{"text":"Overall, I found the paper \"Machine Comprehension Using Match-LSTM and Answer Pointer\" to be well-written and informative. The authors present a comprehensive approach for machine comprehension tasks utilizing Match-LSTM and answer pointer models. The experimental results reported in the paper demonstrate the effectiveness of the proposed approach, achieving state-of-the-art performance on several benchmark datasets.\n\nThe authors provide a clear explanation of the model architecture, with diagrams and equations to help illustrate the concepts. I appreciated the detailed description of each component of the model and the rationale behind their design choices. The use of attention mechanisms to weigh the significance of each word is a particularly elegant solution, and the authors do an excellent job of explaining how it works and its benefits compared to alternative approaches.\n\nThe experimental methodology is also well-described and thorough, with a clear explanation of the datasets used, the evaluation metrics employed, and training procedures. The authors show a clear improvement in performance over previous work that used similar datasets and evaluation metrics, and their analysis of the results is thorough and insightful.\n\nI have a few minor suggestions for improvement. First, the authors could provide more detail on the computational resources required for running their models, including the time and memory required for training and inference. This information would be helpful for researchers and practitioners interested in implementing the proposed approach.\n\nSecond, the authors could provide a more detailed discussion of the limitations of their approach and potential areas for future research. While the results are impressive, there may be scenarios in which the proposed models are not as effective, and the authors could discuss these scenarios and potential remedies to ensure the generalizability of their approach.\n\nOverall, I recommend \"Machine Comprehension Using Match-LSTM and Answer Pointer\" as a valuable contribution to the field of machine comprehension, and I believe it provides a strong foundation for further research in this area.","model":"chatGPT","source":"peerread","label":1,"id":1844}
{"text":"This paper \"Machine Comprehension Using Match-LSTM and Answer Pointer\" addresses the problem of machine comprehension, which is a significant challenge in the field of natural language processing. Machine comprehension refers to the ability of machines to read and understand text, and to answer questions based on that understanding. This paper proposes an architecture that uses a Match-LSTM and Answer Pointer to improve machine comprehension. \n\nStrengths:\n\nThe paper is well-written and clearly explains the proposed architecture in detail. The authors provide a comprehensive evaluation of their approach, which includes a comparison with other state-of-the-art methods on standard datasets. The evaluation shows that the proposed architecture outperforms the existing methods in terms of accuracy, precision, and recall. Furthermore, the authors provide extensive ablation studies, which help to understand the impact of individual components of the proposed architecture.\n\nWeaknesses:\n\nOne weakness of the paper is that it does not discuss the limitations of the proposed architecture. While the evaluation shows that the proposed approach performs well, it is unclear how it would perform on more complex tasks or datasets. Furthermore, the authors do not discuss how the proposed architecture could be improved or extended in future work. Additionally, the paper lacks a detailed discussion of related work, and it could benefit from a more in-depth comparison with other approaches in the field.\n\nOverall, the paper \"Machine Comprehension Using Match-LSTM and Answer Pointer\" presents a well-written and thoroughly evaluated approach to machine comprehension. While there are some limitations to the proposed architecture, the evaluation demonstrates its effectiveness compared to existing state-of-the-art methods.","model":"chatGPT","source":"peerread","label":1,"id":1845}
{"text":"The paper presents an end-to-end neural architecture for machine comprehension of text using the recently released Stanford Question Answering Dataset (SQuAD). The authors propose two models based on the Match-LSTM architecture and the Pointer Net sequence-to-sequence model. The proposed models outperform the best results obtained by previous methods on SQuAD and MSMARCO datasets.\n\nThe authors provide a clear motivation for using SQuAD as an evaluation dataset given the challenging requirements for machine comprehension algorithms. The introduction clearly outlines the significance of the problem and the potential applications of machine comprehension in natural language processing.\n\nThe proposed models are well-justified and the results are convincingly presented. The authors provide a comprehensive analysis of the performance of their models on SQuAD and MSMARCO datasets, comparing their results with the best performing methods in the literature. The comparison is comprehensive, covering accuracy, F1 score, and computation time.\n\nThe authors discuss limitations and potential extensions of their work, including the use of other datasets and the exploration of alternative architectures. Overall, the paper is well-written, the claims are well-supported, and the results are significant.\n\nIn summary, this paper presents a valuable contribution to the field of machine comprehension in natural language processing. The proposed models achieve state-of-the-art results on SQuAD and MSMARCO datasets, providing a strong foundation for further research in this area.","model":"chatGPT","source":"peerread","label":1,"id":1846}
{"text":"This paper proposes an end-to-end neural architecture that uses the match-LSTM and Pointer Net models for machine comprehension of text. The authors aim to improve the performance of existing techniques on the Stanford Question Answering Dataset (SQuAD), which presents a challenging testbed for evaluating machine comprehension algorithms due to its varied answer lengths and unstructured answer space. \n\nOne of the strengths of this paper is the thoroughness with which the authors describe their methodology and experimental setup. They provide clear explanations of both the match-LSTM and Pointer Net models and how they are applied to the task of machine comprehension. The authors also provide detailed comparisons to other existing techniques and results, demonstrating that their models perform substantially better. \n\nAnother strength of the paper is the authors' focus on the practical implications of their work. They consider a real-world application of their models by incorporating them into a search engine scenario, and demonstrate their effectiveness in improving the performance of the search engine by answering user queries more accurately. \n\nOne potential weakness of the paper is its lack of discussion on the interpretability of the model's predictions. While the authors do describe their models as \"end-to-end,\" there is no explanation of how exactly they arrive at their answers. Additionally, while the authors provide clear comparisons to existing techniques, there is no discussion on how the proposed models improve upon these techniques or what their limitations might be. \n\nOverall, this paper presents a strong contribution to the field of machine comprehension of text, demonstrating improved performance on a challenging dataset and providing practical implications for real-world applications.","model":"chatGPT","source":"peerread","label":1,"id":1847}
{"text":"Title: Incorporating long-range consistency in CNN-based texture generation\n\nAuthors: [Authors' Names]\n\nSummary:\nThe paper presents a novel approach for generating textures using Convolutional Neural Networks (CNNs). Specifically, the proposed method incorporates long-range consistency constraints to improve the quality of the generated textures. The authors evaluate their approach by comparing it with two state-of-the-art texture generation methods, and show that their method produces textures with higher perceptual quality and better long-range consistency. \n\nStrengths:\nThe paper presents a well-motivated and innovative approach that addresses a critical issue in CNN-based texture generation - long-range consistency. The authors demonstrate the effectiveness of their proposed method using extensive quantitative and qualitative evaluations. In particular, the results of the evaluation show that the generated textures using the proposed method have improved long-range consistency, which is essential in preserving the global coherence of complex textures such as grass, stone, or wood. The authors also provide a detailed analysis of the effect of the different parameters in their approach on the quality of the generated textures, which is helpful for understanding the proposed method.\n\nWeaknesses:\nThe evaluation of the proposed method in comparison to existing approaches is limited to a few metrics and subjective evaluations. Further investigations on different datasets could strengthen the findings presented in this paper. Additionally, the authors could consider presenting a more comprehensive analysis of the computational complexity of their approach, since real-time performance is critical for some applications.\n\nComments for the authors:\nOverall, the paper presents an interesting and well-executed study on incorporating long-range consistency in CNN-based texture generation. The proposed approach offers several benefits, including the ability to generate realistic textures with improved long-range consistency. The authors offer valuable insights into the different parameters and strategies that work well in the texture generation process. I suggest the authors consider expanding their evaluation by comparing their proposed method with additional state-of-the-art texture generation methods to provide a more robust benchmark. Overall, the technical quality and organization of the paper are very good, and the paper is suitable for publication in its present form with minor revisions.","model":"chatGPT","source":"peerread","label":1,"id":1848}
{"text":"The paper 'Incorporating long-range consistency in CNN-based texture generation' presents an approach to enhance the quality and realism of computer-generated textures by incorporating long-range consistency within the Convolutional Neural Network (CNN) framework. The authors pointed out in their paper that many existing methods focus only on local structure, failing to capture the overall consistency of textures that exists across multiple scales.\n\nOne of the main strengths of the paper is the clarity of the methodology used to achieve this improvement. The authors' approach can leverage the internal representation of the CNNs in order to extract a complete distribution of features that capture the global characteristics of textures. This leads to the generation of textures with a more natural appearance and improved visual quality.\n\nAnother strength of the paper is the detailed evaluation of the proposed approach. The authors provide a quantitative analysis based on statistical tests that compare their approach with existing methods. This evaluation highlights the benefits of incorporating long-range consistency in texture generation, showing that it leads to significant improvements in both the visual quality and diversity of the generated textures.\n\nHowever, the paper also has some weaknesses that need to be addressed. One of the main weaknesses lies in the limitation of the scope of experiments. The authors do not provide sufficient analysis on the generalization ability of their approach, particularly on varying levels of noise, image under different lighting conditions and rotations. The evaluation only focuses on a limited dataset while it will be worthwhile to evaluate the performance over larger datasets.\n\nIn conclusion, the proposed paper presents a valuable contribution in the field of texture generation by introducing a method that improves the overall appearance and consistency of generated textures. The comprehensiveness and clarity of the methodology and the detailed evaluation make it a strong candidate for publication. However, it would benefit from further exploration and analysis of the proposed approach in more diverse contexts to demonstrate its generalizability.","model":"chatGPT","source":"peerread","label":1,"id":1849}
{"text":"The paper \"Incorporating long-range consistency in CNN-based texture generation\" proposes a modification to the representation of image textures using pair-wise products of features in a convolutional network, as previously presented by Gatys et al. (2015). Specifically, the authors suggest a simple modification to this representation that allows for the incorporation of long-range structure into image generation and the rendering of images that exhibit various symmetry constraints.\n\nThe paper presents several experiments demonstrating the effectiveness of this modification in improving the rendering of regular textures and images with symmetric structure, as well as applications to inpainting and season transfer. The experimental results are visually compelling and suggest that the proposed modification is a promising approach for texture generation.\n\nOverall, the paper is well-written, and the proposed modification is a significant contribution to the field of CNN-based texture generation. However, there are some areas where the paper could be improved. Specifically, the authors could provide more details about the implementation of their proposed modification and the specific hyperparameters used in their experiments. Additionally, the authors could more explicitly discuss the limitations of their approach and potential areas for future research.\n\nIn summary, I would recommend this paper for publication, with the caveat that the authors address the aforementioned areas for improvement.","model":"chatGPT","source":"peerread","label":1,"id":1850}
{"text":"The paper titled \"Incorporating Long-Range Consistency in CNN-based Texture Generation\" proposes a modification to the texture generation technique proposed by Gatys et al. (2015). The paper aims to incorporate long-range structure into image generation, and allow for images that satisfy various symmetry constraints. The paper showcases how this modification can improve the rendering of regular textures and images that contain other kinds of symmetric structure, and presents applications to inpainting and season transfer.\n\nStrengths:\n\n1. The paper identifies a problem with the existing texture generation technique and proposes a solution that allows for the incorporation of long-range structure and symmetry constraints in image generation, which can lead to better results in terms of texture generation.\n\n2. The paper provides detailed explanations regarding how the proposed modification can be implemented, which is helpful for readers to understand the approach.\n\n3. The paper presents experiments that show the effectiveness of the proposed modification, outlining its advantages compared to the existing method.\n\nWeaknesses:\n\n1. The paper would benefit from additional experiments or use-cases to showcase the versatility of the proposed modification.\n\n2. The paper could also benefit from a more comprehensive explanation and discussion of the limitations of the proposed modification.\n\nOverall, the proposed modification to the existing technique for texture generation is a useful contribution to the field. The paper provides enough detail for readers to understand the approach, and the experiments presented suggest that the modification can lead to improved results. However, the paper would be strengthened by additional experiments and an in-depth discussion of its limitations. Nonetheless, the paper is well-written and presents valuable insights that can be of interest to those working in the field of image generation and computer vision.","model":"chatGPT","source":"peerread","label":1,"id":1851}
{"text":"Title: Dynamic Coattention Networks For Question Answering\n\nAbstract:\n\nThis paper proposes a novel method for question answering using a dynamic co-attention mechanism, which allows the model to attend to relevant parts of both the question and the passage simultaneously. The proposed model achieves state-of-the-art performance on the Stanford Question Answering Dataset (SQuAD) and other benchmark datasets.\n\nPeer Review:\n\nThis paper proposes a novel method for question answering using a dynamic co-attention mechanism. The main contribution of the paper is the introduction of a dynamic coattention mechanism that attends to relevant parts of both the question and the passage simultaneously. \n\nThe authors demonstrate that their model achieves state-of-the-art performance on the SQuAD dataset, which is a widely used benchmark for machine comprehension. The experimental results show that the proposed method outperforms other state-of-the-art methods on several evaluation metrics, such as F1 score and exact match.\n\nThe paper is well written and organized. The authors clearly explain the proposed method and the experimental results. The related work section provides a comprehensive overview of the existing methods in the field of question answering.\n\nHowever, there are a few limitations to the proposed method that are worth mentioning. The authors did not explore the effectiveness of the proposed method on non-English datasets, which limits the generalizability of the proposed method to different languages. Additionally, the paper did not discuss the limitations of the proposed method and future directions for improvement.\n\nOverall, this paper presents a novel method for question answering that achieves state-of-the-art performance on benchmark datasets. The proposed dynamic co-attention mechanism is a significant contribution that can be used to improve the performance of other natural language processing tasks. This paper is suitable for publication in an NLP conference or journal.","model":"chatGPT","source":"peerread","label":1,"id":1852}
{"text":"The paper \"Dynamic Coattention Networks For Question Answering\" tackles the problem of question answering by proposing a new neural network architecture, called Coattentional LSTM, which incorporates coattention mechanisms to jointly learn representations of a context and a question and then uses them to generate an answer. The paper argues that the proposed model outperforms previous state-of-the-art models on several benchmark datasets, including SQuAD and CNN\/DailyMail.\n\nStrengths:\n\n1. The paper proposes a novel approach for question answering, which addresses the limitations of previous models by using coattention mechanisms to capture the interplay between the question and the context more effectively. The proposed model achieves impressive performance gains compared to previous methods, outperforming the current state-of-the-art approaches.\n\n2. The authors provide a detailed and clear description of the model architecture and the training procedure, which makes it easy to understand the underlying mechanisms and reproduce the experiments.\n\n3. The paper presents extensive experiments on multiple benchmark datasets, demonstrating the effectiveness of the proposed approach across different domains.\n\nWeaknesses:\n\n1. One of the main weaknesses of the paper is the lack of in-depth analysis of the limitations of the proposed model. The authors mainly focus on the experimental results and do not provide in-depth insights into the failure cases of the model.\n\n2. The paper does not provide clear guidelines for choosing the hyperparameters of the model, which makes it difficult for readers to reproduce the experiments and apply the proposed method on other datasets.\n\n3. The experiments are mostly limited to the English language, and there is no investigation of the generalizability of the method to other languages or domains.\n\nIn conclusion, \"Dynamic Coattention Networks For Question Answering\" is an impressive paper that proposes a novel approach for question answering based on coattention mechanisms. The proposed approach achieves state-of-the-art results on several benchmark datasets and constitutes a significant contribution to the field of natural language processing. However, the paper could benefit from more in-depth analysis of the model's limitations, clear hyperparameter tuning guidelines, and investigation of the generalizability of the method to other languages and domains.","model":"chatGPT","source":"peerread","label":1,"id":1853}
{"text":"Peer Review of \"Dynamic Coattention Networks For Question Answering\"\n\nThe paper presents a novel approach, Dynamic Coattention Networks, meant to address the issue of single-pass deep learning models being unable to recover from local maxima corresponding to incorrect answers in question answering. The authors propose a dynamic iterative procedure involving co-dependent representations of questions and documents, which iterates over a range of potential answer spans, augmenting the ability of the model to recover from initial local response peaks corresponding to incorrect answers.\n\nThe paper is organized clearly, with an introduction providing the context, clarity to the methodology, and the experiments demonstrates the improvements over the previous state-of-the-art approaches. The experiments are rigorous, showing the effectiveness of the proposed methodology by providing the results of the DCN approach on the Stanford question answering dataset, where a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1. The results show that the proposed methodology of dynamic coattention networks outperforms the previous state-of-the-art methods.\n\nThe authors have presented their research work well, and their paper is a good contribution to the field of question answering using deep learning models. The paper could benefit from additional insights on the limitations of the proposed method and comparison with other question answering models. Overall, I recommend the acceptance of this paper in the field of question answering, as it showcases a significant improvement in the current state-of-the-art models.","model":"chatGPT","source":"peerread","label":1,"id":1854}
{"text":"The paper titled \"Dynamic Coattention Networks For Question Answering\" addresses the problem of single-pass deep learning models in question answering, which are unable to recover from local maxima representing incorrect answers. The authors propose a Dynamic Coattention Network (DCN) that fuses co-dependent question and document representations and then utilizes a dynamic pointer decoder to locate potential answer spans. The dynamic iterative procedure helps the model to overcome local maxima issues, thus achieving better accuracy levels in question answering.\n\nStrengths:\nThe proposed method of DCN has proven to be effective in overcoming local maxima issues and improving accuracy levels in question answering. The model achieved state-of-the-art results on the Stanford question answering dataset, with a single DCN ensemble obtaining an F1 score of 80.4%, which is significant compared to previous models. The paper provides a detailed explanation of the model architecture and experimental evaluation that could be useful for researchers working in the field.\n\nWeaknesses:\nThe paper, however, lacks a proper comparison of DCN with other models on the same dataset, which limits the understanding of its performance. Additionally, the paper could have included a more detailed analysis of the model to understand the internal workings that make it effective in solving the problem. Finally, while the DCN ensemble achieves state-of-the-art results, it may not be very practical for real-world applications due to its complexity and high computational requirements.\n\nOverall, the paper proposes a promising new approach for question answering that addresses some significant challenges faced by previous models. While there is room for improvement in some areas, the paper provides a solid foundation for further research in this direction.","model":"chatGPT","source":"peerread","label":1,"id":1855}
{"text":"Overview:\n\nThe paper proposes SampleRNN, an end-to-end neural audio generation model that synthesizes waveforms in a fully unconditional manner. The authors assert that SampleRNN is capable of producing audio samples that are much more natural-sounding and diverse than those generated by autoregressive models. Moreover, the proposed method has a substantially reduced need for memory storage, with minimal loss of audio quality. The authors demonstrated the ability of the proposed method to generate audio samples through a range of experiments.\n\nStrengths:\n\nThe paper's most significant strength is the proposed architecture SampleRNN, which synthesizes audio waveforms unconditionally and outperforms existing autoregressive models in terms of audio naturalness and diversity. Additionally, the proposed model has reduced computational requirements, making it well-suited for real-time audio generation applications.\n\nThe authors' approach to generating audio signals by using autoregressive models as a baseline is a good choice, and their evaluation criteria are comprehensive, comparing their proposed method against other state-of-the-art approaches.\n\nThe paper's introduction is suitable, providing a clear and concise background on audio generation and related research topics related to neural networks.\n\nLimitations:\n\nThe paper's experiments are primarily focused on synthesizing simple harmonic signals and real-world speech, with limited research of applications to other domains. While the samples produced by SampleRNN sound realistic and diverse, benchmarking efficiency and perceived quality are challenging owing to the lack of quantity and variety in data sets.\n\nMoreover, it might have been better to provide a more in-depth discussion of the findings, as the proposed method's advantages are often based on qualitative evaluation alone. The paper could be improved by providing a more comprehensive analysis of the techniques used and the comparative results generated.\n\nFinally, the paper could have benefited from more detailed theoretical explanations of the proposed method, especially regarding loss function used, as well as the choice of hyperparameters. The authors presented these elements briefly and could have elaborated further.\n\nConclusion:\n\nIn summary, the authors' proposed method, SampleRNN, is an exciting contribution to the field of neural audio generation. The proposed approach is capable of outperforming existing autoregressive models in terms of naturalness and sound diversity, while also being more computationally efficient. However, the proposed method's implementation is primarily focused on synthesizing simple harmonic signals and speech, and additional research is required to assess the proposed method's effectiveness in other domains.\n\nThe paper is well-written, providing clear and concise explanations of the proposed method and the corresponding experiments. The authors' experimental results are convincing, demonstrating the high-quality samples generated by the proposed method, though the paper could benefit from more detailed analysis and discussions of the results. Overall, the paper constitutes a valuable contribution to the field of neural audio generation.","model":"chatGPT","source":"peerread","label":1,"id":1856}
{"text":"This paper SampleRNN: An Unconditional End-to-End Neural Audio Generation Model addresses the problem of generating raw audio waveforms directly from a neural network. Unlike previous models, which typically generate audio by predicting a spectrogram or a sequence of spectral features, the authors of this paper propose a novel architecture called SampleRNN that is capable of generating raw audio waveforms directly. \n\nStrengths:\n- The proposed architecture achieves state-of-the-art results on unconditional audio generation with a number of objective and subjective evaluation metrics.\n- The authors provide thorough experiments that demonstrate the effectiveness of the proposed model in generating high-quality, diverse audio waveforms.\n- The authors also show that the proposed model is capable of generating audio samples of much longer duration than previously reported models.\n- The paper is well written and provides a clear explanation of the proposed model, making it accessible to researchers in the field.\n\nWeaknesses:\n- While the authors provide a detailed explanation of the proposed SampleRNN model, they do not provide much discussion on the theoretical underpinnings of their approach. A more in-depth discussion of the principles behind the architecture would improve the paper's overall impact.\n- The authors report that the model has a relatively high computational cost and requires significant amounts of memory, but they do not provide a thorough analysis of the trade-offs between model performance and computational resources.\n- The paper does not explicitly compare the SampleRNN model to other state-of-the-art audio generation models. A more detailed comparison would provide a better understanding of the strengths and weaknesses of the proposed model.\n\nOverall, SampleRNN: An Unconditional End-to-End Neural Audio Generation Model is a well-written paper that proposes an effective approach for generating raw audio waveforms directly from a neural network. While there are some weaknesses in the paper, the authors provide thorough experiments that demonstrate the effectiveness of the proposed model in generating high-quality, diverse audio.","model":"chatGPT","source":"peerread","label":1,"id":1857}
{"text":"Peer Review for 'SampleRNN: An Unconditional End-to-End Neural Audio Generation Model'\n\nIn this paper, the authors propose a novel model for unconditional audio generation task. They combine memory-less modules, i.e., autoregressive multilayer perceptron, and stateful recurrent neural networks in a hierarchical structure to capture underlying sources of variations in the temporal domain for very long time on three datasets of different nature. The authors also perform human evaluation on the generated samples to indicate that their model is preferred over competing models.\n\nOverall, the paper is well-written, and the proposed model is carefully designed and evaluated. The authors provide detailed descriptions of the model architecture and training procedures. However, there are some major and minor points to be addressed before the paper can be accepted for publication.\n\nMajor points:\n\n1. Although the proposed model outperforms existing models, the paper lacks a detailed discussion on the limitations of the proposed model. More specifically, the authors need to discuss the limitations of the presented evaluation metrics and provide more insights into future research directions to overcome the limitations.\n\n2. The authors also need to address the use of subjective human evaluation as a single evaluation metric. They should provide more objective evaluation metrics to measure the performance of their proposed model.\n\nMinor points:\n\n1. Some of the figures and tables are hard to read and should be improved.\n\n2. The authors should explain more clearly and in more detail the meaning and justification of some of the technical terms and abbreviations used throughout the paper.\n\n3. The authors should also provide more examples of generated audio samples with different lengths and sources of variations to further illustrate the capabilities of their proposed model.\n\nOverall, given the minor revisions mentioned above, the proposed paper will make a significant contribution to the field of audio generation using NLP techniques.","model":"chatGPT","source":"peerread","label":1,"id":1858}
{"text":"The paper titled \"SampleRNN: An Unconditional End-to-End Neural Audio Generation Model\" proposes a novel model for audio generation. The problem addressed by this paper is to generate audio data of high quality, where the previous models presented faced difficulty in capturing long-term dependencies and variations.\n\nThe strength of this paper lies in the clear description of the model architecture and training process. The hierarchical structure of the model uses a combination of autoregressive multilayer perceptron and stateful recurrent neural networks, which is effective in capturing the source of variations in temporal domain. The proposed model produces high-quality audio samples, and human evaluation on the generated samples indicate that it is preferred over competing models. In addition, the paper provides a thorough analysis of how each component of the model contributes to the exhibited performance.\n\nHowever, the paper can be improved by including more detailed analysis and discussion of the limitations of the proposed model. While the experiment results are impressive, it would be helpful to see how the model performs on other datasets of differing natures. It would also be useful to provide some insights into the computational cost of this model, as it uses a hierarchical structure that can be computationally expensive.\n\nOverall, this paper is a valuable contribution to the field of audio generation. The proposed model is powerful in capturing long-term dependencies and variations and generates high-quality audio samples. The paper can be further improved with additional analysis of its limitations and discussion of its computational cost.","model":"chatGPT","source":"peerread","label":1,"id":1859}
{"text":"Peer Review for \"Metacontrol for Adaptive Imagination-Based Optimization\"\n\nThis paper proposes a method called \"Metacontrol for Adaptive Imagination-Based Optimization\" (MAIO) that incorporates metacontrol into imagination-based optimization (IBO) to improve the performance of the algorithm. The authors propose to use a controller network to select between different IBO policies and improve the exploration of the search space.\n\nThe paper is well-written and well-organized, and the authors provide a thorough analysis of the proposed approach. The experiments are well-designed and the results clearly demonstrate that the proposed MAIO significantly outperforms IBO and other state-of-the-art optimization algorithms on a variety of benchmark problems. The authors also provide a detailed discussion of the strengths and weaknesses of the proposed method, which shows a good understanding of the limitations of the approach.\n\nOverall, I think this is a well-done study that presents an interesting contribution to the field of optimization. The proposed MAIO method is well-motivated and well-executed, and the results are convincing. The authors have clearly put a lot of effort into their work, and their paper is a valuable addition to the literature. I would recommend this paper for publication. \n\nHowever, there are a few points that could be further clarified and\/or addressed in the paper:\n\n1. The authors do not provide any theoretical analysis of the proposed method, which would strengthen the claims they make about its effectiveness. It would be helpful if the authors could provide some theoretical insights or derivations to support their findings.\n\n2. The paper could benefit from more detailed explanations of the implementation of the proposed method. The authors provide a general idea of how the algorithm works, but more details on the specific hyperparameters used and the training procedure would be helpful.\n\n3. The authors only test their method on a limited number of problems. It would be interesting to see if the proposed MAIO approach could be applied to other types of optimization problems, such as multimodal optimization or problems with constraints.\n\nOverall, I found this paper to be a valuable contribution to the field of optimization. The authors tackle an important problem and provide a novel solution that is well-supported by their experiments. With some minor revisions, this paper could be a valuable addition to the literature.","model":"chatGPT","source":"peerread","label":1,"id":1860}
{"text":"The paper \"Metacontrol for Adaptive Imagination-Based Optimization\" addresses the problem of efficiently optimizing complex functions that have multiple local optima. The authors propose a new optimization algorithm based on imagination-based planning (IBP) that incorporates metacontrol to adaptively choose the most appropriate control parameters.\n\nOne strength of the paper is its thorough explanation of the theoretical framework behind imagination-based planning and how it can be used for optimization. The authors provide a clear and thoughtful description of the different components of the IBP algorithm and how they work together to optimize functions with multiple optima.\n\nAnother strength of the paper is the authors' implementation and evaluation of their algorithm on several benchmark optimization problems. The results show that their method outperforms several state-of-the-art optimization algorithms in terms of convergence rate and solution quality.\n\nOne weakness of the paper is that the authors do not provide a detailed discussion of the limitations of their algorithm. For example, it is unclear how the algorithm would perform on functions that have a large number of dimensions or that are very noisy.\n\nOverall, the paper \"Metacontrol for Adaptive Imagination-Based Optimization\" presents a promising new algorithm for optimizing complex functions using imagination-based planning and metacontrol. With further research and development, this approach could potentially have applications in a wide range of fields, including engineering and machine learning.","model":"chatGPT","source":"peerread","label":1,"id":1861}
{"text":"This paper introduces a metacontroller that learns to optimize a sequence of \"imagined\" internal simulations over predictive models of the world in order to construct a more informed, and more economical, solution. The proposed approach is demonstrated to be a powerful framework for using rich forward models for efficient model-based reinforcement learning.\n\nThe paper is clearly written and the methodology is well-explained. The approach takes into account limitations of \"one-size-fits-all\" machine learning systems and addresses the challenge of an agent with a limited computational budget. The introduced metacontroller is a model-free reinforcement learning agent, which decides both how many iterations of the optimization procedure to run, as well as which model to consult on each iteration. \n\nThe experiments conducted in the paper show that the metacontroller can adapt the amount of computation it performs to the difficulty of the task and how to choose which experts to consult by factoring in both their reliability and individual computational resource costs. The results demonstrate that the approach achieves a lower overall cost (task loss plus computational cost) than more traditional fixed policy approaches.\n\nHowever, there are some areas that could be further clarified. For example, the paper would benefit from additional discussion regarding how different model types impact the proposed approach. Specifically, the paper only considers state transition models and action-value functions, and it would be valuable to see how the approach may work with other model types.\n\nOverall, this paper presents a valuable contribution to the field of model-based reinforcement learning and provides insights into how metacontroller algorithms can be developed to optimize computation in machine learning systems.","model":"chatGPT","source":"peerread","label":1,"id":1862}
{"text":"The paper titled \"Metacontrol for Adaptive Imagination-Based Optimization\" addresses the problem of limited computational resources while dealing with a complex reinforcement learning task. The authors propose a metacontroller-based approach to optimize a sequence of \"imagined\" internal simulations over predictive models of the world. The metacontroller learns to adapt the amount of computation it performs to the difficulty of the task in hand and learns to choose which experts to consult by factoring in both their reliability and individual computational resource costs.\n\nOne of the strengths of the paper is the novelty of the proposed approach. The authors have introduced a metacontroller component that decides both how many iterations of the optimization procedure to run, as well as which model to consult on each iteration. They have shown that their approach is a powerful framework for using rich forward models for efficient model-based reinforcement learning. The results of experiments conducted in the paper demonstrate that the proposed approach can solve complex decision-making problems under complex non-linear dynamics while achieving a lower overall cost than more traditional fixed policy approaches.\n\nThe authors have presented a detailed literature review and have clearly explained the methodology utilized in their approach. They have also presented a detailed analysis of the experiments conducted, which provides enough information for the interested readers to reproduce the results.\n\nHowever, there are some weaknesses in the paper that must be addressed. Firstly, the authors have tested their approach on only one dataset and one type of expert model. It is unclear whether the same approach can generalize to other tasks or expert models. Additionally, the authors do not compare their approach with other state-of-the-art approaches for adaptive control.\n\nOverall, the paper \"Metacontrol for Adaptive Imagination-Based Optimization\" is a well-written and informative paper that presents a novel approach for efficient model-based reinforcement learning. The authors' experimental results demonstrate the effectiveness of the proposed approach in solving challenging decision-making problems under complex non-linear dynamics. However, further experimentation and comparison with other state-of-the-art approaches are necessary to establish the superiority of the proposed approach in diverse scenarios.","model":"chatGPT","source":"peerread","label":1,"id":1863}
{"text":"Title: Exploring Sparsity in Recurrent Neural Networks\n\nAuthors: John Smith, Alice Johnson, and Mark Lee\n\nIntroduction:\nThe paper explores the effects of sparsity in recurrent neural networks (RNNs). It investigates the benefits of incorporating sparsity to enhance the performance of RNNs for various natural language processing (NLP) tasks.\n\nApproach:\nThe authors carry out a series of experiments to study the impact of sparsity on RNNs. They use three different NLP datasets and experiment with different levels of sparsity to optimize the performance of the RNNs. The authors compare the performance of the sparse RNNs with the dense RNNs to evaluate the efficacy of sparsity.\n\nAnalysis:\nThe paper presents a thorough analysis of the effects of sparsity on RNNs. The experiments show that sparse RNNs perform better than their dense counterparts. The authors provide a detailed explanation of the factors contributing to the improved performance of sparse RNNs such as reduced overfitting and improved generalization capacity.\n\nThe authors also compare the performance of their method to other state-of-the-art techniques for optimizing RNN performance. However, the authors do not provide a significant improvement over these baseline methods, which limits the novelty of their contribution. Additionally, the paper lacks a theoretical justification for why sparsity improves the performance of RNNs, which could have added to the impact of the paper.\n\nConclusion:\nOverall, the paper provides a comprehensive evaluation of the impact of sparsity on RNNs for NLP tasks. The experiments conducted are sound, and the results presented robustly support the authors\u2019 claims. However, the paper could have made a more significant contribution to the field by proposing a theoretical explanation for the observed improvements in performance. Nonetheless, the paper provides a useful reference for researchers interested in incorporating sparsity into RNNs for NLP tasks.","model":"chatGPT","source":"peerread","label":1,"id":1864}
{"text":"The paper \"Exploring Sparsity in Recurrent Neural Networks\" addresses the problem of optimizing the computational efficiency of recurrent neural networks (RNNs), particularly in the context of natural language processing (NLP) tasks. The authors propose a novel approach for inducing sparsity in the weight matrices of RNNs, which can lead to significant reductions in computational complexity without sacrificing model performance.\n\nOne of the main strengths of this paper is the thoroughness of the experimental evaluations conducted to demonstrate the efficacy of their proposed method. The authors conduct extensive experiments on several different NLP tasks, including sentiment analysis, text classification, and language modeling. In each case, they compare the performance of their sparsity-inducing method to several other approaches for reducing network complexity, demonstrating the superiority of their proposal. \n\nHowever, one potential weakness of the paper is that it may be difficult to fully reproduce the experimental results due to a lack of detailed information on the specific hyperparameters used in training the models. Additionally, while the authors do discuss how their proposed method can induce sparsity in both the weight matrices and the activations of the RNN, they primarily focus on the former, and it is unclear how much the sparsity of activations contributes to the computational savings achieved.\n\nOverall, the paper presents an interesting and promising approach for improving the efficiency of RNNs in NLP tasks through inducing sparsity. The thorough experimental evaluations and clear presentation of results make a compelling case for the proposed method's effectiveness, although more information on the specific training details would be helpful for reproducing the results.","model":"chatGPT","source":"peerread","label":1,"id":1865}
{"text":"The paper \"Exploring Sparsity in Recurrent Neural Networks\" addresses the issue of deploying state-of-the-art recurrent neural networks on mobile phones and embedded devices. The authors propose a technique to reduce the number of parameters of a network by pruning weights during the initial training of the network. The authors demonstrate that the proposed technique results in sparse RNNs that achieve comparable accuracy to the original dense RNNs. Moreover, the network size is reduced by 8\u00d7 and the time needed to train the model remains constant. \n\nThe paper is well-written and organized, providing a clear and detailed explanation of the proposed technique. The authors use a variety of experiments and benchmarks to validate their approach. They demonstrate that their technique can be applied to diverse RNN architectures, including LSTM and GRU, and achieve similar results. The authors also show that the proposed technique can be used to prune larger dense networks and still achieve better than baseline performance while significantly reducing the number of parameters. \n\nOne minor issue with the paper is that it lacks a comparison with existing pruning techniques. Although the authors state that their technique outperforms existing approaches, it would be helpful to see a direct comparison with other methods on the same dataset and task. Additionally, the authors could provide more details on the sparsity structure of the pruned RNNs, as sparsity patterns can have a significant impact on the efficiency of matrix multiplications during inference.\n\nOverall, the paper presents an interesting and practical technique for reducing the size of RNNs while maintaining comparable accuracy, making them more deployable on mobile and embedded devices. The experiments and benchmarks provide strong evidence for the effectiveness of the proposed technique. This paper is a valuable contribution to the field and is recommended for publication.","model":"chatGPT","source":"peerread","label":1,"id":1866}
{"text":"The paper \"Exploring Sparsity in Recurrent Neural Networks\" addresses the challenge of deploying deep learning models, specifically Recurrent Neural Networks (RNNs), on resource-limited devices such as mobile phones and embedded devices. The authors proposed a novel technique of reducing the parameters of a network by pruning weights during the training process, resulting in a sparser network while maintaining high accuracy.\n\nThe strength of this paper lies in its methodology and experiments. The authors provide a clear and detailed explanation of their approach to pruning RNNs during training. They demonstrate the effectiveness of their technique by achieving a significant reduction in the number of parameters in the network while maintaining reasonable accuracy. The benchmarks also show that the proposed technique can help achieve a significant speed-up in inference time using sparse GEMMs.\n\nHowever, there are some weaknesses in the paper that need to be addressed. Firstly, the authors could have provided more details regarding the baseline model and the datasets used in the experiments. This information is essential to evaluate the validity of the proposed technique. Secondly, the authors should have provided more insight into the impact of the proposed technique on the complexity of the training process. The paper lacks a comprehensive analysis of how pruning during training affects parameters like convergence rates and learning curves.\n\nIn conclusion, the paper is a valuable contribution to the field of deep learning and RNNs, and the proposed technique is promising in addressing the challenge of deploying models on resource-limited devices. However, further experiments and analysis are required to understand the impact of this approach on different types of datasets, and more insights can be provided to understand the effect of pruning during training.","model":"chatGPT","source":"peerread","label":1,"id":1867}
{"text":"Overall, I found this paper on Lossy Image Compression with Compressive Autoencoders to be a well-written and informative piece of research. The authors present a comprehensive study on using compressive autoencoders for image compression, which is particularly relevant given the increasing importance of data compression in various applications.\n\nThe authors provide a clear introduction to the problem of lossy image compression and the existing methods for tackling it. They then explain how compressive autoencoders can be used to address this problem by using sparse representations to achieve high compression rates while minimizing loss in image quality.\n\nThe paper includes a detailed description of the proposed method step by step, which is helpful for understanding the process of compression with compressive autoencoders. The authors also provide thorough evaluations of their method on multiple datasets along with comparisons to other state-of-the-art approaches, demonstrating that the proposed method outperforms traditional compression methods and achieves competitive performance compared to other state-of-the-art deep learning-based methods.\n\nMoreover, the authors explain the process of selecting optimal hyperparameters and provide explanation about how to choose between the popular objective functions for compressive autoencoders. This would be particularly valuable to those researchers who are new in this area.\n\nOne minor issue that I would like to point out is that some figures in the paper are not well defined and difficult to interpret. Thus, it would be helpful to improve the quality of the figures to aid in better visualization of the results.\n\nOverall, I would say that this paper is a valuable contribution to the field of image compression with compressive autoencoders as it provides thorough evaluation and comparison of the proposed method with other approaches. I would recommend this paper for publication with minor revisions.","model":"chatGPT","source":"peerread","label":1,"id":1868}
{"text":"The paper \"Lossy Image Compression with Compressive Autoencoders\" addresses the problem of lossy image compression using compressive autoencoder (CAE) models. The goal of the paper is to investigate the performance of CAE models on lossy image compression, compared to the traditional approach of using discrete cosine transform (DCT) followed by quantization and entropy coding.\n\nOne strength of the paper is that it provides a thorough comparison between the CAE approach and the DCT approach on different types of images and compression ratios. The authors also experiment with different architectures and hyperparameters for the CAE model.\n\nHowever, a weakness of the paper is that the evaluation metrics used are not sufficient for assessing the visual quality of the compressed images. The authors use only peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) as metrics, which are known to be imperfect measures of image quality perception. The paper could benefit from using more advanced metrics such as perceptual image quality assessment (PIQA) or subjective evaluations.\n\nAdditionally, the paper could benefit from providing more context on the state of the art in lossy image compression. For example, there are other recent papers that have explored using generative adversarial networks (GANs) for image compression, and it would be interesting to see how the CAE approach compares to these methods.\n\nOverall, the paper provides a solid investigation into the effectiveness of CAE models for lossy image compression, but could be improved with better evaluation metrics and more thorough comparison to other state-of-the-art methods.","model":"chatGPT","source":"peerread","label":1,"id":1869}
{"text":"The paper \"Lossy Image Compression with Compressive Autoencoders\" proposes a novel approach to optimizing autoencoders for lossy image compression through minimal modifications to the compression loss. The authors argue that existing codecs are unable to meet the need for flexible compression algorithms, and autoencoders have the potential to address this gap.\n\nThe paper presents experimental results showing that the proposed method successfully trains deep autoencoders that can effectively compete with JPEG 2000 and outperform approaches based on RNNs. Additionally, the authors demonstrate that the proposed architecture is computationally efficient due to the use of a sub-pixel architecture, making it suitable for high-resolution images.\n\nThe paper is well-written and clearly presents the proposed approach and experimental results. The authors have identified a significant research gap and offer a valuable contribution to the field of lossy image compression. The technical aspects of the paper are also sound, and the experiments are appropriately designed.\n\nOne potential improvement would be to include a more detailed comparison with other state-of-the-art compression methods to provide a more comprehensive evaluation of the proposed approach. Additionally, it may be helpful for the authors to provide more detailed discussions on the limitations and potential future directions of their method.\n\nOverall, the paper presents a valuable contribution to the field of lossy image compression, and I recommend it for acceptance after minor revisions.","model":"chatGPT","source":"peerread","label":1,"id":1870}
{"text":"Summary:\n\nThis paper addresses the problem of optimizing autoencoders for lossy image compression. It highlights that existing codecs are not flexible enough and under changing hardware technology and diverse requirements and content types new compression algorithms are needed. Autoencoders can be a potential solution to this need, but they are difficult to optimize because of the inherent non-differentiability of the compression loss. The authors propose a new approach to solve this problem and demonstrate the superior performance of their technique by comparing it with JPEG 2000 and other recently proposed approaches based on RNNs.\n\nStrengths:\n\nThe paper addresses an important problem of image compression with lossy compression by introducing a new approach that outperforms the existing ones. Additionally, the paper provides a clear motivation and context for the proposed method, which makes it easier for readers to understand the problem domain. The comparison with existing approaches gives the reader a clear understanding of the strengths of the proposed method.\n\nWeaknesses:\n\nWhile the paper provides a clear approach to the problem of optimizing autoencoders, it lacks explanation of the method, algorithm, and architecture in depth. There is no discussion of the limitations of the proposed method, and further, the paper would benefit from more experimental results and visualization to better demonstrate the performance and efficiency of the approach.\n\nSuggestions:\n\nThe paper would be strengthened by more in-depth presentations and discussion about the algorithm, method and architecture. Further proposed experiments and visualization data would support the authors' main argument and points, making the manuscript more convincing. Additionally, a discussion covering the limitations of the proposed method would round out the manuscript.","model":"chatGPT","source":"peerread","label":1,"id":1871}
{"text":"Title: Structured Attention Networks\n\nThe paper presents an interesting approach in the field of natural language processing (NLP) for sentence-level classification tasks. The authors propose a novel neural network architecture called Structured Attention Networks (SAN), which incorporates two important components - a structured attention mechanism and a convolutional neural network (CNN) - to improve the modeling of inter-dependencies between words and their contextual information.\n\nThe paper is well-organized, and the authors do an excellent job of explaining their approach and comparing their results with previous methods. The experiments conducted on various datasets show that SAN outperforms baseline models, achieving state-of-the-art results on three benchmark datasets.\n\nOverall, I find the paper to be of high quality and contribution to the NLP community. The authors provide a clear and well-motivated introduction to the problem of sentence-level classification, and the technical details of SAN are presented with enough clarity for readers familiar with deep learning models. The experiments are thorough and convincing and deliver valuable insights into the usefulness of the proposed approach.\n\nI would, however, recommend some minor edits to improve the readability of the text. For instance, some mathematical notations, such as equations and symbols, are not adequately defined or explained, which may hinder the understanding of the proposed method. Additionally, some explanations are verbose and can be presented more concisely, especially in the later sections of the paper.\n\nIn conclusion, I believe that SAN is a valuable contribution to the field of NLP, providing a novel solution to the problem of sentence-level classification tasks. The paper is well written, and the experiments conducted are thorough and convincing, demonstrating the efficacy of the proposed approach. I recommend this paper to researchers working in the field of deep learning and NLP.","model":"chatGPT","source":"peerread","label":1,"id":1872}
{"text":"The paper \"Structured Attention Networks\" addresses the problem of developing a novel neural network architecture that can effectively model the structured dependencies between different entities in a task. The authors propose a neural network that incorporates structured attention mechanisms allowing the model to capture non-local dependencies between different input elements.\n\nThe paper has several strengths. First, the proposed architecture is effective in modeling structured dependencies between different input elements, and it can be applied in various tasks such as sequence labeling, relation extraction, and text classification. Second, the authors provide a detailed analysis of the proposed model by comparing it to other state-of-the-art architectures on several datasets, showing its effectiveness in various scenarios. \n\nHowever, there are also several weaknesses in this paper. Firstly, while the authors provide an analysis of the model's performance against other state-of-the-art architectures, it would have been helpful to include a more detailed analysis of the model's interpretability, as the proposed structured attention mechanism can be challenging to analyze. Secondly, the paper could benefit from a more extensive discussion of the potential limitations and risks associated with the proposed architecture and its implementation. \n\nOverall, I find this paper to be a valuable contribution to the field of natural language processing. The proposed model offers an impressive and effective approach to modeling structured dependencies between input elements, which can have significant implications for several NLP tasks. While there are areas for improvement, the strengths of this paper outweigh its weaknesses, and I would recommend its acceptance for publication.","model":"chatGPT","source":"peerread","label":1,"id":1873}
{"text":"Title: Peer Review for \"Structured Attention Networks\"\n\nSummary:\nThe paper presents a novel approach for incorporating richer structural dependencies within deep neural networks via structured attention networks. The approach uses graphical models to encode these dependencies, and compared to the basic attention procedure, it allows for extending attention beyond the standard soft-selection approach. Two different classes of structured attention networks are experimented with: linear-chain conditional random fields and graph-based parsing models. Both models are practically implemented as neural network layers, and the experiments show that this approach is effective for incorporating structural biases. The models are found to outperform baseline attention models on a variety of synthetic and real tasks, including tree transduction, neural machine translation, question answering, and natural language inference. The authors provide evidence that training in this way facilitates the learning of interesting unsupervised hidden representations that generalize simple attention.\n\nStrengths:\nThe paper provides a clear and succinct description of the proposed model, and provides sufficient detail for its implementation, which should enable others to replicate and build upon the work. The authors also provide a comprehensive overview of related work in this area, which helps to contextualize the contribution of their work. The experimental results suggest that the proposed model is effective for incorporating structural biases, and that it outperforms baseline attention models on tasks that require richer structural dependencies. The paper also highlights how training in this way can lead to the discovery of interesting unsupervised representations.\n\nWeaknesses:\nThe paper would benefit from a more detailed description of how the proposed method fits into the current state-of-the-art, and how it compares to other approaches for incorporating structural dependencies within neural networks. The authors could also provide a more detailed discussion of the limitations of their approach, and how these limitations might affect its applicability to different types of tasks. Finally, it would be useful if the authors provided more explanation of why certain types of tasks were chosen for evaluation, and what they can tell us about the generalizability of the proposed model. \n\nOverall, I think the paper makes a valuable contribution to the field, and its experimental results suggest that the proposed model has the potential to be applied to a broad range of tasks that require richer structural dependencies.","model":"chatGPT","source":"peerread","label":1,"id":1874}
{"text":"The paper \"Structured Attention Networks\" addresses the problem of incorporating richer structural dependencies within deep neural networks for embedding categorical inference. The traditional attention networks provide effective solutions for embedding categorical inference within deep neural networks; however, they lack the ability to model richer structural dependencies without abandoning end-to-end training. Therefore, the paper proposes two different classes of structured attention networks, linear-chain conditional random fields, and graph-based parsing models, that incorporate structural biases to extend attention beyond the standard soft-selection approach.\n\nThe paper has several strengths. Firstly, the paper presents a novel and effective approach to incorporate richer structural dependencies within deep neural networks. Secondly, the proposed approach is easy to implement and can be used to model a wide range of problems with various structured dependencies. Thirdly, the experiments conducted in the paper show that the proposed structured attention networks outperform the baseline attention models on several synthetic and real tasks, including tree transduction, neural machine translation, question answering, and natural language inference.\n\nHowever, there are a few weaknesses in the paper that can be addressed. Firstly, the paper lacks a detailed comparison with the existing literature on structured attention models. Secondly, the paper's experimental evaluation focuses on a limited set of tasks and data types. It would be interesting to see how the approach performs on a more extensive range of tasks and datasets. Finally, the paper could benefit from providing more insights into the learned representations and their generalization capabilities.\n\nIn conclusion, the proposed structured attention networks are a promising approach for incorporating richer structural dependencies within deep neural networks. The paper provides a clear and thorough explanation of the proposed approach and its practical implementation, and the experiments support the effectiveness of the proposed approach over the traditional attention models. Overall, this paper makes a valuable contribution to the field of natural language processing and deep learning.","model":"chatGPT","source":"peerread","label":1,"id":1875}
{"text":"Title: Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations\n\nAuthors: David Krueger, Nitish Srivastava, Jan Koutn\u00edk, and Ruslan Salakhutdinov\n\nJournal: arXiv:1606.01305 [cs.LG]\n\nKeywords: Zoneout, LSTM, RNNs, regularization, deep learning\n\nSummary:\n\nThe paper \"Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations\" presents a novel regularization technique for recurrent neural networks (RNNs) called Zoneout. This technique extends dropout to temporal domain by randomly preserving hidden activations from the previous timestep instead of dropping them out during training. The authors show that Zoneout provides better regularization than dropout and yields state-of-the-art results on several sequence modeling tasks such as character-level language modeling and speech recognition.\n\nStrengths:\n\n1. Novelty: The paper presents a novel regularization technique for RNNs which builds on the success of dropout regularization in feedforward neural networks. The concept of \"zoneout\" extends the idea of randomly dropping hidden units during training to the temporal domain of RNNs.\n\n2. Experimental Results: The authors provide empirical evidence to support the effectiveness of Zoneout on several benchmark datasets. They show that the proposed technique outperforms both dropout and other state-of-the-art regularization techniques.\n\n3. Clarity: The paper is well written and easy to follow. The authors provide a clear explanation of Zoneout and its motivation as well as a detailed description of the experimental setup and results.\n\nWeaknesses:\n\n1. Lack of analysis: While the authors show the effectiveness of Zoneout for several tasks, they do not provide a detailed analysis of the mechanism by which Zoneout works or why it works better than other regularization techniques.\n\n2. Limited dataset: The experiments were conducted on only a few benchmark datasets which may not be representative of the full range of sequence modeling tasks.\n\n3. Computational overhead: The authors note that implementing Zoneout requires a significant increase in compute time, which may limit its practicality for some applications.\n\nOverall, the paper presents a novel and effective regularization technique for RNNs that provides state-of-the-art results on several benchmark datasets. The authors' experiments show the potential of Zoneout to outperform other techniques like dropout for sequence modeling tasks. However, more detailed analysis of the mechanism behind Zoneout and further investigation on a wider range of datasets would be valuable in future research.","model":"chatGPT","source":"peerread","label":1,"id":1876}
{"text":"The paper \"Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations\" addresses the issue of overfitting in Recurrent Neural Networks (RNNs) by proposing a new regularization method called \"zoneout\". The authors suggest that zoneout can improve the generalization of RNN models by randomly preserving the activation of hidden units over time. The paper assesses the performance of zoneout on several benchmark datasets, and compares it with dropout and other regularization techniques.\n\nStrengths:\n\nThe paper presents a novel regularization technique, zoneout, which is a modification of the dropout method. The authors provide a clear and detailed explanation of zoneout and demonstrate its effectiveness in different RNN architectures over various datasets. They also offer a comprehensive comparison of zoneout with other regularization techniques, such as dropout, L2 regularization, and weight noise. The experimental results show that zoneout performs well in both training and testing phases and outperforms other methods in some instances. Moreover, the paper is well-organized and well-written, which makes it easy to understand the concepts presented.\n\nWeaknesses:\n\nOne possible weakness of the paper is that it does not provide a rigorous comparison of the computational cost of zoneout compared to other regularization methods. Additionally, the paper does not address the effect of zoneout on the interpretability of RNN models, which is an important consideration for many applications. Finally, it would have been useful to have a more extensive discussion of the limitations of zoneout and suggestions for future work.\n\nOverall, \"Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations\" is a well-written and well-organized paper that presents an innovative regularization technique for RNNs. The experimental results demonstrate that zoneout is an effective method for reducing overfitting in various types of RNN models. The paper has some minor weaknesses that could be addressed, but overall, it makes a valuable contribution to the field of neural networks and machine learning.","model":"chatGPT","source":"peerread","label":1,"id":1877}
{"text":"Title: Peer Review for \"Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations\"\n\nThe paper proposes a novel regularization method for recurrent neural networks (RNNs) called \"zoneout\". The authors introduce it as a stochastic regularization method where some hidden units of the RNN are forced to maintain their previous values at each time-step, rather than being dropped like in dropout. The authors claim that zoneout can improve the generalization of RNNs by training a pseudo-ensemble and by preserving gradient and state information over time. \n\nThe paper provides a clear and well-motivated introduction, giving the readers enough context to understand the problem at hand. The authors explain the limitations of the existing regularization methods for RNNs, such as dropout, weight decay and early stopping, and why a new method like zoneout is needed. The authors clearly explain the method and the algorithm used for implementing it. \n\nThe authors present convincing empirical evidence supporting the effectiveness of zoneout across various tasks, including character and word-level language modelling, and permuted sequential MNIST. The authors compare their results with other types of regularization and show that zoneout outperforms these methods. The results are presented in a clear and easy-to-understand manner, and the analysis of the results is thorough.\n\nOverall, the paper is well-written, and the idea is novel and well-supported by empirical results. The authors have done a great job in presenting and explaining the method, and the empirical analysis of the results is convincing. \n\nI recommend accepting the paper for publication after minor revisions. Below are my suggestions for improvement:\n\n- It would be helpful if the authors explain further how zoneout preserves state information compared to dropout. \n- The authors could consider presenting more in-depth analysis of the results or discussing possible extensions of their method. \n- The authors should clarify the interpretation of results for the Text8 dataset, as the improvements in perplexity are minimal over the baseline, unlike for Penn Treebank and permuted sequential MNIST. \n\nOverall, the paper makes an important contribution to the field of recurrent neural networks, and I recommend it for publication.","model":"chatGPT","source":"peerread","label":1,"id":1878}
{"text":"The paper titled \"Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations\" proposes a novel method for regularizing recurrent neural networks (RNNs) called Zoneout. The main goal of this paper is to investigate the effectiveness of zoneout in improving the generalization performance of RNNs. The authors first introduce the mathematical formulation of zoneout and explain how it works to force some hidden units in RNNs to maintain their previous values. This is done by applying a stochastic process that randomly replaces some of the hidden unit values with their previous values during training.\n\nThe paper presents an empirical evaluation of the zoneout method and compares its performance with other regularization techniques such as Dropout, Variational Dropout, and Recurrent Batch Normalization. The authors conduct experiments on several standard benchmark datasets such as Penn Treebank, Text8, and permuted sequential MNIST for word- and character-level language modeling and image classification tasks, respectively. Interestingly, the results show that zoneout outperforms the other regularization techniques across all the tasks considered.\n\nThe paper's strengths lie in the following aspects. Firstly, the authors propose a novel method for regularizing RNNs, called zoneout, which has been shown to outperform other regularization techniques in several benchmark datasets. Secondly, the authors provide a comprehensive and detailed analysis of the performance of different regularization techniques. Finally, the authors demonstrate that zoneout could be combined with recurrent batch normalization to achieve state-of-the-art results on permuted sequential MNIST.\n\nHowever, there are a few weaknesses in the paper that need to be addressed. Firstly, the authors did not perform any theoretical analysis or provide any insights into why zoneout works better than other techniques. Secondly, the authors did not conduct a comprehensive ablation study to understand the individual contributions of each component of the model. Finally, the empirical evaluation does not include other recent regularization techniques such as Zoneout++, scaled-up dropout, and zoneout-dropout, which could be interesting to compare with.\n\nOverall, the paper presents a novel regularization technique that outperforms other techniques, but further theoretical and experimental investigations are necessary to understand and improve this approach.","model":"chatGPT","source":"peerread","label":1,"id":1879}
{"text":"Title: \"Deep Probabilistic Programming: Combining probabilistic programming and deep learning\"\n\nAuthors: John Smith, Jane Doe, and Sam Lee\n\nAbstract:\n\nThe paper presents a novel framework that combines probabilistic programming and deep learning for modelling complex and uncertain systems. The proposed approach, called Deep Probabilistic Programming (DPP), enables the use of deep neural networks as both generative and inference models within a probabilistic programming language. This paper provides a comprehensive overview of DPP and its potential applications.\n\nReview:\n\nThe paper presents an interesting idea of combining probabilistic programming and deep learning. The authors have done a good job in providing a comprehensive overview of DPP and provide insight into its potential applications. The paper is well-written and organized.\n\nThe authors have defined DPP and its components clearly, with examples and diagrams to illustrate the concept. The authors also provide an algorithm for implementing DPP, which is a valuable contribution to the field of probabilistic programming.\n\nThe paper includes a comparison with other approaches to probabilistic programming and deep learning. However, the authors could have provided a more detailed analysis of the strengths and weaknesses of these approaches compared to DPP.\n\nThe authors have presented a case study demonstrating the effectiveness of DPP in modelling complex systems. However, the authors could have provided additional examples to demonstrate the versatility of DPP in different domains.\n\nThe paper concludes with a discussion on the potential applications of DPP. This section provides insight into the potential impact of DPP on machine learning and other fields. However, the authors could have provided more specific examples of potential applications of DPP beyond those presented in the paper.\n\nOverall, the paper is a valuable contribution to the field of probabilistic programming and machine learning. The authors present a clear and comprehensive overview of DPP and its potential applications. However, the paper would benefit from more detailed analysis and examples of DPP compared to other approaches, as well as more diverse examples to demonstrate the versatility of DPP in different domains.","model":"chatGPT","source":"peerread","label":1,"id":1880}
{"text":"\"The paper \"Deep Probabilistic Programming\" by Tuan Anh Le et al. explores the use of deep probabilistic programming for modeling complex generative processes in the context of machine learning. The problem they address is the difficulty of modeling complex, real-world phenomena with deep neural networks and the shortcomings of existing probabilistic programming frameworks in handling large-scale datasets. The authors propose a novel method that combines the power of deep neural networks with the flexibility of probabilistic programming to model such complex datasets.\n\nOne of the major strengths of this paper is its novelty and the significance of the problem it addresses. The authors present a well-formulated approach that synthesizes probabilistic programming and deep neural networks. The paper is well-written, and the results are presented coherently, making it easy to follow the methodology used and the experimental results obtained.\n\nHowever, there are also some limitations to this paper. One of the primary weaknesses is that the proposed approach may not be the most efficient method for modeling complex generative processes compared to other commonly-used approaches, such as GANs and VAEs. Additionally, it would have been helpful to include more comparison to previous works that leverage deep probabilistic programming to understand better how this framework performs compared to various other approaches.\n\nIn summary, the paper \"Deep Probabilistic Programming\" by Tuan Anh Le et al. presents a novel approach to solving the problem of modelling complex generative processes in a deep learning context. Though there are some limitations to this paper, its strengths lie in its novelty and significance, and the authors' ability to present the methodology and results in a clear and concise manner.\"","model":"chatGPT","source":"peerread","label":1,"id":1881}
{"text":"This study presents Edward, a probabilistic programming language that defines two compositional representations: random variables and inference. The authors argue that by treating inference as equal to modeling, probabilistic programming can be as flexible and computationally efficient as traditional deep learning. The paper suggests that Edward offers both flexibility and efficiency in fitting the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. Additionally, Edward can reuse the modeling representation as part of inference, making it straightforward to design rich variational models and generative adversarial networks. The authors demonstrate that Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. \n\nOverall, the paper presents a sophisticated and conceptually interesting probabilistic programming language that appears to be computationally efficient and flexible in its ability to fit various models. The authors provide compelling empirical evidence that Edward is faster than existing systems, such as Stan and PyMC3, providing an important benchmark for future research. The demonstration of integrating Edward into TensorFlow is valuable for the community, as it proposes one possible avenue for scaling probabilistic programming systems to larger and more complex tasks. The study addresses a significant challenge in probabilistic programming: providing efficient inference methods that accommodate large and complex datasets. \n\nHowever, it would be useful if the authors provide more discussion or demonstrate additional empirical evidence for Edward\u2019s performance relative to other probabilistic programming languages beyond Stan, such as Anglican or Venture. Additionally, while the study provides useful examples of the flexibility of the model and rich variational models and generative adversarial networks, it would be valuable to see further empirical evidence on the benefits of this flexibility compared to simpler modeling alternatives. Finally, it would be useful to provide more guidance on how to use Edward, especially for researchers new to probabilistic programming.","model":"chatGPT","source":"peerread","label":1,"id":1882}
{"text":"This paper presents Edward, a probabilistic programming language that aims to provide the flexibility of probabilistic modeling and the computational efficiency of deep learning. The authors have made significant contributions by including the inference as a first-class citizen to make probabilistic programming more flexible and efficient. The authors claim that Edward can address the limitations of existing probabilistic programming systems that are not designed to scale well to large datasets or complex models.\n\nThe strengths of this paper are:\n\n1. ** Novelty:** The authors have made a novel contribution by introducing Edward, which provides significant speedups than existing probabilistic systems. Edward is an interesting addition to the probabilistic programming landscape because it integrates the two compositional representations to achieve flexibility and efficiency simultaneously.\n\n2. ** Compositional Representations:** Edward defines two compositional representations, named as random variables and inference. Including both modeling and inference as first-class citizens of the probabilistic programming framework, provides added flexibility to users in designing complex models.\n\n3. ** Integration with TensorFlow:** Edward is integrated into Tensorflow, which provides several benefits, resulting in significant speed-ups over existing probabilistic systems. It is also no slower than handwritten TensorFlow, making it a practical choice for real-world applications.\n\n4. ** Wide Range of Inference Methods:** Edward makes it easy to fit the same model using several composable inference methods, ranging from point estimation to variational inference to MCMC. This rich set of inference methods allows users with varying levels of expertise to use the appropriate inference method for their problem.\n\nThe weaknesses of this paper are:\n\n1. ** Limited Experiments:** The authors only present two simple examples that demonstrate the usefulness and efficiency of Edward, which is not enough to make definitive claims about the effectiveness of Edward in complex applications.\n\n2. ** Limited Comparison:** The authors have only compared Edward with Stan and PyMC3 on a benchmark logistic regression task, which is not enough to demonstrate the effectiveness of Edward for all types of probabilistic models.\n\nOverall, this paper makes a significant contribution to the field of probabilistic programming by introducing Edward, which provides flexibility and efficiency simultaneously. Given the potential of Edward, additional experiments and comparisons with other state-of-the-art systems are necessary to fully establish the usefulness and performance of this system.","model":"chatGPT","source":"peerread","label":1,"id":1883}
{"text":"Title: LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation\n\nAuthors: [authors' names]\n\nThe paper titled \"LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation\" proposes a novel approach for generating high-quality images using a recursive strategy based on Generative Adversarial Networks (GANs). The authors present their approach using a layered architecture, which they claim improves the quality, diversity, and scalability of the generated images.\n\nThe paper is written in a clear and concise manner, with a good structure that allows the reader to follow the proposed algorithm step-by-step. The authors provide a comprehensive overview of related work on GANs and recursive architectures, which helps to contextualize the contribution of their proposed approach.\n\nMoreover, the authors present a thorough evaluation of their approach on standard image datasets, including CIFAR10, CelebA, and LSUN. They show that their approach achieves state-of-the-art performance in terms of both image quality and diversity metrics. The authors also provide insightful analysis and visualization of the learned features, which demonstrate the effectiveness of their approach.\n\nOne aspect that could be improved in this paper is the comparison with other state-of-the-art approaches, such as StyleGAN or BigGAN. While the authors mention these approaches in the related work section, they do not compare their approach against them in the experiments. This comparison would help to strengthen the claims made in the paper and provide a more comprehensive evaluation of the proposed approach.\n\nIn summary, this paper presents a novel approach for image generation using GANs with a recursive architecture. The approach is well-presented, thoroughly evaluated, and shows promising results. The authors provide insights into the learned features and contribute to the advancement of the field of image generation.","model":"chatGPT","source":"peerread","label":1,"id":1884}
{"text":"The paper \"LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation\" addresses the problem of generating high-quality images with fine-grained details. The authors propose a novel generative model architecture that uses a recursive approach in generating an image by adding layers of detail gradually through multiple stages. The proposed model aims to improve the quality and realism of generated images by generating complex images in a top-down fashion from rough sketches.\n\nThe paper's strengths include the innovative approach to image generation by adopting a recursive structure to create hierarchical representations. The authors have thoroughly tested and evaluated their model using several metrics, comparing it with other state-of-the-art models. The evaluation metrics show that the proposed model provides better quality and diversity in generated images.\n\nOn the other hand, the paper's weakness is the lack of discussion on the impact of hyperparameters on model performance. The authors only briefly mention the parameters used without providing enough justification for their choices. Additionally, the evaluation is limited to visual inspection and quantitative metrics, ignoring the semantic content of generated images.\n\nOverall, the paper presents a promising approach to image generation that can generate high-quality and realistic images with fine-grained details. The proposed model's recursive architecture has shown to be effective in generating complex images, making it a valuable contribution to the field of generative models. However, further research is needed to explore its potential use in applications that require strong semantic consistency. The paper can benefit from a more in-depth discussion of the hyperparameters' impact and a more comprehensive evaluation methodology.","model":"chatGPT","source":"peerread","label":1,"id":1885}
{"text":"General comments:\nThe paper presents a novel approach to image generation by introducing Layered Recursive Generative Adversarial Networks (LR-GAN). The proposed model attempts to generate natural images with objects that are more recognizable to humans by learning to generate image foreground and background separately and recursively. The authors have indicated the research problem clearly, providing sufficient motivation for the work. The technical approach has been described in detail, and the experiments have been carried out to demonstrate the effectiveness of the proposed model in generating natural images. In general, the paper is well-written and the proposed LR-GAN model is sound.\n\nSpecific comments:\n1) The paper lacks clarity in differentiating the proposed LR-GAN from other prior GAN models. The authors should provide insight into how their approach differs from other GAN models in terms of architecture and functionality.\n\n2) The authors should provide more details on how they decided on the architecture of LR-GAN. For instance, why did they choose to use a stack of GANs rather than blending the layers?\n\n3) In the experiments section, it would be helpful to compare the proposed approach to other state-of-the-art methods for generating natural images, such as StyleGAN.\n\n4) The authors should elaborate on how the results were evaluated, other than relying on visual inspection.\n\n5) The authors should provide guidelines for the selection of hyperparameters that influenced the training and testing of the model.\n\n6) The authors should provide a discussion of the limitation of the proposed LR-GAN model.\n\nOverall, the paper presents a novel approach to generating natural images, and the experiments demonstrate that the model is effective in generating visually appealing images. However, the authors should address the concerns mentioned above to enhance the quality of the paper.","model":"chatGPT","source":"peerread","label":1,"id":1886}
{"text":"The paper \"LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation\" addresses the problem of generating natural images with recognizable objects by taking into account the scene structure and context of the image. The authors propose a new type of generative adversarial network that generates image backgrounds and foregrounds separately and recursively, and then combines them in a contextually relevant manner to produce a complete natural image.\n\nThe main strength of this paper is that it presents a novel approach to image generation that takes scene structure and context into account. The proposed LR-GAN model shows significantly better performance in generating natural images with recognizable objects than the baseline GANs. The paper provides detailed experimental results, which demonstrate the effectiveness of the proposed model.\n\nThe paper's weakness is that it does not provide a thorough comparison with other state-of-the-art image generation techniques. Although the LR-GAN model shows better performance than the baseline GANs, it is not clear how it compares to other recent models, such as conditional GANs and variational autoencoders, which have also shown great success in generating high-quality images.\n\nMoreover, the paper's writing can be improved in terms of clarity and organization. The paper can be more concise and avoid some of the repetitions in explaining the proposed model. Additionally, the explanations and terminologies can be made more accessible for readers who are not familiar with the technical details of GAN models.\n\nOverall, \"LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation\" presents a novel approach to address the problem of image generation. The proposed model shows promising results and has the potential to surpass existing state-of-the-art image generation techniques. However, the paper would benefit from a more comprehensive comparison with other recent models and clearer writing.","model":"chatGPT","source":"peerread","label":1,"id":1887}
{"text":"Title: Variational Lossy Autoencoder\n\nAuthors: John Doe and Jane Doe\n\nIntroduction:\nThe paper presents a novel approach to lossy data compression using variational autoencoder. The authors aim to generate compact representations of high dimensional data that can be used for a wide-range of applications ranging from image compression to natural language processing. \n\nStrengths:\nThe paper is well-written and presents clear and concise explanations of the concepts used in variational autoencoder. The experimental results are impressive and show that the proposed method is effective in generating high quality, compressed representations of data. The authors adequately compare their method with other state-of-the-art methods, demonstrating the superiority of the proposed method in terms of the reconstructed quality and compression rates.\n\nWeaknesses:\nWhile the paper provides a thorough explanation of the proposed model and the experimental set-up, some parts of the work need further elaboration. For example, details regarding the hyperparameter selection process could have been provided clearly. Additionally, while the paper demonstrates the potential effectiveness of the model on labeled datasets, its applicability to unlabeled datasets is not discussed.\n\nSuggestions for improvement:\nThe authors could further clarify the selection of hyperparameters used in the experiments to make their approach better reproducible by other researchers. They can also address the generalizability of their proposed method by conducting experiments on large unlabeled datasets. Lastly, it would be useful to provide more detailed instructions on how to implement their approach to help other researchers who may want to try it out.\n\nConclusion:\nOverall, the paper presents a useful and convincing approach to lossy data compression using variational autoencoder. Its experimental results are impressive, and the authors' comparisons with other state-of-the-art methods are thorough. However, some sections of the paper could benefit from further elaboration or clarification, and some areas for improvement have been suggested.","model":"chatGPT","source":"peerread","label":1,"id":1888}
{"text":"The paper \"Variational Lossy Autoencoder\" examines the use of variational autoencoders (VAEs) for the task of lossy compression. Specifically, the authors propose a novel method for training VAEs to achieve better compression performance by introducing a new loss function that takes into account the trade-off between reconstruction accuracy and compression rate.\n\nOne of the strengths of this paper is the clarity of the presentation. The authors do an excellent job of explaining their methodology and results in a clear and concise manner. Additionally, the authors provide a thorough comparison of their proposed method against other state-of-the-art compression techniques, demonstrating the effectiveness of their approach in achieving better compression performance.\n\nAnother strength of the paper is the experimental evaluation of the proposed method. The authors provide an extensive evaluation of their approach on multiple datasets, and they also provide a detailed analysis of the impact of various hyperparameters on the compression performance. This level of analysis is valuable in understanding the behavior of the proposed method and how it can be optimized for different types of data.\n\nHowever, one potential weakness of the paper is the lack of discussion on the limitations of the proposed method. The authors only mention some possible extensions of their work but fail to discuss any inherent limitations of the proposed method or how it might perform worse in certain scenarios.\n\nOverall, the paper \"Variational Lossy Autoencoder\" provides a valuable contribution to the field of lossy compression using VAEs. The proposed method shows promising results and has the potential to be useful for various applications that require efficient compression of high-dimensional data.","model":"chatGPT","source":"peerread","label":1,"id":1889}
{"text":"Overall, the paper presents an interesting and well-motivated approach for learning global representations that discards irrelevant information from observed data by combining Variational Autoencoder (VAE) with neural autoregressive models. The authors successfully demonstrate how their proposed model allows for control over what the global latent code can learn, and by designing the architecture accordingly, they can force the global latent code to discard irrelevant information, resulting in \"lossy\" autoencoding of the data. \n\nThe experimental results of the proposed model on various datasets, including MNIST, OMNIGLOT, Caltech-101, and CIFAR10, show state-of-the-art performance compared to existing approaches. The authors provide a comprehensive analysis of the model's performance and compare it to various baselines to validate their approach's effectiveness. \n\nHowever, there are a few minor issues with the paper. The authors could have provided more details about the hyperparameter tuning process and reported more metrics for each of the experiments conducted. Additionally, the authors have not performed any qualitative analysis of the generated samples, which could have provided further insights into the model's performance.\n\nLastly, while the paper is reasonably well-written, there are some stylistic and grammatical errors that could be addressed to improve the readability of the paper.\n\nIn summary, the paper is a well-motivated and interesting contribution to the field of representation learning that provides state-of-the-art results for various datasets. Addressing the minor issues mentioned above would further improve the paper's quality.","model":"chatGPT","source":"peerread","label":1,"id":1890}
{"text":"The paper titled \"Variational Lossy Autoencoder\" addresses the problem of representation learning, specifically learning global representations that are amenable to downstream tasks while discarding irrelevant information. In particular, the paper proposes a method to achieve this by combining Variational Autoencoder (VAE) with neural autoregressive models. \n\nOne of the strengths of this paper is the clear motivation and relevance of the problem addressed. The authors accurately describe the importance of representation learning in machine learning, and the need to learn global representations that are useful for downstream tasks, while still discarding unimportant information. This is a common problem in many applications of machine learning, and the proposed method suggests a promising way to address this challenge. \n\nAnother strength of this paper is the use of VAEs and autoregressive models to achieve the desired global representation of the data. VAEs are a well-established method for unsupervised learning of generative models, and combining this with autoregressive models provides a strong foundation for the proposed method. The author's use of the global latent code that only autoencodes data in a lossy fashion is well-motivated and provides a means to control what information is retained in the representation.\n\nOne potential weakness of this paper is that it doesn't provide a detailed comparison with other methods for unsupervised representation learning. The paper focuses on demonstrating the performance of the proposed method on MNIST, OMNIGLOT, Caltech-101, and CIFAR-10, but doesn't provide comparative analysis against other similar methods. This makes it difficult to determine how competitive the proposed method is relative to other state-of-the-art methods.\n\nAnother potential weakness of this paper is that it doesn't provide a detailed explanation of the computational complexity of the proposed method. While the authors briefly discuss the computational cost of training the model, it would have been helpful to provide a more detailed analysis of the time and resources required in practical settings. This would have helped the reader to understand the potential limitations of the proposed method in real-world applications.\n\nOverall, the proposed method presented in the paper titled \"Variational Lossy Autoencoder\" is well-motivated and provides a promising way to learn global representations for downstream tasks. The use of VAEs and autoregressive models provides a strong foundation for the proposed method. However, the paper could have provided a more comprehensive comparison with other methods for unsupervised representation learning and a more detailed analysis of the computational cost of the proposed method. Despite these limitations, the paper is a valuable contribution to the literature on unsupervised representation learning.","model":"chatGPT","source":"peerread","label":1,"id":1891}
{"text":"Title: Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\n\nAuthors: Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean\n\nSummary:\nIn this paper, the authors propose a new layer for neural networks, called the Sparsely-Gated Mixture-of-Experts (MoE) layer, which allows for the creation of outrageously large neural networks with efficient training and inference. The MoE layer consists of a gate network that selects a small subset of \"experts\" to process each input, and a mixture network that combines the outputs of the selected experts.\n\nThe authors demonstrate the effectiveness of the MoE layer on a variety of tasks, including language modeling, machine translation, and speech recognition. They show that the MoE layer outperforms existing neural network architectures, even with significantly fewer parameters.\n\nStrengths:\nThe proposed MoE layer is innovative and has the potential to revolutionize the field of neural networks. The authors provide a clear and detailed explanation of the layer, along with empirical evidence of its effectiveness on various tasks. The paper is also well-structured and easy to follow, which makes it accessible to a wide audience.\n\nWeaknesses:\nThe paper does not discuss the limitations of the proposed MoE layer, such as its computational cost and scalability. It would have been useful to see a discussion on how the layer would perform on even larger models or on real-world datasets, as well as potential ways to optimize its training and inference. Additionally, the paper could benefit from a more thorough comparison with existing neural network architectures, rather than just a few selected models.\n\nOverall, the Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer is a well-written and innovative paper that introduces a promising new layer for neural networks. While there are some limitations that could be addressed, the proposed MoE layer has the potential to significantly improve the performance of neural networks and open up new research directions in the field.","model":"chatGPT","source":"peerread","label":1,"id":1892}
{"text":"The paper titled \"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\" talks about a new approach for building neural networks which can achieve state-of-the-art performance while being outrageously large compared to previous neural networks. \n\nStrengths:\nThe paper presents a well-motivated research problem and identifies existing limitations of deep learning models. The authors have proposed an innovative approach called the Sparsely-Gated Mixture-of-Experts (MoE) which can handle  neural models with large number of parameters effectively. The experiments conducted by the authors demonstrate that the proposed approach achieves better results when compared to traditional neural network models on several benchmarks in which the dataset is vast, high-dimensional as well as noisy.\n\nWeaknesses:\nOne of the potential drawbacks of the proposed model is related to the large memory requirement which can pose challenges when deploying these models to low-end computing devices. Also, there are concerns related to the scalability of the proposed approach when it is applied to more complex neural network models. Further, it is not clear whether the proposed approach can be implemented efficiently in practice on the available hardware platforms.\n\nIn conclusion, the paper presents a compelling argument for the proposed Sparsely-Gated Mixture-of-Experts approach for building large neural networks that can achieve state-of-the-art performance. The authors' reasoning and approach to the problem are well supported by the experiments and the discussion that follow. Overall, the strengths of the paper outweigh the limitations of the proposed approach, and this work is surely going to have significant implications in the field of deep learning.","model":"chatGPT","source":"peerread","label":1,"id":1893}
{"text":"This paper proposes a novel neural network architecture, the Sparsely-Gated Mixture-of-Experts Layer, that uses conditional computation to increase model capacity without a proportional increase in computation. The proposed architecture consists of up to thousands of feed-forward sub-networks, and a trainable gating network determines a sparse combination of these experts to use for each example. \n\nThe paper presents experimental results on language modeling and machine translation tasks, demonstrating that the proposed architecture can achieve significant improvements in model capacity while only incurring minor losses in computational efficiency. Specifically, the paper reports achieving greater than 1000x improvements in model capacity, and presents model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. \n\nOverall, this paper presents a significant contribution to the field of neural networks, as it offers a practical solution to the problem of limited model capacity. The proposed architecture has the potential to advance the state-of-the-art in language modeling and machine translation. The experimental results presented by the authors are compelling, and the paper is well written and organized. \n\nHowever, there are some limitations to the proposed approach that could be addressed in future work. For example, while the model architectures presented in this paper achieve better results than state-of-the-art at lower computational cost, it would be interesting to investigate the scalability of the proposed approach to even larger models, and to explore the potential for applying the MoE to other tasks beyond language modeling and machine translation. Additionally, it would be valuable to provide further analysis of the algorithmic and performance challenges associated with conditional computation, and to compare the proposed approach with other methods for increasing model capacity, such as parameter sharing and weight tying. \n\nOverall, I recommend this paper for publication, as it presents a significant contribution to the field of neural networks, and offers a practical and scalable solution to the problem of limited model capacity.","model":"chatGPT","source":"peerread","label":1,"id":1894}
{"text":"The paper \"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\" addresses the problem of limited model capacity of a neural network due to its limited number of parameters. The authors propose conditional computation as a potential solution to this issue by activating only certain parts of the network on a per-example basis. However, implementing this solution in practice has significant algorithmic and performance challenges. The authors aim to address these challenges by introducing a Sparsely-Gated Mixture-of-Experts layer (MoE) that consists of multiple feed-forward sub-networks, and a trainable gating network selectively activates only a sparse combination of these experts for each input example. The authors apply this MoE model to language modeling and machine translation tasks using architectures that stack LSTM layers and convolutions.\n\nThe paper's strengths are in its innovative approach to improving model capacity while reducing computational cost, achieving more than 1000x improvement in model capacity with only minor losses in efficiency. The authors' emphasis on performance optimization solves the challenges of conditional computation, making it practical to apply it to real-world applications. The architecture of the MoE layer is well-explained, and its relevance in natural language processing tasks is established through experiments that demonstrate better performance compared to state-of-the-art models.\n\nHowever, the paper has some weaknesses that need addressing. Firstly, the paper lacks a thorough comparison of the proposed MoE transfer learning approach with other transfer learning approaches on large language modeling and machine translation benchmarks. Secondly, the authors do not perform a rigorous sensitivity analysis of the proposed model's performance to the choice of hyperparameters, leading to the impression that the improved performance is heavily dependent on the specific model and dataset choice. Finally, the paper could benefit from a more detailed discussion on the challenges of scaling up the proposed model to even larger sizes or different types of natural language processing tasks.\n\nIn conclusion, the paper presents an innovative approach to improving model capacity while reducing computational cost through the use of a Sparsely-Gated Mixture-of-Experts layer, leading to improved performance on natural language processing tasks. The paper's strengths are its performance optimization and well-explained MoE architecture, while its weaknesses lie in the lack of a thorough comparison with other transfer learning approaches, a rigorous sensitivity analysis of hyperparameters, and a detailed discussion on the scalability of the proposed model to different tasks and larger sizes.","model":"chatGPT","source":"peerread","label":1,"id":1895}
{"text":"Paper Title: \"Introspection: Accelerating Neural Network Training By Learning Weight Evolution\"\n\nIntroduction: \nThis paper proposes a novel approach for training neural networks by incorporating the concepts of weight evolution and introspection. The authors argue that by allowing networks to self-evaluate and adapt their weights, the training process could be made more efficient and faster. This peer review evaluates the novelty, clarity, and technical soundness of the proposed approach.\n\nNovelty:\nThe introduced approach is novel and represents a significant contribution to the field of neural network training. The concept of using weight evolution and introspection to optimize network performance has not been explored extensively before. The authors have presented a comprehensive methodology for implementing this approach, which is well motivated and based on sound theoretical principles.\n\nClarity:\nThe paper is well structured and easy to follow, with the proposed methodology clearly explained in detail. The authors have also provided a step-by-step description of the experimental setup, which further enhances the clarity of the paper. The figures and tables are intelligibly presented, facilitating a clear understanding of the proposed approach and results.\n\nTechnical Soundness:\nThe technical soundness of the proposed approach is demonstrated through the experimental results presented in the paper. The authors provide evidence that the methodology can accelerate network training times and improve its performance. However, the authors did not provide a comparison with other state-of-the-art techniques used for training neural networks. Thus, it is difficult to evaluate if the proposed approach outperforms other ones. Furthermore, it is unclear how the proposed methodology would perform when applied to more complex tasks, limits its general applicability.\n\nConclusion:\nIn conclusion, the paper presents an innovative approach for training neural networks that can improve the efficiency and effectiveness of the training process. The paper offers a clear description of the proposed methodology and conducts experiments that showcase the efficacy of the approach. However, the lack of comparison with state-of-the-art techniques and limitations of its general applicability limits the novelty and impact of the approach.","model":"chatGPT","source":"peerread","label":1,"id":1896}
{"text":"The paper \"Introspection: Accelerating Neural Network Training By Learning Weight Evolution\" by Muldal, Niklasson, et al., addresses the challenge of accelerating neural network training using weight evolution. The authors propose a novel technique of introducing network introspection to promote the rapid training of deep neural networks.\n\nThe paper identifies a significant problem in the training of neural networks, which is that the learning process is computationally expensive, even for state-of-the-art techniques. The authors approach this problem by proposing a weight evolution algorithm that reduces the computational cost of training while still achieving high levels of accuracy.\n\nThe strength of this paper lies in the authors' attention to detail in presenting their research. The paper is well-structured, and the algorithmic details are presented in a clear and concise manner. Additionally, the experimental results and comparisons with other training methods are generally well-conducted, which lends support to the method's effectiveness.\n\nHowever, a potential weakness of the paper is that it does not provide sufficient justification or motivation for the proposed method. While the authors attempt to link their approach to previous work, they could better elaborate on the novelty of their approach and why it is worth pursuing relative to these previous approaches.\n\nAnother weakness is that the performance comparison with other state-of-the-art techniques could be more thorough. The authors do not provide comprehensive comparisons with other techniques in the literature, which could undermine the validity of their claims regarding the proposed method's effectiveness.\n\nOverall, the proposed method is promising and the experimental results are encouraging. The paper could be improved with a more comprehensive literature review, and a more thorough comparison with other state-of-the-art techniques.","model":"chatGPT","source":"peerread","label":1,"id":1897}
{"text":"Title: Peer Review for \"Introspection: Accelerating Neural Network Training by Learning Weight Evolution\" \n\nThe paper titled \"Introspection: Accelerating Neural Network Training by Learning Weight Evolution\" proposes a novel approach to accelerate the training of deep neural networks. The authors aim to leverage the weight evolution pattern of a pre-trained neural network to speed up the training of novel networks. This is achieved by training a simpler network to learn the weight evolution pattern from the MNIST dataset, a common benchmark dataset for image classification, and then using this learned pattern to speed up the training of networks on larger and more complex datasets, such as CIFAR-10 and ImageNet.\n\nThe paper presents a well-written introduction and clearly identifies the motivation behind the research. The authors have provided a comprehensive literature review, which focuses on previous research focused on accelerating training of neural networks. However, some of the relevant prior work on transfer learning could have been cited to establish the connection with the proposed method.\n\nThe methodology and implementation of the proposed approach are well explained. The authors have provided a detailed description of the experiments conducted in this work. The trained model is evaluated on a range of benchmark datasets, and the results show that the proposed method can significantly speed up the training process without any noticeable loss in the accuracy of the neural network. The authors have also compared the proposed method to some existing state-of-the-art approaches, and the results show that this method is computationally efficient and has a low memory footprint.\n\nOverall, the paper presents a well-executed experiment, and the results are convincing. The proposed approach has a practical application and suggests that learning from the weight evolution pattern could be beneficial in reducing the training time of deep neural networks. \n\nHowever, a few issues should be addressed. Although the authors claim that the proposed approach can be use on different optimizers, only results with ADAM are presented, and it would have been useful to test on other optimizers as well to demonstrate its generalisability. Additionally, more detailed analysis of the learned weight evolution patterns and their interpretation could be added to the paper, although this may not be a primary focus area of this research.\n\nIn summary, I recommend this paper for publication in this journal, with minor revisions based on the aforementioned feedback.","model":"chatGPT","source":"peerread","label":1,"id":1898}
{"text":"The paper titled \"Introspection: Accelerating Neural Network Training by Learning Weight Evolution\" addresses the problem of training time in neural networks. Specifically, the authors explore the idea of utilizing a neural network to learn weight evolution patterns from a simpler network, with the goal of accelerating the training of more complex neural networks. The main strengths of this paper lie in the novelty of the proposed approach and the promising results obtained in terms of improved training time.\n\nThe authors provide a clear and concise explanation of their approach, including the use of the MNIST dataset for learning the weight evolution pattern and the application of this knowledge in accelerating the training of CIFAR-10 and ImageNet classification. The low memory footprint and computational efficiency of the proposed method are also significant strengths of the paper. Moreover, the authors show that the method can be used with other optimizers, further increasing its potential for practical use.\n\nOne limitation of the paper is the lack of a thorough investigation into the generalizability of the proposed approach. Although the results show improved training time for CIFAR-10 and ImageNet classification tasks, further experimentation on different datasets should be conducted to validate the method's applicability to other types of problems.\n\nOverall, the paper presents an innovative approach for accelerating neural network training, and its results offer promising possibilities for practical applications. Therefore, this paper is recommended for publication in its current form with the addition of further experimentation on different datasets to establish its generalizability.","model":"chatGPT","source":"peerread","label":1,"id":1899}
{"text":"Summary:\n\nThe paper presents an efficient approach for hyperparameter optimization (HPO) called Hyperband that uses a bandit-based approach to allocate computational resources. The core idea of Hyperband is to sample and train multiple configurations in parallel and at each iteration, eliminate poorly performing configurations based on a pre-defined halting criteria, and allocate more resources to the top-performing configurations. This way, Hyperband can efficiently explore a large space of hyperparameters and find the best configuration with fewer resources.\n\nStrengths:\n\nThe paper proposes an innovative approach for HPO that is both simple and efficient. The use of bandit-based allocation enables Hyperband to prune configurations early in the process and focus computational resources on promising configurations. The authors provide detailed explanations of the algorithm and the implementation, as well as thorough experimental results on multiple data sets, which demonstrate the effectiveness of Hyperband in comparison to other state-of-the-art algorithms. The paper is well-written and organized, making it easy to follow and understand the proposed approach.\n\nWeaknesses:\n\nAlthough the paper provides experimental results on various datasets, I would have preferred a more in-depth analysis of the algorithm's behavior on larger datasets. Additionally, while Hyperband is shown to be efficient, it would be interesting to see a comparison of its performance with more-complex Bayesian optimization techniques, particularly on problems where noise is an issue. Finally, the paper could have included a more thorough discussion on the cost-benefit trade-off of using Hyperband, particularly regarding its potential drawbacks in terms of model performance.\n\nConclusion:\n\nOverall, the paper presents a novel and practical approach for HPO that can be easily implemented and offers significant improvements in terms of efficiency. The authors provide a detailed explanation of the algorithm and its implementation, as well as extensive experimental results, which demonstrate its effectiveness. While some potential limitations in terms of scalability and performance trade-offs should be further investigated, I think that Hyperband is a promising approach that can have a significant impact on the field of HPO. I recommend this paper for publication in its current form.","model":"chatGPT","source":"peerread","label":1,"id":1900}
{"text":"The paper \"Hyperband: Bandit-Based Configuration Evaluation for Hyperparameter Optimization\" by Li et al. presents a new method for efficiently tuning hyperparameters in machine learning models. The paper addresses the problem of finding the optimal set of hyperparameter values for a given model, which is a crucial and challenging task in machine learning.\n\nOne of the strengths of the paper is the proposed Hyperband algorithm, which aims to optimize the balance between exploring and exploiting different sets of hyperparameters by using a bandit-based approach. The authors demonstrate that Hyperband significantly outperforms state-of-the-art hyperparameter optimization methods on several benchmark datasets, while requiring fewer evaluations of the model.\n\nThe paper is also well-organized, clearly presenting the motivation, technical details and empirical results of the Hyperband algorithm. The authors provide sufficient mathematical and algorithmic details to enable others to fully understand and replicate their work.\n\nOne potential weakness of the paper is the limited scope of experimental evaluation, which mainly focuses on image classification tasks using convolutional neural networks. It would be interesting to see the performance of Hyperband on a wider range of models and datasets, especially on tasks that are more heterogeneous in terms of data types and complexity.\n\nAnother potential limitation of Hyperband is its dependence on a search space defined by the user. Although the authors provide some guidelines for choosing a search space, it is unclear how robust the algorithm would be to different search spaces that may have a different structure or dimensionality.\n\nOverall, this paper provides an important contribution to the field of hyperparameter optimization, and the Hyperband algorithm is a promising method that can lead to significant improvements in the performance of machine learning models.","model":"chatGPT","source":"peerread","label":1,"id":1901}
{"text":"Overall, this paper presents an interesting approach to hyperparameter optimization that focuses on speeding up random search through adaptive resource allocation. The proposed algorithm, Hyperband, is novel, simple, flexible, and theoretically sound. The authors show that Hyperband performs well compared to state-of-the-art Bayesian Optimization methods on several hyperparameter optimization problems for neural networks and kernel-based learning.\n\nThe manuscript is well-organized and clearly written. The introduction provides a good overview of the problem of hyperparameter optimization and motivates the need for efficient algorithms. The authors present a comprehensive literature review and discuss the limitations of existing approaches which sets up a strong basis for the proposed algorithm.\n\nThe authors provide a thorough explanation of the Hyperband algorithm, including the different parameters and steps involved. The presented theoretical analysis of the algorithm is sound and provides insights into the behavior of the algorithm. The authors also provide empirical analyses, which include comparisons with state-of-the-art Bayesian Optimization methods on a variety of hyperparameter optimization problems for neural networks and kernel-based learning. The empirical results demonstrate that Hyperband can provide over an order of magnitude speedups compared to competitor methods.\n\nOne limitation of this paper is that the analysis is largely empirical and lacks details on the algorithm's performance on a wider class of problems or datasets. The authors could have provided more comparisons with other optimization algorithms to better understand the strengths and limitations of the approach.\n\nOverall, this paper makes a valuable contribution to the field of hyperparameter optimization, and the proposed algorithm, Hyperband, is a promising approach that is simple, flexible, and theoretically sound. The research is well-executed, and the results are convincing. I would recommend this paper for publication in the current form, subject to minor revisions.","model":"chatGPT","source":"peerread","label":1,"id":1902}
{"text":"The paper \"Hyperband: Bandit-Based Configuration Evaluation for Hyperparameter Optimization\" addresses the problem of finding a good set of hyperparameters for the performance improvement of machine learning algorithms. The authors focus on the speed of random search through adaptive resource allocation as opposed to recent approaches that use Bayesian Optimization.\n\nOne of the strengths of this paper is the presentation of a novel algorithm that is simple, flexible, and theoretically sound. The authors introduce Hyperband as a principled early-stopping method that adaptively allocates a predefined resource to randomly sampled configurations. The empirical results presented compare the performance of Hyperband with state-of-the-art Bayesian Optimization methods. The authors show that Hyperband can provide over an order of magnitude speedups over competitors on a variety of neural network and kernel-based learning problems.\n\nOne of the weaknesses of this paper is that the empirical evaluation focuses only on neural network and kernel-based learning problems. It would be helpful to include other types of problems to demonstrate the generalized performance of the algorithm. Additionally, the paper could have included further discussion on the pros and cons of random search and Bayesian Optimization methods compared to Hyperband.\n\nOverall, this paper presents a novel algorithm in Hyperband that provides competitive results over state-of-the-art methods. The empirical evaluation of the algorithm is thorough, and the theoretical foundation of the algorithm is convincing. Therefore, I recommend this paper for publication after addressing the above weaknesses.","model":"chatGPT","source":"peerread","label":1,"id":1903}
{"text":"Title: Lie-Access Neural Turing Machines\n\nIntroduction:\nThe paper introduces a new architecture for Neural Turing Machines that allows for more efficient and effective data processing by applying Lie algebraic principles to computation. The authors propose a new memory matrix operations and an improved controller with the help of Lie algebra. In addition, they test the proposed architecture on a range of benchmark tasks, including copying, sorting, and associative recall.\n\nMethodology:\nThe authors provide a clear and concise explanation of the new architecture and how it differs from traditional Neural Turing Machines. They have effectively demonstrated the effectiveness of the proposed method by comparing it with vanilla NTMs, adding a number of benchmarks to show its good performance.\n\nResults and Discussion:\nThe performance of the proposed method on all benchmarks is highly promising, exceeding the traditional NTMs on all tasks. The authors have provided extensive analysis and comparison with previous methods, which helps in understanding the strengths and limitations of the proposed model. The authors also provide a thoughtful discussion of the possible limitations of their model and ways in which it can be improved in future work.\n\nConclusion:\nOverall, this paper is well-written, comprehensive and provides sound evidence that the proposed Lie-Access Neural Turing Machines architecture has a great potential for improving the performance of NTMs. The authors have successfully demonstrated beyond question that the proposed Lie-algebraic approach provides a new way of thinking in designing memory-augmented neural architectures, as well as opening up new research possibilities to explore.","model":"chatGPT","source":"peerread","label":1,"id":1904}
{"text":"The paper \"Lie-Access Neural Turing Machines\" addresses the problem of improving the capabilities of Neural Turing Machines (NTMs) by incorporating the concept of Lie groups, which allow for more efficient and flexible transformations of data. Specifically, the paper applies Lie theory to extend the memory and read\/write operations of NTMs, resulting in a novel Lie-Access Neural Turing Machine (LANTM) architecture.\n\nOne strength of this paper is the thorough explanation of the Lie theory concepts and their application to NTMs, making it accessible for both NTM experts and Lie theory theorists. The paper provides clear examples and figures to illustrate the key ideas of the LANTM architecture, which helps in understanding the implementation details.\n\nHowever, a potential weakness of this paper is the limited analysis of the LANTM model's performance compared to a standard NTM. While the paper provides empirical results demonstrating the effectiveness of LANTM, it only compares them to a simple NTM rather than state-of-the-art models. Therefore, it is unclear how well the LANTM model would perform in comparison to other existing models on more complex tasks.\n\nMoreover, while the paper provides an overview of the benefits of applying Lie theory to NTMs, it does not delve into the potential drawbacks or limitations of this approach. Additionally, the paper could have benefit from more discussion on the potential future applications of the LANTM model beyond its current implementation.\n\nIn summary, the paper \"Lie-Access Neural Turing Machines\" provides a novel approach to improving the capabilities of Neural Turing Machines using Lie groups, which can be potentially beneficial in real-world applications. The paper's strength lies in the clear explanations of the LANTM architecture, but it could benefit from additional analysis and discussion of the model's performance and limitations.","model":"chatGPT","source":"peerread","label":1,"id":1905}
{"text":"Review of \"Lie-Access Neural Turing Machines\"\n\nThe paper proposes an alternative model for external neural memory structures which is specifically designed for the neural setting. The authors argue that among the various neural memory systems, the existing ones lack specific structure that is important for relative indexing. They propose a solution, called Lie-access memory, in which memory is accessed using a continuous head in a key-space manifold. The head is moved via Lie group actions, such as shifts or rotations, generated by a controller, and memory access is performed by linear smoothing in key space. \n\nThe paper is well written and structured, and the authors provide a clear, precise description of their proposed approach. The concept of Lie groups as a natural generalization of discrete memory structures is interesting, and the authors provide a convincing argument for the advantages of using such groups in the context of external neural memory. The implementation of a simplified Lie-access neural Turing machine (LANTM) with different Lie groups is well described and the experimental results are promising, showing that the proposed approach performs well on a range of algorithmic tasks.\n\nHowever, there are some areas in which the paper could be improved. Firstly, while the authors argue that the existing neural memory systems lack specific structure important for relative indexing, they could more clearly explain why this is the case and what the limitations of these systems are. Secondly, while the proposed approach is motivated by the need for relative indexing, it is not clear how this approach would be extended to incorporate other types of memory access, such as absolute indexing. Finally, while the experimental results are promising, the authors could provide more detailed analysis of the strengths and limitations of their proposed approach when compared to existing external neural memory structures.\n\nOverall, the proposed approach of Lie-access memory for external neural memory structures is an interesting and potentially valuable contribution to the field. The paper is well written, and the experimental results are promising, although there are areas in which the paper could be improved.","model":"chatGPT","source":"peerread","label":1,"id":1906}
{"text":"This paper explores the utilization of external neural memory structures for algorithmic deep learning. The authors argue that current neural memory systems lack specific structure necessary for relative indexing, and propose an alternative model, Lie-access memory, that is explicitly designed for the neural setting. This paradigm utilizes a continuous head in a key-space manifold, moved via Lie group actions, and linear smoothing in key space, for memory access. The authors argue that Lie groups provide a natural generalization of discrete memory structures such as Turing machines, while maintaining differentiability. \n\nThe main strength of this paper is the innovative proposal of Lie-access memory, which addresses the limitations of existing neural memory structures. The use of Lie groups and manifold-based memory access allows for efficient use of memory and provides greater interpretability of the memory operation. Furthermore, the implementation of a Lie-access neural Turing machine (LANTM) allows for practical validation of the proposed approach, with promising results shown for a range of algorithmic tasks.\n\nOne potential weakness of the paper is the lack of comparison with other state-of-the-art memory structures, such as memory networks or differentiable neural computers. Another weakness is the limited analysis of the efficiency and scalability of the proposed approach compared to traditional memory structures.\n\nIn summary, the proposed Lie-access memory approach is an innovative and promising direction for external neural memory structures. Further research can build on this work by comparing it with other state-of-the-art memory structures and exploring its efficiency and scalability in larger-scale tasks.","model":"chatGPT","source":"peerread","label":1,"id":1907}
{"text":"Title: Quasi-Recurrent Neural Networks\n\nAuthors: James Bradbury, Stephen Merity, Caiming Xiong, Richard Socher\n\nAbstract:\n\nThe Quasi-Recurrent Neural Network (QRNN) is a neural network architecture that can process sequential data more efficiently than the traditional RNN architecture. The QRNN is able to achieve this by combining the strengths of the convolutional and recurrent neural network architectures. In this paper, we describe the QRNN architecture and evaluate its performance on a variety of sequence modeling tasks, including language modeling and machine translation. We show that the QRNN outperforms both traditional RNNs and convolutional neural networks in these tasks.\n\nPeer Review:\n\nThe paper titled \"Quasi-Recurrent Neural Networks\" presents an interesting new neural network architecture for sequence modeling, the Quasi-Recurrent Neural Network (QRNN). The authors have demonstrated through experiments on several sequence modeling tasks that the QRNN is more efficient and outperforms traditional RNNs and convolutional neural networks. The paper provides a clear description of the QRNN architecture and the experiments are well-designed and comprehensive.\n\nThe authors have provided a detailed comparison of the QRNN with the traditional RNN and convolutional neural network architectures, highlighting the strengths and weaknesses of each. The results of the experiments presented in the paper show that the QRNN is able to learn better representations of sequential data and can do so with fewer parameters than the traditional RNN and convolutional neural network architectures.\n\nOne issue that I found with the paper is that the authors do not provide details on how they selected the hyperparameters for their experiments. It would be helpful for readers to better understand how the authors arrived at their choice of hyperparameters and how this may have affected the results.\n\nOverall, the paper is well-written and the experimental results are convincing. The QRNN architecture has the potential to improve the efficiency and performance of many sequence modeling tasks, and this paper makes a significant contribution in introducing and evaluating the QRNN approach.","model":"chatGPT","source":"peerread","label":1,"id":1908}
{"text":"The paper \"Quasi-Recurrent Neural Networks\" addresses an important problem in natural language processing - the challenge of processing sequential data with long-range dependencies. The authors propose a new neural network architecture, called Quasi-Recurrent Neural Network (QRNN), that seeks to address some of the limitations of existing recurrent architectures such as the Long Short-Term Memory (LSTM) network.\n\nOne strength of the paper is the clear explanation of the proposed architecture and the comparison with other neural network models. The authors demonstrate that QRNN is faster, more memory-efficient, and yields comparable or better performance than LSTM and other state-of-the-art models on a range of benchmark tasks, including language modeling and sentiment analysis.\n\nAnother strength of the paper is the thorough analysis of the impact of hyperparameters on the performance of QRNN. The authors conduct a series of experiments and provide clear explanations of the results, which will be useful for researchers and practitioners seeking to use the model for their own applications.\n\nOne of the limitations of the paper is the lack of analysis on the interpretability of the model. While the authors provide evidence that QRNN outperforms existing models on various benchmark tasks, it is unclear how the model is making its predictions. This is an important consideration in natural language processing, where model interpretability is important for real-world applications.\n\nOverall, the paper presents a promising new neural network architecture for sequential data processing tasks in natural language processing. The authors provide evidence that QRNN is faster, more memory-efficient, and yields comparable or better performance than LSTM and other state-of-the-art models on a range of benchmark tasks. However, further work is needed to enhance the interpretability of the model.","model":"chatGPT","source":"peerread","label":1,"id":1909}
{"text":"This paper introduces quasi-recurrent neural networks (QRNNs) as an alternative approach to modeling sequential data that allows for increased parallelism compared to traditional recurrent neural networks (RNNs). The authors accomplish this by alternating convolutional layers, which operate in parallel across time steps, with a minimalist recurrent pooling function that operates in parallel across channels. \n\nThe authors have provided comprehensive experimental results on various applications, such as language modeling, sentiment classification, and character-level neural machine translation. The results show that QRNNs achieve better predictive accuracy than stacked LSTMs of the same hidden size while also being up to 16 times faster at train and test time.\n\nThe paper is well-organized and easy to follow, making it accessible to readers from different backgrounds. The authors provide a clear and concise explanation of the motivation, methodology, and results. The key contributions of the paper are well-presented and supported by experiments. \n\nHowever, the authors could have provided more details on the limitations and potential future directions of QRNNs. Additionally, it would have been valuable to compare QRNNs with other established approaches, such as Gated Recurrent Units (GRUs), and highlight the advantages\/disadvantages of each approach.\n\nOverall, I believe this paper is novel, well-written, and provides strong empirical evidence for the effectiveness of QRNNs. I highly recommend this paper to researchers working in the field of natural language processing and sequence modeling.","model":"chatGPT","source":"peerread","label":1,"id":1910}
{"text":"The paper \"Quasi-Recurrent Neural Networks\" addresses the problem of the limitations in the parallel processing of Recurrent Neural Networks (RNNs) due to their dependence on previous timesteps. Specifically, the authors propose an alternative approach, Quasi-Recurrent Neural Networks (QRNNs), that alternates between convolutional and pooling layers to achieve higher parallelism and better predictive accuracy than conventional RNNs.\n\nThe main strength of this paper is the introduction of an alternative approach that significantly improves the efficiency of neural sequence modeling. The authors show that even in the absence of trainable recurrent layers, QRNNs achieve better predictive accuracy than RNNs of the same hidden size. Additionally, QRNNs are up to 16 times faster at train and test time, making them significantly more practical for real-world sequence modeling applications.\n\nAnother strength is the thoroughness of the experiments conducted by the authors, which demonstrate the efficacy of QRNNs for a range of different sequence tasks, including language modeling, sentiment classification, and character-level neural machine translation.\n\nOne weakness of the paper is the lack of a detailed analysis of the mechanism by which QRNNs achieve their superior performance. Although the authors explore the idea that the alternate convolutions and pooling layers allow for better parallelization, it would be useful to have a more detailed analysis of how this process works in practice.\n\nOverall, \"Quasi-Recurrent Neural Networks\" presents a promising alternative to conventional RNNs for sequence modeling, with compelling results in terms of both accuracy and efficiency. The authors could further strengthen their argument by providing a more detailed analysis of the mechanisms underlying the improved performance of QRNNs.","model":"chatGPT","source":"peerread","label":1,"id":1911}
{"text":"Title: Recurrent Environment Simulators: Learning Dynamics from Pixels\n\nAuthors: Jane Doe and John Smith\n\nSummary:\n\nIn this paper, the authors introduce a new approach to model-based reinforcement learning using recurrent environment simulators. The proposed method, which combines a convolutional neural network with a recurrent neural network, is able to learn the underlying dynamics of an environment from raw pixel observations. The authors demonstrate the effectiveness of this approach on various benchmarks in Atari games and continuous control tasks.\n\nOverall Assessment:\n\nThe paper presents a novel and interesting approach for learning the dynamics of an environment without explicit knowledge of the underlying physics. The use of a recurrent neural network allows for the model to capture the temporal dynamics of the environment, which is a valuable contribution to the field. The experimental results are well presented and demonstrate the superiority of the proposed method over existing approaches for both discrete and continuous control problems.\n\nHowever, the paper does not provide an in-depth analysis of the limitations and potential failure modes of the approach. It would be useful to investigate the sensitivity of the model to different initialization and regularization techniques, and to study how the performance of the proposed method deteriorates as the complexity of the environment increases. Additionally, it would be interesting to explore the interpretability of the learned model and how it can be used to explain observed phenomena in the environment.\n\nMoreover, it would be beneficial if the paper discussed potential applications and real-world use cases for the proposed method, beyond the experimental benchmarks presented in the paper.\n\nThe writing is generally clear and concise, though there are a few minor grammatical errors that should be corrected.\n\nIn summary, the proposed recurrent environment simulator is a valuable contribution to the field of model-based reinforcement learning, and the experimental results presented in the paper are promising. However, further investigation into the limitations and potential applications of the proposed method would strengthen the paper.","model":"chatGPT","source":"peerread","label":1,"id":1912}
{"text":"The \"Recurrent Environment Simulators\" paper proposes a new deep learning approach to address the challenge of simulating complex environments, such as those encountered in robotics or gaming. The authors introduce a novel architecture called \"Recurrent Environment Simulators\" (RES), which combines recurrent neural networks and variational autoencoders to generate realistic and diverse environment trajectories.\n\nStrengths:\n- The paper provides a clear and comprehensive description of the proposed RES architecture, explaining how it works and how it compares to existing approaches in the field.\n- The authors perform extensive experiments to test the performance and accuracy of RES, comparing it to several baselines and demonstrating its superior results.\n- The paper discusses the potential applications of RES in several domains, including robotics and game development, highlighting its flexibility and usefulness.\n\nWeaknesses:\n- While the paper provides detailed descriptions of the experiments, the authors do not provide sufficient explanation or analysis of the results, making it difficult to interpret the significance of the findings.\n- The paper lacks a clear discussion of the limitations or potential weaknesses of the proposed approach, leaving some unanswered questions about its generalizability or scalability.\n- Finally, the authors do not provide any information about the code or data used in their experiments, making it difficult for others to reproduce or build upon their work.\n\nOverall, the \"Recurrent Environment Simulators\" paper makes a valuable contribution to the field of deep learning and simulation, offering a novel approach that could be useful in several domains. While there are some limitations and unanswered questions, the authors provide a solid foundation for future research in this area.","model":"chatGPT","source":"peerread","label":1,"id":1913}
{"text":"Peer review for \"Recurrent Environment Simulators\"\n\nThe paper \"Recurrent Environment Simulators\" presents an interesting approach to improve environment simulators for agents to plan and act efficiently. The authors propose to use recurrent neural networks to make coherent predictions for hundreds of time-steps into the future from high-dimensional pixel observations. The proposed approach is tested on ten Atari games, a 3D car racing environment, and complex 3D mazes, and the results show that this method can be used to improve exploration while being adaptable to diverse environments. \n\nThe paper is well-written and the authors provide an extensive analysis of the factors affecting performance to advance the understanding of the properties of these models. However, some issues in the paper need to be addressed. \n\nFirstly, the authors do not clearly explain why their approach outperforms previous methods. A detailed comparison with existing state-of-the-art methods would help to better understand the significance of the proposed approach. Furthermore, the authors do not provide any comparison with humans or other RL algorithms in terms of performance. Without these comparisons, it is difficult to understand the practical implications of the proposed technique.\n\nSecondly, the authors should justify the claim of computational efficiency more thoroughly. Although the proposed model is claimed to generate high-dimensional images only if necessary, the computational burden is not clearly stated. Moreover, the authors should specify the hardware requirements and runtime complexity to train and test the model.\n\nFinally, the authors only tested their approach on a limited number of environments. Besides Atari games and 3D mazes, more diverse and complex environments should be considered to better understand the generality and scalability of the proposed technique.\n\nOverall, the paper presents an interesting approach to improving environment simulators for agents. The authors provide a thorough analysis of the factors affecting performance and demonstrate the potential benefits of using an RNN-based approach. However, some issues need to be addressed and clarified to make the paper more valuable and informative.","model":"chatGPT","source":"peerread","label":1,"id":1914}
{"text":"The paper \"Recurrent Environment Simulators\" addresses the problem of developing models that can simulate how environments change in response to actions, which can be used by agents to plan and act efficiently. The paper strengthens the previous environment simulators from high-dimensional pixel observations by introducing recurrent neural networks that are able to generate temporally and spatially coherent predictions for hundreds of time-steps into the future. Additionally, the paper presents an in-depth analysis of the factors that affect this model's performance, providing the most extensive attempt to advance the understanding of the properties of these models.\n\nThe paper's strengths include the innovative use of recurrent neural networks for generating temporally and spatially coherent predictions for hundreds of time-steps into the future. The paper also addresses the issue of computational inefficiency, which is a significant concern when generating high-dimensional images at each time-step. Moreover, the paper demonstrates the wide applicability of the proposed approach, as it works well in various environments, namely 10 Atari games, a 3D car racing environment, and complex 3D mazes.\n\nA potential weakness of the paper is that the experimental results are limited to simulation environments, and the paper lacks empirical evidence to demonstrate the proposed method's performance in real-world settings.\n\nOverall, the paper \"Recurrent Environment Simulators\" is a well-written and informative article that makes a significant contribution to the literature on environment simulators. It offers a unique and innovative approach to the task of generating simulation environments, demonstrating its effectiveness in various environments. The authors' in-depth analysis of the factors affecting the proposed method's performance is particularly noteworthy and provides valuable insights into the properties of these models. Nonetheless, additional empirical evidence is required to determine its effectiveness in real-world settings.","model":"chatGPT","source":"peerread","label":1,"id":1915}
{"text":"Overall, I found the paper on EPOpt: Learning Robust Neural Network Policies Using Model Ensembles to be well-written and informative. The paper presents a novel approach to learning neural network policies using model ensembles, which is an interesting direction for research in this field. The authors describe the methodology in detail, providing clear and concise explanations of the various components of their proposed method.\n\nOne of the strengths of this paper is the thoroughness of the experimental evaluation. The authors test their approach on a variety of environments and provide a comprehensive analysis of the results. The experiments show that EPOpt outperforms other state-of-the-art approaches in terms of robustness and stability, which is a significant contribution to the field.\n\nHowever, there are a few areas where I think the paper could be improved. While the authors provide good explanations of their methodology, there were some parts where the presentation could have been clearer. For example, in the section on the objective function, it would have been helpful to include a clear explanation of the different terms and their purpose.\n\nAdditionally, I would have liked to see a more extensive discussion of the limitations of this approach. While the paper identifies some concerns related to scalability and computational cost, there could be other limitations that are not addressed. For instance, it is unclear how EPOpt might perform in more complex environments, and whether there are any trade-offs between performance and interpretability.\n\nOverall, I believe this paper makes a valuable contribution to the field of neural network policy learning. The experiments demonstrate that EPOpt is a promising approach that can improve the robustness and stability of neural network policies. I would recommend this paper for publication, with the suggestion to clarify some of the methodology and provide a more thorough discussion of the limitations of the approach.","model":"chatGPT","source":"peerread","label":1,"id":1916}
{"text":"The paper \"EPOpt: Learning Robust Neural Network Policies Using Model Ensembles\" presents a novel approach to developing robust neural network policies. The authors outline the need for developing robust policies that can perform well in the presence of uncertainties and variation in the environment. The paper addresses this need by proposing a method that uses model ensembles to achieve robustness.\n\nOne of the key strengths of this paper is the clarity of the proposed approach. The authors provide detailed descriptions of the various steps involved in developing robust policies using model ensembles. They also demonstrate the effectiveness of their approach through comprehensive experimental evaluations on several benchmark tasks, which shows that the proposed approach outperforms the state-of-the-art methods. \n\nAnother strength of the paper is the novelty of the proposed approach. The authors present a unique method for incorporating model ensembles to develop robust policies while highlighting the importance of the ensemble size in achieving good performance. They also conducted necessary ablation studies to investigate the contribution of each component of the proposed method. \n\nHowever, there are also some potential weaknesses in the paper. One potential weakness is the limited discussion on the limitations of the proposed approach. It would have been valuable to discuss the scenarios where the proposed approach may not be effective or applicable, particularly given the complexities of real-world environments. \n\nAnother potential weakness is the limited comparison with other baselines that explicitly utilize uncertainty estimation, such as dropout, which is increasingly popular in recent research. While the proposed approach performs better than other reinforcement learning algorithms, more comparison with such methods may better understand the relative advantage of using model ensembles.\n\nOverall, this paper is valuable and contributes meaningfully to the field of reinforcement learning by providing a unique and effective approach to develop robust neural network policies using model ensembles. The authors have provided strong empirical evidence to support their claims, and the proposed method offers potential applicability in a variety of real-world scenarios.","model":"chatGPT","source":"peerread","label":1,"id":1917}
{"text":"Overall, this paper presents a well-written and well-motivated approach to learning robust neural network policies using model ensembles. The author highlights the importance of sample complexity and safety in learning policies with reinforcement learning for real-world tasks, particularly when policies are represented using deep neural networks.\n\nThe author employs model-based methods with simulated training data, which has the potential to provide a richer set of data for learning policies. However, the author acknowledges the challenges posed by discrepancies between the simulated source domain and target domain.\n\nTo address this challenge, the author proposes the EPOpt algorithm, which successfully uses an ensemble of simulated source domains and adversarial training to learn policies that are robust and generalize to a wide range of possible target domains, including unmodeled effects. The author also introduces the concept of adapting the probability distribution over source domains in the ensemble using data from the target domain and approximate Bayesian methods. \n\nOverall, the presented approach appears promising and the author provides sufficient details and results to support the effectiveness of the EPOpt algorithm. However, the author could provide some additional discussion points to further strengthen the paper. For example, further explanation of the choice of ensemble size and how it affects model performance, a discussion on the limitations and trade-offs in using simulations, and a more detailed comparison with existing approaches for handling discrepancies between source and target domains. \n\nIn conclusion, this paper presents a promising approach to address challenges in learning policies using deep neural networks for real-world tasks, and its novelty lies in the combination of model ensembles and source domain adaptation to achieve both robustness and learning.","model":"chatGPT","source":"peerread","label":1,"id":1918}
{"text":"The paper \"EPOpt: Learning Robust Neural Network Policies Using Model Ensembles\" addresses one of the major challenges faced when learning policies with reinforcement learning for real-world tasks, which is sample complexity and safety. The authors argue that to address this challenge, model-based methods can be employed, where a simulated source domain is created to augment real data. However, using a simulated source domain can lead to discrepancies between simulated training and real-world applications, which makes training challenging. The authors propose a new algorithm, EPOpt, that uses an ensemble of simulated source domains and adversarial training to learn policies that are robust and can generalize to a broad range of possible target domains.\n\nStrengths:\nThe paper proposes a new method, EPOpt, that addresses a significant challenge faced in reinforcement learning, and that is sample complexity and safety. The authors use a model ensemble and adversarial training to address discrepancies between the simulated source domain and real-world applications. The use of this ensemble is an innovative approach to improve the performance of the algorithm, and it allows the model to deal with unmodeled effects that can occur in the target domain. Additionally, the paper highlights and addresses the problem of dataset bias and its effect on the learned policy.\n\nWeaknesses:\nThe authors provide limited details on how the ensemble of models was created and how the adversarial training was conducted. More information on these steps would make it easier for readers to replicate the results of the study. Additionally, the paper does not provide any results on the performance of EPOpt when compared to other state-of-the-art algorithms. It would be useful if the authors provided this comparison to demonstrate the effectiveness of their proposed algorithm.\n\nIn summary, the paper presents a novel algorithm, EPOpt, that addresses a major challenge faced in reinforcement learning, and that is sample complexity and safety. The use of a model ensemble and adversarial training to learn a policy that is robust to various target domains is an innovative approach that contributes to the field of reinforcement learning. However, further details on the method should be provided, and comparisons with other state-of-the-art algorithms should be conducted.","model":"chatGPT","source":"peerread","label":1,"id":1919}
{"text":"Title: Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain\n\nThe paper \"Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain\" proposes a new method for adaptive transfer learning from multiple sources in the same domain. The authors introduce a deep architecture that uses attention mechanisms to selectively attend to different sources and adapt to target tasks. \n\nOverall, the paper is well-written and clearly describes the proposed method. The experimental results demonstrate the effectiveness of the proposed method compared to several baseline methods on multiple datasets. The authors also provide a thorough analysis of the proposed method's components and parameters, which can help guide future research.\n\nHowever, there are some areas that the authors could improve upon. First, the authors could provide more information on the datasets used in the experiments. Specifically, the authors could discuss the characteristics of each dataset, including how many samples are in each dataset, the distribution of labels, and any imbalances. Additionally, the authors could provide more details on the experimental setup, such as how the hyperparameters were selected, whether any additional data augmentation was used, and how many trials were run.\n\nFurthermore, the authors might want to clarify some of the technical details in the paper. For instance, the use of different techniques like fine-tuning can be made clearer, and the intuition behind the attention mechanism they propose could be better explained.\n\nIn conclusion, this paper presents a novel and effective approach for adaptive transfer learning from multiple sources in the same domain. While there is room for improvement in certain areas, the overall contribution of the paper is significant and can benefit the field of transfer learning.","model":"chatGPT","source":"peerread","label":1,"id":1920}
{"text":"Paper Title: Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain\n\nProblem\/Question Addressed:\n\nThe paper addresses the problem of transfer learning, which is the ability of learning from one or multiple sources and then adapt the knowledge in a new, related but distinct, domain. More specifically, the paper proposes a novel attention-based deep learning framework that can adaptively transfer knowledge from multiple sources in the same domain to a target task, with the aim of improving the performance and reducing the need for large amounts of labeled data.\n\nStrengths:\n\nThe paper presents a well-motivated and comprehensive approach to transfer learning, designing a novel attention-based deep learning architecture that can attend, adapt, and transfer knowledge from multiple sources in the same domain to a target task. The proposed method is evaluated using a wide range of experiments on several benchmark datasets, and the results demonstrate the effectiveness and robustness of the approach compared to several state-of-the-art transfer learning methods. The paper is well-structured, clearly written, and the authors provide an extensive literature review that contextualizes their contribution and highlights the novelty and relevance of their work.\n\nWeaknesses:\n\nThe main weakness of the paper is the lack of clarity and transparency in detailing some of the key technical aspects of the proposed approach. For instance, the paper would benefit from providing more detailed descriptions of the motivation and intuition behind the various design choices, such as the selection of the different layers or the choice of the specific attention mechanisms. Additionally, the paper would benefit from providing a more in-depth analysis and interpretation of the experimental results, providing more insights into the relative contributions and limitations of the proposed approach.\n\nOverall, the paper offers an interesting and well-executed approach to adaptive transfer learning, providing a novel attention-based deep learning framework that can leverage multiple related sources to improve performance in a target task. With some minor revisions and additional clarifications, this paper can make a valuable contribution to the literature on transfer learning and deep learning techniques.","model":"chatGPT","source":"peerread","label":1,"id":1921}
{"text":"The paper \"Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain\" addresses the important problem of knowledge transfer from source tasks to a new target task. The authors propose a generic architecture, called A2T, that is able to selectively transfer knowledge from multiple source tasks in the same domain. The paper presents thorough experiments on different learning algorithms, demonstrating that A2T is an effective approach that can avoid negative transfer while transferring selectively.\n\nThe paper is well-written, and its structure is clear and easy to follow. The authors present a good overview of related work in the area of transfer learning, and they clearly explain the two main challenges of transfer learning: avoiding negative transfer and selectively transferring relevant knowledge.\n\nThe authors provide a detailed description of their proposed architecture, A2T, which consists of three modules: an attention module, an adaptation module, and a transfer module. The attention module is used to selectively attend to the relevant parts of the source tasks, while the adaptation module is used to adjust the transferred knowledge to fit the new target task. The transfer module is used to transfer the knowledge from the source tasks to the target task.\n\nThe experiments conducted by the authors are comprehensive and include different learning algorithms and evaluation metrics. They show that A2T is able to avoid negative transfer, and it can selectively transfer knowledge from multiple source tasks in the same domain. The results also demonstrate that A2T outperforms other baseline methods for transfer learning.\n\nIn conclusion, the paper \"Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain\" presents a well-motivated and effective approach for transfer learning. The proposed architecture, A2T, is generic and flexible, and it is shown to be able to avoid negative transfer and selectively transfer knowledge from multiple source tasks. The experiments are thorough and convincing, making the paper an important contribution to the field of transfer learning.","model":"chatGPT","source":"peerread","label":1,"id":1922}
{"text":"The paper \"Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain\" addresses the problem of transferring knowledge from prior source tasks to a new target task in several learning applications. It is recognized that transfer learning is an effective way of improving the learning process when dealing with new tasks. However, the paper points out two challenges that need to be addressed in the transfer learning process, avoiding negative transfer and selectively transferring from multiple source tasks.\n\nThe authors propose a generic approach for Adapt and Transfer (A2T) mechanism which is an attentive deep architecture for selecting and transferring knowledge from multiple source tasks. The paper claims that A2T solves the problem of negative transfer by being able to transfer selectively from different source tasks only where it is beneficial to the learning process of the target task. The authors provide empirical evaluations using different learning algorithms to demonstrate the effectiveness of A2T in transfer learning.\n\nThe paper's strengths include the creative solution it proposes for selective transfer learning which is applicable to both policy and value functions. The authors provide sufficient detail on how their model operates and provides empirical evaluations on different learning algorithms. The paper's organization makes it easy to understand and follow the proposed solution.\n\nHowever, the paper also has some weaknesses. It does not provide detailed explanation or argument as to why selective transfer learning is so crucial over other transfer learning methods in different settings. Also, while the authors provide empirical evaluations, they do not compare their results to existing transfer learning approaches or do ablation tests to better understand the contribution of each component of their framework.\n\nOverall, \"Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain\" paper provides an interesting solution to an important problem of transfer learning, and it is well-written and well-presented. However, more work is needed to better explain the strengths and weaknesses of this paper in the context of the broader literature on transfer learning.","model":"chatGPT","source":"peerread","label":1,"id":1923}
{"text":"Introduction:\n\nThe paper titled \"Multi-view Recurrent Neural Acoustic Word Embeddings\" presents an innovative approach for creating word embeddings using multiple viewpoints. The authors propose that using different acoustics created by various sources can help create more robust and accurate embeddings, thus improving the overall performance of the resulting model.\n\n\nStrengths:\n\nThe paper presents a novel technique for creating word embeddings, which is different from the traditional approaches used in NLP. The authors provide a convincing argument for using multiple viewpoints, which is supported by the results obtained in their experiments. The Multi-view Recurrent Neural Network architecture presented in the paper is well explained and seems promising. \n\nThe authors' method achieves better results compared to baseline methods, and the evaluation is comprehensive. The paper presents a good analysis of the learned embeddings and provides a thorough understanding of the benefits of using multiple viewpoints in training the model.\n\nWeaknesses:\n\nHowever, the paper could be improved by discussing more the limitations of the proposed method. Although the results are better than those of the baselines, some experimental results are still far from perfect, and there is still room for further improvement.\n\nAnother issue is the lack of a comparison among the different sources used as input. The paper shows that combining different views leads to better results, but it remains unclear which sources contribute the most to the overall improvement in accuracy. Is it the combination of the different sources that is essential, or are some sources more critical than others? The authors do not provide a clear answer to this question.\n\nConclusion:\n\nOverall, the paper presents an innovative approach for creating word embeddings using multiple viewpoints. The experiments show that using different acoustics can indeed help create more robust and accurate embeddings. Although some issues remain unclear, and the method is yet to be evaluated on other datasets, this paper is well-written and presents promising results. With some improvement, the proposed method could be a valuable addition to the field of NLP.","model":"chatGPT","source":"peerread","label":1,"id":1924}
{"text":"The paper \"Multi-view Recurrent Neural Acoustic Word Embeddings\" addresses the problem of robustly representing words in the context of spoken language processing. The authors propose a new method for creating word embeddings that can handle multiple acoustic views of the same spoken word. \n\nOverall, I found the paper to have several strengths. Firstly, the authors provide a detailed description of their proposed method, including a clear explanation of the architecture and model training process. They also provide thorough evaluation metrics, including both a quantitative comparison with existing methods, as well as a qualitative analysis of the embeddings themselves. \n\nIn terms of weaknesses, I noted that the paper could have provided a more thorough discussion of the limitations of their method. Additionally, while the authors provide some evidence of the usefulness of their embeddings, it would have been helpful to see more detailed analysis of their potential applications, such as in speech recognition or speaker identification. \n\nIn conclusion, the paper \"Multi-view Recurrent Neural Acoustic Word Embeddings\" provides a compelling new method for creating word embeddings in the context of spoken language processing. While there may be some limitations to the approach, the authors present a convincing case for the potential usefulness of this method in a variety of applications.","model":"chatGPT","source":"peerread","label":1,"id":1925}
{"text":"Title: Peer Review for \"Multi-view Recurrent Neural Acoustic Word Embeddings\" \n\nThe paper provides an interesting and novel approach to learning neural acoustic word embeddings, combining information from multiple views. The idea of jointly learning to embed acoustic sequences and their corresponding character sequences using deep bidirectional LSTM embedding models and multi-view contrastive losses is promising. The paper is well organized and clearly explains the objectives, methods, and results.\n\nThe experimental results presented in the paper support the effectiveness of the proposed approach. The authors demonstrate that their multi-view approach to learning acoustic word embeddings outperforms previous approaches in word discrimination task. Also, cross-view word discrimination and word similarity tasks are enabled, which further show the potential of the proposed approach.\n\nHowever, there are some limitations of the study that need to be addressed. Firstly, although the paper provides a comparison with previous approaches, it would be good to have more details about the dataset used for experiments, such as the size and type of data. Furthermore, while the authors explore different loss functions, they do not provide conclusive evidence that a specific variant works better than others. Providing a more detailed analysis and comparison of different loss variants would strengthen the paper.\n\nOverall, the paper is well-written and provides a significant contribution to the field of neural acoustic word embeddings. The proposed multi-view approach shows potential for improving speech processing tasks, and the experimental results are promising. However, some limitations need to be addressed to further strengthen the paper.","model":"chatGPT","source":"peerread","label":1,"id":1926}
{"text":"The paper \"Multi-view Recurrent Neural Acoustic Word Embeddings\" addresses the problem of neural acoustic word embeddings, which are vector representations of arbitrary-length speech segments corresponding to words, for speech recognition and retrieval tasks. The main idea of this work is to create a fixed-dimensional vector space in which words with similar acoustic representations are mapped to nearby points, while words with different acoustic representations are mapped to far away points.\n\nThe paper has several strengths. First, the multi-view approach to embedding acoustic sequences and corresponding character sequences is novel and shows promising results. Second, the use of deep bidirectional LSTM embedding models is well-suited to the task and results in high accuracy. Third, the analysis of different loss variants, including fixed-margin and cost-sensitive losses, provides useful insights into the effectiveness of different approaches.\n\nHowever, there are also some weaknesses in the paper. One weakness is that the experiments are only conducted on a single dataset, which limits the generalizability of the results. Additionally, the paper would benefit from a more comprehensive comparison with other state-of-the-art methods.\n\nOverall, the paper \"Multi-view Recurrent Neural Acoustic Word Embeddings\" presents a well-written and well-executed approach to the problem of neural acoustic word embeddings, with several notable strengths and some minor weaknesses.","model":"chatGPT","source":"peerread","label":1,"id":1927}
{"text":"Title: Learning Features of Music From Scratch \n\nThe paper presents an interesting approach to learning features of music by using a deep learning technique with a convolutional neural network (CNN) architecture. The authors provide a detailed description of the methodology along with a comprehensive evaluation of the model, showing results that demonstrate the efficacy of their approach.\n\nThe experimental design is well-structured, and the authors provide sufficient detail about the dataset, the pre-processing and the CNN architecture used in the study. The authors also explain the reasoning behind the selection of the particular CNN architecture, which is helpful in understanding the rationale behind the methodology.\n\nThe results demonstrate that the proposed approach can effectively learn discriminative features that are specific to different genres of music. The evaluation metrics used in the study are appropriate for assessing the performance of the model, and the authors have provided a thorough analysis of the obtained results.\n\nThe paper could benefit from further explanation of some of the technical details, particularly in the description of the CNN architecture used in the study. It would also be helpful to provide more information about how the dataset was collected, the number of instances per genre, and whether the data was stratified or not.\n\nOverall, the paper presents a novel approach to learning features of music, and the methodology is sound. The results are promising and demonstrate the potential for the proposed approach to be used in various applications such as music recommendation systems or music classification. Therefore, I would recommend the paper for publication after addressing the minor technical issues mentioned above.","model":"chatGPT","source":"peerread","label":1,"id":1928}
{"text":"Review of \"Learning Features of Music From Scratch\"\n\nThe paper aims to investigate the effectiveness of different models to learn the musical structure of a piece of music from scratch, without relying on any prior knowledge of the musical grammar. The authors argue that there is a lack of research in this area and suggest that this paper aims to alleviate this issue by generating a new dataset composed of more than 1500 pieces of music, from several different musical genres, and testing various models against them. \n\nStrengths:\n- The paper presents a comprehensive overview of the existing literature regarding music feature learning, which highlights the significance of this new dataset for this particular research question.\n- The paper compares different models, ranging from traditional unsupervised algorithms to more novel approaches using deep learning architectures, which ensures a broad scope of the investigation.\n- The authors use various evaluation metrics to assess the performance of the models, which provides a robust evaluation of the models' effectiveness.\n\nWeaknesses:\n- The models tested in this research rely on the presence of a melody,which may not exist in all genres of music or in all pieces of music. It would be useful to consider other aspects of music, such as harmony or rhythm, in future research.\n- The authors report the average performance of each model rather than analyzing the performance of each model with respect to individual pieces of music, which limits the insights that can be gained from the study.\n- Although the dataset used in this study is large and diverse, it only includes one type of encoding, which could lead to potential bias in the dataset.\n\nOverall, the paper makes a valuable contribution to the literature on music feature learning. It presents a new dataset that could be useful for future studies in this area, and the comparison of different models provides valuable insights into the strengths and weaknesses of each approach. While there are some limitations to the study's methodology, the authors were able to generate meaningful results and provide solid evidence for their findings.","model":"chatGPT","source":"peerread","label":1,"id":1929}
{"text":"General Comments:\n\nThis paper introduces a new large-scale music dataset, MusicNet and proposes a multi-label classification task that aims to predict notes in musical recordings. The authors benchmark three machine learning architectures \u2013 spectrogram-based learning, end-to-end learning with a neural net and end-to-end learning with a convolutional neural net \u2013 and report their findings. By experimental results, the study concludes that end-to-end models trained for note prediction learn frequency-selective filters as a low-level representation of audio. Overall, the paper is well-structured, and the arguments presented are clear and easy to follow. Though, some additional details could be added to enhance the clarity of the study.\n\nMajor comments:\n\n1. One significant concern with the paper is regarding the data augmentation techniques and how they could impact the training and performance of the models. It is suggested that the authors should discuss the augmentation techniques, which could include techniques such as time-stretching, pitch-shifting, noise injection, and room simulation. The authors may also want to consider the implication of these techniques on the performance of the models being trained on this dataset.\n\n2. The paper provides a multi-label classification task to predict notes in musical recordings, which is an exciting area of research. However, the paper does not explain why the chosen task is essential for the field. The authors may want to discuss its significance explicitly, its challenges, and its future applications. What are the benefits that this task can provide to the society if it is accurately predicted?\n\nMinor comments:\n\n1. The authors could include a paragraph explaining the novel contributions of the study, which will help readers in understanding the potential impact of this work on the field of music research.\n\n2. When discussing the experimental results, the authors could use more descriptive statistics and comparisons with other existing approaches to emphasize the superiority of their proposed method.\n\n3. To enhance the understanding of the dataset, it will be more helpful if the authors could explain more in detail about the annotation process and how the annotations were verified. \n\n4. The paper could benefit from more concrete examples and practical illustrations, which could make the study more accessible to a wider audience.\n\n5. The paper would benefit from having a section that discusses potential future research directions to further improve upon the present work.","model":"chatGPT","source":"peerread","label":1,"id":1930}
{"text":"This paper addresses the problem of using machine learning methods to learn features of music from scratch; that is, without prior knowledge or pre-processing of the raw music data. The paper introduces the MusicNet dataset, which consists of hundreds of freely-licensed classical music recordings with instrument\/note annotations. The goal is to predict notes in musical recordings using multi-label classification and evaluate the performance of different machine learning architectures.\n\nThe strengths of this paper include the introduction of a large-scale music dataset for machine learning research, which is a valuable contribution to the field. The paper defines a well-formulated task and evaluation protocol for the note prediction task, which provides a standardized benchmark for future studies. The experiments conducted in the paper demonstrate the effectiveness of end-to-end learning with a neural net and convolutional neural net architecture for note prediction, and the insights gained from these models show that they learn frequency selective filters as a low-level representation of audio.\n\nHowever, there are also some weaknesses in this paper. Firstly, the paper could have included a discussion of the limitations of the MusicNet dataset, such as the dataset only consisting of classical music recordings and not including other genres. Secondly, the paper does not provide a thorough comparison of the results obtained with other state-of-the-art methods on the same task.\n\nOverall, this paper is well-written and makes a valuable contribution to the field of music research with machine learning. It provides a standardized benchmark and useful insights for how machine learning architectures can learn features of music from scratch. The weaknesses identified are minor and do not detract from the overall impact of the paper.","model":"chatGPT","source":"peerread","label":1,"id":1931}
{"text":"General comments:\nThe authors provide an interesting and important contribution to the field of deep learning by proposing a straightforward baseline for detecting misclassified and out-of-distribution examples in neural networks. The presentation of the paper is clear, concise, and well-organized, with a comprehensive experimental validation.\n\nStrengths:\nThe paper presents a well-designed baseline method to detect misclassified and out-of-distribution examples in neural networks. The authors use two different datasets, CIFAR-10 and CIFAR-100, and a variety of deep learning models to illustrate their approach, comparing it with other state-of-the-art methods in the literature. The proposed method follows a two-step process that efficiently classifies examples that were correctly classified by the neural network and clearly labels those that are misclassified. The proposed method is shown to be quite robust and effective in detecting and correcting misclassified examples that were not contained in the training dataset.\n\nWeaknesses:\nThe paper does not explicitly establish the shortcomings and limitations of the proposed method, notably how the approach would behave in more complex datasets or a more challenging setting. Furthermore, while the paper contains comprehensive experiments, it would be beneficial to test the proposed approach on even more challenging datasets to establish its generalizability to a broader range of machine learning tasks.\n\nRecommended changes:\nIt would be worthwhile for the authors to discuss further the insights that can be generated from the proposed approach besides detecting misclassifications and out-of-distribution examples. For example, the authors could highlight how the approach can provide important insights into the strengths and vulnerabilities of neural network models in the training data. Additionally, the authors may enhance their discussion of some of the weaker performing methods in the literature to identify areas of ongoing research that may further improve the proposed approach.","model":"chatGPT","source":"peerread","label":1,"id":1932}
{"text":"Paper Title: A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks\n\nProblem\/Question Addressed:\nThe paper aims to address the problem of misclassification and identifying out-of-distribution examples in Neural Networks. As Neural Networks are being used widely in various areas such as self-driving cars, medical diagnosis, and fraud detections, it is crucial to detect the misclassified and out-of-distribution examples. \n\nStrengths:\nThe paper presented an excellent baseline for addressing the issue of detecting misclassified and out-of-distribution examples in Neural Networks. The authors have proposed two baselines, the Trained Confidence Scores and Outlier Exposure (OE). The experiments and results demonstrated that these baselines are effective and efficient for detecting the misclassified and out-of-distribution examples in the Neural Networks. The paper also provides a detailed evaluation of the proposed methods using different datasets, architectures, and metrics.\n\nWeaknesses:\nAlthough the proposed baseline methods are effective, the paper does not provide much insight into the neural network models and architectures used in the experiments. Also, the paper did not address the potential limitations or drawbacks of the proposed methods. Additionally, the paper could have provided a more detailed explanation of how to apply the proposed methods for detecting the misclassified and out-of-distribution examples in real-world scenarios.\n\nConclusion:\nOverall, the paper offers a valuable contribution to the field of detecting misclassified and out-of-distribution examples in Neural Networks. The proposed baselines provide a good starting point for further research in this area. The evaluation of the proposed methods is extensive, and the results are promising. However, there is still room for improvement in terms of the limitations, drawbacks, and applicability of the proposed methods. Therefore, I recommend this paper for publication with minor revisions addressing the mentioned weaknesses.","model":"chatGPT","source":"peerread","label":1,"id":1933}
{"text":"Target paper: \"A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks\"\n\nIn the target paper, the authors present a simple baseline for detecting misclassified and out-of-distribution examples in neural networks. The authors demonstrate that correctly classified examples generally have higher maximum softmax probabilities than incorrectly classified examples or out-of-distribution examples. The authors present several tasks in computer vision, natural language processing, and automatic speech recognition and show the effectiveness of the proposed baseline across all the tasks. Furthermore, the authors demonstrate that the proposed baseline can sometimes be surpassed, indicating the need for further research on these detection tasks.\n\nOverall, the target paper is well-written and clearly presented. The proposed baseline is simple yet effective and can potentially serve as a starting point for researchers exploring misclassification and out-of-distribution detection in neural networks. The authors provide a detailed methodology for their experiments and provide sufficient evidence of the effectiveness of the proposed baseline on several tasks. Moreover, the authors provide helpful insights regarding the limitations of the proposed baseline and identify areas where further research is needed.\n\nOne potential drawback of the proposed baseline is that it requires the use of softmax probabilities and may not be applicable in scenarios where other types of probabilities are used. The authors acknowledge this limitation but do not provide suggestions or insights about how to extend the proposed baseline to handle other types of probabilities.\n\nOverall, the target paper provides a valuable contribution to the field of machine learning and neural networks. I recommend this paper for publication.","model":"chatGPT","source":"peerread","label":1,"id":1934}
{"text":"The paper \"A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks\" addresses the important problem of detecting if an example is misclassified or out-of-distribution, which are critical for ensuring the reliability and robustness of neural networks. The paper presents a simple yet effective baseline method that utilizes the probabilities from softmax distributions to differentiate between correctly classified, misclassified and out-of-distribution examples. The proposed approach is evaluated on several computer vision, natural language processing, and automatic speech recognition tasks, demonstrating its effectiveness in detecting misclassified and out-of-distribution examples.\n\nThe strengths of this paper lie in its simplicity and effectiveness. The proposed baseline approach is easy to implement, does not require any additional data, and achieves competitive performance across different tasks. The paper's contribution of exploring a new angle in detecting misclassified and out-of-distribution examples is also appreciated, which can help researchers and practitioners better understand the limits and potentials of their models.\n\nThe weaknesses of this paper are relatively minor. The paper mainly focuses on the effectiveness of the proposed method and does not delve into the details of why it works, which may limit its theoretical understanding and inspire future research. Additionally, the paper only evaluates the proposed approach under a limited set of tasks and datasets. Further studies are needed to generalize and validate the baseline on more challenging scenarios.\n\nOverall, \"A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks\" is a well-written paper that offers a simple yet effective approach to a critical problem. The proposed method can be of practical importance to ensure the reliability and robustness of neural networks, and the potential for future research on this topic is promising. I recommend this paper for publication on the condition that the authors address the minor weaknesses mentioned above.","model":"chatGPT","source":"peerread","label":1,"id":1935}
{"text":"Title: Learning to superoptimize programs\n\nAbstract:\n\nThe paper presents a novel technique for program optimization by learning to predict the best sequence of optimization passes for a given program. This is done using a neural network-based approach which takes as input the program code and produces a list of optimization passes to perform on it.\n\nOverall, the paper presents an interesting idea for program optimization that has the potential to significantly improve the performance of programs. The approach of using a neural network to learn the sequence of optimization passes is an innovative way to solve the problem of optimizing programs. The experiments conducted show that the technique can produce significant improvements in program performance compared to traditional optimization techniques. \n\nOne potential limitation of the paper is that the experiments were only done on a small set of programs, which may not be representative of real-world programs. It would be interesting to see how the technique performs on a larger set of programs and whether it can be applied to more complex programs.\n\nAnother concern is that the paper does not provide a detailed analysis of the neural network architecture used for learning the optimization passes. While the paper describes the architecture briefly, it would be helpful to have more information on the specific design choices made and how they contribute to the effectiveness of the approach.\n\nOverall, I believe that the paper presents a promising approach for program optimization that has the potential to significantly improve the efficiency of programs. The use of neural networks for predicting optimization passes is an innovative solution to a difficult problem, and the results of the experiments are encouraging. However, more work needs to be done to test the technique on a broader set of programs and to provide a more detailed analysis of the neural network architecture.","model":"chatGPT","source":"peerread","label":1,"id":1936}
{"text":"This paper \"Learning to superoptimize programs\" addresses the problem of program optimization, which is a crucial aspect of software development. The authors propose a novel method for automatic program optimization, utilizing a machine learning approach to learn a superoptimizer, which is a program that optimizes code sequences to execute faster, using the smallest number of instructions possible. The paper aims to demonstrate the effectiveness of their proposed method by comparing it with state-of-the-art techniques.\n\nOne primary strength of this paper is the novelty of the proposed technique. The idea of using machine learning to learn a superoptimizer is a unique approach that has not been extensively explored in prior research. The authors provide a detailed description of their method and an empirical evaluation that shows its superiority over other optimization techniques. Furthermore, the research was conducted using a diverse set of benchmarks, which improves the generalizability of the results.\n\nOn the other hand, one potential weakness of this paper is the limited scope of the empirical evaluation. Although the authors compared their method with various optimization techniques, the evaluation was conducted on a relatively small set of benchmarks. Therefore, it is challenging to assess the generalizability of the results to more complex and realistic programs. Additionally, the authors did not provide a thorough analysis of the limitations or potential drawbacks of their proposed method.\n\nIn summary, this paper presents an innovative approach to program optimization through machine learning. The proposed technique has shown superior results compared to existing optimization methods. However, the empirical evaluation could be improved by conducting experiments on a broader range of benchmarks and providing a more in-depth analysis of the limitations of the proposed method. Overall, this paper contributes to the field of program optimization by introducing a unique and promising approach that warrants further investigation.","model":"chatGPT","source":"peerread","label":1,"id":1937}
{"text":"The paper proposes a novel learning-based approach for code super-optimization by utilizing unbiased estimators of the gradient of the expected improvement. The authors compare their proposed method with existing rule-based transformation strategies and stochastic search-based methods on benchmarks comprising of automatically generated and existing programs. The results demonstrate that the proposed method significantly outperforms the state-of-the-art approaches for code super-optimization.\n\nOverall, the paper is well-written, and the proposed method is technically sound. The authors provide a good motivation for their work and clearly present the problem of code super-optimization. Moreover, the authors discuss their proposed method in detail and compare it with existing approaches. The experiments are well-designed and the results are reported effectively. \n\nHowever, there are a few areas where the paper could be improved. First, in the related work section, the authors can further discuss other learning-based approaches for code super-optimization and how their method compares to those methods. Second, the authors can discuss the limitations of their proposed method and potential areas for future work. Finally, the authors can provide more details on the specific benchmarks they used and how they were generated.\n\nOverall, this paper presents an interesting and promising approach to code super-optimization. It addresses a challenging problem in computer science and demonstrates superior performance compared to existing approaches.","model":"chatGPT","source":"peerread","label":1,"id":1938}
{"text":"The paper \"Learning to superoptimize programs\" focuses on the task of code super-optimization. Specifically, it proposes a novel learning-based approach for transforming programs to a more efficient version while preserving its input-output behavior. The paper highlights the limitations of current stochastic search-based methods for code optimization and posits that a learning-based method that can leverage the semantics of the program under consideration could be more effective. The proposed method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement.\n\nThe strengths of this paper are considerable. Firstly, the problem addressed is of significant importance in computer science, and the proposed approach provides an innovative solution that outperforms the existing state-of-the-art methods. Moreover, the paper is well-structured and easy to follow. It provides a clear motivation for the proposed method and incorporates a comprehensive evaluation of the algorithm on a range of benchmarks.\n\nHowever, there are some weaknesses to consider. One limitation of the paper is that the experiments are conducted only on generated datasets and existing programs (\"Hacker's Delight\"). Future work could focus on testing the proposed method on more varied and complex programs. Additionally, the paper could benefit from a more detailed discussion of the limitations and challenges of the proposed method.\n\nOverall, the paper \"Learning to superoptimize programs\" presents an innovative approach to the code super-optimization problem. The proposed method effectively utilizes learning-based approaches to surpass the limitations of current stochastic search-based methods. The paper is well-written, provides clear motivations, and a comprehensive evaluation of the proposed method.","model":"chatGPT","source":"peerread","label":1,"id":1939}
{"text":"Title: Sigma Delta Quantized Networks\n\nAuthors: John Smith, Jane Doe\n\nJournal: arXiv:2101.12345v1 [cs.LG]\n\nOverview:\n\nThe authors propose a novel methodology for training neural networks with limited bit-width quantization, referred to as Sigma Delta Quantized Networks (SDQN). The proposed method relies on the use of sigma-delta modulation, a type of noise-shaping technique, to reduce the quantization error caused by low-bit representation in the network layer. The authors demonstrate the effectiveness of the proposed method through a series of experiments on several benchmark datasets, achieving comparable accuracy to traditional full-precision networks with significantly reduced model size, memory footprint and computational complexity.\n\nStrengths:\n\nThe paper presents a unique and innovative approach to address the challenge of high computational cost in deep learning, driven by the increasing demand for real-time, low-power computation. The paper is well-written and demonstrates thorough experimental results that provide compelling evidence demonstrating the effectiveness of the proposed method.\n\nWeaknesses:\n\nThere are a few areas that could be improved in the paper. First, although the authors provide an intuitive explanation of sigma-delta modulation, a more formal treatment would be valuable to gain a deeper understanding of the approach. Second, while the authors compare the performance of SDQN method to other state-of-the-art compression methods, it would be helpful to see more detailed comparison with other quantization schemes for neural networks. Finally, the authors do not provide any insights on the interpretability of the learned weights in the SDQN method, which is an important factor for applications where model interpretability is critical.\n\nOverall, this paper presents an innovative methodology for training neural networks with low bit representation, and the experimental results demonstrate its effectiveness. With some minor revisions, the paper has the potential to make a significant contribution to the field of deep learning for edge devices.","model":"chatGPT","source":"peerread","label":1,"id":1940}
{"text":"The paper \"Sigma Delta Quantized Networks\" by authors S. Srinivasan, A. Bommannan, H. Yesil-Celiktutan and S. Garg focuses on reducing the storage requirements of full-precision parameters in deep neural networks by using the sigma-delta quantization technique. The authors aim to evaluate the effectiveness of this technique in improving the accuracy of the network while maintaining the compression ratio.\n\nStrengths:\nThe paper addresses an important problem in the field of deep learning - the need for efficient storage of model parameters. The sigma-delta quantization technique proposed by the authors is innovative and has the potential to significantly improve the storage efficiency of neural networks, while maintaining their accuracy. The authors provide a thorough analysis of the technique, including experimental results that demonstrate the effectiveness of this approach on various datasets. The metrics used to evaluate the performance of the proposed method are well-defined and appropriate. \n\nWeaknesses:\nWhile the authors provide evidence for the benefits of the sigma-delta quantization technique, the paper does not compare it against other parameter compression methods, which would be valuable to provide context and compare the performance of the proposed method against other state-of-the-art methods. In addition, it is unclear how the proposed method scales for larger or more complex models, as the authors only evaluate the SIGMA-Delta technique on smaller, simple networks. The paper lacks a formal discussion of the limitations of the proposed method, and future research directions to address these limitations are not clearly defined.\n\nOverall, the paper presents an interesting and promising approach to reduce the storage requirements of deep neural networks through the sigma-delta quantization technique. However, more studies and analysis would be necessary to validate its effectiveness compared to other state-of-the-art compression methods across a wide range of models and datasets.","model":"chatGPT","source":"peerread","label":1,"id":1941}
{"text":"The paper \"Sigma Delta Quantized Networks\" proposes a novel approach to address the issue of large computation requirements of deep neural networks, specifically in processing video data. The authors introduce Sigma-Delta networks, which change the computation flow compared to conventional deep neural networks, reducing the amount of repetitive computations. The paper presents an optimization algorithm for converting pre-trained deep networks into Sigma-Delta networks, and experiments show that the approach significantly reduces computational costs for processing video data. \n\nThe paper is well-written and clearly presents the proposed method and its benefits. The motivation for the work is sound, and the authors provide a convincing argument that the proposed approach can significantly reduce the computational requirements and cost of processing video data. However, I would suggest more discussion and analysis of the trade-offs of the proposed method, e.g., the possible impact on accuracy or robustness of the model, compared to traditional deep learning approaches. \n\nThe evaluation of the proposed method includes experiments on two datasets for video processing. Still, I would suggest more thorough experimental evaluation, including comparative analysis with several state-of-the-art approaches and varying hardware configurations, to further support the proposed method's effectiveness and generalizability. \n\nOverall, the paper presents a promising method for reducing computational costs in video processing, and I recommend it for publication in this field.","model":"chatGPT","source":"peerread","label":1,"id":1942}
{"text":"The paper \"Sigma Delta Quantized Networks\" addresses the problem of wasteful processing of deep neural networks for video data. In particular, the authors propose a new type of network, Sigma-Delta networks, which would scale the computation of each layer with the amount of change in the input and layer activations, rather than the size of the network. The authors also introduce an optimization method for converting pre-trained deep networks into Sigma-Delta networks, which they demonstrate could significantly reduce the computational cost of processing video data.\n\nThe strengths of this paper lie in the authors' innovative proposal for scaling the computation of deep neural networks, as well as the demonstrated potential for significant computational cost reduction. The optimization method presented for converting pre-trained deep networks into Sigma-Delta networks is also a significant contribution and could have practical applications in reducing the computational burden for video processing tasks. The results are promising and suggest that Sigma-Delta networks are a viable alternative to traditional convolutional networks for video processing tasks.\n\nHowever, there are also some weaknesses to the paper. One issue is that the authors do not provide a thorough comparison to other existing methods for reducing the computational cost of deep neural networks. It would be helpful to have a more detailed discussion of the advantages and disadvantages of Sigma-Delta networks compared to other methods, such as pruning, quantization, or low-rank approximation. Additionally, the proposed hardware requirements for running the algorithm at the scale required for video processing may limit the practical applicability of Sigma-Delta networks in certain contexts. The authors could benefit from discussing potential hardware limitations and future directions for making Sigma-Delta networks more widely available and practical.\n\nOverall, the paper provides an innovative proposal for optimizing deep neural networks for video processing tasks. While there are some limitations and challenges to the proposed method, Sigma-Delta networks have the potential for significant computational cost reduction, making the paper a valuable contribution and worth further exploration.","model":"chatGPT","source":"peerread","label":1,"id":1943}
{"text":"Overall examination:\n\nThe paper, \"Regularizing CNNs with Locally Constrained Decorrelations,\" focuses on a regularization technique that reduces internal covariate shift in convolutional neural networks (CNNs) and improves the generalization ability of CNNs. The approach is to incorporate a locality constraint into the decorrelation process of the learned convolutional filters. The authors evaluate this technique on several standard image classification datasets, and the results show that their approach improves the performance of CNNs and reduces overfitting. The paper is well written and structured, and the research is well motivated, clearly explained, and well-executed. However, there are several areas where the paper could be improved.\n\nStrengths:\n\nThe authors provide a thorough literature review and clearly explain the motivation for their approach. The paper is well structured and easy to follow. The experimental setup is well-defined and the results are presented clearly. The authors conduct experiments on several standard image classification datasets and their results demonstrate improvements in classification accuracy and reduced overfitting compared to baseline models. The proposed method is computationally efficient, and the authors provide a detailed discussion of the computational complexity of their approach.\n\nWeaknesses:\n\nThe authors do not provide details on the hyperparameter tuning process of their method. The choice of hyperparameters is important for the effectiveness of the proposed method, so it is important to provide more information on this process. Additionally, the authors do not discuss the limitations of their approach or areas for future work. It would be beneficial to include a discussion of potential extensions of the proposed method or ways in which the method could be further improved.\n\nConclusion:\n\nIn summary, the paper presents a well-motivated and well-executed regularization technique for convolutional neural networks. The experimental results demonstrate the effectiveness of the approach for improving classification accuracy and reducing overfitting. However, the paper could be improved by providing more information on the hyperparameter tuning process and including a discussion of the limitations of the proposed method and areas for future work.","model":"chatGPT","source":"peerread","label":1,"id":1944}
{"text":"The paper \"Regularizing CNNs with Locally Constrained Decorrelations\" proposes a method for regularizing Convolutional Neural Networks (CNNs) by decorrelating nearby feature maps. The authors argue that this is a more effective regularization technique than others commonly used for CNNs, such as DropOut or Weight Decay.\n\nStrengths:\n\nOne of the strengths of this paper is that it presents a novel regularization technique that is conceptually sound and well-motivated. The authors provide a clear description of the proposed method and its theoretical underpinnings, and they provide convincing experimental evidence that it works better than existing regularization techniques on several benchmark datasets. They also compare their approach to other decorrelation-based regularization techniques (such as ZCA whitening) and show that their method outperforms them as well.\n\nWeaknesses:\n\nOne potential weakness of this paper is that the experiments are somewhat limited in scope. Although the authors demonstrate that their regularization method works well on several benchmark datasets, they do not provide a detailed analysis of how the method performs under different conditions (e.g., different architectures, datasets, etc.). Additionally, the authors do not provide any insights into why their method is more effective than other regularization techniques, beyond the fact that it is motivated by decorrelation.\n\nAnother weakness of this paper is that the evaluation metrics used in the experiments are somewhat limited. Although the authors report the standard classification accuracy, they do not provide any analysis of other performance metrics that are commonly used in the literature (e.g., F1 score, recall, precision, etc.). This makes it difficult to assess the generalizability of their method and its usefulness in practical applications.\n\nConclusion:\n\nIn summary, \"Regularizing CNNs with Locally Constrained Decorrelations\" is a well-written paper that presents a novel regularization technique for CNNs that is based on decorrelation. The paper provides a clear description of the proposed method and its theoretical underpinnings, and it provides convincing experimental evidence that it outperforms existing regularization techniques on several benchmark datasets. However, the experiments are somewhat limited in scope, and the evaluation metrics used are somewhat limited as well. Therefore, the paper would benefit from additional experiments and analysis to demonstrate the generalizability and usefulness of the proposed method in practical applications.","model":"chatGPT","source":"peerread","label":1,"id":1945}
{"text":"The paper \u201cRegularizing CNNs with Locally Constrained Decorrelations\u201d presents a novel regularization technique, OrthoReg, which enforces feature orthogonality to better leverage all the capacity of deep neural networks while reducing overfitting. \n\nThe authors explain the motivation for developing OrthoReg, which stems from the observation that while many regularizations rely on reducing the effective number of parameters, feature decorrelation is an underexplored alternative that may leverage the full capacity of a model. However, the authors argue that previous attempts at decorrelation introduce too much overhead and produce narrow margins of overfitting reduction, which limits their effectiveness. \n\nTo address this limitation, the authors propose OrthoReg, which enforces feature orthogonality locally, thereby removing interferences between negatively correlated feature weights and allowing for higher decorrelation bounds. The authors demonstrate the effectiveness of their approach through experiments on CIFAR-10, CIFAR-100, and SVHN datasets. \n\nOverall, this paper is well-written and presents an interesting and potentially impactful contribution to the field of deep learning. The methodology is clearly presented, and the results are convincing. While the paper could benefit from more detailed discussion of related work and possible limitations, the significance and relevance of the proposed approach are well-supported. \n\nAccordingly, I recommend acceptance of this paper pending minor revisions.","model":"chatGPT","source":"peerread","label":1,"id":1946}
{"text":"This paper \"Regularizing CNNs with Locally Constrained Decorrelations\" addresses a crucial problem in deep learning, which is the efficient and effective reduction of overfitting. Regularization techniques serve as a key approach to solving this problem, and this paper presents a novel regularization approach called OrthoReg that locally enforces feature orthogonality to remove interferences between negatively correlated feature weights. The paper demonstrates that OrthoReg leads to higher decorrelation bounds and more effective overfitting reduction.\n\nOne strength of this paper is the proposed OrthoReg regularization technique, which is a novel approach that leverages the full capacity of the models and reduces overfitting more effectively. The paper provides a clear and detailed explanation of how OrthoReg works and incorporates its intuition into the experimental design. Another strength is that the authors showcase experiments on different datasets, which shows that OrthoReg performs well on images of varying complexity.\n\nHowever, one weakness of this paper is that the experimental evaluation could have been more comprehensive. The paper does not present a detailed comparison with other state-of-the-art regularization techniques such as weight decay, early stopping, and data augmentation. Furthermore, the experiments were not performed on more challenging datasets like ImageNet, which may affect the generalizability of the proposed approach.\n\nOverall, this paper offers an innovative regularization approach that improves the performance of state-of-the-art deep learning models. Still, more work needs to be done in evaluating OrthoReg against other regularization techniques and demonstrating its robustness to more challenging datasets.","model":"chatGPT","source":"peerread","label":1,"id":1947}
{"text":"Title: Maximum Entropy Flow Networks\n\nAuthors: John Doe, Jane Smith\n\nGeneral Feedback:\nThe authors present an interesting approach for modelling flow networks using maximum entropy principles. The paper is well-written and the methodology is clear and well-explained. However, there are certain issues that need to be addressed before this work can be considered for publication.\n\nStrengths:\n- The authors present a novel approach for modelling flow networks.\n- The maximum entropy principles used in this work are well explained and relevant to the problem being addressed.\n- The experimental results indicate that the proposed approach can achieve comparable performance to state-of-the-art methods.\n\nWeaknesses:\n- The novelty of the approach is based on the application of maximum entropy principles to flow networks. However, this may not be seen as a significant contribution in the field of NLP.\n- The authors do not provide sufficient justification for the choice of the maximum entropy approach over other modeling techniques.\n- The experimental evaluation lacks comparisons with a wider range of state-of-the-art methods, which would be required to further validate the effectiveness of the proposed approach.\n- The paper lacks a thorough analysis of the computational complexity of the proposed method, which could be an important factor to consider in practical applications.\n\nSuggestions for Improvement:\n- The authors could consider providing a more detailed comparison of the proposed method with other relevant techniques, such as graph convolutional networks or maximum flow-based methods.\n- It would be beneficial for the authors to provide a more detailed discussion of the computational complexity of the proposed method, especially in comparison to other techniques.\n- Clarification of the potential applications of the proposed approach would be a valuable addition to the conclusions section.\n\nOverall, the authors have presented an interesting approach to modelling flow networks but some of the weaknesses highlighted need to be addressed to strengthen the paper for publication.","model":"chatGPT","source":"peerread","label":1,"id":1948}
{"text":"The paper \"Maximum Entropy Flow Networks\" by Kevin Murphy addresses the problem of constructing a flow network from a given set of edge capacities and demands, which can be used in various applications such as transportation planning, network flow simulation, and power grid modelling. The paper introduces a novel approach to this problem using the principle of maximum entropy.\n\nThe paper first provides a clear definition of the problem and the related concepts of flow networks and maximum entropy, which is helpful for readers who are not familiar with the topic. The paper then presents the mathematical formulation of the maximum entropy flow network (MEFN) model and describes the algorithm for solving it using convex optimization techniques. The paper also provides a detailed analysis of the performance and efficiency of the algorithm in various experiments and compares it with other popular algorithms for flow network construction.\n\nOne of the major strengths of this paper is its clear and concise presentation of the problem and the solution, which makes it accessible to a wide range of readers. The paper also provides a rigorous mathematical analysis of the MEFN model, demonstrating the optimality of the solution and the convergence of the algorithm. The paper also provides extensive experimental results showing the superiority of the MEFN algorithm over other algorithms in terms of accuracy and speed.\n\nHowever, there are several weaknesses of this paper that should be addressed. First, the paper lacks a detailed discussion of the limitations and assumptions of the MEFN model and the implications for its practical use. Second, the paper does not provide a comparison with more recent and state-of-the-art algorithms in the field, which limits the relevance and impact of the research. Third, the paper could benefit from a more detailed explanation of the implications of the theoretical results and their relationship to practical applications.\n\nOverall, this paper presents a promising approach to the problem of flow network construction using maximum entropy, but further research is needed to address its limitations and to compare it with other algorithms in the field. The paper could benefit from more discussion of the practical implications of the results and their relevance for real-world applications.","model":"chatGPT","source":"peerread","label":1,"id":1949}
{"text":"The manuscript presents a novel approach to maximum entropy modeling using normalizing flow networks. The authors propose to learn an invertible transformation that maps a simple distribution to the desired maximum entropy distribution instead of the traditional method of optimizing the continuous density directly. The proposed method is shown to be effective in simulations and flexible in application to finance and computer vision.\n\nOverall, the manuscript is well-structured and clearly written. The authors provide a comprehensive introduction to the topic and articulate their research questions and objectives. The methodology and approach are described in detail and are justified by the relevant literature. The authors provide the necessary theoretical background and explain the technical details of their approach in a clear and concise way. The results are presented and discussed thoroughly, and the authors provide insight into the effectiveness and limitations of their approach.\n\nOne strength of the manuscript is the novelty and significance of the proposed approach. The use of normalizing flow networks to solve the maximum entropy problem is innovative, and the manuscript presents a contribution to the field. The experiments and applications included demonstrate the flexibility and accuracy of the approach, and the authors provide relevant comparisons to other methods.\n\nHowever, there are some areas that could be improved. The authors could have provided more detail on the simulation setup and parameter choices. Additionally, the manuscript could benefit from additional examples or case studies that help to illustrate the efficacy and flexibility of the method in practice. Finally, the authors could have provided more information on the potential limitations or drawbacks of their approach.\n\nIn conclusion, the manuscript presents a promising approach to maximum entropy modeling using normalizing flow networks. The proposed approach is well-justified and technically sound, and the results demonstrate its effectiveness and flexibility. The manuscript could benefit from some additional details and examples, but overall it is a valuable contribution to the field.","model":"chatGPT","source":"peerread","label":1,"id":1950}
{"text":"The paper \"Maximum Entropy Flow Networks\" addresses the problem of formulating statistical models using maximum entropy modeling, given partial knowledge. The authors propose a novel approach that learns a smooth and invertible transformation, which maps a simple distribution to the desired maximum entropy distribution, rather than optimizing over the continuous density directly. They exploit recent developments in normalizing flow networks and cast the problem into a finite-dimensional constrained optimization. The authors solve the problem by combining stochastic optimization with the augmented Lagrangian method. The authors also present simulation results that demonstrate the effectiveness of their method and show the flexibility and accuracy of maximum entropy flow networks in the context of finance and computer vision.\n\nThe paper has several strengths. The authors present a novel approach to formulate statistical models using maximum entropy modeling. They adapt the normalizing flow networks to cast the maximum entropy problem into a finite-dimensional constrained optimization, which makes the problem easier to solve. They also use the augmented Lagrangian method to solve the problem, which is a powerful optimization technique. The simulation results presented by the authors demonstrate the effectiveness of their method and its flexibility and accuracy in different contexts.\n\nThere are also some limitations to the paper. The paper may lack sufficient detail for readers who are not familiar with the techniques they use. The paper does not provide a clear explanation of why the augmented Lagrangian method is a suitable choice for solving the optimization problem. And the paper could benefit from additional examples or analyses to support the generalizability of their approach beyond finance and computer vision.\n\nOverall, the presented approach of using maximum entropy flow networks is innovative and demonstrates potential for formulating statistical models in various contexts. The authors use appropriate methods and present convincing experimental results. However, additional explanations and examples may help readers fully understand and generalize the approach.","model":"chatGPT","source":"peerread","label":1,"id":1951}
{"text":"Title: Unrolled Generative Adversarial Networks\n\nAuthors: Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein\n\nSummary:\n\nThe paper presents a novel approach to training generative adversarial networks (GANs) by unrolling the optimization procedure for a fixed number of steps. The proposed method, named as Unrolled GANs (UGANs), involves using an iterative optimization process where the generator and discriminator networks are modified based on the back-propagation of fixed number of steps. The authors claim that this approach can solve the issue of mode collapse and improve training stability, leading to better image generation results.\n\nStrengths:\n\nThe paper addresses a significant problem in training GANs which is mode collapse, and demonstrates that UGANs approach can mitigate it. The experimental results presented are convincing and demonstrate that the proposed technique can produce qualitative and quantitative improvements over the vanilla GAN approach. The paper is well-written, and the proposed approach is clearly explained.\n\nWeaknesses:\n\nOne possible limitation of the paper is that the proposed method requires a fixed number of steps for unrolling the optimization procedure. Although the authors suggest that this can be set to a small value, future research could investigate whether the results are sensitive to the choice of this hyperparameter.\n\nAnother potential limitation is that the experimental evaluation is limited to image generation tasks. Future work could consider applying UGANs to other applications such as text generation or reinforcement learning.\n\nConclusion:\n\nOverall, this is an interesting and significant contribution to the field of generative adversarial networks. The proposed approach, UGANs, tackles a major limitation of GANs in a convincing way by unrolling the optimization procedure. The paper is well-written, and the experimental results are compelling. The authors have done a good job in presenting a clear and effective approach to training GANs. I would recommend this paper for acceptance to the conference\/journal.","model":"chatGPT","source":"peerread","label":1,"id":1952}
{"text":"Paper: Unrolled Generative Adversarial Networks\n\nThe problem and question addressed in this paper are how to improve the training stability and convergence of Generative Adversarial Networks (GANs). The authors propose the Unrolled Generative Adversarial Network (UGAN) model, which unrolls the optimization process of the generator neural network by a fixed number of steps. The main idea is to use multiple steps of optimization of the generator network, where the discriminator network is fixed, to update the generator network. This unrolling process leads to a more stable and efficient optimization compared to the traditional GAN algorithm, and the authors demonstrate its efficacy in several image generation tasks.\n\nThe strengths of this paper are that the proposed model addresses an important problem in the GAN literature, which is the training instability of GANs. The authors presented a clear and convincing argument for the effectiveness of the unrolling process through an extensive experimental evaluation. The experiments show that UGANs can generate high-quality images that are both diverse and realistic, and they outperform the traditional GAN algorithm in several benchmark datasets.\n\nHowever, there are also some weaknesses in this paper. Firstly, the authors did not show the generalization ability of the proposed model in generating images of unseen categories or domains. Secondly, the evaluation criteria of the generated images are mainly based on visual inspection and subjective assessment, which may not be enough to judge the true effectiveness of the proposed model. More objective evaluation metrics such as Inception Score or Frechet Inception Distance could have been employed to provide a more comprehensive evaluation of the model.\n\nOverall, this paper provides a valuable contribution to the GAN literature by proposing a simple yet effective modification to the traditional GAN algorithm. Despite some limitations, the proposed model's experimental results and benefits show a promise in improving the training stability and convergence of GANs.","model":"chatGPT","source":"peerread","label":1,"id":1953}
{"text":"The paper titled \"Unrolled Generative Adversarial Networks\" introduces a method for stabilizing and improving the performance of Generative Adversarial Networks (GANs). The authors address a common problem in GANs, namely mode collapse, by defining the generator objective as an unrolled optimization of the discriminator.\n\nThe paper presents convincing empirical evidence to support the effectiveness of the proposed method. The authors demonstrate that their technique stabilizes training for GANs with recurrent generators and improves the diversity and coverage of the data distribution.\n\nWhile the paper addresses an important problem in the field of GANs, some aspects of the methodology and analysis could be further elaborated. For example, the authors may consider providing more insight into the gradient computation and optimization process used for the discriminator unrolling. Additionally, the paper could benefit from a more thorough discussion of the limitations and potential disadvantages of the proposed method.\n\nOverall, the contribution of the paper is significant, and the proposed method is promising for improving stability and performance of GANs. The manuscript is well-structured, and the experimental results and analysis presented are convincing. Hence, I recommend this paper for publication, with minor revisions recommended as discussed above.","model":"chatGPT","source":"peerread","label":1,"id":1954}
{"text":"The paper titled \"Unrolled Generative Adversarial Networks\" presents a novel method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. The problem this paper addresses is the instability of GAN training caused by using the current value of the discriminator in the generator's objective instead of the optimal discriminator. The paper shows how this technique solves the common problem of mode collapse and stabilizes the training of GANs with complex recurrent generators. \n\nOne of the strengths of this paper is the clarity of explanation and the use of diagrams to illustrate the process. The paper provides empirical evidence that the proposed technique improves the performance of GANs in image generation tasks. The experiments conducted by the authors show that the method can generate diverse and high-quality images and better cover the data distribution compared to other GAN models.\n\nHowever, the paper has a weakness in terms of the lack of comparison to other recent methods aimed at solving the problem of GAN instability, which limits the generalizability of the method. Additionally, the paper's focus on image generation makes it unclear how well the proposed method would perform on other types of data such as sequences or graphs. \n\nOverall, the paper presents a promising technique for stabilizing GAN training and improving image generation. The authors should consider addressing the weaknesses highlighted above to broaden the applicability and significance of the proposed method.","model":"chatGPT","source":"peerread","label":1,"id":1955}
{"text":"Title: Frustratingly Short Attention Spans in Neural Language Modeling\n\nAuthors: John Doe, Jane Smith, and Adam Johnson\n\nJournal: Neural Networks\n\nIn this paper, John Doe, Jane Smith, and Adam Johnson investigate the impact of attention spans in neural language modeling. They argue that existing models have a frustratingly short attention span, which limits their ability to capture long-term dependencies in the input text.\n\nThe authors present a series of experiments to support their argument. They compare several state-of-the-art language models with different attention spans, ranging from 100 to 1000 tokens. They show that models with longer attention spans perform significantly better on several benchmark datasets, especially on tasks that require capturing long-range dependencies, such as machine translation and summarization.\n\nOverall, I find the paper to be well-written and thorough. The experiments are well-designed and the results are convincing. The authors provide clear explanations for their findings and discuss the implications for future work in language modeling.\n\nOne minor issue with the paper is the lack of discussion on the computational cost of using longer attention spans. It would be helpful to know how much additional computational resources are required to train and evaluate models with longer attention spans, and whether the performance gains are worth the added cost.\n\nOverall, I recommend this paper for publication in Neural Networks. The authors make a valuable contribution to the field of neural language modeling and offer insights that will aid future research in this area.","model":"chatGPT","source":"peerread","label":1,"id":1956}
{"text":"Paper Title: Frustratingly Short Attention Spans in Neural Language Modeling\n\nProblem Description:\nThe paper \"Frustratingly Short Attention Spans in Neural Language Modeling\" addresses the challenge of short attention spans in neural language modeling. Specifically, the author focuses on the ways in which modeling for natural language processing can be improved by addressing limitations in current attention-based models. Attention-based models have become increasingly popular in recent years. However, they often suffer from insufficient attention span, which hinders their utility in practical NLP applications. Addressing this issue has the potential to greatly improve NLP model performance.\n\nStrengths:\nThe paper is well-structured and presents strong arguments for the need to address the challenge of short attention spans in neural language modeling. The author does an excellent job of clearly presenting the technical aspects of the models discussed in the paper in a way that is accessible to readers who may not be NLP experts. They provide compelling arguments about the potential impact of addressing short attention spans in NLP modeling.\n\nWeaknesses:\nOne weakness of this paper is that it focuses exclusively on attention-based models and does not consider alternative approaches to overcoming the challenge of short attention spans in NLP. Additionally, the paper does not provide empirical evidence to support its claims. Further research could be conducted to evaluate the effectiveness of the proposed approaches in comparison to other methods.\n\nOverall, the paper \"Frustratingly Short Attention Spans in Neural Language Modeling\" provides a valuable contribution to the field of NLP by highlighting the challenge of short attention spans in neural language modeling and suggesting potential approaches to address this issue. While it doesn't offer empirical evidence to support its claims, the paper is still valuable as a starting point for further research into this important area.","model":"chatGPT","source":"peerread","label":1,"id":1957}
{"text":"Title: Peer Review for \"Frustratingly Short Attention Spans in Neural Language Modeling\"\n\nGeneral Comments:\nThe authors propose a key-value attention mechanism that produces separate representations for the key and value of a memory, and for a representation that encodes the next-word distribution. The proposed method outperforms existing memory-augmented neural language models on two corpora. However, the authors found that it mainly utilized past memory only of the previous five representations, which led to their unexpected finding that a much simpler model is on par with more sophisticated memory-augmented neural language models.\n\nStrengths:\n- The paper presents a novel key-value attention mechanism that shows improved performance compared to existing memory-augmented neural language models on two corpora. \n- The authors clearly state and explain their main findings and contributions. \n- The authors provide experimental results showing the effectiveness of their proposed method over existing methods. \n\nWeaknesses:\n- The paper could have provided more thorough explanations of key-value attention mechanism in simpler terms, as it can be somewhat difficult to understand for readers with less experience in the field.\n- The paper would have benefitted from more detailed and comprehensive experiments, for example, exploring variations of the key-value attention mechanism or making comparisons with other related work.\n- The authors do not provide sufficient analysis of why the proposed model only utilizes past memory of the previous five representations. \n\nSuggestions for Improvement:\n- To make the proposed method more accessible to a wider audience, the authors could have provided more intuitive explanations of the key-value attention mechanism or included figures to illustrate their approach.\n- The authors could have tested their proposed method on a wider variety of datasets and complex tasks and presented the results obtained, as this would provide more insights into its efficacy and challenges.\n- A detailed discussion of the reasons why the proposed model only utilizes the past memory of the previous five representations, including potential directions for future research, would enhance the contribution of the paper.\n\nOverall, the paper presents an interesting approach for incorporating differentiable memory into language modeling architectures by introducing the key-value attention mechanism. The experimental results show that the proposed method outperforms existing methods. However, more analyses are needed to show why the proposed method performs better and to provide a better understanding of the underlying mechanisms.","model":"chatGPT","source":"peerread","label":1,"id":1958}
{"text":"The paper \"Frustratingly Short Attention Spans in Neural Language Modeling\" addresses an important problem in the field of natural language processing: the difficulty of predicting the next token in a sentence based on the previous history, especially when considering long-range dependencies. The paper proposes a key-value attention mechanism that addresses this problem by producing separate representations for the key and value of a memory, as well as an encoding for the next-word distribution. The main finding of the paper is that this approach beats existing memory-augmented neural language models on two corpora, but only utilizes past memory from the previous five representations.\n\nThe strength of this approach lies in the use of a separate representation for the key and value of a memory, which allows for greater flexibility in accessing past history when predicting the next token. Additionally, the paper provides empirical evidence that this approach outperforms existing memory-augmented neural language models, which could be useful for future development in the field.\n\nHowever, a weakness of the paper is that it focuses too much on comparing the proposed approach to existing memory-augmented models rather than exploring other potential solutions. For example, the paper could have compared the proposed approach to models that directly incorporate long-term memory, such as those based on a neural Turing machine. Additionally, the paper does not provide clear intuition for why the simpler model of using a concatenation of output representations from the previous three-time steps performs as well as more sophisticated memory-augmented models.\n\nOverall, the paper makes a valuable contribution to the field of neural language modeling and presents an intriguing key-value attention mechanism to address the issue of short attention spans. However, further investigation may be required to determine the optimal approach for addressing long-term dependencies in language modeling.","model":"chatGPT","source":"peerread","label":1,"id":1959}
{"text":"Title: Recurrent Hidden Semi-Markov Model\n\nAuthors: John Doe and Jane Smith\n\nSummary:\n\nIn this paper, the authors propose the use of a Recurrent Hidden Semi-Markov Model (RHMM) for modeling time series data with complex and variable temporal dependencies. The RHMM is an extension of the standard Hidden Semi-Markov Model (HSMM) that is able to capture the recurrent structure of time series data. The authors demonstrate the effectiveness of the RHMM on several applications, including speech recognition and handwriting recognition.\n\nPeer review:\n\nThe paper by Doe and Smith proposes a Recurrent Hidden Semi-Markov Model for modeling complex and variable temporal dependencies in time series data. The paper is well-written and clearly presents the proposed model and its advantages over the standard HSMM. The authors also provide a thorough analysis of the effectiveness of the RHMM on several applications, demonstrating its superiority over other models.\n\nOne weakness of the paper is that the authors do not provide a detailed comparison of the RHMM with other state-of-the-art models for modeling time series data. It would be helpful if the authors could include a comparison with other models, such as LSTM and GRU, to better understand the strengths and weaknesses of the proposed approach.\n\nOverall, I find this paper to be a valuable contribution to the field of time series modeling. The proposed RHMM is a novel and effective approach for modeling complex and variable temporal dependencies in time series data, and the authors demonstrate its effectiveness on several applications. I recommend this paper for publication, with the suggested improvement of including a comparison with other state-of-the-art models.","model":"chatGPT","source":"peerread","label":1,"id":1960}
{"text":"The paper \u201cRecurrent Hidden Semi-Markov Model\u201d attempts to address the problem of modelling sequential data where the underlying structure is non-Markovian in nature. The authors propose a novel approach which combines the sequence modelling capacity of Recurrent Neural Networks (RNNs) with the interval-duration modelling capacity of Hidden Semi-Markov Models (HSMMs).\n\nStrengths of the paper include the clear articulation of the problem, and the development of a methodology that attempts to solve this problem. The authors provide a thorough description of Recurrent Hidden Semi-Markov Model (RHSMM) and its implementation. The experimental results that the authors provide demonstrate that RHSMMs can be effective in generating sequences that do not exhibit Markovian structure, with improved performance over existing RNN and HSMM models.\n\nHowever, there are also a few weaknesses of the paper. Firstly, the authors do not provide a detailed comparison of the computational efficiency of RHSMMs with respect to other models or algorithms. Secondly, a more in-depth analysis of the performance of the proposed method compared to other state-of-the-art models should be provided. Finally, some additional discussion around the limitations of RHSMMs and potential future research directions could be included.\n\nIn conclusion, the paper presents a promising approach for modelling non-Markovian sequential data. The proposed methodology, RHSMM, has demonstrated effective performance in generating such sequences. The strengths of this paper lie in the clear presentation of the problem, the development of a new methodology, and the experimental results demonstrating its effectiveness. However, the paper could benefit from additional comparison of computational efficiency and performance with other state-of-the-art models, and further discussion around the limitations and opportunities for future research.","model":"chatGPT","source":"peerread","label":1,"id":1961}
{"text":"The paper proposes a Recurrent Hidden Semi-Markov Model (R-HSMM) for segmenting and labeling high dimensional time series data in an unsupervised way. The proposed method incorporates a Recurrent Neural Network (RNN) to model the generative process in HSMM, which addresses the limitation of HSMM in capturing the nonlinear and complex dynamics within segments. The authors also introduce a structure encoding function to accelerate the inference while preserving accuracy. Furthermore, they generalize the penalty method to distribution space for simultaneous training of the model and the encoding function. Empirical results demonstrate that the proposed R-HSMM provides state-of-the-art performance on both synthetic and real-world datasets.\n\nOverall, the paper presents a significant contribution to the field of unsupervised segmentation and labeling of high dimensional time series data. The proposed R-HSMM method surpasses the existing HSMM and its variants in capturing the nonlinear and complex dynamics within the segments. The incorporation of RNN, structure encoding function, and generalization of the penalty method to distribution space are novel and effective ways to address the limitations of HSMM.\n\nThe paper is well-written, and the authors presented their methodology, experiments, and results in a clear and organized manner. However, there are a few areas of concerns that the authors should address:\n\n1. The authors should provide more details about the experimental setup, including the dataset description, feature selection, and parameter tuning. This information will help the readers to better understand the experiments and make comparisons with other related work.\n\n2. The authors should provide a detailed comparison of their proposed method with related work in the field, particularly state-of-the-art methods, to demonstrate the superiority of R-HSMM.\n\n3. The authors should evaluate the proposed method on more real-world datasets to demonstrate its applicability in various domains.\n\n4. Additional analysis and discussion should be provided to explain the reasons behind the observed differences in performance between the proposed method and the baseline methods.\n\nOverall, I recommend this paper for publication with minor revisions once the above-mentioned concerns are addressed.","model":"chatGPT","source":"peerread","label":1,"id":1962}
{"text":"The paper \"Recurrent Hidden Semi-Markov Model\" addresses the problem of segmentation and labeling of high dimensional time series data. The authors suggest that this problem is critical in various domains such as medical diagnosis and behavior understanding. While Hidden Semi-Markov Model (HSMM) is a classical tool for this problem, existing HSMM and its variants have limited ability to capture the nonlinear and complex dynamics within segments due to simple conditional assumptions of observations. The authors suggest that to fix this limitation, the Recurrent Neural Network (RNN) can be incorporated in HSMM, resulting in Recurrent HSMM (R-HSMM). The authors also designed a structure encoding function to mimic the exact inference, thus achieving accurate inference while preserving the speed of the algorithm. Finally, they generalize the penalty method to the distribution space, which enables them to train the model and the encoding function simultaneously. \n\nThe paper's strengths are its original contribution in proposing a new model for segmentation and labeling of high-dimensional time series data by incorporating RNN in HSMM. This combination results in more accurate results due to the ability of RNN to capture the nonlinear and complex dynamics, making the method superior to HSMM and its variants. The encoding function is an interesting innovation as it enables accurate inference while preserving the speed of the algorithm. The authors' experiments on synthetic and real-world datasets demonstrate the effectiveness of their proposed R-HSMM's ability to achieve state-of-the-art performances. \n\nHowever, the paper also has a few weaknesses that need to be addressed. Firstly, there is a lack of clarity in describing the encoding function's mechanism, and the paper could benefit from more detailed and non-mathematical explanation. Additionally, this paper's contribution is not highlighted enough or compared to the existing literature; hence it is not clear what the novelty of this method is. Lastly, it would be beneficial to provide a visual presentation for better understanding and interpretation of the results.\n\nOverall the paper \"Recurrent Hidden Semi-Markov Model\" is a valuable contribution to the segmentation and labeling of high dimensional time series data, presenting novel ideas and demonstrating improvements over classical models. Therefore, this paper deserves to be accepted, pending the authors' revisions to address the noted limitations, and clarification of the novelty of their method.","model":"chatGPT","source":"peerread","label":1,"id":1963}
{"text":"Title: Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data\n\nAuthors: Frederic Kor and Tom\u00e1\u0161 Pevn\u00fd\n\nIn \"Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data,\" Frederic Kor and Tom\u00e1\u0161 Pevn\u00fd present an innovative approach to unsupervised learning of state space models called the Deep Variational Bayes Filters. The authors propose a deep neural network architecture that can learn jointly the parameters of the state space model as well as the model of the observation. The paper includes an extensive experimental section, where the proposed approach is evaluated on a number of synthetic and real datasets.\n\nOverall, I found the paper to be well-written, clear, and concise. The authors provide a comprehensive introduction to state space models and the problem of unsupervised learning from raw data, and they motivate the importance of their contribution. The proposed approach is innovative and brings a new perspective to this challenging problem. I particularly appreciated the thorough experimental section, which demonstrated the effectiveness of the proposed approach on several datasets.\n\nHowever, I do have a few suggestions for improvement. Firstly, the authors could provide more details about the hyperparameters used in their experiments. Although the authors mention the hyperparameters used in their approach, it would be beneficial to explicitly state the values used. This would allow other researchers to reproduce the experiments and compare their results to those reported in the paper.\n\nSecondly, it would be helpful to include a discussion of the limitations of their approach. While the experimental section demonstrates the effectiveness of the proposed approach, it would be useful to discuss the scenarios where it might not perform well.\n\nFinally, the authors could expand their discussion on the practical applications of their approach. Although the authors mention potential applications to control systems and robotics, it would be beneficial to elaborate on the specific scenarios in these areas where their approach could be useful.\n\nIn conclusion, I believe that \"Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data\" is a well-written and innovative paper that proposes a novel approach to the challenging problem of unsupervised learning of state space models. The experimental results demonstrate the effectiveness of the proposed approach. However, further detail about hyperparameters, limitations, and practical applications could be added to strengthen the paper.","model":"chatGPT","source":"peerread","label":1,"id":1964}
{"text":"The paper \"Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data\" presents a new approach to unsupervised learning of state space models (SSMs) using variational autoencoders (VAEs). The authors aim to address several challenges in modeling complex systems such as high-dimensional raw data and the need for accurate inference of latent states. They propose a method that combines a VAE with a Kalman filter-based state space model, called the deep variational Bayes filter (DVBF).\n\nOne of the strengths of this paper is the novelty of the proposed method. The authors demonstrate that their DVBF outperforms other state-of-the-art unsupervised learning methods for SSMs. Additionally, they provide a thorough evaluation of their model on both synthetic and real-world datasets, showing that it achieves high accuracy in estimating latent states from raw data.\n\nHowever, there are some weaknesses in this paper. The main limitation is that the proposed method assumes a linear Gaussian state space model. This assumption may not be appropriate for all types of data, and future work should investigate extending this method to non-linear dynamic systems. Additionally, the paper lacks a clear explanation of the intuition behind DVBF's performance advantage over other state-of-the-art methods, and some of the technical details could be explained more clearly for readers who may not be familiar with the method.\n\nOverall, the paper presents a promising new approach to unsupervised learning of SSMs using VAEs, and the results are impressive. However, further improvements and analysis are necessary to fully demonstrate the effectiveness and applicability of the method.","model":"chatGPT","source":"peerread","label":1,"id":1965}
{"text":"Overall, this paper presents an interesting approach to unsupervised learning and identification of latent Markovian state space models, using Deep Variational Bayes Filters (DVBF). The authors utilize recent advances in Stochastic Gradient Variational Bayes to handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge, which is impressive. The experiments demonstrate that enabling backpropagation through transitions improves information content of the latent embedding and enables realistic long-term prediction.\n\nOne strength of this paper is that the authors present a detailed explanation of the proposed method, making it easy to understand the steps involved in building the DVBF. The paper also highlights how DVBF improves upon existing approaches through its ability to overcome intractable inference distributions. Additionally, the experimental results are presented in a clear and organized manner, making it easy to follow the findings.\n\nHowever, there are also some weaknesses of the paper. For instance, the authors do not provide a comparison with other related methods that could outline the strengths and weaknesses of the proposed method. A comparison with existing methods would have helped in demonstrating the superiority of the proposed method. Additionally, the paper could improve by providing further insights into the use of DVBF in real-world applications, and highlighting potential limitations or challenges of applying this method to large-scale datasets.\n\nOverall, the paper presents a promising approach to unsupervised learning of latent Markovian state space models. However, I would suggest the authors address the concerns above to strengthen the paper further.","model":"chatGPT","source":"peerread","label":1,"id":1966}
{"text":"The paper titled \"Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data\" by Krishnan et al. addresses the problem of unsupervised learning of latent Markovian state space models. The authors introduce a new method called Deep Variational Bayes Filters (DVBF) that leverages Stochastic Gradient Variational Bayes to overcome intractable inference distributions via variational inference. This method is capable of handling highly nonlinear input data with temporal and spatial dependencies, such as image sequences, without domain knowledge.\n\nThe main strengths of this paper include its clear and concise writing style, thorough explanation of the proposed method, and well-designed experiments that demonstrate the effectiveness of the proposed method. The authors have provided well-defined equations and algorithms that make the proposed method easy to understand and implement. Their use of different datasets with varying complexities to evaluate the effectiveness of DVBF is praiseworthy, and their results demonstrate that DVBF outperforms existing methods for state space modeling.\n\nHowever, one major weakness of the paper is the lack of detailed comparison with other state-of-the-art methods for unsupervised learning and identification of latent Markovian state space models. In particular, the authors could have compared the proposed method with Nonlinear Dynamical Systems (NDS) and Hidden Markov Models (HMM), which are commonly used for state space modeling tasks. The paper could also have benefited from more discussion on how the proposed method relates to existing techniques in the literature.\n\nIn summary, the paper presents a novel method for unsupervised learning and identification of latent Markovian state space models. The DVBF method presented in the paper is effective in handling highly nonlinear input data with temporal and spatial dependencies without domain knowledge. The results of the experiments reported in the paper demonstrate the superiority of the proposed method. However, additional evaluation with other state-of-the-art techniques could have resulted in stronger conclusions.","model":"chatGPT","source":"peerread","label":1,"id":1967}
{"text":"Title: Generative Multi-Adversarial Networks\nAuthors: X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, P. Abbeel\n\nThe paper \"Generative Multi-Adversarial Networks\" proposes an innovative approach for the generation of natural images based on a new type of generative adversarial network (GAN) architecture, called Mixture Density Generative Adversarial Network (MD-GAN). The authors provide strong theoretical foundations for their approach, and experimentally show that the MD-GAN outperforms existing state-of-the-art methods in terms of image quality, diversity, and stability.\n\nThe paper is well-written and well-structured. The introduction provides a clear motivation for the proposed approach, and the authors do an excellent job of introducing the key concepts and relevant background information necessary to understand their proposed approach. In particular, the authors provide a thorough explanation of GANs and the challenges associated with their training, as well as a detailed overview of the limitations of existing GAN architectures.\n\nThe proposed approach is presented in a clear and concise manner, and the authors provide a thorough explanation of the technical details of the MD-GAN. The authors use a mixture of Gaussian distributions to model the output distribution of the generator network, which allows the network to generate images with more diverse and meaningful features. The MD-GAN also incorporates a multi-scale architecture, which helps to improve the stability of the training process and generate higher quality samples.\n\nThe experimental results presented in the paper are compelling, and the authors use a variety of objective and subjective metrics to evaluate the performance of MD-GAN against existing state-of-the-art methods. The authors demonstrate that their approach produces images with better visual quality and more diverse features, which is an important contribution to the field of generative modeling.\n\nOverall, I consider this paper to provide a valuable contribution to the field of generative modeling, and I strongly recommend it for publication. The authors have done a great job of presenting a well-motivated and technically sound approach, and the experimental results clearly demonstrate the effectiveness of the proposed method.","model":"chatGPT","source":"peerread","label":1,"id":1968}
{"text":"The paper \"Generative Multi-Adversarial Networks\" attempts to address the problem of generating high-quality images using a deep learning approach. Specifically, the authors propose a variant of the Generative Adversarial Networks (GANs) architecture that incorporates multiple discriminators and noise injection to improve the quality and diversity of the generated images.\n\nStrengths:\n- The proposed architecture shows promising results in generating high-quality and diverse images compared to the standard GANs architecture.\n- The authors provide a thorough analysis and evaluation of their approach, including both quantitative and qualitative measures.\n- The paper is well-organized and clearly presents the proposed method and its evaluation.\n\nWeaknesses:\n- The paper lacks an in-depth comparison with other state-of-the-art image generation methods. It would be beneficial to compare the proposed approach with other relevant methods to determine its superiority.\n- The authors could provide more information about the hyperparameters used in the experiments and how they were chosen.\n- The paper could benefit from a more detailed discussion about the limitations and potential extensions of the proposed approach.\n\nOverall, the paper presents an interesting and well-executed approach to image generation that demonstrates promising results. However, further analysis and in-depth comparisons are needed to establish its superiority over other relevant methods. Additionally, more information about hyperparameters and potential limitations would be helpful for readers looking to apply the proposed approach.","model":"chatGPT","source":"peerread","label":1,"id":1969}
{"text":"This paper proposes a framework named Generative Multi-Adversarial Network (GMAN) that extends the traditional Generative Adversarial Network (GAN) to multiple discriminators. The authors demonstrate that GMAN can be trained reliably using the original minimax objective. Additionally, the authors explore different discriminator roles and compare the proposed framework with standard GANs in image generation tasks. The authors show that GMAN produces higher quality samples in fewer iterations when measured by a pairwise GAM-type metric.\n\nOverall, this paper presents a valuable contribution to the field of machine learning and generative modeling. The proposed framework of GMAN is well-motivated and well-explained. The paper is well-organized, clear, and easy to follow. The authors provide sufficient details of the architecture, experiments, and evaluation metrics, making it possible to reproduce the results.\n\nThe comparative experiments with standard GANs demonstrate an improvement in sample quality with GMAN. However, the authors should include a more comprehensive analysis of the experimental results, comparing the performance of GMAN and standard GANs against a larger set of evaluation metrics, such as Inception Score, Frechet Inception Distance, and Structural Similarity Index. Combining multiple evaluation metrics in the analysis can provide a more robust comparison between GMAN and GANs.\n\nMoreover, the authors should discuss the complexity of their proposed framework in terms of computation and model size. Adding multiple discriminators to the GAN framework could possibly increase the computational requirements for training and inference. The authors should discuss the trade-off between sample quality improvement and the computational costs of the proposed framework.\n\nIn conclusion, this paper is well-written, presents interesting contributions to the field, and provides a well-explained solution for the extension of GANs to multiple discriminators. The authors' experiments and analysis are promising, but more thorough evaluations and discussions are necessary.","model":"chatGPT","source":"peerread","label":1,"id":1970}
{"text":"Peer review for \"Generative Multi-Adversarial Networks\"\n\nThe paper proposes a novel framework called \"Generative Multi-Adversarial Networks\" (GMAN) that extends the generative adversarial network (GAN) framework to include multiple discriminators. The objective of the paper is to introduce a framework that enables reliable training of GANs with the original untampered objective. The authors demonstrate empirically that GMAN produces higher quality samples when compared to standard GANs, while requiring a fraction of the iterations in image generation tasks.\n\nStrengths:\n- The authors introduce a novel framework, which extends the GAN framework to include multiple discriminators. The proposed framework is designed to reliably train GANs without the need for modified objectives.\n- The authors provide a clear and concise explanation of GMAN by comparing it to standard GANs. They provide illustrative diagrams and provide insights into the design perspectives of the discriminator role.\n- The paper presents empirical results that demonstrate the effectiveness of GMAN in image generation tasks. The authors provide comparisons of GMAN with standard GANs, which highlight the superiority of the proposed framework in producing higher quality samples in fewer iterations.\n\nWeaknesses:\n- The paper lacks a discussion of the limitations of GMAN. It would benefit from an analysis of the scenarios where the proposed framework may not work well and potential challenges in real-world applications.\n- The paper would also benefit from a more in-depth analysis of the design perspectives of the discriminator role. While the authors briefly mention the role of the discriminator, a more in-depth discussion could provide insights into how it affects GMAN's performance.\n- Although the pairwise GAM-type metric is used to compare the quality of samples between GMAN and standard GANs, the authors do not provide any additional metrics to support their claims. The addition of other metrics or experiments could strengthen their argument.\n\nOverall, the paper presents a novel framework that extends the GAN framework to include multiple discriminators. The authors provide clear insights into their proposed framework with illustrative diagrams and comparison with standard GANs. The empirical results demonstrate that the proposed framework produces higher quality samples while requiring fewer iterations. However, additional analysis in the form of limitations and design perspectives is needed to strengthen the paper further.","model":"chatGPT","source":"peerread","label":1,"id":1971}
{"text":"Title: Mollifying Networks: Mildly Relaxed Neural Networks with a Barrier Function\n\nAuthors: John Smith, Jane Doe\n\nJournal: Neural Information Processing Systems Conference (NIPS)\n\nLink to paper: [insert link here]\n\nSummary:\n\nThe paper proposed a novel approach to improve the robustness and generalization capabilities of neural networks. The approach, called mollifying networks, introduces a barrier function to the optimization objective of neural networks, which encourages a small margin of separation between the decision boundary and the data points. The authors demonstrate that mollifying networks can improve the performance of various neural network architectures on both benchmark datasets and real-world applications.\n\nStrengths:\n\nOverall, the paper is well-written and clearly explains the motivation, approach, and results of the proposed mollifying networks. The authors provide a solid theoretical foundation for the approach and demonstrate its effectiveness through extensive experiments. Specifically, the results show that mollifying networks can reduce the overfitting of neural networks, increase their robustness to adversarial attacks, and improve their performance on out-of-distribution data. Moreover, the authors demonstrate that the approach is computationally efficient and easy to implement.\n\nWeaknesses:\n\nWhile the paper provides convincing evidence for the effectiveness of mollifying networks, it would be helpful to have a more detailed discussion on the limitations of the approach. For example, the authors mention that the approach may not be effective for certain types of neural network architectures or datasets, but do not provide specific examples or explanations. Additionally, the paper could benefit from a more thorough comparison of the proposed approach to other related works on improving the robustness and generalization capabilities of neural networks.\n\nSuggestions:\n\nOverall, the paper is well-written and presents an important contribution to the field of deep learning. However, a few minor revisions could strengthen the paper:\n\n- Provide a more detailed discussion on the limitations of mollifying networks and potential future directions for research.\n- Include a more thorough comparison of the proposed approach to other related works on improving the robustness and generalization capabilities of neural networks, particularly those that use a similar barrier function approach.\n- Provide more details on the experimental setup, particularly the hyperparameters used for the models and the choice of datasets for evaluation. \n\nIn conclusion, the paper is a valuable contribution to the field of deep learning and provides a novel and effective approach for improving the robustness and generalization capabilities of neural networks.","model":"chatGPT","source":"peerread","label":1,"id":1972}
{"text":"The paper titled \"Mollifying Networks\" addresses the problem of neural network models' inability to capture the intrinsic properties of functions with singularities or sharp transitions. The authors propose a novel approach to mollify the networks that can accurately approximate such functions by a smooth function with appropriately scaled gradients. The paper's strengths lie in its innovative approach to solving the problem of the inability of neural networks to handle singularities. The proposed approach is applied to several tasks, including image classification and regression, and provides competitive results compared to state-of-the-art methods.\n\nAdditionally, the authors provide theoretical analysis and empirical results to support their approach. The theoretical analysis suggests that mollifiers can improve the expected gradients' smoothness, leading to better generalization of the neural network models. The empirical results presented in the paper demonstrate that the proposed approach can significantly improve the accuracy of prediction on various datasets over alternative methods. \n\nHowever, the paper's weaknesses are related to the explanations of some computational details. For instance, the authors do not provide clear explanations of the implementation details of the mollifier, which could make it challenging to reproduce their results. Furthermore, some of the experiments in the paper seem to lack a sufficient description of the hyperparameters used, which may affect the reproducibility and evaluation of the proposed approach.\n\nOverall, the paper presents a novel approach that addresses the significant problem of singularity and sharp transitions in neural network models. Although there are a few minor weaknesses in the explanations of some computational details, the authors' theoretical and empirical analyses provide strong evidence for the effectiveness of the proposed approach.","model":"chatGPT","source":"peerread","label":1,"id":1973}
{"text":"Review of \"Mollifying Networks\"\n\nThe paper proposes a novel method termed \"mollifying networks\" for addressing the problem of optimizing highly non-convex neural network objectives. This is an important problem as the non-convex nature of loss functions can lead to pathological landscapes that can be challenging to escape from using gradient-based optimization methods. The proposed method addresses this problem by starting with a mollified loss function that is simpler to optimize and gradually becomes more complex as the training progresses. The complexity of the mollified network is controlled by a single hyperparameter that is annealed during training.\n\nThe paper is well written and clearly presents the motivation, approach and the experimental results. The experiments show significant improvements on various difficult optimization tasks compared to traditional optimization strategies. The authors also provide a discussion on the relationship between their approach and recent works on continuation methods for neural networks. \n\nOne of the strengths of the paper is the simplicity of the approach which requires the introduction of only a single hyperparameter. Additionally, this approach is easily implementable and can be integrated into existing deep learning frameworks. \n\nHowever, one limitation of the proposed approach is that it may not be applicable to all deep learning architectures. For example, the authors mainly evaluate their approach on fully-connected networks and convolutional neural networks, but it is unclear how it would perform on other architectures such as recurrent neural networks or attention-based models. \n\nOverall, the paper presents a promising method for optimizing highly non-convex neural network objectives. The results are impressive and demonstrate the effectiveness of the proposed approach in addressing the problem of optimization of deep neural networks.","model":"chatGPT","source":"peerread","label":1,"id":1974}
{"text":"The paper \"Mollifying Networks\" addresses the challenging problem of optimizing deep neural networks, which can be more difficult than traditional convex optimization problems due to the highly non-convex nature of the loss function. The authors propose a solution to this problem by starting with a smoothed or \"mollified\" objective function, which becomes more complex as the training proceeds. They draw inspiration from continuation methods and curriculum methods and let the mollified networks evolve during training until they eventually become the original, difficult-to-optimize objective function.\n\nThe strengths of this paper include the innovative approach to deep neural network optimization and the clear and concise presentation of the proposed solution. The use of mollifiers, a concept familiar to mathematicians but relatively unknown in machine learning, is a novel idea in the field of network optimization. The paper also provides experimental results on various optimization tasks that demonstrate the effectiveness of mollified networks. The authors clearly establish a link between their method and continuation methods for neural networks.\n\nOne potential weakness of this paper is the limited scope of experimental evaluation. While the authors demonstrate improvements on difficult optimization tasks, it would be helpful to see further evaluations on larger datasets and more complex models. Additionally, there could be some limitations in the effectiveness of mollifiers in certain types of optimization problems. Further exploration and experimentation may be needed to identify these limitations.\n\nOverall, \"Mollifying Networks\" presents an innovative approach to deep neural network optimization that is well-supported by experimental results. While there are potential limitations to the effectiveness of mollifiers in certain optimization problems, the proposed solution has the potential to be a useful tool for neural network optimization.","model":"chatGPT","source":"peerread","label":1,"id":1975}
{"text":"Title: Offline bilingual word vectors, orthogonal transformations and the inverted softmax\n\nAuthors: John Doe, Jane Smith, and Mary Lee\n\nSummary:\nThe paper presents a new approach for bilingual word vectors, involving the use of offline representations and orthogonal transformations. The proposed algorithm also makes use of the inverted softmax technique to improve the performance of the model. The authors have conducted experiments in several languages and obtained promising results, which demonstrate the effectiveness of the proposed method.\n\nStrengths:\nThe paper presents a novel approach for learning bilingual word vectors that is based on orthogonal transformations and the inverted softmax, which is an interesting contribution to the field of natural language processing. The authors also conducted experiments in various languages, which add credibility to their proposed method. The results of the experiments are promising and suggest that the proposed approach can outperform existing methods.\n\nWeaknesses:\nAlthough the authors have presented a novel approach for learning bilingual word vectors, the paper lacks a detailed explanation of the proposed method. The authors also didn't provide clear motivation for the use of orthogonal transformations and inverted softmax. Moreover, the paper doesn't compare the proposed method with existing state-of-the-art methods in detail, which leaves room for improvement. Finally, the authors need to improve the writing style and use scientific terminology accurately.\n\nRecommendations:\nTo improve the paper, the authors should provide more details on the proposed approach and clarify the motivation behind the use of orthogonal transformations and inverted softmax. Additionally, the authors should conduct a more extensive evaluation and comparison against existing state-of-the-art methods. Furthermore, the paper needs to be edited for clarity and scientific accuracy. Finally, it would be useful to provide a concrete implementation of the proposed method to facilitate adoption by other researchers.\n\nOverall, the proposed approach has potential to make significant contributions in the field of bilingual word vector learning. However, the mentioned shortcomings should be addressed to make the paper more impactful and credible.","model":"chatGPT","source":"peerread","label":1,"id":1976}
{"text":"The paper \"Offline bilingual word vectors, orthogonal transformations and the inverted softmax\" by Guillaume Lample and Alexis Conneau, presents an approach to learning bilingual word embeddings without the need for parallel text data. The proposed method is based on orthogonal transformations and the inverted softmax technique, and is evaluated on a range of cross-lingual tasks.\n\nStrengths:\n- The paper provides a clear motivation for the problem of learning bilingual word embeddings in the absence of parallel data.\n- The proposed approach is well-motivated and builds upon previous work in the field. It is also explained in a clear and concise manner.\n- The evaluation of the proposed approach on a range of cross-lingual tasks is thorough and convincing, demonstrating the effectiveness of the method.\n- The paper includes a comparison with state-of-the-art methods, which further reinforces the contributions of the proposed approach.\n\nWeaknesses:\n- While the paper provides a clear motivation for the problem, there could be more discussion on the significance and potential impact of the proposed approach in addressing the problem.\n- The experimental setup could be more detailed, including information on the specific datasets used and how they were preprocessed.\n- While the proposed method provides a promising solution to the problem, it would be useful to discuss its limitations and\/or assumptions, and potential directions for future work.\n\nOverall, the paper presents a well-motivated and effective approach to learning bilingual word embeddings, and provides a thorough evaluation of the method. Despite a few minor shortcomings, the proposed approach has the potential to be a valuable contribution to the field of cross-lingual NLP.","model":"chatGPT","source":"peerread","label":1,"id":1977}
{"text":"Review of \u201cOffline bilingual word vectors, orthogonal transformations and the inverted softmax\u201d\n\nThis paper presents a novel method for aligning bilingual word embeddings. The proposed approach follows the offline strategy, which differs from the online strategy commonly used in previous approaches. The authors suggested that orthogonal transformations should be used to align pre-trained embeddings from two different languages. The method was tested using Italian-English language pairs, and the results showed that this proposed orthogonal transformation-based method outperformed the previous alignment method. Additionally, the authors introduced a novel \u201cinverted softmax\u201d approach to identify translation pairs, which further improved the performance of the alignment.\n\nOverall, the paper is well-organized and well-written. The problem of bilingual word embedding alignment is clearly stated, and the proposed method provides a comprehensive solution to this problem. The paper is also well-motivated, as the authors provided a detailed background on the current state-of-the-art techniques for bilingual word embedding alignment.\n\nThe experiments and results section in the paper are also well-done. The authors provided comprehensive quantitative measures to evaluate their proposed method, comparing it to the previous approach, which provides a baseline for comparison purposes. Different metrics, such as precision and recall, were used to evaluate the effectiveness of the proposed method. The results showed that the proposed method has significantly improved the performance of the alignment, demonstrating its superiority.\n\nOne minor point of improvement for the paper would be to provide more detailed explanations for the technical terms or concepts used in the paper. For example, the term \u201conline\u201d and \u201coffline\u201d used throughout the paper might be unclear to some readers, and therefore, the authors should provide a clear definition or explanation of these terms.\n\nOverall, this paper is a valuable contribution to the field of natural language processing, and it has the potential to inspire further research on this topic.","model":"chatGPT","source":"peerread","label":1,"id":1978}
{"text":"This paper titled \u201cOffline bilingual word vectors, orthogonal transformations and the inverted softmax\u201d aims to address the problem of bilingual word vector alignment using an algorithm that does not require expert knowledge, is more robust to noise, and provides higher precision. The paper proposes a method to align pre-trained word embeddings in bilingual settings. The authors argue that unlike previous online training methods, their \u201coffline\u201d method which uses two pre-trained word embeddings and an orthogonal transformation technique can have high precision while being more robust to noise. \n\nThe strengths of this paper include the use of singular value decomposition (SVD) to compute the orthogonal transformation and the focus on finding a method that does not require expert knowledge. These approaches are novel and can imply that such algorithms can be applied even when expert resources are not readily available. Furthermore, the authors introduced a new \"inverted softmax\" method for better identification of translation pairs which improved the precision @1 from 34% to 43% when translating a test set composed of common and rare English words into Italian. \n\nHowever, there are some weaknesses in this paper. One weakness is that the results are reported only on a single dataset, which prevents a broad evaluation of the proposed algorithm's generalizability. The algorithms may perform differently in other bilingual settings or with different languages. The paper could have done a better job of testing the algorithm on more datasets to prove its effectiveness. Additionally, there could have been more discussion and experimentation to determine what impact the pseudo-dictionary approach had on the performance of the algorithm. Finally, the presented research does not show the limitations of the approach, such as the increased computational time, which could be a drawback in some practical applications.\n\nOverall, this paper is well written and presents a novel approach to align pre-trained word embeddings in bilingual settings. Although there are some weaknesses, the novel techniques used are promising and provide interesting leads for further research in this area.","model":"chatGPT","source":"peerread","label":1,"id":1979}
{"text":"Abstract:\n\nThis paper presents a method for visualizing and understanding deep neural network decisions through prediction difference analysis. The authors introduce a visualization tool, called Prediction Difference Analysis (PDA), that enables users to explore the differences between the classifications made by a deep neural network and a reference model by identifying the key features that drove the model's decisions. The authors demonstrate the effectiveness of their method through experiments on several image classification datasets.\n\nOverall, this paper presents an innovative and useful tool for visualizing the decision-making process of deep neural networks. The idea behind PDA is simple yet effective, and the authors provide a thorough evaluation of its usefulness. The experiments presented in this paper demonstrate that PDA can be used to gain insights into the underlying patterns and features that drive deep neural network decisions.\n\nOne strength of this paper is that it is well-structured and easy to follow. The authors provide a clear description of their method and the experiments they conducted to evaluate it. The use of visualizations to demonstrate the effectiveness of their approach is particularly valuable, as it allows the reader to easily grasp the key insights gained from their work.\n\nOne potential limitation of the paper is the lack of discussion on the generalizability of their method to different types of deep neural networks architectures. It would have been interesting to see whether PDA works equally well across a range of network architectures or whether it is more effective for certain types of neural networks.\n\nOverall, this paper is a valuable contribution to the field of deep neural network research, and I would recommend it for publication in this field. The authors' work opens up new possibilities for understanding the decision-making processes of deep neural networks and may have practical implications for the development of more transparent and interpretable AI models.","model":"chatGPT","source":"peerread","label":1,"id":1980}
{"text":"The paper \"Visualizing Deep Neural Network Decisions: Prediction Difference Analysis\" aims to address the lack of transparency and interpretability issues that arise in deep neural networks. Specifically, it proposes a method for visualizing the decisions made by deep neural networks using Prediction Difference Analysis (PDA). \n\nStrengths:\nOne of the main strengths of this paper is that it provides a detailed description of the PDA method for visualizing deep neural network decisions. The authors demonstrate the effectiveness of the proposed method through experiments on various datasets, including image classification and sentiment analysis, and provide visualizations of the decision-making process of deep neural networks. \n\nAdditionally, the paper provides a comprehensive literature review on the interpretation and visualization of deep neural network decisions, which helps contextualize the contribution of the proposed method. \n\nWeaknesses:\nOne potential weakness of this paper is that the authors do not provide an in-depth discussion of the limitations of the PDA method. While the proposed method is effective for visualizing the decision-making process of deep neural networks, it is not clear how generalizable these visualizations are across different types of models and datasets. \n\nFurthermore, the paper does not provide a comparison to other methods for visualizing deep neural network decisions, which would help demonstrate the superiority of the PDA method. \n\nOverall, this paper provides a valuable contribution to the field of deep neural network interpretation and visualization through the proposed PDA method. However, future work should focus on addressing the limitations and extending the method to other types of models and datasets.","model":"chatGPT","source":"peerread","label":1,"id":1981}
{"text":"Peer Review:\n\nThe paper \"Visualizing Deep Neural Network Decisions: Prediction Difference Analysis\" presents an interesting method for visualizing the decision-making process of deep neural networks. The authors propose a prediction difference analysis method that highlights areas in an input image that provide evidence for or against a certain class during the classification process. This method is shown to provide important additional insights into the decision-making process of classifiers and could be useful in making neural network decisions more interpretable.\n\nThe paper is well-written and clearly presents the prediction difference analysis method. The authors have done a good job of explaining the motivation for this method and its advantages compared to previous methods. The method is also illustrated in experiments on both natural images (ImageNet data) and medical images (MRI brain scans), which shows its potential usefulness across different application areas.\n\nHowever, one potential limitation of this paper is that the experiments are limited to only two types of images. It would be interesting to see if the method works equally well on other types of images and domains. Additionally, the paper does not discuss the potential limitations or challenges of this method, which would be valuable information for other researchers who may want to use or expand upon this work.\n\nOverall, the paper presents an interesting and useful method for visualizing the decision-making process of deep neural networks. The authors have done a good job of explaining the method and its potential benefits, and the experiments illustrate its usefulness in two different domains. I recommend this paper for publication with minor revisions to address the potential limitations and challenges of the method.","model":"chatGPT","source":"peerread","label":1,"id":1982}
{"text":"This paper addresses the problem of making neural network decisions interpretable through visualization. More specifically, it presents a method called prediction difference analysis that visualizes the response of a deep neural network to a specific input, highlighting areas in the input image that provide evidence for or against a certain class when classifying images. The motivation behind this work is to provide additional insight into the decision making process of classifiers, which is important to improve models and accelerate the adoption of black-box classifiers in application areas such as medicine.\n\nThe main strength of this paper is that it presents a new method for visualizing the response of neural networks that overcomes several shortcomings of previous methods. The method is being tested on both natural images (ImageNet data) and medical images (MRI brain scans) which makes the paper very relevant and applicable to real-world problems with important implications in the medical field.\n\nHowever, a weakness of the paper is that it could have provided more details on how the prediction difference analysis method was evaluated in comparison with other relevant techniques. In addition, this paper justifies the need for making neural network decisions interpretable through visualization, but it does not discuss in detail the implications of their findings and how they can be applied in practice.\n\nOverall, this paper presents a significant contribution to the field of deep learning and explainable AI, and it is well-written with clear explanations and demonstrations of the proposed method. I would recommend this paper for publication with minor revisions to address the mentioned weaknesses.","model":"chatGPT","source":"peerread","label":1,"id":1983}
{"text":"Title: Latent Sequence Decompositions\n\nGeneral comment:\nThe paper presents an interesting approach for decomposing sequential data. The proposed method is well-developed, and the experiments conducted demonstrate the effectiveness of the approach. However, there are a few important issues that need to be addressed before the paper can be considered for publication.\n\nSpecific comments:\n\n1. The paper could be improved by providing more detail on the proposed approach. For example, more information could be provided on how the latent space is initialized and how the model parameters are optimized. Additionally, the impact of the chosen hyperparameters could be further examined.\n\n2. The paper would benefit from providing additional insight into how the approach compares to other state-of-the-art methods. While the experiments provided show that the method outperforms some approaches, the comparison is limited to a few methods. Further comparison with other methods would better establish the robustness and generalizability of the proposed approach.\n\n3. The paper would benefit from providing more explanation for how the proposed approach can be used in practice. Specifically, the authors could provide more detail on how the approach could be used for downstream tasks such as classification or prediction.\n\n4. The paper could benefit from improved clarity in some sections, particularly in the experiments section. The experimental setup and results could be more clearly explained, including the choice of datasets and metrics used.\n\nOverall, the paper presents a promising approach for decomposing sequential data. However, the authors should address the issues raised above to further establish the robustness, applicability, and generalizability of the approach.","model":"chatGPT","source":"peerread","label":1,"id":1984}
{"text":"The paper \"Latent Sequence Decompositions\" introduces a new approach for characterizing the content of time series data. The authors propose a method that decomposes a sequence into a set of latent patterns and corresponding weights, which they call Latent Sequence Decompositions (LSD). The proposed framework is compared against other state-of-the-art approaches, such as Dynamic Time Warping (DTW) and Matrix Profile (MP), on a variety of time series datasets, including stock prices, speech recognition, and image recognition.\n\nStrengths:\nOne of the major strengths of the paper is that it addresses an important problem in the field of time series analysis. Time series data is abundant in various domains, such as finance, healthcare, speech recognition, and internet of things, and it is crucial to develop efficient and effective methods to characterize and predict its behavior. The authors provide a novel approach that aims to provide a robust and interpretable representation of sequential data.\n\nThe authors have performed a thorough evaluation of the proposed framework, comparing its performance against state-of-the-art methods on several datasets. The results show that the proposed method outperforms DTW and MP on several datasets, while demonstrating competitive performance on others. The authors provide detailed analysis and insights into the strengths and limitations of the proposed approach.\n\nWeaknesses:\nThe paper could benefit from more explicit discussions about the limitations and potential drawbacks of the proposed approach. While the authors provide some analysis on the impact of different parameters on the performance of the method, it would be useful to discuss the sensitivity of the method to different assumptions, such as the size of the latent patterns, the choice of similarity measure, and the impact of noise and missing data.\n\nAdditionally, the paper could provide more context and discussion on the contributions of the proposed approach compared to existing work in the field. While the authors do compare their method with some well-known approaches, such as DTW and MP, additional comparisons to other related approaches and methods could provide a better understanding of the proposed approach's novelty and potential impact.\n\nOverall, \"Latent Sequence Decompositions\" is a well-written paper that presents a novel approach to address the problem of time series characterization. The authors provide a comprehensive evaluation of the proposed method, highlighting its strengths and limitations. The paper may benefit from additional discussions on the limitations of the approach and context on the contribution compared to existing literature.","model":"chatGPT","source":"peerread","label":1,"id":1985}
{"text":"The paper presents an interesting approach to address the limitations of fixed decomposition of target sequences in sequence-to-sequence models. The authors propose a framework called Latent Sequence Decompositions (LSD), in which the model learns the decomposition of sequences into constituent tokens during training, taking into account both the input and the output sequences. This framework incrementally samples decompositions, from left to right, by locally sampling between valid extensions. \n\nThe experimental results on the Wall Street Journal speech recognition task showed that the proposed LSD model outperforms the character baseline with a 14.8% WER, achieving a 12.9% WER. When combined with a convolutional network on the encoder, the WER further improved to 9.6%. These results indicate that LSD can effectively learn an optimal decomposition of the target sequences, leading to improved performance.\n\nThe paper is well written and structured, and the methodology is presented clearly. The experimental set up is appropriate, and the results are discussed thoroughly. One minor suggestion for improvement would be to provide a more detailed discussion of the underlying principles of the model in the introduction section.\n\nOverall, this study makes an important contribution to the field of sequence-to-sequence models and their application to speech recognition tasks. The proposed framework shows promising results and has the potential to be applied to other domains beyond speech recognition. Therefore, I recommend this paper for publication.","model":"chatGPT","source":"peerread","label":1,"id":1986}
{"text":"The paper \"Latent Sequence Decompositions\" presents a new framework, LSD, for learning the sequence decomposition of the target sequences into a sequence of tokens. The main problem addressed is the issue of sub-optimal choice of token dictionaries in sequence-to-sequence models, which is often static and independent of the input\/output data domains. The paper proposes a solution, LSD, that enables learning the decomposition of sequences into constituent tokens during the training of the model and depends on both input and output sequences. \n\nThe paper presents several strengths that make it a valuable contribution to the field. Firstly, the paper clearly articulates the problem of sub-optimal choice of token dictionaries and proposes a solution that is based on a principled statistical framework. Secondly, the proposed framework is innovative and sound, as it allows for the decomposition of sequences to be learned dynamically during the training process, which is an improvement over the current approach employed in existing sequence-to-sequence models. Finally, the results presented in the paper are convincing, the model achieves a 12.9% WER compared to a character baseline of 14.8% WER, and when combined with a convolutional network on the encoder, the WER improves further to 9.6%. \n\nDespite its strengths, the paper has some weaknesses that limit its wider applicability. Firstly, the paper lacks clarity in the description of specific details about the model, such as the choice of hyperparameters, the choice of token dictionaries, and the loss function used. Secondly, the paper does not compare the performance of LSD to that of other existing sequence-to-sequence models, which makes it difficult to assess the relative performance of LSD. Finally, it is not clear whether the proposed framework can be successfully applied to other NLP tasks, such as machine translation or natural language inference. \n\nOverall, the paper \"Latent Sequence Decompositions\" presents an innovative framework, LSD, for learning the sequence decomposition of target sequences into a sequence of tokens. The paper provides strong empirical evidence to support the proposed framework by demonstrating that it outperforms the baseline character model on the Wall Street Journal speech recognition task. However, the weaknesses of the paper suggest future work is required to improve the applicability of LSD to other NLP tasks and provide a more comprehensive analysis of its performance in comparison to existing models.","model":"chatGPT","source":"peerread","label":1,"id":1987}
{"text":"Peer review for the paper \"Paleo: A Performance Model for Deep Neural Networks\"\n\nThe submitted paper titled \"Paleo: A Performance Model for Deep Neural Networks\" presents a novel performance model that predicts the execution time and energy consumption of deep neural networks. The paper demonstrates the effectiveness of Paleo in predicting the performance of four different deep neural networks on two different hardware platforms.\n\nOverall, the paper is well-structured and provides a comprehensive overview of the proposed model along with the evaluation methodology. The approach of modeling the execution time and energy consumption of deep neural networks is significant and can help optimize the performance of these networks on different hardware platforms.\n\nThe experimental evaluation is thorough and demonstrates the usefulness of the proposed model. The authors have effectively shown that the Paleo model can accurately estimate the energy consumption and execution time of deep neural networks on different hardware platforms, which can reduce the time and cost of optimizing the network's performance.\n\nThe authors have also highlighted the limitations of their work, such as the lack of consideration of the impact of software optimizations and communication overheads, and suggest future research directions to address these issues.\n\nOverall, the paper is of high quality and provides significant contributions to the field of deep learning. The proposed performance model has the potential to aid in the optimization of deep neural networks for different hardware platforms, which is critical in achieving high-performance computing.\n\nThe paper is recommended for publication in this journal, and I suggest minor revisions to improve the clarity of certain sections of the paper.","model":"chatGPT","source":"peerread","label":1,"id":1988}
{"text":"The paper, \"Paleo: A Performance Model for Deep Neural Networks,\" addresses the problem of accurately predicting the performance of deep neural networks. In particular, the paper proposes a new performance model that takes into account the impact of various factors, such as the computational complexity of the network and the hardware being used, on the performance of the network. The authors evaluate the effectiveness of this model on a variety of neural network models and datasets.\n\nStrengths:\n \nOne of the key strengths of this paper is the use of a novel performance model for deep neural networks. The model takes into account various factors that are often ignored in other performance models, such as the impact of input size, the number of layers in the network, and the type of hardware being used. The authors also provide extensive experimental results that demonstrate the effectiveness of their model on various neural network models and datasets.\n\nAnother strength of the paper is the clarity and completeness of the experimental methodology. The authors provide detailed descriptions of the neural network models used, the datasets, and the hardware configurations. They also provide a clear description of the training process used to generate the data for the experiments.\n\nWeaknesses:\n\nOne potential weakness of the paper is the lack of comparison with other performance models. Although the authors demonstrate the effectiveness of their model on various neural network models and datasets, it is difficult to evaluate how much of an improvement their approach represents over existing approaches.\n\nAnother weakness of the paper is the lack of discussion of potential limitations or areas for future work. For example, the authors do not explore the impact of other factors, such as the specific training algorithm used or the impact of different types of activation functions, on the performance of deep neural networks.\n\nOverall, \"Paleo: A Performance Model for Deep Neural Networks\" is a well-written and well-researched paper that proposes a novel approach to predicting the performance of deep neural networks. The authors provide extensive experimental results that demonstrate the effectiveness of their approach, though additional comparison to other performance models and further exploration of limitations are areas for future research.","model":"chatGPT","source":"peerread","label":1,"id":1989}
{"text":"Review of \"Paleo: A Performance Model for Deep Neural Networks\"\n\nThis paper presents an analytical performance model called Paleo, which aims to efficiently and accurately model the expected scalability and performance of deep learning systems. The authors point out that it remains unclear how to best leverage parallel and distributed computing infrastructure to accelerate the training and deployment of deep learning systems, and the effectiveness of existing systems varies widely based on the neural network architecture and dataset under consideration. To address this challenge, the authors extract the computational requirements associated with training and evaluation of a neural network architecture and map them to a specific point within the design space of software, hardware, and communication strategies.\n\nThe approach proposed in this paper is novel and has the potential to be useful for deep learning researchers and practitioners. The authors demonstrate the versatility of Paleo by showing that it is robust to the choice of network architecture, hardware, software, communication schemes, and parallelization strategies. They further demonstrate its ability to accurately model various scalability results for recently published convolutional neural networks (CNNs) such as NiN, Inception, and AlexNet.\n\nHowever, there are a few areas where the paper could be improved. The authors could provide more details on the computational requirements extracted from different neural network architectures and how they are mapped to the design space of software, hardware, and communication strategies. It would also be interesting to see more examples of how Paleo can be used to guide the selection of software, hardware, and communication strategies to optimize the performance of a deep learning system for a given problem instance. Additionally, it would be useful to discuss the limitations of the approach and potential future directions of research.\n\nOverall, this paper presents a promising approach for modeling the expected scalability and performance of deep learning systems. With some further development and refinement, Paleo could become a valuable tool for deep learning researchers and practitioners in optimizing the performance of their systems.","model":"chatGPT","source":"peerread","label":1,"id":1990}
{"text":"The paper \"Paleo: A Performance Model for Deep Neural Networks\" addresses the problem of how to effectively leverage parallel and distributed computing infrastructure for the training and deployment of scalable deep learning systems. The authors propose an analytical performance model called \"Paleo\" that aims to efficiently explore the space of scalable deep learning systems and diagnose their effectiveness for a given problem instance. \n\nStrengths:\n- The paper proposes a novel and valuable approach to modeling the expected scalability and performance of a deep learning system.\n- The authors thoroughly evaluate the effectiveness of Paleo using various network architectures, hardware, software, communication schemes, and parallelization strategies, demonstrating robustness and accuracy.\n- The paper is well-written and easy to understand, providing clear explanations of the proposed approach and the results obtained.\n\nWeaknesses:\n- Although the authors demonstrate the effectiveness of Paleo in modeling scalability for CNNs such as NiN, Inception, and AlexNet, it is unclear how well Paleo would perform with other types of neural networks and datasets.\n- The paper does not provide a comparison of Paleo with other existing analytical performance models for deep learning.\n\nOverall, the paper presents an innovative approach to efficiently exploring and diagnosing deep learning system performance, with strong experimental evidence supporting its effectiveness. The limitations of the approach should be addressed in future work.","model":"chatGPT","source":"peerread","label":1,"id":1991}
{"text":"Title: Combining policy gradient and Q-learning\n\nOverall, the paper presents an interesting approach for combining policy gradient and Q-learning to improve the efficiency of reinforcement learning algorithms. The experimental results show that the proposed method outperforms traditional Q-learning and policy gradient methods in various simulated environments.\n\nStrengths:\n\n- The paper presents a clear and well-structured approach for combining policy gradient and Q-learning, and provides an in-depth analysis of each component of the algorithm.\n- The experimental results demonstrate the effectiveness of the proposed method in improving the performance of reinforcement learning algorithms.\n- The authors provide a thorough discussion of related work and they explain the novelty of their approach in the context of previous research.\n- The paper is well-written and easy to follow.\n\nWeaknesses:\n\n- The evaluation of the proposed method is limited to simple simulated environments, and it is unclear how well the method would perform in more complex environments or with real-world data.\n- There are few details provided on the implementation of the algorithm, such as choice of hyperparameters and the specific architecture of the neural networks used.\n- The comparison with other state-of-the-art reinforcement learning methods is not as comprehensive as it could be, and it would be useful to have more detailed analysis of the advantages and limitations of the proposed method compared to other approaches.\n\nOverall, the paper presents a novel approach for combining policy gradient and Q-learning that can improve the efficiency of reinforcement learning algorithms. However, further research is needed to fully evaluate the effectiveness of the proposed method in more complex environments and to better understand its strengths and limitations compared to other state-of-the-art reinforcement learning methods.","model":"chatGPT","source":"peerread","label":1,"id":1992}
{"text":"Problem\/Question: \n\nThe paper aims to address the challenge of optimizing continuous control problems in an episodic setting using a combination of Q-learning and policy gradient methods. Specifically, the authors proposed a hybrid algorithm that can handle both continuous action and state spaces.\n\nStrengths:\n\nThe paper provides a clear description of the proposed hybrid algorithm that uses the best of both Q-learning and policy gradient methods to address the optimization problem. The authors demonstrated that the proposed algorithm outperforms other state-of-the-art algorithms on a benchmark of continuous control tasks.\n\nTheoretical properties of the new method are well presented in the paper. The authors show that the method leverages benefits of both policy gradient and Q-learning enabling faster convergence to optimal solution and optimal exploration in a large action selection space which is otherwise difficult to achieve.\n\nWeaknesses: \n\nThe paper could be improved by providing a more detailed theoretical analysis of the proposed algorithm. While the authors briefly discuss the theoretical properties of the combined algorithm, more in-depth discussions of the theoretical guarantees will be helpful in understanding the effectiveness of the algorithm under different scenarios.\n\nThe paper could also be strengthened by providing a more detailed comparison of the proposed hybrid algorithm with other hybrid algorithms in the field. This will help the reader to understand the unique strengths of the algorithm and how it can be used to improve future research on this topic.\n\nOverall, the paper provides a valuable contribution to the field of optimization in an episodic setting of continuous control problems. By combining two different methods, the proposed algorithm can provide a better balance between exploration and exploitation, superior convergence speed and better optimal policies in complex control environments.","model":"chatGPT","source":"peerread","label":1,"id":1993}
{"text":"The paper titled \"Combining policy gradient and Q-learning\" proposes a new technique called PGQL that combines policy gradient with off-policy Q-learning. The authors motivate the development of the technique by highlighting the limitations of vanilla online variants that are on-policy only and cannot take advantage of off-policy data.\n\nThe authors establish a connection between the fixed points of the regularized policy gradient algorithm and the Q-values, which allows them to estimate the Q-values from the action preferences of the policy. Q-learning updates are then applied to the estimated Q-values. The paper argues that the new technique, PGQL, can improve data efficiency and stability of reinforcement learning algorithms.\n\nThe paper is generally well-written and the authors provide a clear motivation for their work. The technique proposed is well-described, and its implementation in a suite of Atari games demonstrates improved performance over other techniques.\n\nHowever, some points can be improved. The authors could provide more detail on the numerical examples presented, particularly the parameters used in the experiments. A robust parameter search procedure could help readers to better understand the proposed technique's adaptability to different settings. Additionally, the authors provide a brief discussion on the limitations of the proposed technique, but the discussion could be expanded to include more details on the limitations and suggestions for future research.\n\nOverall, the paper makes a significant contribution to the reinforcement learning community by proposing a new technique that combines policy gradient and Q-learning with off-policy data. The proposed technique improves data efficiency and stability, with the implementation in a suite of Atari games demonstrating its effectiveness. The paper is recommended for publication with the above-mentioned suggestions.","model":"chatGPT","source":"peerread","label":1,"id":1994}
{"text":"This paper aims to address the limitations of vanilla online policy gradient algorithms in reinforcement learning settings. Specifically, the authors propose a new technique called PGQL, which combines off-policy Q-learning with policy gradient, drawing on experience from a replay buffer. This technique enables estimation of Q-values from action preferences of the policy, enabling Q-learning updates. The authors also demonstrate an equivalence between action-value fitting techniques and actor-critic algorithms.\n\nStrengths:\nThe paper offers a novel approach to addressing the limitations of vanilla online policy gradient algorithms. The combination of policy gradient and off-policy Q-learning, resulting in the PGQL technique, is an innovative solution that demonstrates improved data efficiency and stability in numerical examples compared to other algorithms. The authors clearly lay out the motivation behind their approach and provide a strong theoretical foundation, backed by equations and proofs.\n\nWeaknesses:\nSome parts of the paper are difficult to follow without prior knowledge of reinforcement learning and policy gradient algorithms, making it less accessible to readers outside the field. Moreover, the paper does not provide an in-depth analysis or comparison of PGQL with other reinforcement learning methods on a wider range of environments, which limits the generalizability of their findings.\n\nOverall, this paper offers a new approach to addressing the limitations of vanilla online policy gradient algorithms, with a strong theoretical foundation and promising numerical results. However, further analysis and comparison with other methods on a wider range of environments would strengthen the paper.","model":"chatGPT","source":"peerread","label":1,"id":1995}
{"text":"Introduction:\nThis paper presents a new approach to density estimation using Real NVP (Real Non-Volume Preserving) flow, which is a type of normalizing flow model. The authors demonstrate the effectiveness of their approach through experimental comparisons on several datasets, including MNIST, CIFAR-10, and CelebA.\n\nStrengths:\nThe paper is well-motivated and well-written, and the authors provide a thorough review of related work in the field of density estimation. The experimental results show that the proposed method outperforms existing approaches on several datasets. The authors also provide a detailed description of the architecture and training procedure, making it easy for others to replicate their results.\n\nWeaknesses:\nOne weakness of the paper is the limited discussion on the limitations of the proposed method. Although the authors demonstrate the effectiveness of their approach on several datasets, it is unclear how well their method would perform on other, more complex datasets. Additionally, the paper does not provide a comparison to state-of-the-art approaches for each dataset, which makes it difficult to assess the true potential of the proposed method.\n\nConclusion:\nOverall, Density estimation using Real NVP presents a promising new approach to density estimation using normalizing flow models. While the paper could benefit from a more in-depth analysis of the limitations of the proposed method and comparisons with state-of-the-art approaches, the experimental results demonstrate the effectiveness of the approach on several datasets.","model":"chatGPT","source":"peerread","label":1,"id":1996}
{"text":"The paper \"Density Estimation using Real NVP\" presents a new approach for building generative models by proposing a deep neural network architecture called Real NVP (Real-valued Non-Volume Preserving) for density estimation. The main goal is to approximate a probability distribution by creating a function from the input space to an output space. The paper investigates the effectiveness of this architecture on multiple datasets and compares it with other state-of-the-art techniques like Masked Autoregressive Flow (MAF) and Glow.\n\nStrengths:\n- The paper presents a well-written and well-organized study\n- The Real NVP architecture provides an efficient solution for density estimation without the need of expensive computation resources\n- The experimental results show that Real NVP outperforms other methods on some datasets, and achieves competitive performance on others\n- The authors also provide insights into the architecture's sensitivity to hyperparameter tuning, which could be helpful to practitioners\n\nWeaknesses:\n- The evaluation metrics used in the paper are limited to log-likelihood, which is not a holistic indicator of generative model performance\n- The paper lacks theoretical justification for why the proposed approach works well\n- The authors do not compare the quality of generated samples visually, which could give a better idea of model quality\n- While the paper thoroughly investigates the impact of the number of coupling layers on model performance, it could benefit from a deeper analysis of the scalability of the approach to larger and more complex datasets\n\nOverall, \"Density Estimation using Real NVP\" provides an interesting and promising approach to density estimation, and the experimental results are impressive. However, more theoretical justification, alternative evaluation metrics, and visual sampling comparison would significantly enhance the contribution.","model":"chatGPT","source":"peerread","label":1,"id":1997}
{"text":"Title: Peer review for \u201cDensity estimation using Real NVP\u201d\n\nThe paper presents an unsupervised learning algorithm for density estimation using real-valued non-volume preserving (real NVP) transformations. The authors use these transformations to extend the space of models with tractable learning, sampling, inference, and evaluation. \n\nThe authors' approach to density estimation is innovative and extends the capabilities of existing methods. They demonstrate that their method has several advantages such as exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. Additionally, they provide experimental results on four datasets to showcase the performance of their method in modeling natural images through sampling, log-likelihood evaluation, and latent variable manipulations.\n\nThe paper is well-written and the methodology is clearly explained. The authors conduct an adequate literature review and provide detailed experimental results. However, there are a few areas that could be improved.\n\nFirst, the authors could discuss the computation time required for their method. Although the authors mention the efficiency of their method, they do not provide details on the computation time required for training and inference. Additionally, the authors could also compare their approach to existing methods in terms of complexity and computation time.\n\nSecond, the authors could discuss the generalizability of their method to other types of datasets beyond images. Although the authors demonstrate the performance of their method on four image datasets, it would be interesting to see if their method could also be applied to other types of datasets such as audio or text.\n\nFinally, the authors could provide more information on the interpretability of the latent space. Although the authors mention that the latent space is interpretable, they do not provide any concrete examples or analysis to support this claim.\n\nOverall, the paper presents an innovative approach to density estimation using real NVP transformations. The methodology is well-explained and the experimental results are promising. However, some areas could be improved, such as including computation time details, discussing generalizability to other datasets, and providing more information on the interpretability of the latent space.","model":"chatGPT","source":"peerread","label":1,"id":1998}
{"text":"The paper \"Density Estimation using Real NVP\" addresses the critical issue of unsupervised learning of probabilistic models in machine learning. The authors aim to design models that enable tractable learning, sampling, inference, and evaluation. Their approach consists of extending the space of models using real-valued non-volume preserving (real NVP) transformations, which are invertible and learnable transformations. Through this extension, the authors aim to create an unsupervised learning algorithm that has exact log-likelihood computation, exact sampling, exact inference of latent variables, and provides an interpretable latent space. They demonstrate the application of the model in natural image modeling and evaluate the results through sampling, log-likelihood evaluation, and latent variable manipulations.\n\nThe main strength of this paper is that it introduces a novel approach for density estimation, which offers a principled and interpretable way to create probabilistic models. The method is based on invertible transformations and provides an excellent framework for matching the density of the target distribution. The results demonstrate that the approach is effective in modeling the distribution of natural images and offers impressive performance in terms of sampling, log-likelihood evaluation, and latent variable manipulations.\n\nAnother strength of the paper is the use of the exact likelihood function, enabling the model to evaluate the goodness of fit directly. This feature is crucial for the correct selection of the optimal model fit, which is necessary for many applications. Additionally, the proposed model has an interpretable latent space, which is useful for understanding the structure of the model and provides insights into the data.\n\nThere are also some areas for improvement in this paper. Firstly, the authors should describe the experiments more explicitly, detailing the datasets used and the training and testing methodology. Secondly, the authors should include a discussion regarding the scalability of the model, where they evaluate the model's performance on large datasets with a large number of latent variables. Lastly, the authors should include comparison results with other state-of-the-art models for density estimation.\n\nOverall, the paper \"Density Estimation using Real NVP\" proposes a compelling approach for density estimation in unsupervised learning. The results demonstrate that the proposed method is effective in modeling natural images and provides useful insights into the data through its interpretable latent space. With some minor adjustments, this paper has the potential to become an essential contribution to the field of machine learning.","model":"chatGPT","source":"peerread","label":1,"id":1999}
{"text":"Title: Recurrent Batch Normalization\n\nAuthors: XYZ\n\nSummary:\nThe paper proposes a method called Recurrent Batch Normalization (RBN) for training Recurrent Neural Networks (RNNs). The method extends the Batch Normalization (BN) technique to the recurrent connections in the RNNs by modifying the calculation of the mean and variance in the BN layer. The authors argue that this technique helps RNNs to better capture long-term dependencies and improves the training of RNN models in various tasks.\n\nStrengths:\n1. The paper provides a detailed explanation of the proposed technique and its implementation details.\n2. The authors have performed extensive experiments on various RNN models and datasets to evaluate the effectiveness of their technique.\n3. The results show that the proposed method significantly improves the performance of RNNs in various tasks, such as character-level language modeling, machine translation, and speech recognition.\n\nWeaknesses:\n1. The paper lacks a discussion on the theoretical underpinnings of the proposed method. It would be beneficial for the readers if the authors could provide some insights into why RBN works better than the standard BN technique in RNNs.\n2. The authors could have included more detailed comparisons with other techniques that aim to improve the performance of RNNs, such as Layer Normalization and Weight Normalization.\n3. The authors have mostly focused on the improvement in the training and performance of RNNs, but there is little discussion on the computational overhead introduced by the proposed method.\n\nOverall, the paper presents an interesting and innovative technique for improving the training and performance of RNNs. The experimental results are convincing and show that the proposed method outperforms the standard BN and other commonly used normalization techniques. However, addressing the weaknesses mentioned above would further strengthen the paper and make it more valuable to the readers in the field.","model":"chatGPT","source":"peerread","label":1,"id":2000}
{"text":"The paper \"Recurrent Batch Normalization\" addresses the problem of internal covariate shift in recurrent neural networks (RNNs). Internal covariate shift occurs when the distribution of features at each layer of the network changes as the input changes, hindering the convergence of the network. The paper proposes using batch normalization in RNNs to mitigate this problem.\n\nOne strength of the paper is that it presents a detailed analysis of the internal covariate shift in RNNs and explains the mechanism through which batch normalization can help alleviate this issue. The experiments conducted in the paper demonstrate that recurrent batch normalization (RBN) can significantly improve the performance of RNNs on various tasks, including language modeling and speech recognition. These results are reproducible and provide strong evidence in support of the proposed approach.\n\nA weakness of the paper is that it does not provide a thorough comparison with other normalization techniques that have been proposed for RNNs, such as layer normalization and weight normalization. While the paper does compare RBN with regular batch normalization and weight normalization in some experiments, more extensive comparisons with other techniques would have strengthened the paper's contribution.\n\nAnother weakness is that the paper does not discuss the potential drawbacks of RBN, such as increased computational cost and the need for careful tuning of hyperparameters. While these issues are mentioned briefly in the paper, a more detailed discussion would have made the paper more informative for practitioners.\n\nOverall, \"Recurrent Batch Normalization\" is a well-written and informative paper that proposes a novel approach to address internal covariate shift in RNNs. The paper's strengths lie in its detailed analysis of the problem and its convincing experimental results, but some weaknesses in comparison and discussion of drawbacks could be addressed in future work.","model":"chatGPT","source":"peerread","label":1,"id":2001}
{"text":"Peer Review for \u201cRecurrent Batch Normalization\u201d Paper:\n\nThe paper presents a reparameterization of Long-Short Term Memory (LSTM) models, where batch normalization technique is applied to the hidden-to-hidden transition, in addition to the input-to-hidden transformation. The proposed method aims to reduce the effect of internal covariate shift between time steps in recurrent neural networks (RNNs), thus achieving faster convergence and improved generalization in various sequential problems, such as sequence classification, language modeling, and question answering.\n\nOverall, the paper is well-written, the method is novel and technically sound with clear motivations, a well-structured exposition of the approach, and comprehensive evaluation. The authors provided sufficient experimental details and sufficient results analysis to support their claims, which are generally convincingly demonstrated.\n\nOne strength of the paper is that it provides a detailed comparison with existing RNN models using a variety of baseline datasets, which allows direct comparison and clearly demonstrates the superiority of the proposed Recurrent Batch Normalization (RBN) method. The results presented in the paper are significant, and the proposed RBN method outperforms the baselines consistently in terms of both convergence speed and generalization accuracy.\n\nThe paper is easy to understand for those familiar with RNNs and batch normalization, but a brief introduction to the presented concepts might be beneficial for newcomers to the field. Additionally, some details about the choice of hyperparameters, learning rate schedules, and optimization strategies were not provided explicitly, which could make it harder to reproduce the results. \n\nIn summary, this paper presents an important contribution to the field of recurrent neural networks, which has the potential to significantly improve various sequence learning tasks. The authors demonstrated the effectiveness of the proposed RBN method with convincing experimental results and well-supported discussion. Therefore, I recommend this paper for acceptance after minor revisions addressing the points mentioned above.","model":"chatGPT","source":"peerread","label":1,"id":2002}
{"text":"In \"Recurrent Batch Normalization\", the authors propose a reparameterization of LSTM that incorporates batch normalization to improve the performance of recurrent neural networks. Specifically, the authors suggest that batch normalization can be applied not only to the input-to-hidden transformation of RNNs, but also to the hidden-to-hidden transition. The paper examines the effects of this approach on various sequential problems, including sequence classification, language modeling, and question answering.\n\nOne of the strengths of this paper is the authors' clear and effective presentation of the proposed method. The authors provide a detailed explanation of the reparameterization, including the derivation of the gradients, and provide empirical results that demonstrate the effectiveness of the approach. Additionally, the authors compare their approach to previous methods that apply batch normalization only to the input-to-hidden transformation, providing a valuable point of comparison.\n\nOne potential weakness of the paper is its focus on empirical evaluation without providing a thorough theoretical analysis of the proposed method. While empirical results are important, it would be helpful for the authors to provide some insight into how and why batch normalization improves performance in recurrent neural networks beyond reducing internal covariate shift.\n\nOverall, \"Recurrent Batch Normalization\" is a well-written and well-argued paper that proposes a novel approach to improving the performance of LSTM architectures. The paper presents a clear and effective explanation of the proposed method and provides empirical evidence to support the authors' claims.","model":"chatGPT","source":"peerread","label":1,"id":2003}
{"text":"Title: SGDR: Stochastic Gradient Descent with Warm Restarts\n\nThe paper \"SGDR: Stochastic Gradient Descent with Warm Restarts\" proposes a new optimization algorithm to improve the performance of stochastic gradient descent (SGD) by introducing warm restarts. The authors argue that traditional SGD methods can occasionally converge to suboptimal local minima and become trapped in a region of the parameter space. This paper proposes a scheduling mechanism called \"warm restarts\" to prevent SGD from getting stuck in these regions.\n\nOverall, the paper is very well written, and the proposed SGDR method seems promising for improving SGD optimization. The authors provide a clear and thorough explanation of the problem they are solving, and they do an excellent job of outlining the technical details of their method. They also provide a comprehensive empirical evaluation of SGDR, comparing it to other well-established optimization algorithms.\n\nOne criticism of the paper is that it would have been helpful if the authors had provided a more detailed comparison of SGDR with other SGDR-like optimization algorithms. Additionally, while the authors do provide some general guidelines for choosing the optimal value of the hyperparameters involved in the SGDR algorithm, a more thorough discussion of parameter tuning would have been appreciated.\n\nIn conclusion, the paper \"SGDR: Stochastic Gradient Descent with Warm Restarts\" presents a novel approach for improving the performance of SGD optimization. The experimental results demonstrate that the proposed algorithm is effective in improving the convergence rate of SGD and reducing its sensitivity to initial conditions. I recommend this paper for publication.","model":"chatGPT","source":"peerread","label":1,"id":2004}
{"text":"The paper SGDR: Stochastic Gradient Descent with Warm Restarts proposes a novel technique for optimizing the stochastic gradient descent algorithm using warm restarts. \n\nThe problem that the paper addresses is the difficulty of finding the optimal learning rates for deep neural networks. The paper proposes a technique called Stochastic Gradient Descent with Warm Restarts (SGDR) that cyclically increases and decays the learning rates, making it easier to find a good initial learning rate for the algorithm.\n\nOne of the major strengths of the paper is the clarity of its presentation. The authors do an excellent job of explaining the theory behind the SGDR technique and how it can be applied to deep learning problems. Furthermore, the experimental section of the paper includes extensive evaluation of the technique on a number of different datasets, providing evidence of its effectiveness.\n\nHowever, one weakness of the paper is that the authors do not provide a clear comparison of the SGDR technique with other state-of-the-art techniques. While the authors do evaluate the technique against a number of baselines, a more extensive comparison against other recent optimization techniques would have strengthened the paper's contribution.\n\nOverall, the paper makes a valuable contribution to the field of deep learning optimization with its introduction of the SGDR technique. The clarity of the presentation and the experimental evaluation make the paper a strong candidate for publication.","model":"chatGPT","source":"peerread","label":1,"id":2005}
{"text":"The paper presents a simple warm restart technique for stochastic gradient descent, referred to as SGDR, to improve the anytime performance when training deep neural networks. The authors also conducted empirical studies to evaluate the performance of SGDR on different datasets, including CIFAR-10, CIFAR-100, a dataset of EEG recordings and a downsampled version of the ImageNet dataset. The results demonstrate new state-of-the-art results for CIFAR-10 and CIFAR-100, as well as advantages on other datasets.\n\nThe paper is well-written and well-organized. The introduction provides a clear motivation for the study and highlights the importance of the proposed technique. The related work section is comprehensive and properly discusses the related techniques. The proposed technique is introduced and explained clearly in the method section. The experiments section is detailed, and the results are presented in a clear and concise way.\n\nThe authors conducted extensive experiments to evaluate the performance of SGDR on different datasets, and the results are convincing. The improvements in performance in terms of both classification accuracy and anytime performance are noticeable. Moreover, the ablation study results show the effectiveness of the proposed technique. However, some details should be added to clarify the reported results. For example, it is not clear which models were used to obtain the results reported in Table 3. Additionally, the reported results in Table 5 are not complete, and only the test accuracies are reported.\n\nIn conclusion, the paper presents a simple but effective technique for warm restarting stochastic gradient descent to improve the anytime performance when training deep neural networks. The empirical results demonstrate the effectiveness of the proposed technique, and the paper is well-written and organized. Therefore, I recommend the paper for publication with minor revisions to clarify some details.","model":"chatGPT","source":"peerread","label":1,"id":2006}
{"text":"The paper presents a warm restart technique for stochastic gradient descent named SGDR, which improves the performance of the algorithm when training deep neural networks. The paper aims to address the challenge of optimizing the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. Moreover, the paper attempts to establish the effectiveness of partial warm restarts in gradient-based optimization.\n\nThe paper's strength is in its proposal of a simple and effective technique that requires minimal computational cost for implementation. The proposed algorithm improves performance on deep neural networks on different datasets such as EEG, CIFAR-10, CIFAR-100, and a downsampled version of the ImageNet dataset. The experimental results are convincing, demonstrating that the proposed technique outperforms other state-of-the-art algorithms.\n\nThe paper's drawbacks include a lack of insights into the theoretical analysis underlying the proposed technique. The authors could have provided more details regarding the implications of the proposed approach concerning optimization convergence properties. Additionally, the paper could have benefited from more empirical studies to demonstrate the robustness of the proposed technique under various conditions.\n\nOverall, the paper makes a valuable contribution to the field of deep learning optimization, providing a simple and effective warm restart technique that outperforms state-of-the-art methods concerning anytime performance. The paper's strengths significantly outweigh its weaknesses, demonstrating the potential impact of the proposed algorithm on optimizing deep learning models' performance.","model":"chatGPT","source":"peerread","label":1,"id":2007}
{"text":"Overview:\n\nThe paper proposes a novel approach to building natural language interfaces using neural programmer-interpreter. The authors develop a system that allows users to interact with a database by posing natural language questions. Their approach uses a neural programming framework that can learn to generate executable programs from sentences in natural language, effectively bridging the gap between natural language and programming languages. The authors provide experimental results on the accuracy of the system and perform a human evaluation of the system's usability.\n\nStrengths:\n\nThe paper presents a well-motivated, novel approach to natural language interfaces that uses neural programming techniques. The authors provide a thorough explanation of the neural programming framework and how it can be adapted to natural language processing tasks. The experimental results and human evaluations show that the system is effective in generating executable programs from natural language sentences.\n\nThe authors also contribute a significant dataset of natural language queries and their translations to executable programs. This dataset is valuable as it facilitates future research in the field and enables researchers to easily compare results across different approaches.\n\nWeaknesses:\n\nThe paper lacks a detailed explanation of how the authors arrived at specific parameter settings for the neural programming framework. It would be helpful if the authors explained their reasoning for choosing specific hyperparameters, optimization algorithms, etc. Additionally, the authors do not address how their approach may scale to larger datasets or more complex databases, which may limit the applicability of their system in real-world scenarios.\n\nAlthough the human evaluations are promising, the authors do not provide an extensive analysis of their results, such as examining whether certain types of natural language queries are more difficult for the system to handle than others.\n\nOverall, the paper presents a promising approach to natural language interfaces that uses neural programming techniques. The contribution of the dataset is particularly valuable, and the experimental results and human evaluations demonstrate the effectiveness of the proposed system. However, further analysis and exploration are needed to fully evaluate the potential of this approach.","model":"chatGPT","source":"peerread","label":1,"id":2008}
{"text":"The paper \"Learning a Natural Language Interface with Neural Programmer\" addresses the problem of designing a natural language interface (NLI) for programming languages. The authors propose a neural network model that can parse natural language queries and translate them into executable code.\n\nThe authors have several strengths in their study. First, they provide a comprehensive review of related work on NLIs and programming language processing, which demonstrates that their proposed model is a significant contribution to the field. Moreover, the authors present a well-designed neural network architecture consisting of key components (such as sequence-to-sequence model, attention mechanism, and tree traversal algorithm). \n\nOne of the significant strengths of this paper is its thorough evaluation of the proposed model on various benchmarks. The experiments presented in the paper reveal that the model performs well in handling different complexities of natural language queries and can translate them into the corresponding code successfully. The authors also compare their model with other state-of-the-art models, and the results show that their method outperforms those models in terms of accuracy.\n\nHowever, the paper also has some weaknesses. Firstly, the authors did not discuss the limitation of their model, and this can be an area for further improvement. Moreover, the authors tested their model only for specific programming languages, which limits its generalizability to other programming languages. Also, the complexity of generating programs from natural language inputs can further be addressed to enhance the practicality of the system.\n\nIn conclusion, the paper \"Learning a Natural Language Interface with Neural Programmer\" features a promising approach to designing a natural language interface for programming languages. Its strengths lie in its literature review, neural network architecture, and evaluation while the weaknesses are in the limited discussion of limitations and testing specific programming languages. Overall, this paper contributes significantly to the field and provides valuable insights into developing NLIs for programming languages.","model":"chatGPT","source":"peerread","label":1,"id":2009}
{"text":"The paper entitled \"Learning a Natural Language Interface with Neural Programmer\" presents a novel approach to tackle the challenging task of learning a natural language interface for accessing information in databases. The authors propose a weakly supervised end-to-end neural network model that induces programs through an enhanced objective function of the Neural Programmer on a real-world dataset. The paper provides an experimental evaluation of the proposed model, comparing its performance to traditional natural language semantic parser and achieving competitive results.\n\nAs an NLP expert, I find the paper's approach and experiments to be well-motivated and sound. The authors address a crucial problem in natural language processing, making it easy for non-experts to access the data stored in databases by asking simple questions. The challenge of learning a natural language interface for database tables is a challenging problem that requires deep language understanding and multi-step reasoning. \n\nOne of the strengths of this work is the use of the Neural Programmer model, which is designed to handle discrete operations in a natural language. The model works by taking natural language questions and learning the program that is used to retrieve the answer from a database. \n\nMoreover, the authors use weak supervision of question-answer pairs, which is more efficient than traditional methods that require domain-specific grammars, rules, or annotations. The experiments conducted show that a single Neural Programmer model achieves 34.2% accuracy, while an ensemble of 15 models achieves 37.7% accuracy. These results are competitive with the current state-of-the-art achieved by traditional semantic parsers.\n\nOverall, the paper is well-written and presents a novel approach to the challenging problem of learning a natural language interface for database tables. The results of the experiments are promising, and the proposed model could have significant practical applications in the future. I recommend this paper for publication in its current form.","model":"chatGPT","source":"peerread","label":1,"id":2010}
{"text":"Overview:\n\nThe paper presents a novel approach to tackling the problem of natural language interface for database tables by inducing programs on a real-world dataset. By enhancing the objective function of Neural Programmer, the authors apply it on a natural language question-answering dataset, WikiTableQuestions. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction.\n\nStrengths:\n\nThe paper addresses a challenging problem of deep language understanding and multi-step reasoning for learning a natural language interface for database tables. The approach is novel and unique in that it does not require domain-specific grammars, rules and annotations that are typically required in other approaches. The fact that the model achieves a competitive state-of-the-art accuracy is impressive given that it uses only 10,000 examples with weak supervision. The application of the Neural Programmer model on WikiTableQuestions and the results obtained demonstrate the effectiveness and generalizability of the proposed approach.\n\nWeaknesses:\n\nThe paper could be improved by providing more details on the design and structure of their proposed technique. The authors have not provided enough information on the neural network architecture and how they have enhanced the objective function of Neural Programmer. They could have also provided a more detailed discussion on the limitations of the proposed approach and future directions for further exploration. Overall, the paper is innovative and well-written, which provides insights into the feasibility of learning a natural language interface for database tables using weakly supervised, end-to-end neural network models. \n\nRecommendations:\n\nI recommend accepting this paper for publication with minor revisions. The authors should provide more details on the neural network architecture and the enhancements made to the objective function of the Neural Programmer. Additionally, the authors should expand the discussion section to include the limitations of the proposed approach and highlight future directions for research in this area.","model":"chatGPT","source":"peerread","label":1,"id":2011}
{"text":"Title: Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU\n\nSummary: \n\nThe paper proposes a novel algorithm, Asynchronous Advantage Actor-Critic (A3C), to enable reinforcement learning on a Graphics Processing Unit (GPU). The authors demonstrate the capabilities of the A3C algorithm by applying it to Atari 2600 games and providing empirical evidence of its superior speed and effectiveness in learning compared to other similar algorithms.\n\nReview:\n\nThe paper is well-written and presents a clear description of the proposed algorithm, A3C. The authors provide a comprehensive background on reinforcement learning and its challenges, followed by a detailed explanation of the A3C architecture. The paper includes a good number of figures and diagrams that effectively illustrate the algorithm and its implementation. \n\nThe authors provide a detailed comparison of A3C with other reinforcement learning algorithms, demonstrating its efficacy in overcoming several limitations of existing algorithms. This comparison establishes the relevance of the work and its potential for practical applications.\n\nThe experimental results show that A3C outperformed state-of-the-art algorithms, achieving significant speed-up in learning and improved overall performance in Atari 2600 games. The authors provide sufficient details on the experimental setting and the methodology used to evaluate the algorithm\u2019s performance.\n\nOverall, the paper is well-structured, and the arguments are well-supported by empirical evidence. The authors\u2019 contribution is significant, and the proposed algorithm appears to be a promising solution to overcome the computational bottleneck of reinforcement learning. The paper is highly recommended for publication, with no major technical or methodological issues. \n\nMinor feedback:\n\nThe authors could consider incorporating some additional details on the implementation of A3C on a GPU, as this aspect of the algorithm is key to achieving high-performance computing. Additionally, the authors could provide a more in-depth discussion of the limitations of A3C and the potential directions for future research in this area.","model":"chatGPT","source":"peerread","label":1,"id":2012}
{"text":"The paper \"Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU\" explores the use of asynchronous advantage actor-critic (A3C) reinforcement learning to improve the performance of deep neural networks on a GPU. The authors address the issue of slow convergence during reinforcement learning and propose the use of A3C to optimize the learning process on a parallel structure, which is facilitated by GPU technology.\n\nOne of the major strengths of this paper lies in the comprehensive explanation of the A3C algorithm and its practical application on a GPU. The authors provide a detailed explanation of the neural network architecture, hyperparameters, and experimental setups used to conduct the experiments. Furthermore, the authors demonstrate experimentally that their method significantly outperforms other deep reinforcement learning methods in terms of both performance and convergence speed.\n\nHowever, a potential weakness of this paper is the limited analysis of the reasons behind the improvement in performance and convergence speed. While the authors claim that using a parallel A3C structure on a GPU significantly improves the convergence rate, they do not provide a detailed analysis of the factors that contributed to this improvement. Furthermore, the authors use only one type of environment to test their model, which limits the generalizability of their findings.\n\nOverall, this paper presents an interesting and well-executed approach to improving reinforcement learning performance and convergence speed through asynchronous advantage actor-critic techniques on a GPU. Despite some minor limitations, the authors provide a useful contribution to the understanding of efficient reinforcement learning algorithms with practical applications.","model":"chatGPT","source":"peerread","label":1,"id":2013}
{"text":"Title: Peer review of Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU\n\nIntroduction:\n\nThe paper proposes a hybrid CPU\/GPU version of the state-of-the-art method in reinforcement learning, Asynchronous Advantage Actor-Critic (A3C). The authors analyze the computational traits of the existing A3C algorithm and identify the aspects critical to leveraging the GPU\u2019s computational power. They introduce a system of queues and a dynamic scheduling strategy, which can potentially benefit other asynchronous algorithms as well. The hybrid CPU\/GPU implementation of A3C based on TensorFlow achieves a significant speedup compared to the CPU implementation.\n\nStrengths:\n\n1. Novelty: The authors introduce a novel method that leverages GPU's computational power to speed up the A3C algorithm. Although several studies have attempted to parallelize the A3C algorithm using GPUs, the proposed method presents a hybrid CPU\/GPU implementation with a unique approach.\n\n2. Impact: The proposed approach has practical implications in various gaming tasks where the A3C algorithm is used as the state-of-the-art method in reinforcement learning.\n\n3. Open access: The authors have made their implementation publicly available, making it easier for other researchers to achieve faster learning performance in their controlled experiments.\n\nWeaknesses:\n\n1. Evaluation: Although the authors have compared the performance of the proposed method with a CPU implementation, the evaluation only includes a few games (Breakout, Pong, and Space Invaders) and does not compare with other parallel implementations of A3C.\n\n2. Complexity: The proposed method introduces a system of queues and a dynamic scheduling strategy, which may be complex and difficult for other researchers to implement.\n\n3. Reproducibility: While the authors have made their implementation publicly available, it may be difficult for researchers to reproduce the experiments due to possible variations in hardware and software configurations. \n\nConclusion:\n\nThe proposed method in this paper presents a novel hybrid CPU\/GPU implementation of the A3C algorithm that achieves a significant speedup compared to the CPU implementation. The study highlights the importance of leveraging the GPU's computational power in reinforcement learning and introduces a system of queues and a dynamic scheduling strategy that can potentially benefit other asynchronous algorithms. The authors have made their implementation publicly available, which is commendable. However, the evaluation could have been more extensive and comparative to other parallel implementations of A3C.","model":"chatGPT","source":"peerread","label":1,"id":2014}
{"text":"no cost.\n\nPeer Review:\n\nThe paper \"Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU\" by the authors, introduces a hybrid CPU\/GPU version of the Asynchronous Advantage Actor-Critic (A3C) algorithm for reinforcement learning. The paper analyzes the computational advantages of leveraging the GPU's power for the A3C algorithm and also introduces a system of queues and dynamic scheduling for other asynchronous algorithms. The CPU\/GPU version of A3C achieves significant speed up compared to a CPU implementation, and the source code is made publicly available for further research.\n\nStrengths:\nThe paper is well-structured and clear in its presentation, and the authors achieve their objective of optimizing the computational efficiency of the A3C algorithm by using a hybrid CPU\/GPU implementation. They also introduce an innovative dynamic scheduling strategy that can potentially be used for other asynchronous algorithms. In addition, the authors provide thorough analysis and evaluation of their implementation, demonstrating the superiority of the hybrid CPU\/GPU version over the CPU-only version.\n\nWeaknesses:\nAlthough the authors present their work with good clarity, they do not provide comprehensive comparisons with other existing optimization techniques. Some of the analysis and evaluation focus on only one baseline implementation, which raises questions about the generalization of their findings. Additionally, there is limited discussion on the implications of their findings on the wider research community or the practical applications beyond gaming tasks.\n\nOverall, the hybrid CPU\/GPU version of the Asynchronous Advantage Actor-Critic algorithm presented in the paper is a significant contribution to the field of reinforcement learning. The authors demonstrate a novel way to leverage GPU's computational power, providing promising directions for future research in this area. However, there is still room for further analysis and evaluation, particularly with respect to comparing the proposed method with other existing optimization techniques and testing on non-gaming tasks.","model":"chatGPT","source":"peerread","label":1,"id":2015}
{"text":"Title: DeepCoder: Learning to Write Programs\n\nAuthors: Matej Balog, Alexander L. Gaunt, Lachlan Gunn and Phil Blunsom\n\nAbstract:\n\nThis paper presents DeepCoder, a deep learning model that can write small programs from scratch. The model is trained on a large dataset of input-output examples, and learns how to synthesize code by detecting patterns in the data. We evaluate DeepCoder on a set of programming problems and show that it outperforms previous state-of-the-art models in program synthesis.\n\nReview:\n\nThe paper presents a novel method for program synthesis using deep learning models. The authors have demonstrated the successful implementation of the DeepCoder model, which is able to write small programs by leveraging pattern recognition in large datasets of input-output examples. The evaluation results show that DeepCoder outperforms existing models in program synthesis.\n\nThe paper provides detailed descriptions of the model architecture, the training procedure, and the evaluation metrics. The authors have also analyzed the limitations of their model and provided suggestions for further research in the area of program synthesis.\n\nOverall, the paper is well-written and clearly presents the novel contributions of the DeepCoder model to the field of program synthesis. The authors have provided sufficient experimental results to support their claims, and they have compared their model with existing state-of-the-art models. However, there are a few concerns that need to be addressed regarding the methodology and implementation. \n\nFirst, the data acquisition and preparation process is not thoroughly explained. The authors have mentioned that they used a large dataset of input-output examples, but there is no information about the source or quality of the dataset. It is unclear how the authors selected the data and ensured its correctness. \n\nSecond, the evaluation metrics used in the paper are limited to a few benchmark problems, and it is unclear how well the model generalizes to other programming tasks. There is a need for more rigorous evaluation methods to assess the performance of the model and to compare it with other models in different domains.\n\nIn conclusion, DeepCoder presents a significant contribution to the field of program synthesis, and the paper is well-written and easy to follow. However, there are some concerns about the methodology, evaluation metrics, and generalization of the model that need to be addressed in future research.","model":"chatGPT","source":"peerread","label":1,"id":2016}
{"text":"The paper \"DeepCoder: Learning to Write Programs\" addresses the problem of automating the process of software development. The authors propose a novel approach to programming using recurrent neural networks (RNNs) trained on a large database of programming examples to produce code in response to a natural language input.\n\nOne of the strengths of this paper is its comprehensive evaluation of the proposed approach. The authors conducted experiments using a large corpus of programming problems to demonstrate the effectiveness of their method. They also compared their approach to existing programming systems, including human-written code and state-of-the-art program synthesis systems. Additionally, they performed an analysis of the quality of the generated code, which highlights the strengths of the approach.\n\nOne of the weaknesses of the paper is that it does not provide specific information on the size of the database of programming examples used in training the RNNs. While it is mentioned that the database was large, no specific details were provided. Also, the paper did not consider certain scenarios and edge cases that are often encountered in real-world software development.\n\nOverall, \"DeepCoder: Learning to Write Programs\" is a well-written paper that proposes a novel approach to programming using RNNs. The comprehensive evaluation and comparison with existing programming systems make this paper an interesting read for researchers in the field of natural language processing (NLP) and programming language design. However, further research is needed to address some of the shortcomings identified in this paper.","model":"chatGPT","source":"peerread","label":1,"id":2017}
{"text":"Overall Impression:\nThe paper introduces an innovative approach to solve programming problems by employing deep learning techniques. The proposed method, DeepCoder, trains a neural network to predict properties of the program that generated the outputs from the inputs. The predictions of the neural network help with augmenting search techniques. The research is significant as it can have a vast impact on the programming industry. The results obtained through experimentation compare favorably with other state-of-the-art techniques, making it a suitable candidate for solving programming competition-style problems.\n\nStrengths:\nThe paper is well-structured and easy to follow, with a clear and concise introduction of the problem and the proposed solution. The authors have described the related work in the field, which provides a good context for the study. The citation of the previous works has been done appropriately, and the assumptions and limitations of the method have been mentioned explicitly. The authors have provided a detailed description of the datasets used and the features extracted to train the neural network. The experimental evaluation is thorough, and the results obtained are significant, indicating that the proposed method outperforms other non-augmented baselines and a Recurrent Neural Network approach. \n\nWeaknesses:\nWhile the study is significant, it has certain weaknesses that need to be addressed. The authors have not compared the proposed method with other deep learning techniques. It would be interesting to know how DeepCoder compares with other deep learning-based approaches that solve similar problems. Additionally, the performance of the model on the datasets may not generalize well to all programming problems as it is limited to certain types of programming problems. Furthermore, the study lacks a detailed analysis of the interpretability of the predictions of the model; the authors could improve the manuscript by providing a detailed examination of the explanations provided by the model for the predictions it generates. \n\nConclusion:\nOverall, the study presented in the paper is significant and provides a promising approach to solving programming problems using deep learning methods. The proposed method is novel and outperforms other non-augmented baselines and a Recurrent Neural Network approach. Nevertheless, the authors could improve their work by addressing the weaknesses in the study, such as providing a comparison with other deep learning techniques and explaining the predictions of the model.","model":"chatGPT","source":"peerread","label":1,"id":2018}
{"text":"The paper \"DeepCoder: Learning to Write Programs\" addresses the problem of solving programming competition-style problems using deep learning. Specifically, the authors propose a method that trains a neural network to predict the properties of the program that generated the outputs from the inputs, and uses those predictions to augment search techniques from the programming languages community, such as enumerative search and an SMT-based solver. The authors empirically evaluate their approach and compare it against strong non-augmented baselines and a Recurrent Neural Network approach.\n\nOne strength of this paper is that it offers a novel approach for solving programming problems using deep learning. The authors clearly articulate the motivation for their work and provide a comprehensive evaluation of their proposed methods. The experimental results demonstrate the effectiveness of their approach, especially when compared against strong baselines.\n\nHowever, there are also some weaknesses in this paper. The authors do not fully explore or discuss the limitations of their proposed method, which leaves some unanswered questions. Additionally, the paper could benefit from a more detailed discussion of related work in this area, as well as further exploring the implications and potential applications of their research.\n\nOverall, \"DeepCoder: Learning to Write Programs\" offers a promising approach for solving programming problems using deep learning, and demonstrates significant improvements over existing methods. With further development and exploration, this approach may have important applications in the field of programming and computer science.","model":"chatGPT","source":"peerread","label":1,"id":2019}
{"text":"The paper \"Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks\" presents an interesting approach to reinforcement learning in stochastic systems using Bayesian neural networks (BNNs). The authors propose a method that incorporates model uncertainty in the learning process by considering both the prediction of the next state and the policy selection as a joint problem.\n\nThe paper is well-organized and the methodology is clearly described. The authors provide a comprehensive introduction to the field of reinforcement learning, explaining the challenges that arise when dealing with stochastic systems and discussing the limitations of existing methods. The proposed approach is motivated by the desire to overcome these limitations by modeling the uncertainty inherent in stochastic systems with BNNs.\n\nThe experimental results presented in the paper are compelling, demonstrating that the proposed method outperforms existing approaches on a variety of benchmarks. The authors also perform a thorough sensitivity analysis to demonstrate the robustness of the proposed approach.\n\nDespite these strengths, there are a few areas that could be improved. Firstly, the authors could provide more detail in the description of the methodology. While the paper is generally clear, some technical details could be further elaborated. Additionally, there are a few typos and grammatical errors that could be addressed in a revision.\n\nOverall, \"Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks\" is a valuable contribution to the field of reinforcement learning. The proposed approach is conceptually sound and the experimental results are convincing. With some minor revisions, this paper has the potential to be an excellent addition to the literature.","model":"chatGPT","source":"peerread","label":1,"id":2020}
{"text":"This paper, \"Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks,\" addresses the problem of learning and policy search in stochastic dynamical systems using Bayesian Neural Networks (BNNs). The authors explore how BNNs can be used to capture uncertainty in the model, which is a critical issue in dynamical systems with stochastic dynamics. The paper presents theoretical analysis, simulation studies, and real-world experimental results demonstrating the efficacy of their approach.\n\nThe strength of this paper lies in its presentation of an efficient and scalable Bayesian reinforcement learning algorithm that can solve complex problems with high-dimensional state and action spaces. The authors introduce the use of stochastic gradient Markov Chain Monte Carlo for decentralized training of BNNs, allowing for scalable inference of high-dimensional parameter spaces. The empirical results presented in the paper also demonstrate the practical application and potential of the approach for robotic control problems.\n\nHowever, the paper also has several weaknesses. Firstly, the mathematical notation and derivations could be challenging to understand for readers without advanced mathematical knowledge. Additionally, while the algorithm introduced in the paper is shown to be scalable, the experimental results are limited to only two robotic control problems, and the performance comparison with other reinforcement learning algorithms is not comprehensive enough.\n\nIn summary, this paper presents a novel and promising approach for learning and policy search in stochastic dynamical systems with BNNs. Theoretical analysis, simulation studies, and experimental results suggest that the approach offers potential for solving complex robotic control problems. However, the presentation of the paper could be more accessible to a broader audience, and the empirical evaluation of the algorithm could be more extensive.","model":"chatGPT","source":"peerread","label":1,"id":2021}
{"text":"Title: Peer Review for \"Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks\"\n\nThe presented work contributes to the field of model-based reinforcement learning by using Bayesian neural networks (BNNs) that incorporate stochastic input variables to capture complex statistical patterns in the transition dynamics of stochastic dynamical systems. The authors demonstrate that using this approach, they are able to learn the dynamics of a challenging problem where model-based approaches usually fail, and obtain promising results in real-world scenarios including the control of a gas turbine and an industrial benchmark.\n\nThe paper is well organized and clearly written with a good introduction and comprehensive literature review. The methodology used in the paper is sound and the authors have provided the necessary details to replicate their experiments. They have also compared their approach with other conventional methods and explained the reasons for using specific techniques such as minimizing \u03b1-divergences with \u03b1 = 0.5 to train their BNNs.\n\nThe empirical studies in this paper are impressive and provide evidence that the proposed algorithm performed well on the challenging problem and in real-world scenarios. However, the authors could have provided more insights into the interpretability of the learned dynamics from the BNNs. Furthermore, they could have explored the scalability of their approach and the impact of different levels of model complexity on the performance of the algorithm.\n\nOverall, this paper presents a valuable contribution to the field of model-based reinforcement learning, and it is recommended for publication in its current form with some minor revisions as mentioned above.","model":"chatGPT","source":"peerread","label":1,"id":2022}
{"text":"The paper titled \"Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks\" addresses the problem of policy search in stochastic dynamical systems using a model-based reinforcement learning approach. The authors have proposed an algorithm that presents a new technique of modeling the system dynamics using Bayesian neural networks (BNNs) that also capture the stochastic input variables. This approach allows the authors to capture complex statistical patterns in transition dynamics, such as multi-modality and heteroskedasticity, which cannot be detected with alternative modeling techniques.\n\nThe authors have illustrated the performance of their method by solving a challenging problem where other model-based approaches usually fail. Additionally, they have also obtained promising results in real-world scenarios, including the control of a gas turbine and an industrial benchmark. \n\nStrengths:\n\n1. The authors have proposed a novel approach to policy search in stochastic dynamical systems using Bayesian neural networks. This technique enables the portrayal of complex statistical patterns, which are usually missed by other modeling approaches, making it highly plausible for real-world implementations.\n\n2. The authors have demonstrated the efficacy of their proposed approach by solving challenging problems where other model-based approaches failed.\n\n3. The authors have provided a detailed experimental analysis, making it easier for readers to understand the experimental setting and to replicate the results.\n\nWeaknesses:\n\n1. The paper could have been more precise in explaining the technical details of Bayesian neural networks and the $\\alpha$-divergence measure.\n\n2. The authors could have an elaborated discussion about the limitations of their approach.\n\nOverall, this paper is well-written, and the proposed method appears to be promising in solving complex real-world issues. The experiments conducted in this work illustrate the effectiveness of the proposed algorithm. Therefore, I recommend this paper for publication in the target journal with minor revisions.","model":"chatGPT","source":"peerread","label":1,"id":2023}
{"text":"Title: Variable Computation in Recurrent Neural Networks\n\nAuthors: [authors\u2019 names]\n\nJournal: [journal name]\n\nDate of submission: [date]\n\nDate of review: [date]\n\nReview:\n\nThe authors present an interesting and innovative approach to overcome the limitations of recurrent neural networks (RNNs) in managing variable length inputs. They propose a new architecture that takes into account the varying length of input sequences and assigns computation resources accordingly. The idea of dynamically allocating computation resources based on the length of input sequences is unique and has great potential for improving the accuracy and efficiency of RNNs.\n\nThe proposed approach uses a gating mechanism to determine which portions of the input sequence require the most computation resources. This is achieved by introducing two additional intermediate hidden states in the RNN architecture that are used as gating signals to dynamically control the flow of information through the network. The authors also introduce new learning mechanisms to train the proposed architecture and show experimental results that demonstrate the effectiveness of their approach in various applications.\n\nThe paper is very well-written and the authors make a compelling case for their approach. The experiments are thorough and the results presented are convincing. The authors also provide a detailed discussion of the limitations and potential future directions of their work, which enhances the overall value of the paper.\n\nOverall, this paper makes a significant contribution to the field and has the potential to inspire further research on variable computation in RNNs. I recommend its publication after minor revisions. Specifically, the authors should provide more detailed explanations of the architectural design and the learning mechanisms used in the proposed approach, to assist readers in understanding the technical details of the method. Additionally, the authors should include a comparison of their approach to other state-of-the-art methods in the field to demonstrate the effectiveness and competitiveness of their method.","model":"chatGPT","source":"peerread","label":1,"id":2024}
{"text":"The paper \"Variable Computation in Recurrent Neural Networks\" addresses the problem of efficiently computing and updating the hidden states in recurrent neural networks (RNNs), which are widely used for a variety of natural language processing tasks. Specifically, the paper proposes a novel approach that allows RNNs to compute and update only the necessary part of the hidden states that is relevant to the input, rather than computing the entire hidden state vector.\n\nOne of the major strengths of the paper is its originality and significance. The proposed approach is a promising way to greatly improve the efficiency of RNNs without sacrificing accuracy, which can enable faster and more effective natural language processing applications. The paper also provides a detailed description of the proposed approach and presents a thorough experimental analysis to demonstrate its effectiveness on various benchmark datasets.\n\nHowever, there are also some weaknesses of the paper that should be taken into account. The paper lacks a thorough comparison with related works in the field, which could provide a better understanding of the novelty and contribution of the proposed approach. Additionally, the paper could benefit from a more detailed explanation of the theoretical foundations behind the proposed approach and the underlying assumptions.\n\nOverall, the paper \"Variable Computation in Recurrent Neural Networks\" presents a novel and promising approach for improving the efficiency of RNNs for natural language processing, but would greatly benefit from further elaboration and comparison with related work.","model":"chatGPT","source":"peerread","label":1,"id":2025}
{"text":"Peer Review for \"Variable Computation in Recurrent Neural Networks\"\n\nThis paper explores a modification to existing recurrent neural network (RNN) units, which allows them to vary the amount of computation performed per time step in response to the input sequence. The authors argue that while traditional RNN models consume input features at a constant rate and perform a constant number of computations per time step, this approach can be detrimental to both speed and model capacity when dealing with highly variable information flow found in many types of sequential data. \n\nOverall, the paper is well written and clearly presents the motivation, approach, and experimental results. The contribution of the paper is significant as it provides a novel approach to address an important issue in the field of RNNs. The paper is also well organized with sections that lead smoothly from one topic to the next, making it easy to follow.\n\nOne of the strengths of the paper is the thorough evaluation of the proposed models on several benchmark datasets. The authors show experimentally that their modified RNNs not only require fewer operations, but they also lead to better performance overall on evaluation tasks. This is supported by the extensive experimental results showing that the modified RNN models achieve higher accuracy with fewer computations on all datasets tested.\n\nHowever, a few areas for improvement are identified. First, the authors need to provide more detailed explanations and justifications of some of the design choices made in the experiments. For example, the authors did not explain why they chose to use the specific optimization procedures, training parameters, and hyperparameters. Additionally, the authors may want to elaborate on the potential limitations of their proposed approach and provide some insights into the effect of varying computation on training stability or convergence behavior.\n\nOverall, this is an interesting and well-executed work that provides a valuable contribution to the field of RNNs. The authors' results appear to be significant and can be useful to other researchers and practitioners working in this area. I recommend this paper for publication with minor revisions as mentioned above.","model":"chatGPT","source":"peerread","label":1,"id":2026}
{"text":"The paper \"Variable Computation in Recurrent Neural Networks\" addresses a major problem in recurrent neural networks (RNNs) - the constant rate of input feature consumption and the constant number of computations per timestep which limits the speed and model capacity. The paper introduces a modification to existing recurrent units that enables them to learn to vary the amount of computation they perform at each step without prior knowledge of the sequence's time structure. \n\nOverall, the paper presents some interesting findings and presents this modification to recurrent neural networks in a clear and coherent way. The strengths of this paper include the thoroughness of the experiments and the rigor with which results are reported. The paper presents a range of experiments demonstrating that the introduced method improves the model's overall performance as well as reduces the number of computations required. The experiments are well-designed, and the models and evaluation methods are well-established in the literature.\n\nHowever, the paper's weakness is that the contributions are relatively incremental, and its novelty may not be sufficient to motivate further work. The improvement in performance and computational efficiency is relatively modest, and there are potential practical issues related to implementing the variable computation feature in large-scale systems. Additionally, the paper does not fully address how the variable computation feature can be generalized to other types of sequential data beyond the ones presented in the experiments.\n\nOverall, \"Variable Computation in Recurrent Neural Networks\" presents a useful improvement to recurrent neural networks and adds to the ongoing research on improving the efficiency of machine learning models. However, further work is needed to explore the practical implications of this modification and its generalizability to different types of sequential data.","model":"chatGPT","source":"peerread","label":1,"id":2027}
{"text":"Title: \"Deep Variational Information Bottleneck\"\n\nAuthors: John Doe, Jane Smith\n\nJournal: IEEE Transactions on Neural Networks and Learning Systems\n\nSummary:\n\nThe \"Deep Variational Information Bottleneck\" paper proposes a method to extract compressed representations of high-dimensional data via deep variational autoencoders (VAEs). The paper argues that VAEs, which combine the power of deep learning with probabilistic modeling, are well suited for the challenging task of compressing complex data while still preserving its salient features. The proposed method optimizes a tradeoff between the amount of information retained in the compressed representation and the amount of information about the input data required to generate this representation. The experiments conducted on several image classification datasets demonstrate the effectiveness of the approach in achieving state-of-the-art performance in terms of classification accuracy.\n\nSignificance:\n\nVAEs have become increasingly popular in recent years as a powerful technique for generative modeling and representation learning, but their application to the task of data compression has received less attention. The proposed method provides a novel and effective approach for compressing high-dimensional data while preserving its information content. The paper's contribution is significant and has the potential to inform future research in the field of deep learning and data compression.\n\nStrengths:\n\nThe paper is well-written, and the authors provide a clear explanation of the method and its theoretical underpinnings. The experiments conducted on various datasets demonstrate the effectiveness of the proposed method, and the authors compare it to several baseline methods, providing a thorough evaluation. The results demonstrate that the proposed method achieves state-of-the-art performance in terms of classification accuracy while retaining more information than other methods.\n\nWeaknesses:\n\nThe paper could benefit from more in-depth analysis of the tradeoff between information retention and information required, particularly in the context of real-world data. The authors also do not discuss the computational complexity of the proposed method, which could be of concern when dealing with large datasets.\n\nOverall Assessment:\n\nThe \"Deep Variational Information Bottleneck\" paper is a well-written and significant contribution to the field of deep learning and data compression. The proposed approach is effective and achieves state-of-the-art performance on several image classification datasets. Further research could explore the applicability of the method to other types of data and provide a more in-depth analysis of the computational complexity of the approach. Overall, I recommend this paper for publication in IEEE Transactions on Neural Networks and Learning Systems.","model":"chatGPT","source":"peerread","label":1,"id":2028}
{"text":"The paper \"Deep Variational Information Bottleneck\" addresses the problem of learning compressed representations in deep neural networks. Specifically, the authors propose a novel approach called the Deep Variational Information Bottleneck, which imposes a bottleneck on the information flow between the input and the output layers of a neural network, while still maintaining high accuracy on prediction tasks. By doing so, the network learns a low-dimensional representation of the input that captures its essential features, while discarding extraneous information.\n\nThe paper presents several strengths. First, the authors provide a theoretical justification for their approach, showing that it is closely related to information bottleneck theory. Furthermore, the authors conduct extensive experiments on benchmark datasets, demonstrating that their approach outperforms several state-of-the-art methods in terms of both compression rate and accuracy on prediction tasks. Additionally, the paper provides a comprehensive analysis of the impact of hyperparameters on the performance of the network, which provides insight into how to tune the model for practical applications.\n\nHowever, there are also a few weaknesses of the paper. First, while the authors provide a thorough explanation of the mathematical framework underlying their approach, the text can be challenging to follow for readers without a strong background in information theory. Additionally, while the authors provide a detailed analysis of the impact of hyperparameters, it would be helpful to see more discussion of how their approach compares to other compression methods in terms of computational efficiency and memory requirements. Finally, it would be interesting to see an analysis of the interpretability of the representations learned by the network, which could shed light on how the approach captures essential features of the input.\n\nOverall, the paper \"Deep Variational Information Bottleneck\" makes a valuable contribution to the field of deep learning by proposing a novel approach for learning compressed representations in neural networks. The extensive experimental evaluation and analysis of hyperparameters provide evidence for the effectiveness of the approach, although some improvements in the clarity and scope of the paper could further strengthen the work.","model":"chatGPT","source":"peerread","label":1,"id":2029}
{"text":"Peer review for \"Deep Variational Information Bottleneck\" paper: \n\nThe paper presents a new approach called \"Deep Variational Information Bottleneck\" (Deep VIB), which is based on a variational approximation to the information bottleneck of Tishby et al. (1999) model. The authors use a neural network to parameterize the model and leverage the reparameterization trick for efficient training. The main contribution of this paper is to demonstrate that the proposed method outperforms other forms of regularization in terms of generalization performance and robustness to adversarial attack. \n\nOverall, the paper is well-written and organized. The experiments are well-designed and conducted, and the results are presented clearly. The proposed Deep VIB method is compared with several other approaches, including L2 regularization, Dropout, and batch normalization. The authors also show that Deep VIB can be used to improve adversarial training, which is becoming increasingly important in machine learning applications. \n\nOne potential limitation of this paper is that the experiments are mainly focused on image classification tasks. It would be interesting to see how Deep VIB performs in other domains, such as natural language processing or speech recognition. Additionally, the authors could provide more detailed information on the hyperparameters used in their experiments, as these were not fully specified in the paper.\n\nDespite these limitations, the paper makes a valuable contribution to the field of machine learning. The proposed Deep VIB method has the potential to be a useful tool for improving generalization performance and robustness to adversarial attack, and could be used in a variety of real-world applications. Therefore, I recommend the acceptance of the paper for publication in a peer-reviewed journal.","model":"chatGPT","source":"peerread","label":1,"id":2030}
{"text":"The paper \"Deep Variational Information Bottleneck\" addresses the problem of learning representations from high-dimensional data that capture only the necessary information for the tasks at hand. It builds on the Information Bottleneck (IB) theory proposed by Tishby et al. (1999), which suggests minimizing the mutual information between the input and bottleneck while maximizing the mutual information between the bottleneck and output. The authors propose a variational approximation to IB, which enables parameterizing the IB model using a neural network and utilizing the reparameterization trick for efficient training. They term this method \"Deep Variational Information Bottleneck\" or Deep VIB.\n\nThe strengths of the paper include the clear exposition of the method, the use of recent innovations such as the reparameterization trick and the demonstration of its effectiveness on various classification tasks. The authors provide adequate clarity about the model's detail and the various elements of the training and evaluation criteria. The paper establishes a link between IB and recent ideas of Deep Learning and is expected to generate excellent interest in the NLP and Machine Learning communities.\n\nOne weakness of the paper is the evaluation methodology, where there was a lack of a robustness analysis to reduce the specificity of the task to optimize specific metrics. More addition in the verification of the independence of the model's performance on the selected dataset needs to be provided with more details on how the datasets were chosen, how the experimental setup was done for reusability, and a comparison of the VIB method with other regularization techniques.\n\nOverall, the paper presents an interesting approach to learning compressed and relevant representations for NLP tasks, building on recent advances in deep learning. The proposed method exhibits a large enough improvement over other regularization techniques and has potential implications for a wide range of NLP applications.","model":"chatGPT","source":"peerread","label":1,"id":2031}
{"text":"Title: \"The Neural Noisy Channel: A Probabilistic Framework for Generating Natural Language Adversarial Examples\"\n\nThe paper titled \"The Neural Noisy Channel: A Probabilistic Framework for Generating Natural Language Adversarial Examples\" presents an interesting and relevant approach to generating adversarial examples in natural language processing (NLP) using a probabilistic framework. The authors present an alternative method to existing approaches by introducing noise into the input sequence using techniques from information theory.\n\nOne of the strengths of this paper is its motivation for the importance of adversarial examples in NLP. The authors convincingly argue that generating adversarial examples is crucial for improving the robustness of NLP models, resulting in better performance in real-world scenarios. This is a well-established area of research in computer vision, and the authors introduce a necessary and innovative approach for applying similar concepts to the field of NLP.\n\nMoreover, the experimental results presented in this paper are well-articulated, and the authors demonstrate the effectiveness of their approach to generate adversarial examples in NLP tasks, including sentiment analysis, text classification, and question answering. The authors also discuss the limitations of their approach, which provides insight into potential directions for future research.\n\nOverall, the paper provides an original and significant contribution to the field of NLP, highlights an area of investigation that is necessary for the development of robust NLP models, and presents convincing experimental results. However, the paper would benefit from a more in-depth discussion and analysis of the potential implications of their approach for NLP applications, and how it compares to existing adversarial generation techniques.\n\nTherefore, I would recommend that the paper be accepted for publication after minor revisions.","model":"chatGPT","source":"peerread","label":1,"id":2032}
{"text":"The paper \"The Neural Noisy Channel\" by Li Dong and Mirella Lapata tackles the problem of neural machine translation by proposing a new neural noisy channel model. Specifically, the authors aim to build a more accurate generative model of the target language by incorporating the idea of noisy channel models, which is commonly used in the field of information theory.\n\nThe main strength of this paper is its ability to achieve state-of-the-art results on neural machine translation tasks. The authors conducted extensive experiments on different languages, and demonstrated the effectiveness of the proposed model compared to several competitive baselines. Moreover, the theoretical foundation of the approach is well-motivated and well-explained, making it easy for readers to understand the model.\n\nHowever, there are several weaknesses of the paper that should be addressed. Firstly, there is a need for better explanation of the intuition behind some of the parameters used in the model, such as the use of the inverse square root of the length to normalize the attention weights. Secondly, the paper could benefit from more rigorous analysis of the trade-off between accuracy and efficiency of the model, as the proposed method requires more computational resources compared to some of its competitors. Lastly, although the results are impressive in terms of the BLEU score, further analysis on the quality of the output translations, especially with respect to idiomatic expressions or rare words, would be valuable.\n\nOverall, this paper presents a novel approach to tackle the problem of neural machine translation, and provides promising results. With further refinement and analysis, this model has the potential to become a significant contribution to the field of natural language processing.","model":"chatGPT","source":"peerread","label":1,"id":2033}
{"text":"The paper \"The Neural Noisy Channel\" proposes a novel method for sequence to sequence transduction by formulating it as a noisy channel decoding problem, with the help of Recurrent Neural Networks (RNN). The authors suggest that this approach has certain advantages over direct modeling techniques, as noisy channel models must produce outputs that explain their inputs during both training and inference. The proposed model consists of a source model and a channel model which can be trained with both paired and unpaired samples from the marginal output distribution.\n\nThe authors introduce a latent variable that controls the amount of conditioning sequence the channel model should read in order to generate the subsequent symbol. This variable leads to a tractable and effective beam search decoder. The efficacy of the proposed method is demonstrated through experiments on three different sequence to sequence transduction tasks, namely abstractive sentence summarization, morphological inflection, and machine translation.\n\nOverall, the paper presents a well-motivated and well-executed approach to sequence to sequence transduction. The authors provide a clear discussion of the theoretical underpinnings of the approach and the practical details of their method. The experimental evaluation is thorough and convincingly demonstrates the superiority of the proposed method over direct modeling techniques. \n\nThere are, however, some minor issues that could be addressed in a revision of the paper. Firstly, while the paper makes the case for the advantages of noisy channel models in general terms, it would help to have more concrete examples or case studies to illustrate the theoretical argument. Secondly, the description of the model architecture could be a bit more detailed and clearer in places. Finally, the authors should discuss limitations of their proposed method and suggest some directions for future work. \n\nOverall, this is an interesting and well-presented paper that contributes to the development of sequence to sequence transduction models. Therefore, I recommend this paper for publication, with minor revisions.","model":"chatGPT","source":"peerread","label":1,"id":2034}
{"text":"The paper \"The Neural Noisy Channel\" proposes a novel approach to sequence-to-sequence transduction using a noisy channel decoding framework. The authors use recurrent neural networks to parameterize the source and channel models, and employ a latent variable to control the amount of conditioning the channel model needs to generate subsequent symbols.\n\nThe main strength of the paper is its innovative approach to sequence-to-sequence transduction, as the use of a noisy channel model solves the issues of explaining-away effects that plague direct models. In addition, the authors show that their model can produce outputs that explain their inputs, and can use unpaired samples from the marginal output distribution in their training procedure.\n\nThe experimental results conducted for abstractive sentence summarization, morphological inflection, and machine translation show that the noisy channel model outperforms direct models and that there are significant benefits to be gained from using increased amounts of unpaired output data that direct models cannot easily use. This provides further evidence of the effectiveness of their approach.\n\nOne major weakness of the paper is the lack of discussion around the limitations of their method. For instance, the paper does not address how the noisy channel model could be adapted to handle out-of-vocabulary words, or how the model might handle input sequences with varying lengths. It would have been valuable to see how the model could be extended to accommodate these challenges.\n\nOverall, \"The Neural Noisy Channel\" presents a well-written and insightful paper on an innovative approach to sequence-to-sequence transduction. Although there are limitations to the method, the authors provide compelling evidence of its effectiveness and the benefits it can bring in certain contexts.","model":"chatGPT","source":"peerread","label":1,"id":2035}
{"text":"Title: Dialogue Learning With Human-in-the-Loop\nAuthors: [Insert authors' names]\n\nAs an NLP expert in the field of dialogue systems, I have reviewed the paper \"Dialogue Learning With Human-in-the-Loop\" with great interest. The paper presents an innovative approach to dialogue learning by incorporating a human-in-the-loop system to improve the quality of automated dialogues.\n\nThe paper is well-written and provides a clear explanation of the problem the authors are trying to solve. The authors have also proposed a novel solution to the problem by using a self-learning dialogue system that can adjust its responses based on user feedback received through a human-in-the-loop system. This approach is intended to improve the quality of the automated dialogue system by allowing it to learn from feedback and adapt to the user's needs over time.\n\nThe methodology used in this study is well-designed and thorough. The authors have conducted a series of experiments to evaluate the effectiveness of their approach with promising results. They have also provided a comprehensive comparison with existing dialogue systems and demonstrated that their approach outperforms these systems.\n\nHowever, there are a few areas where I think the paper could be improved. First, while the authors have shown that their approach is effective on a small dataset, it remains to be seen whether it can scale to larger datasets. Additionally, the authors have not discussed the limitations of their approach and the potential challenges that could arise in real-world scenarios.\n\nOverall, I believe that this paper is a valuable contribution to the field of dialogue systems, and the approach proposed by the authors has the potential to improve the quality of automated dialogues in various applications. I would recommend this paper for publication after addressing the minor issues outlined above.","model":"chatGPT","source":"peerread","label":1,"id":2036}
{"text":"This paper, titled \"Dialogue Learning With Human-in-the-Loop\", addresses the problem of developing dialogue agents that can learn and adapt to new situations through interaction with a human. The authors propose a new approach for training dialogue agents that combines supervised learning, active learning, and reinforcement learning with human evaluations to improve the quality of the generated responses.\n\nThe key strength of this paper is its novel approach for dialogue learning, which integrates different learning techniques with a human-in-the-loop. The authors provide a thorough explanation and demonstration of each component of their approach, which adds to the clarity and replicability of their methodology.\n\nHowever, the paper could be improved in terms of its presentation and organization. While the authors provide clear explanations of each approach and technique, the paper lacks a cohesive structure that ties these approaches together. This makes it difficult for the reader to understand how the different components of the proposed approach work in concert with each other.\n\nAdditionally, the authors do not provide enough detail about the data or metrics used in their experiments, which makes it difficult to draw conclusions about the effectiveness of their approach. A more detailed discussion of the experimental setup and results, with appropriate statistical analysis, would help to strengthen the validity of their findings.\n\nOverall, \"Dialogue Learning With Human-in-the-Loop\" makes a valuable contribution to the field of dialogue agent development, and its approach has potential for further research in this area. However, the authors could improve the clarity and organization of the paper, as well as provide more detail for their experimental results, in order to strengthen the validity of their approach.","model":"chatGPT","source":"peerread","label":1,"id":2037}
{"text":"Title: Peer Review for \"Dialogue Learning With Human-in-the-Loop\"\n\nGeneral Comments:\nThe paper presents an interesting approach that explores a new direction for machine learning - learning from human feedback in conversational settings. The authors have developed a simulator that enables testing of various aspects of such learning in a synthetic environment and introduced models that work in this regime. The paper concludes with experiments with Mechanical Turk that validate the approach. \n\nOverall, the paper is well-written and well-structured, and the work is promising. The research addresses an important problem and has the potential to drive new directions in conversational agents. However, some important aspects need further improvement, and the authors should address the following issues:\n\nMajor Comments:\n\n1. The authors should provide more empirical results, particularly in the mechanical Turk experiments section. The experiments section does not detail the experiments sufficiently for the reader to understand the dataset and the comparisons with the state-of-the-art baselines.\n\n2. The paper does not mention the ethical and social implications of the proposed approach. The authors should consider discussing the impact of the proposed approach on privacy, security, and accountability.\n\n3. There is a lack of clarity regarding the simulation environment. The authors should provide a more detailed explanation of the environment and how it works. The authors should include a flowchart or a diagram to illustrate the simulation process.\n\nMinor Comments:\n\n1. The introduction section should include a more detailed overview of conversational agents and the challenges involved in building them.\n\n2. The authors should provide more clarity about the motivation for considering the reinforcement learning setting in which the bot improves its question-answering ability from feedback a teacher gives following its generated responses.\n\n3. The authors should clarify why the study only focuses on question-answering and how the approach can be extended to other conversational settings. \n\n4. The authors should check the consistency of the writing style throughout the paper. \n\nOverall, the paper presents an exciting direction for machine learning and conversational agents, and the study has the potential to drive new research in this direction. With some improvements based on the aforementioned comments, the paper can become an even stronger contribution to the field.","model":"chatGPT","source":"peerread","label":1,"id":2038}
{"text":"This paper investigates the problem of dialogue learning with human-in-the-loop in order to improve the question-answering ability of a chatbot. The main contribution of this work is the development of a reinforcement learning framework where a bot can interact with a human teacher, receive feedback, and learn from mistakes. The authors simulate several learning scenarios in a synthetic environment and introduce models that can learn from human feedback. The effectiveness of this approach is validated through real experiments with Mechanical Turk.\n\nThe paper has several strengths. First, the research question is relevant and interesting in the context of developing conversational agents that can interact with humans. Second, the reinforcement learning framework proposed in this paper is well-designed and comprehensive, addressing various aspects of learning with human feedback. Third, the use of a synthetic environment to simulate learning scenarios provides a controlled and reproducible experimental setup for testing the proposed models. Fourth, the experimental results demonstrate that the proposed models are effective in improving the question-answering ability of the chatbot.\n\nDespite these strengths, there are also some weaknesses of the paper that need to be addressed. First, the evaluation of the proposed models is limited to the specific task of question-answering, and it would be interesting to see if the framework can be extended to other tasks such as dialogue generation or natural language understanding. Second, the paper would benefit from a more detailed description of the proposed models, particularly regarding the architecture and implementation of the reinforcement learning framework. Finally, the paper could be improved by providing more discussion on the limitations and future directions of the proposed framework.\n\nOverall, the paper presents a valuable contribution to the field of conversational AI by proposing a reinforcement learning framework for human-in-the-loop learning. With some improvements, this work could have significant practical implications for the development of conversational agents that can learn from human feedback.","model":"chatGPT","source":"peerread","label":1,"id":2039}
{"text":"Title: Adversarially Learned Inference\n\nIntroduction:\nThe paper proposes a novel approach to unifying generative modeling and inference using an adversarial training process. The approach is based on training a generator network to generate samples indistinguishable from a true data distribution, while simultaneously training an inference network to infer latent code from the generated samples.\n\nStrengths:\nOverall, the paper presents an innovative approach to learning generative models that is grounded in solid theoretical foundations. Specifically, the authors provide a clear description of the underlying theory, as well as a thorough presentation of the experimental results that demonstrates the potential of this approach. Additionally, the paper is well written and easy to follow, making it accessible to a broad audience.\n\nWeaknesses:\nWhile the paper is comprehensive, there are a few areas that could benefit from additional clarification. For instance, it may be helpful to provide more detail on the particular inference network architecture that was used, and how it was optimized. Additionally, the paper could benefit from a more detailed discussion of how the proposed approach compares to existing methods in terms of computational efficiency.\n\nConclusion:\nOverall, the paper presents a promising new approach to adversarially learned inference, which has the potential to significantly advance our understanding of generative modeling and inference. With further refinement and additional experimental validation, this approach could prove to be highly useful in a wide range of practical applications. I would recommend this paper for publication, with only minor revisions needed.","model":"chatGPT","source":"peerread","label":1,"id":2040}
{"text":"The paper \"Adversarially Learned Inference\" proposes a new approach for semi-supervised learning that combines both generative models and discriminative models. The paper addresses the problem of how to leverage unlabeled data to improve the performance of a model in a supervised learning task. The authors propose a new framework that utilizes a generative model to learn the underlying distribution of the data and a discriminative model to make predictions on the labeled data. The two models are trained in an adversarial way, where the discriminative model tries to distinguish between the real labeled data and the generated data from the generative model while the generative model tries to fool the discriminative model by generating realistic-looking data.\n\nOne of the strengths of this paper is the novelty of the proposed framework. The authors propose an innovative approach for semi-supervised learning that combines the strengths of both generative and discriminative models. The adversarial training of the two models provides a way to improve the quality of the generated data, leading to better performance on the supervised learning task.\n\nAnother strength of the paper is the thorough experimental evaluation of the proposed approach. The authors test their approach on multiple datasets and compare it with other state-of-the-art methods. The results show that the proposed method outperforms other methods in terms of the accuracy achieved on the supervised learning task.\n\nOne potential weakness of the paper is the lack of a detailed explanation of the underlying architecture of the models used in the experiments. While the authors provide some details on the architecture, a more in-depth description would be helpful to understand the implementation of the proposed approach.\n\nIn summary, \"Adversarially Learned Inference\" presents a promising approach for semi-supervised learning that combines generative and discriminative models in an adversarial way. The proposed approach outperforms other state-of-the-art methods on multiple datasets, making it a valuable contribution to the field of machine learning.","model":"chatGPT","source":"peerread","label":1,"id":2041}
{"text":"Peer Review of \"Adversarially Learned Inference\"\n\nThis paper introduces the adversarially learned inference (ALI) model, which is a generative model that consists of two neural networks, a generation network and an inference network, trained together using an adversarial process. The model is evaluated on the semi-supervised SVHN task and the authors provide evidence that the ALI model produces competitive results.\n\nOverall, I think the paper is well-written and the idea of learning a generation network and an inference network jointly using an adversarial process is interesting. The paper is well-structured and gives a clear description of the model, its components, and the training process. The authors also provide a thorough explanation of the adversarial process and how it works.\n\nOne of the strengths of the paper is the extensive evaluation of the model. The authors provide detailed visualizations of the generated and reconstructed images, which help to demonstrate the model's ability to learn mutually coherent inference and generation networks. The authors also provide comparative results with other approaches on the semi-supervised SVHN task, which shows that the ALI model is competitive.\n\nHowever, one limitation of the paper is that the results are only evaluated on one task (semi-supervised SVHN task), and it is unclear whether the model would generalize to other tasks or datasets. It would have been helpful if the authors had provided additional evaluations on other tasks or datasets to demonstrate the model's generalizability.\n\nAnother limitation is that the paper lacks a clear explanation of the hyperparameters used in the experiments. It would have been helpful if the authors had provided a table or appendix with the hyperparameters used in the experiments, as this information can be useful for researchers who want to reproduce the experiments.\n\nOverall, I believe this paper is a valuable contribution to the field of generative models and the adversarial process. The model is innovative and produces competitive results on the semi-supervised SVHN task. However, additional evaluations on other tasks or datasets and a clear explanation of the hyperparameters used in the experiments would have further strengthened the paper.","model":"chatGPT","source":"peerread","label":1,"id":2042}
{"text":"The paper \"Adversarially Learned Inference\" presents a novel approach to jointly learn a generation network and an inference network using an adversarial process. The main focus of the paper is to introduce the adversarially learned inference (ALI) model and demonstrate its ability to learn mutually coherent inference and generation networks.\n\nStrengths:\n\nThe paper presents a well-motivated and novel approach to learning jointly a generation network and an inference network. The concept of an adversarial game between the two networks and a discriminative network adds depth to the model learning.\n\nThe experiments undertaken in this paper appear to be extensive and demonstrate the advantages of the ALI model over other comparable semi-supervised models such as ladder networks and generative moment matching networks.\n\nThe authors provide evidence that the learned representations using the ALI model can be applied to a range of tasks and that the proposed approach is useful in real-world applications. \n\nWeaknesses:\n\nWhile the paper is well written and the concepts are well explained, there is a lack of comparison with other state-of-the-art models that perform similar tasks, which could limit the potential scope for the ALI model.  \n\nThe paper could be improved by providing more detail on how the authors derived their results, such as by providing specific hyperparameters or architectures for the models used in this study. \n\nOverall, this paper presents a novel and innovative approach to learning a generation network and an inference network using an adversarial process. It offers a new direction for research in neural networks and presents promising results for semi-supervised learning tasks. However, there is room for improvement, such as through providing more detailed and comparative results, to make the study even stronger.","model":"chatGPT","source":"peerread","label":1,"id":2043}
{"text":"Title: Learning through Dialogue Interactions by Asking Questions\n\nThe paper introduces a novel approach to enable machine learning systems to improve by asking questions and receiving feedback in the form of dialogue interactions. The authors propose a question-answering agent that learns to ask questions that improve its understanding of a given problem. The approach is evaluated on two different datasets and the results demonstrate that the proposed approach is effective in improving the performance of the system.\n\nOverall, the paper presents an interesting and potentially useful approach for machine learning. The idea of using dialogue interactions to improve a system's understanding of a problem is compelling and has potential applications in a wide range of domains. The paper is generally well-written and structured, with a clear description of the proposed approach and its experimental evaluation.\n\nThe authors provide a comprehensive evaluation of the proposed approach and compare its performance with baseline methods. However, there are some limitations and potential areas for improvement that could be addressed in future work. First, the datasets used in the experiments are relatively small and limited, which may not fully capture the complexity of real-world problems. Second, the proposed approach relies heavily on the quality of the feedback provided by the human agents, which could introduce bias or noise in the learning process. Third, there is no discussion on the usability or practicality of the proposed approach and how it can be adapted to different domains and applications.\n\nDespite these limitations, the paper presents an interesting and useful approach that has potential for further development and applications. The authors provide a clear and detailed description of the proposed method, which makes it possible for other researchers to replicate and build upon the work. Therefore, I recommend this paper for acceptance for further publication.","model":"chatGPT","source":"peerread","label":1,"id":2044}
{"text":"The paper \"Learning through Dialogue Interactions by Asking Questions\" addresses the question of how language models can learn to ask questions to improve their understanding of a particular task. The paper presents a method for question generation by training a language model to generate questions based on the input text and the task domain. The proposed method is evaluated on a dataset of dialogue interactions, and the results show that the generated questions improve the overall quality of the dialogue.\n\nOne of the strengths of this paper is the clear presentation and explanation of the proposed method. The authors provide detailed descriptions of the training process and the evaluation metrics used, which makes it easier for other researchers to replicate and build upon their work. Additionally, the authors provide a comprehensive evaluation of the proposed method, which includes both quantitative and qualitative analyses.\n\nHowever, there are also some weaknesses in this paper. Firstly, the evaluation of the proposed method is limited to a specific dataset, which may not be representative of all types of dialogue interactions. Further evaluation on different types of datasets could have strengthened the paper. Additionally, the authors do not provide any analysis of the limitations of their proposed method or suggestions for future research.\n\nOverall, this paper presents an innovative approach to language learning through dialogue interactions, and the proposed method demonstrates promising results. However, further evaluation on different datasets is necessary to assess its generalizability.","model":"chatGPT","source":"peerread","label":1,"id":2045}
{"text":"Review of \"Learning through Dialogue Interactions by Asking Questions\"\n\nThis paper investigates the use of questions in dialogue interactions between a learner and a teacher. The authors focus on the movie domain and use a simulator and a set of synthetic tasks to investigate the benefits of asking questions for both offline and online reinforcement learning settings.\n\nThe paper is well-written and the research is well-motivated. The authors clearly explain the importance of dialogue agents that can both respond to and ask questions, and the potential benefits of incorporating questions into the learning process. The research design is also logical and clearly explained, with a focus on developing a comprehensive set of synthetic tasks in the movie domain.\n\nThe results of the experiments are also interesting and indicate that the learner does improve when asking questions. The authors provide a thorough discussion of the potential limitations of their approach and avenues for future research.\n\nOverall, this paper makes a valuable contribution to the field of dialogue agents and the use of questions in the learning process. The research is well-designed and the results are significant. There are a few small issues with clarity in certain sections, but overall the paper is well-written and structured. I would recommend this paper for publication in a reputable venue.","model":"chatGPT","source":"peerread","label":1,"id":2046}
{"text":"The paper \"Learning through Dialogue Interactions by Asking Questions\" addresses the issue of developing a good dialogue agent that can learn from interactions with its users, both by responding to questions and asking questions. The authors explore the potential benefits that could be gained from providing an agent with the ability to ask relevant questions in a reinforcement learning setting. The paper presents a set of synthetic tasks in the movie domain that allow for interactive learning between a learner and a teacher, and evaluates how the learner benefits from asking questions in both offline and online reinforcement learning scenarios.\n\nOne of the strengths of this paper is that it explores an important aspect of interactive learning in the development of dialogue agents. The authors provide a valuable contribution to the field by describing the design of a simulator and a set of synthetic tasks that allow for interactive learning between a learner and a teacher. Furthermore, they provide empirical evaluations that demonstrate the efficacy of their proposed approach.\n\nHowever, there are also some weaknesses that should be addressed. One key weakness is the limited scope of the evaluation, which is limited to a specific set of synthetic tasks in the movie domain. Although this is a good starting point, it is important to investigate the generalizability of the approach to other domains and tasks. Additionally, the evaluation could be improved by including comparison studies with other state-of-the-art methods for interactive learning.\n\nIn summary, the paper \"Learning through Dialogue Interactions by Asking Questions\" is a valuable contribution to the field of interactive learning for dialogue agents. The authors provide a good starting point for future research in this area, but more work is needed to establish the generality and effectiveness of their approach across a broader range of domains and tasks.","model":"chatGPT","source":"peerread","label":1,"id":2047}
{"text":"The paper \"FractalNet: Ultra-Deep Neural Networks without Residuals\" presents a novel approach to constructing deep neural networks without the use of residual connections. The authors argue that their approach, which is based on a fractal architecture, performs better than the state-of-the-art residual-based networks on various image classification tasks.\n\nThe paper is well-written and provides a thorough description of the proposed approach, including the architecture of the fractal network and the training procedure. The authors also provide a detailed analysis of their approach using several benchmark datasets, demonstrating its superior performance compared to residual-based networks.\n\nOne noteworthy aspect of the paper is the use of visualizations to help explain the fractal architecture. These visualizations make the approach more understandable and provide insight into how the network is able to learn hierarchical representations.\n\nHowever, there are a few limitations to the study. First, the authors only compare their approach to residual networks and do not consider other state-of-the-art architectures, such as DenseNet or SqueezeNet. Additionally, the authors do not provide an extensive analysis of the computational complexity of their approach, which is important when considering the practical applicability of the method.\n\nOverall, the paper is a valuable contribution to the field of deep learning and provides an intriguing alternative to residual-based architectures. However, further analysis and comparison with other state-of-the-art methods is necessary to fully evaluate the effectiveness of the proposed method.","model":"chatGPT","source":"peerread","label":1,"id":2048}
{"text":"The paper \"FractalNet: Ultra-Deep Neural Networks without Residuals\" by Larson et al. addresses the problem of building ultra-deep neural networks without using residual connections. Residual connections have been popular for building very deep networks, but they also add complexity and require careful management in training. FractalNet, on the other hand, presents a way of building extremely deep networks by repeat application of a simple module.\n\nOne of the strengths of the paper is the thoroughness of the authors' experimentation with FractalNet. The experiments show that FractalNet can achieve state of the art results on several image classification tasks with less complexity than comparable architectures using residual connections. Moreover, the use of a simple module allows for easy scaling of the network depth.\n\nA potential weakness of the paper is the lack of investigation into the efficacy of FractalNet on non-image classification tasks. The authors mention a single example of applying FractalNet to a natural language processing task but offer no empirical evaluation. Given that the benefits of FractalNet stem from its ability to handle deep networks, it would be interesting to see how effective it is on tasks that are not solely image-oriented.\n\nAnother weakness is that the paper could benefit from more in-depth analysis of the FractalNet architecture. While the authors describe the architecture and its potential advantages, they do not offer much insight regarding why it works so well in practice. Additionally, it would be useful to understand how the FractalNet architecture compares to other ultra-deep architectures, beyond just the residual connections.\n\nOverall, \"FractalNet: Ultra-Deep Neural Networks without Residuals\" presents an interesting and promising approach to building deep neural networks with less complexity. The paper's strength lies in its thorough experimentation and state-of-the-art results, but it could benefit from investigation into non-image classification tasks and deeper analysis of the architecture.","model":"chatGPT","source":"peerread","label":1,"id":2049}
{"text":"This paper presents an interesting design strategy for constructing ultra-deep neural networks, which are based on fractal macro-architecture. The authors employ a simple expansion rule to create truncated fractals that consist of interacting subpaths of different lengths, without the use of residual connections. The authors further demonstrate that their fractal networks can achieve comparable performance to standard residual networks on CIFAR and ImageNet classification tasks.\n\nThe study's results signify that residual connections may not be necessary for attaining high accuracy on challenging image recognition tasks. Instead, it is critical to transition from shallow to deep during the network training process. The authors also note that their approach has some similarities with student-teacher behavior.\n\nHowever, the paper could benefit from several improvements. First, it would be helpful to provide a more in-depth explanation of how the fractal networks' structure affects the model's performance. Second, it would be interesting to see experiments that compare fractal networks' performance with that of other state-of-the-art network architectures. Lastly, the paper could benefit from a more thorough evaluation of the proposed regularization technique, drop-path, and its impact on model performance.\n\nOverall, the paper presents a unique perspective on the design of ultra-deep neural networks and offers compelling experimental results. With the suggested improvements, this study has the potential to make significant contributions to the field of deep learning.","model":"chatGPT","source":"peerread","label":1,"id":2050}
{"text":"The paper \"FractalNet: Ultra-Deep Neural Networks without Residuals\" by Larsson et al. presents a new design strategy for neural network macro-architecture based on self-similarity. This approach generates deep networks with precisely truncated fractals, containing interacting subpaths of different lengths, but without any residual connections. The authors argue that residual connections may not be fundamental to the success of extremely deep convolutional neural networks, and instead, their ability to transition from effectively shallow to deep is a key factor. The paper presents experiments in which fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks.\n\nStrengths:\n\nOne of the strengths of this paper is that the proposed approach is based on a simple expansion rule that generates deep networks with fractal-shaped architecture without the need for residual connections. This is an innovative approach that challenges the commonly held view that residual connections are essential for training ultra-deep neural networks.\n\nThe paper also presents experimental results that show the performance of fractal networks is comparable to that of residual networks on CIFAR and ImageNet classification tasks, and that the anytime property of fractal neural networks allows for quick and more accurate answers.\n\nThe authors also introduce a regularization technique called drop-path, which allows for the extraction of high-performance fixed-depth subnetworks. This is an interesting extension of the dropout technique and shows the potential of fractal networks for network compression.\n\nWeaknesses:\n\nOne weakness of the paper is that it does not provide a detailed comparison of the performance of fractal networks with other state-of-the-art deep neural networks, such as DenseNet or MobileNet. While the authors compare their approach with residual networks, it would have been useful to see how the proposed approach compares to other architectures.\n\nAnother weakness is that the paper does not discuss the computational costs of fractal networks in comparison to residual networks. While the authors briefly mention that fractal networks are computationally efficient and require less storage space, a more detailed analysis of the computational costs would have been helpful.\n\nOverall, the paper presents a novel approach for designing ultra-deep neural networks based on fractal architecture that achieves comparable performance to residual networks. The proposed regularization technique, drop-path, is also a notable contribution that could have wider applications beyond the fractal network architecture. However, further comparative analysis and investigation of the computational costs of the proposed approach would enhance the contribution of this work.","model":"chatGPT","source":"peerread","label":1,"id":2051}
{"text":"Title:\nRevisiting Classifier Two-Sample Tests\n\nAbstract:\nIn this paper, the authors propose a new method for comparing two distributions of labeled data using supervised classifiers. They revisit the existing approach of classifier two-sample tests and propose using the maximum mean discrepancy (MMD) as a metric for testing the difference between distributions. They also introduce a novel method called the Residual Bootstrapping MMD (RB-MMD) which provides improved performance over the traditional MMD approach. The authors demonstrate the effectiveness of their proposed method through experiments on both simulated and real-world data sets.\n\nGeneral Comments:\nThe paper presents an interesting approach for comparing two labeled data distributions using supervised classifiers. The use of MMD as a metric for testing differences between the distributions is well-motivated and the proposed RB-MMD method is shown to provide improved performance over the traditional MMD approach. The experiments conducted on simulated and real-world data sets demonstrate the effectiveness of the proposed method.\n\nSpecific Comments:\n- Introduction: The introduction provides a clear and concise overview of the problem being addressed, and the motivation behind the proposed approach. However, the authors can provide more context on the importance of comparing two labeled data distributions in various fields (e.g. natural language processing, computer vision) and how the proposed method can contribute to those fields.\n\n- Methodology: The section on methodology is well-written and provides a detailed description of the proposed approach. It would be helpful if the authors could provide more explanation on how the RB-MMD method provides improved performance over the traditional MMD approach.\n\n- Experiments: The experiments presented in the paper are thorough and well-designed. However, some of the figures are not clearly labeled and would benefit from additional explanation in the figure caption. Furthermore, the authors can provide a more detailed discussion on the practical implications of the results obtained from the experiments.\n\n- Related Work: The related works section can be expanded to include more recent and relevant literature on comparing two labeled data distributions using supervised classifiers.\n\nOverall, the paper presents a well-motivated and interesting approach for comparing two labeled data distributions using supervised classifiers. The proposed RB-MMD method is shown to provide improved performance over the traditional MMD approach through experiments on simulated and real-world data sets. However, the authors can provide more context on the importance of comparing two labeled data distributions in various fields and expand the related works section to include more recent and relevant literature.","model":"chatGPT","source":"peerread","label":1,"id":2052}
{"text":"The paper \"Revisiting Classifier Two-Sample Tests\" addresses the problem of assessing the performance of two classifiers with different distributions. The authors propose a novel method to compare the accuracy of two classifiers based on their predicted outputs.\n\nOne of the strengths of this paper is that it describes a new method for classifier two-sample tests that is more effective than previously used methods. The proposed approach is based on comparing the predicted outputs of two classifiers rather than the actual labels. The authors demonstrate the superiority of their approach by comparing it against existing methods using simulated and real datasets.\n\nAnother strength of this paper is that it provides clear and concise descriptions of the proposed method and the experiments. The authors also provide sufficient experimental results to support their claims about the superiority of their approach. Additionally, the authors discuss the limitations of their approach and suggest future research directions to address those limitations.\n\nOne of the weaknesses of this paper is that the proposed method requires a significant amount of computational resources, which could pose a challenge for practical applications. Additionally, the authors do not provide a thorough analysis of the statistical properties of their proposed method, which is an essential aspect of any statistical method.\n\nOverall, the paper \"Revisiting Classifier Two-Sample Tests\" makes a significant contribution to the field of classifier evaluation by proposing a new approach to compare the accuracy of two classifiers. The strengths of this paper outweigh its weaknesses, and I would recommend its publication with some minor revisions to address its limitations.","model":"chatGPT","source":"peerread","label":1,"id":2053}
{"text":"Overall, the paper \"Revisiting Classifier Two-Sample Tests\" proposes a method for building two-sample tests using binary classifiers, called Classifier Two-Sample Tests (C2ST) and evaluates their properties, performance, and uses. The paper is well-written and organized, with clear explanations and derivations. The authors provide a comprehensive analysis of the theoretical properties of C2ST and compare their performance against various state-of-the-art alternatives in real-world datasets.\n\nThe authors' approach of using binary classifiers to construct the C2ST is intriguing, and it is exciting to see that it provides multiple benefits over other methods. I appreciate that the authors provide a thorough explanation of the construction of the C2ST and how it works, which makes it accessible to readers with little expertise in machine learning.\n\nOne of the strengths of the paper is the extensive evaluation of the proposed method against various alternatives. The authors show that C2ST performs well compared to many state-of-the-art methods, and it provides interpretable results. The evaluation on generative models is impressive, and the novelty of causal discovery shows the versatility of C2ST. \n\nThe paper could be improved by providing a more in-depth presentation of the interpretation of the predictive uncertainty in identifying where P and Q differ. Furthermore, it would be useful to discuss the limitations of the proposed approach, such as its sensitivity to the choice of classifier architecture and the size of the dataset.\n\nIn conclusion, the paper \"Revisiting Classifier Two-Sample Tests\" presents an exciting and practical approach for constructing two-sample tests using binary classifiers. The authors' work provides valuable insights into the properties, performance, and use cases of C2ST, and I believe this paper would be of interest to the machine learning and statistics communities.","model":"chatGPT","source":"peerread","label":1,"id":2054}
{"text":"The paper \"Revisiting Classifier Two-Sample Tests\" addresses the problem of assessing whether two samples, $S_P \\sim P^n$ and $S_Q \\sim Q^m$ are drawn from the same distribution. The authors propose a relatively unexplored method to build two-sample tests using binary classifiers. They construct a dataset by labeling $S_P$ as positive and $S_Q$ as negative, and if the null hypothesis \"P = Q\" is true, then the classification accuracy of a binary classifier on a held-out subset of this dataset should remain near chance-level. The aim of this paper is to establish the properties, performance, and uses of \"Classifier Two-Sample Tests\" (C2ST).\n\nThe strengths of this paper lie in its detailed analysis of the theoretical properties of C2ST and its comparative evaluation against state-of-the-art alternatives. The paper provides a comprehensive explanation of the benefits of using C2ST, including its ability to learn a suitable representation of the data on the fly, return test statistics in interpretable units, have a simple null distribution, and allow for the interpretation of where $P$ and $Q$ differ in terms of predictive uncertainty. The authors showcase the use of C2ST to evaluate the sample quality of generative models with intractable likelihoods and introduce a novel application of GANs together with C2ST for causal discovery.\n\nThe weaknesses of this paper are limited. One possible issue is the potential oversimplification of the datasets used for evaluation compared to real-world scenarios. Further, the authors could have provided more information on the computational complexities of C2ST versus alternative methods.\n\nOverall, this is a well-written and well-researched paper that provides valuable insights into the performances, strength, and benefits of C2ST.","model":"chatGPT","source":"peerread","label":1,"id":2055}
{"text":"Overall, the paper on Topology and Geometry of Half-Rectified Network Optimization provides an insightful investigation of the connection between half-rectified networks and topology and geometry. The authors present a range of experiments and analyses that comprehensively demonstrate the potential of this approach.\n\nThe paper is well-organized and clearly written, making it accessible to readers with some background in deep learning and neural network optimization. The authors provide a thorough discussion of related work and provide a detailed explanation of their research methods and results. In particular, the authors do an excellent job of highlighting the importance of topological and geometric features in half-rectified networks and how their findings can be applied to the development of more efficient and effective neural network models.\n\nOne issue that could be addressed in future versions of the paper is the scope of the experiments conducted. While the authors provide a detailed and convincing analysis of their results, the experiments are limited to a specific set of architectures and datasets, and it would be valuable to explore the generalizability of their findings to other domains.\n\nOverall, the paper is a valuable contribution to the field of neural network optimization and deep learning. The clear presentation of the methodology and results makes it a good resource for researchers and practitioners interested in this topic, and the novel insights presented by the authors suggest a range of important avenues for future research.","model":"chatGPT","source":"peerread","label":1,"id":2056}
{"text":"Problem or Question\nThe paper \"Topology and Geometry of Half-Rectified Network Optimization\" seeks to address the challenge of understanding the topology and geometry of neural network optimization. The study focuses on analyzing the behavior of the Half-Rectified Net optimization algorithm, which has emerged as a powerful and widely adopted technique in deep learning applications.\n\nStrengths\nOne of the significant strengths of this paper is the depth of analysis and rigorous mathematical techniques employed in analyzing the Half-Rectified Net algorithm. The authors provide a detailed overview of the algorithm and describe its optimization behavior using the concepts of convex and non-convex optimization. They also use geometric optimization techniques to investigate the properties of the optimization landscape and the role of saddle points, which are crucial in determining the complexity and convergence properties of the algorithm.\n\nThe authors' use of visualizations and examples to illustrate the findings is also commendable. With clear and well-designed graphs, they were able to communicate the complex mathematical concepts succinctly and improve the readability of the paper.\n\nWeaknesses\nOne of the weaknesses of the paper is its lack of experimental validation of the theoretical results presented in the study. While the authors' analysis is thorough and presents convincing arguments, the absence of practical experiments to demonstrate the applicability of their findings to real-world problems limits their ability to generalize the results.\n\nAnother weakness is the limited scope of the study, which only focuses on the Half-Rectified Net algorithm. Although the authors provide some insights into the landscape of the optimization problem, more extensive research is needed to understand the stability and convergence properties of other optimization techniques and how their properties vary across different network architectures.\n\nConclusion:\nIn summary, the paper \"Topology and Geometry of Half-Rectified Network Optimization\" provides a valuable contribution to the understanding of the optimization behavior of deep neural networks. Although the study has some limitations, such as the limited scope and the absence of experimental validation, the authors' rigorous analysis and use of geometric optimization concepts provide important insights into the behavior of the Half-Rectified Net algorithm. Overall, this paper is a valuable resource for researchers and practitioners interested in optimizing neural networks with Half-Rectified Net optimization.","model":"chatGPT","source":"peerread","label":1,"id":2057}
{"text":"The paper \"Topology and Geometry of Half-Rectified Network Optimization\" makes significant theoretical contributions to the understanding of deep neural network optimization. The authors rigorously study the loss surface of deep half-rectified networks and prove that certain conditions on the data distribution and model architecture prevent the existence of bad local minima. The paper's main theoretical contribution is the proof that half-rectified single layer networks are asymptotically connected, and explicit bounds are provided. The authors also address the conditioning of gradient descent, studying the geometry of level sets and introducing an algorithm to efficiently estimate their regularity.\n\nThe paper is well-written and organized, with clear explanations of the theoretical concepts and rigorous proofs presented. The empirical results are also informative and support the theoretical findings. The work contributes significantly to the understanding of deep neural network optimization, and the results have significant implications for practical applications.\n\nThere are, however, several areas that could be improved. While the paper's main contribution is the proof that half-rectified single layer networks are asymptotically connected, it would be helpful to include some discussion of the implications of this result for practical applications. Additionally, while the empirical results are informative, more detailed information about the experimental setup and methodology could be provided. Finally, it would be helpful if the authors could discuss some of the limitations of their approach, and potential directions for future research. Overall, however, this is an impressive and important paper that makes significant contributions to the field.","model":"chatGPT","source":"peerread","label":1,"id":2058}
{"text":"Problem\/question addressed:\nThe paper titled \"Topology and Geometry of Half-Rectified Network Optimization\" aims to explore the loss surface of deep neural networks, which is considered a non-convex, high-dimensional problem. The study focuses on the conditions of the data distribution and model architecture that prevent bad local minima in deep half-rectified networks, a type of neural network often used in practice but whose landscape topology has not been well-understood. In particular, the paper aims to provide a theoretical contribution to prove that half-rectified single layer networks are asymptotically connected and to explore the conditioning of gradient descent through the geometry of the level sets.\n\nStrengths:\nThe paper provides a thorough theoretical analysis of deep half-rectified networks, addressing several important questions related to the loss surface and properties of single-layer networks. The paper's strengths lie in its rigorous mathematical analysis and theoretical contributions that use explicit bounds to reveal the interplay between the smoothness of the data distribution and model over-parametrization. In particular, the results provide a deeper understanding of the landscape topology of deep half-rectified networks, which have not been well-understood previously. Furthermore, the paper presents an algorithm to estimate the regularity of level sets in large-scale networks, indicating a near-convex behavior, which is supported by empirical results.\n\nWeaknesses:\nWhile the paper addresses several important questions related to deep half-rectified networks, the results may not be generalizable to other types of neural networks. The study also relies heavily on theoretical analysis, and the empirical results could be further strengthened by including a broader range of experiments on different datasets and architectures to validate theoretical predictions. The paper could also benefit from additional discussion and interpretation of the empirical results, particularly with respect to practical implications for optimizing deep half-rectified networks.\n\nOverall, the paper provides valuable contributions to understanding the loss surface and properties of deep half-rectified networks. The authors' theoretical analysis and explicit bounds offer a deeper understanding of the network's topology and geometry, providing groundwork for optimizing such networks while avoiding bad local minima. Further empirical validation and discussion of the practical implications of the results could strengthen the paper's impact on the field of neural network optimization.","model":"chatGPT","source":"peerread","label":1,"id":2059}
{"text":"The paper \"Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning\" presents a new approach to improving the learning speed and efficiency of deep reinforcement learning agents. The authors propose a new mechanism for fine-grained action repetition, which allows for more effective exploration and more accurate value estimation for states that require a sequence of actions to solve. \n\nIn general, the paper is well-written and thoroughly researched. The authors provide compelling evidence for the effectiveness of their approach, both empirically and theoretically. The experiments are well-designed, with clear evaluation metrics and detailed results, and the discussion of related work is comprehensive and informative.\n\nOne potential concern is the lack of comparison to other state-of-the-art approaches. Although the authors compare their approach to several baselines, including vanilla DQN and A3C, they do not compare their results to other recent work that has focused on improving exploration in deep reinforcement learning. It would be helpful to see how their approach stacks up against recent methods, such as intrinsic curiosity or exploration bonuses.\n\nAnother concern is that the experiments may not generalize to other environments. The paper only tests their approach on two games in the Atari benchmark suite, and it is unclear whether the same results could be achieved in other environments or with real-world tasks. The authors acknowledge this limitation in the discussion, but it would be helpful if they provided more detailed analysis or speculation on how their approach might perform in other domains.\n\nOverall, I believe that the paper makes a valuable contribution to the field of deep reinforcement learning, and the proposed approach has potential for broader applications beyond the Atari suite. With some additional experiments and comparisons to other recent approaches, this paper could significantly impact the state-of-the-art in exploration for deep reinforcement learning.","model":"chatGPT","source":"peerread","label":1,"id":2060}
{"text":"The paper \"Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning\" addresses the problem of achieving better long-term performance in deep reinforcement learning tasks by introducing a fine-grained repetition mechanism. Specifically, the paper proposes a novel approach to enable an agent to learn to repeat specific actions multiple times in succession, which can lead to better exploration and exploitation of the environment.\n\nOne strength of the paper is the introduction of a new and innovative mechanism within the reinforcement learning paradigm. The approach taken by the authors is quite promising and could have a significant impact on several domains where reinforcement learning is applied. The experiments conducted by the authors are thorough and show that their proposed algorithm outperforms existing algorithms on a range of tasks.\n\nHowever, one potential weakness is that the paper focuses solely on the Atari benchmark suite. While this is a widely-used and well-respected benchmark, it is unclear whether the proposed approach can be generalized to other, more complex environments. Additionally, while the experimental results show improved performance, it is not clear how the proposed method scales to real-world applications, especially given the additional computational resources required to implement the fine-grained repetition mechanism.\n\nOverall, the paper presents a compelling and innovative approach to deep reinforcement learning, with promising results on a well-established benchmark. Nonetheless, further studies and experimentation are needed to determine how well the proposed approach generalizes to more complex environments, and whether the computational requirements make it feasible for real-world applications.","model":"chatGPT","source":"peerread","label":1,"id":2061}
{"text":"The paper of \"Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning\" proposes a novel framework called Fine Grained Action Repetition (FiGAR) that enables agents to decide both the action and the time scale of repeating it. The authors demonstrate that FiGAR can be used to improve existing Deep Reinforcement Learning algorithms by enabling temporal abstractions in the action space and implicitly enabling planning through sequences of repetitive macro-actions. \n\nThe paper provides a comprehensive overview of related work and motivations for the proposed approach, which allows readers to understand the research question and contextualize the contributions of the paper. The authors clearly state the significance of their proposed framework and demonstrate the empirical efficacy of this approach in Atari 2600 domain, Mujoco domain, and TORCS car racing domain. The experimental results exhibit the superiority of FiGAR over the state-of-the-art methods, and the authors provide detailed analyses and discussions of the results. \n\nThe methodology proposed in the paper is technically sound, and the experiments are well-designed and conducted with appropriate baselines. The paper also provides intuitive explanations and visualizations of the experimental results, which are helpful for readers to understand and interpret findings. In general, the paper is well-written and clearly presented.  \n\nHowever, some minor issues need to be addressed. Firstly, it would be helpful if the authors can provide more technical details on how FiGAR chooses the optimal action repetition horizon. Secondly, the paper would benefit from a more in-depth discussion of the limitations and future directions of the proposed approach. \n\nOverall, \"Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning\" presents a well-motivated and technically sound approach to enabling temporal abstractions in Deep Reinforcement Learning. The empirical evaluation demonstrates its efficacy in improving the performance of several state-of-the-art algorithms in different domains. This paper is a valuable contribution to the field of Deep Reinforcement Learning, and I recommend it for publication after addressing the minor issues mentioned above.","model":"chatGPT","source":"peerread","label":1,"id":2062}
{"text":"This paper proposes a novel approach called Fine Grained Action Repetition (FiGAR) which enables agents in sequential decision-making tasks to repeat actions over a certain time scale. This is achieved by enabling temporal abstractions in the action space and planning through sequences of repetitive macro-actions, improving Deep Reinforcement Learning algorithms that maintain an explicit policy estimate. The empirical results show that FiGAR outperforms three policy search algorithms on three different domains, namely Atari 2600, Mujoco, and TORCS car racing.\n\nThe strengths of this paper include the novelty of the approach, which has been demonstrated to effectively improve the performance of Deep Reinforcement Learning algorithms across different domains. Additionally, the paper is well-written and the methodology is clearly explained, making it easy for readers to understand the proposed approach.\n\nOne weakness of the paper is the lack of comparison with other relevant approaches in the literature. Although the empirical results show that FiGAR outperforms the three policy search algorithms, it would have been valuable to compare it with other similar approaches. This would have provided readers with a better understanding of the impact and novelty of the proposed approach.\n\nOverall, this paper presents a novel approach that addresses an important problem in Reinforcement Learning. The empirical results show that FiGAR is a promising approach that can effectively improve the performance of Deep Reinforcement Learning algorithms.","model":"chatGPT","source":"peerread","label":1,"id":2063}
{"text":"Title: Loss-aware Binarization of Deep Networks\n\nAuthors: John Doe, Jane Smith\n\nJEL Classification: C45, C63, D81\n\nExecutive Summary:\n\nThe paper presents a new binarization method for deep neural networks using a loss-aware approach. The proposed method is compared against existing binarization techniques and is shown to achieve state-of-the-art performance on various image classification tasks. The authors claim that their approach can reduce the storage and computation costs of deep neural networks while providing comparable performance to full-precision networks. \n\nStrengths:\n\n1. The paper presents a novel binarization method that extends an existing method and achieves superior results on a number of benchmark datasets.\n\n2. The authors provide detailed explanations of the proposed algorithms and its components, making it easy to understand and replicate their results.\n\n3. The experiments performed in this paper are comprehensive, including a detailed comparison with existing binarization methods and a thorough analysis of different factors that can impact the performance of the proposed approach.\n\n4. The authors demonstrate that their method outperforms several state-of-the-art methods by a significant margin on various machine learning tasks.\n\nWeaknesses:\n\n1. While the authors provide a thorough comparison with existing binarization methods, there is no comparison with deep neural networks that use full-precision weights. This limits the conclusion that the proposed approach can provide comparable performance to full-precision networks.\n\n2. The approach is evaluated on image classification tasks only. It would be interesting to see how it performs on other tasks such as object detection or segmentation. \n\n3. The paper could benefit from an analysis of the computational and memory overhead associated with the proposed method.\n\nOverall Assessment:\n\nThe paper provides a novel and well-explained method for binarizing deep neural networks. The authors present comprehensive experimental results and demonstrate that their approach significantly outperforms existing methods. While the paper has some weaknesses that could be addressed in future work, it makes a valuable contribution to the field of binarization of deep neural networks. Therefore, I recommend that the paper be accepted for publication after minor revisions.","model":"chatGPT","source":"peerread","label":1,"id":2064}
{"text":"The paper titled \"Loss-aware Binarization of Deep Networks\" addresses the problem of reducing memory usage and computational complexity in deep neural networks (DNNs) while maintaining their accuracy. Specifically, the paper presents a method for binarizing the weights and activations of DNNs by minimizing the loss function with a soft-binarization approach, resulting in a more compact network.\n\nThe strengths of this paper are that it proposes a novel technique for binarizing DNNs with an emphasis on loss-awareness, which allows for better accuracy retention. The authors compare their method with existing binarization techniques and demonstrate superior performance in terms of model size, speed, and accuracy on benchmark datasets. Additionally, they provide a detailed analysis of the effects of loss-aware binarization on DNNs' various layers and activation functions.\n\nHowever, there are also some potential weaknesses that need further clarification. First, the experiments are only conducted on a limited number of datasets, which makes it difficult to evaluate the proposed method's effectiveness on a larger scale. Also, the paper lacks a thorough discussion of the practical implications and limitations of the proposed method.\n\nOverall, the paper is well-written, and the proposed technique for loss-aware binarization of DNNs is promising. Further research in this area will undoubtedly lead to more effective and practical methods for reducing memory usage and computational complexity in deep neural networks.","model":"chatGPT","source":"peerread","label":1,"id":2065}
{"text":"The paper titled \"Loss-aware Binarization of Deep Networks\" proposes a proximal Newton algorithm that minimizes the loss with respect to the binarized weights. The authors argue that existing binarization schemes ignore the effect of binarization on the loss and are based on simple matrix approximations. The proposed algorithm directly addresses this issue by using second-order information to obtain diagonal Hessian approximation that is more efficient than existing binarization schemes. The authors applied the proposed algorithm on both feedforward and recurrent networks and demonstrated its superiority compared to existing binarization schemes.\n\nThe paper is well written and easy to follow. The motivation behind the proposed algorithm is clear and the experiments are well designed to evaluate its effectiveness. The authors provide extensive experimental results to support their claims. The paper also provides a detailed description of the proposed algorithm and the algorithm's advantages over existing binarization schemes.\n\nHowever, there are several areas that could be improved. First, the authors could provide more details of the experimental setup, especially the pre-processing steps and the initialization setting. Secondly, the authors could include a comparative analysis of the proposed algorithm with state-of-the-art non-binarization algorithms. Finally, the paper could be strengthened by providing more theoretical justification of the algorithm's superiority.\n\nIn summary, the proposed algorithm for loss-aware binarization of deep networks is an interesting approach that addresses a fundamental issue in existing binarization schemes. The experiments demonstrate the algorithm's effectiveness, and the theoretical basis is well presented. The paper could benefit from additional details and theoretical analysis.","model":"chatGPT","source":"peerread","label":1,"id":2066}
{"text":"The paper \"Loss-aware Binarization of Deep Networks\" proposes a novel binarization algorithm that not only reduces the size of deep neural network models but also directly minimizes the loss with respect to the binarized weights. Existing binarization schemes are based on simple matrix approximations, which ignore the effect of binarization on network loss. The authors have proposed a proximal Newton algorithm that uses a diagonal Hessian approximation to minimize the loss w.r.t. binarized weights. The underlying proximal step has an efficient closed-form solution, and second-order information is obtained from the Adam optimizer's second moments. Their proposed algorithm has been benchmarked on both feedforward and recurrent networks and outperforms existing binarization schemes.\n\nThe strengths of this paper are:\n- The proposed algorithm addresses the existing limitations of other binarization schemes and considers the effect of binarization on network loss, which is an essential aspect previously ignored by other approaches.\n- The authors have also provided a thorough mathematical explanation for their proposed algorithm in the paper.\n- The results presented in this paper demonstrate that the proposed algorithm performs exceptionally well compared to existing binarization schemes.\n- They have shown that the proposed algorithm is robust for wide and deep networks, which is a significant contribution.\n\nHowever, there are some potential weaknesses of this paper, which can be improved in future work:\n- The authors have used only Adam optimizer for all of their experiments. It would have been beneficial if they had compared their proposed algorithm with other optimization methods to evaluate its performance robustness.\n- The authors did not provide a detailed comparison of their proposed algorithm with state-of-the-art pruning methods. Their proposed algorithm could be compared with the recently proposed neural network pruning algorithms to assess its competitiveness.\n- The authors have evaluated their proposed algorithm for feedforward and recurrent networks only. Extending the evaluation on more complex architectures such as convolutional neural networks and transformer models would make the findings more generalizable.\n\nOverall, this paper's contributions take a significant step towards efficient deep neural network model designing and training, and it would be interested to see continued work in this direction.","model":"chatGPT","source":"peerread","label":1,"id":2067}
{"text":"Title: A Review of Energy-based Generative Adversarial Networks\n\nThe paper provides a comprehensive review of energy-based generative adversarial networks (EBGANs), which is an extension of traditional GANs. The authors compared the EBGAN with different GAN models and provided a thorough analysis of its strengths and limitations.\n\nThe introduction is well-written, providing the necessary background on GANs and its challenges. However, some parts of the introduction are too technical and may be challenging for readers not familiar with GANs. The authors did an excellent job of describing the EBGAN architecture and how energy-based models can be used to mitigate some of the challenges of GANs, such as unstable training and mode collapse.\n\nThe authors presented several experiments to evaluate the performance of EBGAN. The results showed that EBGANs can produce high-quality images and generate diverse samples. The authors also discussed how EBGANs can improve upon the quality of generated images compared to conventional GANs.\n\nThe paper has some limitations that can be addressed in future work. For instance, the authors could provide a more detailed discussion of the impact of different hyperparameters on the performance of EBGAN. Additionally, the authors could include a more comprehensive comparison of EBGANs with other generative models, such as Variational Autoencoders (VAEs), PixelCNNs or normal GANs.\n\nOverall, the manuscript is well-written with clear explanations and figures. It provides an informative review of EBGANs, suggesting the model's potential and how it outperforms other generative models. I recommend this paper to researchers, professionals, and students interested in the area of generative models, energy-based models, and deep learning.","model":"chatGPT","source":"peerread","label":1,"id":2068}
{"text":"The paper \"Energy-based Generative Adversarial Networks\" proposes a new approach for training Generative Adversarial Networks (GANs), which is based on energy-based models (EBMs). GANs have shown impressive results in generating realistic images, but they are difficult to train due to their non-convex objective function. By contrast, EBMs allow the use of powerful optimization techniques such as Markov Chain Monte Carlo (MCMC) to learn complex probability distributions.\n\nThe paper presents an algorithm, referred to as EBGANs, which alternates between updating a generator and a discriminator, based on the energy function defined by the generator-encoder pair. The proposed method is evaluated on several benchmark datasets, and it is shown to achieve competitive results in terms of image quality, diversity, and stability, compared to state-of-the-art GAN variants.\n\nOverall, the paper is well-written and provides a compelling argument for the use of EBMs in GAN training. The authors provide a clear motivation for their approach, and they demonstrate its effectiveness through extensive experimentation.\n\nHowever, there are also several weaknesses that need to be addressed. Firstly, the paper lacks a comprehensive theoretical analysis of why EBMs can lead to improved GAN training. Although the authors provide a brief argument for the benefits of EBMs, it would be helpful to have a more rigorous and detailed explanation of this perspective.\n\nSecondly, the paper does not provide a detailed comparison to existing EBM-based models. Although the authors mention some related work, it would be interesting to see a more extensive analysis of their approach in comparison to state-of-the-art EBMs.\n\nFinally, while the proposed method shows competitive performance across several benchmark datasets, it would be interesting to see how it performs on more challenging datasets, such as high-resolution images or videos.\n\nIn summary, the paper presents a promising approach for improving the training of GANs, based on energy-based models. Although there are some weaknesses, the paper provides a compelling case for this approach, and I would recommend it for publication after addressing the aforementioned issues.","model":"chatGPT","source":"peerread","label":1,"id":2069}
{"text":"Peer Review for \"Energy-based Generative Adversarial Networks\":\n\nIn this paper, the authors introduce an Energy-based Generative Adversarial Network (EBGAN) model which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. The generator is trained to produce contrastive samples with minimal energies while the discriminator is trained to assign high energies to these generated samples. This approach allows for the use of a wide range of architectures and loss functions beyond the traditional binary classifier with logistic output.\n\nOne of the main strengths of the paper is the use of an auto-encoder architecture with the energy being the reconstruction error. The proposed EBGAN framework exhibits more stable behavior than regular GANs during training. Additionally, the authors show that a single-scale architecture can be trained to generate high-resolution images.\n\nHowever, there are some limitations to the paper that could be addressed in future work. Firstly, the authors only show one instantiation of the EBGAN framework using an auto-encoder architecture, and it would be interesting to see how the proposed approach works with other architectures and loss functions. Secondly, the paper lacks a thorough comparison to existing GAN models, and it would be helpful to see how the proposed EBGAN model performs against state-of-the-art models on various datasets.\n\nOverall, the authors present an interesting approach to GANs that allows for more flexibility in the choice of architectures and loss functions. The proposed auto-encoder EBGAN framework shows promising results and the paper provides a good foundation for future research in this area.","model":"chatGPT","source":"peerread","label":1,"id":2070}
{"text":"Problem\/Question:\nThe paper introduces the Energy-based Generative Adversarial Network (EBGAN) model with the aim of addressing the issue of instability during training that typical Generative Adversarial Networks (GANs) encounter. The EBGAN model utilizes an energy function-based discriminator and a generator trained to produce contrastive samples with minimal energies.\n\nStrengths:\nOne of the key strengths of this paper is the EBGAN's ability to address the problem of instability during training, which is a well-known issue with traditional GANs. The use of an energy-based discriminator in the EBGAN model provides a stable training environment, enabling the generator to produce high-quality output with less training time. Furthermore, the paper provides a clear and detailed explanation of the EBGAN model, its key components, and their respective roles in training the generator.\n\nAnother strength of the paper is the use of an auto-encoder architecture as a form of EBGAN instantiation that utilizes the reconstruction error as the basis for energy calculation. The experimental results showed that this approach provides a more stable training process than traditional GANs, further demonstrating the effectiveness of the EBGAN model.\n\nWeaknesses:\nOne area of weakness for the paper is the lack of comparison to other methods for addressing the instability issue in GANs, such as Wasserstein GANs or Gradient Penalty GANs. Additionally, the limited experimental results presented in the paper only consider image generation, which may not fully capture the generalizability of the EBGAN model to other applications.\n\nRecommendation:\nOverall, the paper provides a clear and well-explained approach to addressing the instability issue in GANs, which is a significant contribution to the field of generative modeling. However, to strengthen the work, it is recommended to compare the EBGAN model to other state-of-the-art methods for tackling the instability issue in GANs. Additionally, more experimental results on different types of data can be presented to demonstrate the generalizability of the EBGAN model.","model":"chatGPT","source":"peerread","label":1,"id":2071}
{"text":"General comments:\nThis paper on the Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning presents a novel approach to address the domain shift problem in representation learning. The paper is well written and presents a convincing argument for the use of the proposed method. The experiments conducted on various datasets provide a thorough evaluation of the proposed method and demonstrate its effectiveness in achieving domain-invariant representations. Overall, this is a well-researched and well-presented study with important contributions to the field of representation learning.\n\nStrengths:\nThe paper clearly presents a novel approach to address the domain shift problem in representation learning. The proposed method, Central Moment Discrepancy (CMD), is well-motivated and grounded in theoretical principles. The authors provide a comprehensive analysis of the CMD method and explain how it differs from other existing methods. Additionally, the authors conduct extensive experiments on various datasets and evaluate the CMD method against several state-of-the-art methods. The evaluation metrics used are appropriate and the results demonstrate the effectiveness of the CMD method in achieving domain-invariant representations.\n\nWeaknesses:\nWhile the paper clearly presents the CMD method and its advantages, there are some limitations to the study that could be addressed in future work. One limitation is that the paper does not clearly explain why other methods, such as Deep CORAL, fail to achieve domain-invariant representations in certain scenarios. Additionally, the authors could provide more details on the computational complexity and scalability of the CMD method. Finally, the paper could provide more discussion on the practical applications of domain-invariant representations and how the CMD method can be applied in real-world scenarios.\n\nSuggestions for improvement:\nOne area of improvement for the paper would be to provide a more detailed quantitative comparison of the CMD method with other state-of-the-art methods. Specifically, the paper could provide a more thorough analysis of the limitations of existing methods and how the CMD method overcomes these limitations. Additionally, the authors could provide more details on the implementation of the CMD method and any potential challenges that may arise during implementation. Finally, the paper could discuss the potential impact of the CMD method on real-world applications and provide recommendations for future research in this area.\n\nOverall, the paper presents a novel and well-motivated approach to address the domain shift problem in representation learning. The proposed method is well-evaluated and the results demonstrate its effectiveness in achieving domain-invariant representations. The paper is well-written and provides a thorough explanation of the proposed method and its advantages. The limitations of the study could be addressed in future work, but this does not detract from the overall quality of the research presented.","model":"chatGPT","source":"peerread","label":1,"id":2072}
{"text":"The paper titled \"Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning\" addresses the critical issue of domain shift, which is a significant problem in most practical machine learning applications. Domain shift tends to occur when the distribution of the training data differs from the distribution of the testing data, leading to suboptimal performance of machine learning models. Thus, the paper proposes a novel method for domain-invariant representation learning, called Central Moment Discrepancy (CMD), which is claimed to be robust to domain shift in various real-world scenarios.\n\nThe paper's strength lies in its theoretical foundation and empirical evaluation. The CMD is theoretically grounded, and the authors provide a detailed explanation of its formulation and properties. Furthermore, the authors evaluate CMD on several standard benchmark datasets and demonstrate that it outperforms state-of-the-art domain adaptation methods. The authors further demonstrate the generalization capability of CMD across different domains and application scenarios.\n\nHowever, the paper has a few weaknesses that need to be addressed. Firstly, the paper is not well-organized, making it challenging to follow the authors' arguments. Secondly, the empirical validation is not extensive enough to conclude CMD's widespread applicability and generalize to all datasets and applications. A larger-scale evaluation with multiple datasets and additional comparison methods will enable us to draw more robust conclusions.\n\nIn conclusion, the paper \"Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning\" has the potential to make significant contributions to the field of domain adaptation in machine learning. Despite some weaknesses, the paper presents a compelling argument for the effectiveness of CMD and its ability to outperform existing domain adaptation methods on several benchmark datasets. The authors' future work should aim to address the weaknesses highlighted in this review and further demonstrate its effectiveness in real-world applications.","model":"chatGPT","source":"peerread","label":1,"id":2073}
{"text":"Review of \"Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning\"\n\nThis paper proposes a new regularization method, called Central Moment Discrepancy (CMD), to learn domain-invariant representations for domain adaptation with neural networks. Specifically, the authors propose to match the higher order central moments of probability distributions by means of order-wise moment differences, which can be computed efficiently without requiring distance and kernel matrix computations. The authors show that CMD is a metric on the set of probability distributions on a compact interval and prove that convergence of probability distributions on compact intervals w.r.t. the new metric implies convergence in distribution of the respective random variables. The proposed approach is tested on two benchmark datasets for object recognition and sentiment analysis, and achieves better performance compared to existing methods, such as Maximum Mean Discrepancy, Variational Fair Autoencoders, and Domain Adversarial Neural Networks. \n\nOverall, the paper is well written, and the proposed method is novel and well-motivated. The authors provide a clear explanation of the proposed approach and its relationship with existing distribution matching methods. The theoretical analysis of CMD is also rigorous and insightful, providing a solid foundation for the proposed method. The experiments on benchmark datasets demonstrate the effectiveness of CMD in improving domain adaptation performance, and the authors provide a post-hoc analysis of parameter sensitivity, which adds further credibility to the proposed method. \n\nHowever, there are some limitations and questions that need to be addressed. Firstly, the paper lacks a detailed analysis of the computational complexity and scalability of CMD. The authors claim that CMD is computationally efficient compared to existing methods, but it would be helpful to provide a more detailed comparison of the runtime and memory requirements. Secondly, the authors focus on the matching of central moments, and it is unclear whether higher-order central moments provide better discriminative power compared to other types of moments (e.g., raw moments or cumulants). It would be interesting to investigate the performance of CMD with other types of moments and compare them to central moments. Finally, the experiments are limited to two benchmark datasets, and it is unclear how the proposed method would generalize to other domains and tasks. \n\nOverall, I recommend accepting the paper with minor revisions. The authors are encouraged to address the limitations and questions raised in this review, and provide more details and insights in the revised version of the paper.","model":"chatGPT","source":"peerread","label":1,"id":2074}
{"text":"The paper proposes a new regularization method called Central Moment Discrepancy (CMD) for domain-invariant representation learning in the context of domain adaptation with neural networks. The authors aim to match the higher order central moments of probability distributions by means of order-wise moment differences to minimize the domain-specific feature representations directly in the hidden activation space. The authors claim that their approach is computationally efficient and stable, and it achieves state-of-the-art performance on two different benchmark data sets for object recognition and sentiment analysis of product reviews.\n\nThe paper has several strengths. Firstly, the proposed approach is novel and addresses an important problem in machine learning, which is domain adaptation. Secondly, the authors provide a rigorous mathematical foundation for their approach, proving that CMD is a metric on the set of probability distributions on a compact interval, and that convergence of probability distributions on compact intervals w.r.t. the new metric implies convergence in distribution of the respective random variables. Thirdly, the authors conduct experiments on two different benchmark data sets and compare the performance of CMD with other standard methods. The results show that CMD outperforms other methods on both data sets.\n\nHowever, there are also some weaknesses in the paper. Firstly, the authors could have provided more detailed explanations of the CMD approach and how it differs from other standard approaches. Secondly, the authors could have conducted more experiments on different data sets to further evaluate the generalizability of their approach. Thirdly, the authors could have provided more discussion on the limitations of their approach and potential areas for future improvement.\n\nOverall, the paper presents a novel and effective approach for domain-invariant representation learning in the context of domain adaptation with neural networks. The approach is supported by a mathematical foundation and achieves state-of-the-art performance on two different benchmark data sets. However, further research is needed to investigate the generalizability of the approach and its limitations.","model":"chatGPT","source":"peerread","label":1,"id":2075}
{"text":"Title: Entropy-SGD: Biasing Gradient Descent Into Wide Valleys\n\nAuthors: Jian-Guo Zhang, Ioannis Mitliagkas, Christopher R\u00e9\n\nJournal\/Conference: International Conference on Learning Representations (ICLR), 2017\n\nSummary:\n\nThis paper presents a new optimization algorithm named Entropy-SGD. The proposed algorithm is designed to prevent gradient descent from getting trapped in narrow valleys that are suboptimal for convergence. To achieve this, Entropy-SGD introduces a regularization term in the objective function that encourages the gradient directions to be more uniformly distributed across the parameter space. The authors compare the performance of Entropy-SGD with several existing optimization methods, including stochastic gradient descent (SGD), Adam, Adagrad and Adadelta, on various deep learning tasks, and show that in many cases, Entropy-SGD can outperform the other methods in terms of both convergence speed and final accuracy.\n\nStrengths:\n\nThis paper presents a novel optimization algorithm that addresses a common problem in deep learning, namely, getting stuck in suboptimal valleys. The authors provide clear and detailed explanations of the algorithm, and derive the theoretical properties of the regularization term introduced in Entropy-SGD. The experimental results presented in the paper are comprehensive and demonstrate the advantages of Entropy-SGD over other optimization algorithms on various deep learning tasks.\n\nWeaknesses:\n\nThe paper could benefit from a more thorough comparison with state-of-the-art optimization algorithms such as AdamW and LAMB, which have been proposed since the publication of this paper. In particular, it would be interesting to see how Entropy-SGD performs when compared to these newer methods on large-scale deep learning tasks. Additionally, the authors could provide more insights into the mechanism of how Entropy-SGD prevents gradient descent from getting trapped in narrow valleys.\n\nOverall, this is a well-written paper that proposes a promising optimization algorithm for deep learning. The results demonstrate that Entropy-SGD is a competitive method for various deep learning tasks. The proposed algorithm and empirical evaluation provide useful insights into the design and performance of optimization algorithms for deep learning.","model":"chatGPT","source":"peerread","label":1,"id":2076}
{"text":"The paper \"Entropy-SGD: Biasing Gradient Descent Into Wide Valleys\" addresses the problem of optimization in deep learning, specifically the issue of getting stuck in sharp, narrow valleys during the gradient descent process. The authors propose a new optimization algorithm, Entropy-SGD, that introduces entropy regularization to bias the optimization towards larger regions of the loss landscape, thereby avoiding these narrow valleys.\n\nOne strength of this paper is the thoroughness of the experimental evaluation, which includes a comparison with several existing optimization algorithms on a variety of datasets and models. The results demonstrate that Entropy-SGD consistently performs well, achieving better performance and convergence rates than the other algorithms in many cases. The authors also provide clear explanations and visualizations of the impact of entropy regularization on the optimization process, which helps to clarify their approach.\n\nOne potential weakness of the paper is that the theoretical analysis of Entropy-SGD is somewhat limited, with a focus primarily on the impact of entropy regularization on the variance of the gradients. The authors acknowledge this limitation and suggest that further analysis is needed to fully understand the properties of the algorithm. Additionally, while the results are impressive, it is not entirely clear how broadly applicable Entropy-SGD will be to other deep learning tasks and architectures.\n\nOverall, the paper presents a promising new approach to optimization in deep learning that addresses an important problem. The experimental evaluation provides strong evidence of the effectiveness of Entropy-SGD, and the authors' clear explanations and visualizations make the approach accessible to readers. However, further theoretical analysis and more extensive evaluations on a wider range of tasks would help to ensure the applicability and robustness of the algorithm.","model":"chatGPT","source":"peerread","label":1,"id":2077}
{"text":"Review of \u201cEntropy-SGD: Biasing Gradient Descent Into Wide Valleys\u201d\n\nThis paper presents a new optimization algorithm, Entropy-SGD, for training deep neural networks. The proposed algorithm is motivated by the local geometry of the energy landscape, where local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. The authors leverage this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. \n\nThe authors demonstrate that Entropy-SGD has a smoother energy landscape than SGD and achieves improved generalization under certain assumptions. They present experiments on convolutional and recurrent neural networks that show Entropy-SGD outperforms state-of-the-art techniques in terms of generalization error and training time.\n\nOverall, this is a well-written paper with clear explanations of the proposed algorithm and its motivation. The authors adequately explain the intuition behind the proposed objective function and demonstrate its effectiveness through experiments. \n\nHowever, a major limitation of this paper is that the assumptions under which Entropy-SGD achieves improved generalization are quite restrictive. The authors assume that the loss function is Lipschitz continuous, and the weight updates are stable under a certain noise perturbation. While these assumptions may hold in certain cases, it is important to assess the effectiveness of Entropy-SGD under more general conditions.\n\nMoreover, the choice of the Langevin dynamics as a way to estimate the gradient of the local entropy may not be the most efficient one, and there are relatively simple methods that have been proposed in the literature that can be used to obtain unbiased estimates of the gradient of the entropy that may help reduce training time.\n\nTherefore, while Entropy-SGD presents an interesting approach to training deep neural networks, further research is needed to assess its effectiveness under more general conditions and to explore alternative methods for estimating the gradient of the local entropy.","model":"chatGPT","source":"peerread","label":1,"id":2078}
{"text":"This paper entitled \"Entropy-SGD: Biasing Gradient Descent Into Wide Valleys\" addresses the problem of optimizing deep neural networks, which have multiple local optima with a varying level of generalization error. The authors propose a novel optimization algorithm called Entropy-SGD, which leverages the observation that extrema with low generalization error are usually located in large flat regions of the energy landscape, and constructs a local-entropy-based objective function to favor these solutions while avoiding poorly generalizable solutions located in sharp valleys.\n\nThe strength of this paper lies in its novel approach to constructing an objective function that biases gradient descent into wide valleys, which has the potential to improve the generalization performance of deep neural networks. The authors provide a solid theoretical foundation for their approach and conduct thorough experiments on convolutional and recurrent neural networks to demonstrate the effectiveness of Entropy-SGD.\n\nHowever, there are also some weaknesses of the paper. The main limitation is that the proposed method has a higher computational cost compared to standard SGD due to the use of Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. The authors partially address this issue by proposing a parallel implementation of the algorithm, but it still remains as a limitation.\n\nOverall, this paper presents a promising new optimization algorithm called Entropy-SGD for training deep neural networks. It is an interesting and valuable contribution to the field of neural network optimization, particularly in addressing the generalization issue. However, the computational cost and scalability of this approach would require further investigation in future research.","model":"chatGPT","source":"peerread","label":1,"id":2079}
{"text":"Title: Deep Multi-task Representation Learning: A Tensor Factorisation Approach\n\nAbstract:\n\nThe paper presents a novel approach for deep multi-task representation learning using tensor factorisation. The proposed approach employs a deep neural network to form a tensor of the input data and factorises it using a tensor factorisation algorithm to learn shared representations across multiple related tasks. The proposed approach is evaluated on several multi-task learning datasets and compared against state-of-the-art multi-task learning methods.\n\nOverall Assessment:\n\nThe paper presents an interesting and novel approach to tackle the problem of multi-task learning by leveraging tensor factorisation. The proposed method is well-motivated, clearly explained and convincingly demonstrated with the help of sufficient experimentation on multiple datasets. The paper makes a significant contribution to the research in multi-task learning and has the potential to attract the interest of researchers working in this field. \n\nStrengths:\n\nThe paper provides a thorough literature review of recent developments in multi-task learning and explains how the proposed approach complements the existing methods in this area. The proposed approach is conceptually sound, and the authors provide detailed explanations of the employed tensor factorisation algorithm, which is critical for readers to understand the approach. The proposed method is evaluated on multiple multi-task learning datasets, and the experimental results are compared against state-of-the-art methods, which confirms the effectiveness of the proposed approach. \n\nWeaknesses:\n\nThe work could be improved by clearly specifying the hyperparameters used in the experimental evaluation, including the learning rate for the neural network, the number of hidden layers, and other similar parameters used for the tensor factorization algorithm. Lack of explanation for these important decisions could deter other researchers from replicating or building upon the proposed approach. Furthermore, the experimental evaluation could have been strengthened with additional performance comparisons against other related multi-task learning approaches to provide more rigorous assessments. \n\nConclusion:\n\nIn summary, the paper presents an innovative and effective approach for deep multi-task representation learning using tensor factorisation. The proposed method could potentially provide better results for many real-world applications requiring multi-task learning. However, the lack of clear explanation of the hyperparameter choices and comparison against more competitive baselines are the areas that could be improved in future work. Overall, the paper presents a promising direction for future research, and its significant contributions to the field of multi-task learning recommends it to the attention of researchers working in this area.","model":"chatGPT","source":"peerread","label":1,"id":2080}
{"text":"The paper \"Deep Multi-task Representation Learning: A Tensor Factorization Approach\" proposes a novel method for multi-task learning using tensor factorization. The authors address the problem of learning representations for multiple tasks simultaneously, which requires handling inter-dependencies between tasks and identifying common patterns across them.\n\nOne of the strengths of this paper is that the proposed method is well-motivated, and the authors provide a clear description of the key challenges in multi-task learning. They also provide a detailed explanation of tensor factorization and its application to multi-task learning. The experimental results show that the proposed method outperforms several baseline methods on multiple datasets and tasks.\n\nHowever, there are some weaknesses in this paper that need to be addressed. The authors do not provide a thorough comparison of their method with related work on multi-task learning that used alternative techniques. Additionally, the authors do not provide a detailed description of the hyperparameters used in their experiments, which could have a significant impact on their results. Finally, the authors could improve the clarity of their writing and the organization of their experimental results section to make their findings more accessible and interpretable.\n\nOverall, this paper presents a promising new technique for multi-task learning, but there are areas for improvement in the presentation and analysis of the results.","model":"chatGPT","source":"peerread","label":1,"id":2081}
{"text":"The paper titled \"Deep Multi-task Representation Learning: A Tensor Factorisation Approach\" presents a new deep multi-task representation learning framework that learns cross-task sharing structure in deep networks. The authors introduce a new approach that generalizes the matrix factorization technique used by many existing multi-task learning (MTL) algorithms to tensor factorization. This approach allows the automatic learning of end-to-end knowledge sharing in deep networks compared to existing deep learning approaches that require user-defined multi-task sharing strategies. The authors evaluate the efficacy of their approach through experiments that measure the accuracy and design choices for both homogeneous and heterogeneous MTL.\n\nThe paper is well written and is organized in a logical and coherent manner. The authors provide a comprehensive literature review, which helps to establish the novelty and significance of the proposed approach. The paper has a clear introduction, where the research question and objectives are well articulated. The authors provide a detailed description of their approach, including the mathematical formulations and algorithms used to implement the proposed method. The figures included in the paper are informative and well-structured, which helps readers to better understand the authors' approach.\n\nThe authors provide a detailed experimental analysis to evaluate the efficacy of their approach. The experimental results show that their approach delivers higher accuracy than existing MTL algorithms while also requiring fewer design choices. However, the authors could have provided a more extensive qualitative analysis to support their experimental findings.\n\nOverall, the proposed approach in this paper is novel and has the potential to advance the state of the art in MTL. The experimental analysis is compelling and provides evidence of the efficacy of the authors' approach. I recommend this paper for publication in its current form with minor revisions to improve clarity and readability.","model":"chatGPT","source":"peerread","label":1,"id":2082}
{"text":"The problem this paper addresses is the shallow approach that contemporary multi-task learning (MTL) methods employ in the era of deep learning. The authors propose a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. The paper presents an approach that is based on generalising matrix factorisation techniques that are commonly used by conventional MTL algorithms to tensor factorisation, which allows for the automatic learning of end-to-end knowledge sharing in deep networks. The authors argue that their approach is superior to the existing deep learning approaches, which rely on a user-defined multi-task sharing strategy.\n\nOne of the strengths of this paper is the novelty of the proposed approach. The paper fills a gap in the literature by presenting a new method for deep multi-task representation learning that overcomes some of the limitations of existing approaches. The use of tensor factorisation techniques allows for automatic learning of end-to-end knowledge sharing in deep networks, which eliminates the need for a user-defined multi-task sharing strategy.\n\nThe authors do a good job of presenting the theoretical framework and provide a detailed description of the proposed method. They also provide a thorough evaluation of their approach with experiments that demonstrate its efficacy in terms of both higher accuracy and fewer design choices.\n\nOne potential weakness of the paper is that the experiments were conducted on a limited number of datasets. The generalisability of the proposed approach to other datasets and tasks is not clear. Additionally, while the authors present a comparison with some related work, a more extensive comparison with other state-of-the-art methods in the field of deep MTL would have strengthened the paper.\n\nOverall, this paper presents a novel approach to deep multi-task representation learning that has the potential to overcome some of the limitations of existing methods. The authors provide a thorough evaluation of their approach with experiments that demonstrate its efficacy. With the inclusion of a more extensive comparison with other state-of-the-art methods and evaluation on more datasets, this paper has the potential to make a solid contribution to the field of deep multi-task learning.","model":"chatGPT","source":"peerread","label":1,"id":2083}
{"text":"Title: Temporal Ensembling for Semi-Supervised Learning\n\nAuthors: Laine, Samuli and Aila, Timo\n\nThe paper presents a novel approach for improving the performance of semi-supervised learning (SSL) algorithms by introducing a method named Temporal Ensembling (TE). The proposed technique incorporates multiple predictions from a single model for unlabeled samples over different iterations to increase the model's confidence in its predictions. This ensembling aims to address the high variance of the model's prediction, which hinders the SSL model's performance.\n\nThe authors provided an analysis of the proposed approach on benchmark datasets, including CIFAR-10 and SVHN, comparing TE with other state-of-the-art SSL algorithms. The results indicate that the proposed method outperforms existing SSL approaches and achieves competitive performance with fully supervised learning methods. Additionally, the authors analyzed the impact of different hyperparameters on the performance of the TE and provided insightful discussions regarding the possible reasons behind the results.\n\nThe methodology and analysis of the experiments are presented in a clear and concise manner, making the paper easy to understand. The authors provided enough evidence to justify the proposed approach's effectiveness, and the experiments conducted were thorough, including multiple baseline comparisons and hyperparameter tuning.\n\nOverall, the paper provides a valuable contribution to the SSL field and is well-written, with the experimental results being convincing. I recommend this paper for publication in this field.","model":"chatGPT","source":"peerread","label":1,"id":2084}
{"text":"This paper addresses the problem of semi-supervised learning, where the dataset consists of a small number of labeled samples and a large number of unlabeled samples. The authors propose a novel approach called \"temporal ensembling\" to improve the performance of semi-supervised learning.\n\nThe strength of this paper lies in the clear and detailed explanation of the proposed method. The authors have provided a thorough description of the algorithm and the associated mathematical notations, which makes it easy to follow and understand. Additionally, the experimental results are impressive and show that the proposed method outperforms the state-of-the-art techniques on several benchmark datasets.\n\nOne possible weakness of this paper is that the proposed method may be computationally expensive, especially for larger datasets. The authors have not provided any information about the computational cost of their method or the time required for training on different datasets. It would be useful to provide some insights into the scalability and efficiency of the method in future works.\n\nFurthermore, the authors could discuss the limitations of their method and how it can be improved in future work. They also could include a more in-depth analysis of the proposed method's generalization capacity and robustness across different network architectures and hyperparameters. \n\nOverall, this paper is a well-written and informative contribution to the field of semi-supervised learning, and the proposed method has the potential to be a valuable addition to the existing algorithms for this learning paradigm.","model":"chatGPT","source":"peerread","label":1,"id":2085}
{"text":"Summary:\nThis paper proposes a method called \"Temporal Ensembling\" for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. This method utilizes self-ensembling to form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and under different regularization and input augmentation conditions. The paper shows that this ensemble prediction can be used as a target for training and outperforms the output of the network at the most recent training epoch. The proposed method sets new records for two standard semi-supervised learning benchmarks and provides a clear improvement in CIFAR-100 classification accuracy in the presence of incorrect labels.\n\nStrengths:\nThe paper presents a well-motivated idea of self-ensembling to generate a consensus prediction of the unknown labels for semi-supervised learning. The experimental results support the effectiveness of the proposed method as it sets new records for two standard semi-supervised learning benchmarks and improves the classification accuracy in CIFAR-100 classification even in the presence of incorrect labels. The paper provides details on the experimental set-up and is easy to follow in terms of explaining the method and the results.\n\nWeaknesses:\nAlthough the results are promising, the proposed method's advantage over other semi-supervised techniques is not clear. Also, the paper does not provide a thorough analysis of the performance of the method concerning the size of the labeled dataset. It would have been beneficial to see the performance of the proposed method in the case of a smaller labeled dataset. \n\nConclusion:\nIn summary, the paper presents a simple and efficient method for training deep neural networks in a semi-supervised setting. The proposed method's promising results suggest that it is a useful and practical method for semi-supervised learning. However, further investigation is needed to understand its advantage over other semi-supervised learning techniques and the method's performance concerning smaller labeled datasets.","model":"chatGPT","source":"peerread","label":1,"id":2086}
{"text":"The paper \"Temporal Ensembling for Semi-Supervised Learning\" presented a novel method for training deep neural networks in a semi-supervised setting with only a small portion of labeled data. The authors proposed a self-ensembling approach, which is based on forming a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs and with different regularization and input augmentation conditions. The authors showed that this ensemble prediction can be a better predictor for the unknown labels than the output of the network at the most recent training epoch and can hence be used as a target for training.\n\nThe paper's main strength lies in the innovative approach proposed to tackle the problem of semi-supervised learning. The self-ensembling technique enables better utilization of the unlabeled data and improves the classification accuracy significantly. The paper's experimental results are impressive as it reduces the (non-augmented) classification error rate in SVHN from 18.44% to 7.05% and from 18.63% to 16.55% in CIFAR-10 with a small number of labeled data. Additionally, the authors demonstrated the effectiveness of their proposed method in increasing the CIFAR-100 classification accuracy.\n\nHowever, the paper's empirical evaluation could be improved in a few areas. Firstly, the authors can provide visualization of the self-ensembling method and the effect of the regularization techniques and input augmentation conditions on the network's output. This could help readers better understand the proposed approach. Secondly, the authors could have provided more details on the implementation of their method, such as the initialization of parameters, the batch size, and the optimization algorithm used.\n\nOverall, the paper \"Temporal Ensembling for Semi-Supervised Learning\" presents an innovative approach for semi-supervised learning that has demonstrated significant improvements on standard benchmarks. With its promising results, the proposed method could inspire further research in the field of semi-supervised learning.","model":"chatGPT","source":"peerread","label":1,"id":2087}
{"text":"Title: Peer Review for \"Training deep neural-networks using a noise adaptation layer\"\n\nThe paper \"Training deep neural-networks using a noise adaptation layer\" presents a novel approach to training deep neural networks using a noise adaptation layer, which is added to the input layer of a neural network. The authors demonstrate the effectiveness of their approach by applying it to image classification tasks using the CIFAR-10 and CIFAR-100 datasets.\n\nOverall, the paper is well written and organized. The authors clearly state their research questions and provide adequate background information on related work. The experimental setup and methodologies are well explained, and the results are presented in a clear and concise manner. There are, however, a few areas where the authors could improve their presentation.\n\nOne potential issue with the paper is that the authors do not provide enough detail on the noise adaptation layer itself. While they provide a brief overview of how the layer is constructed, they do not provide enough detail on the architecture or parameters used. It would be helpful if the authors could provide more information on how the noise adaptation layer was designed and how different configurations were tested. Additionally, it would be helpful to compare the noise adaptation layer to other common techniques used for training neural networks, such as dropout or batch normalization.\n\nAnother issue with the paper is that the authors only apply their approach to image classification tasks. It would be interesting to see if their approach is also effective for other types of tasks, such as natural language processing or speech recognition. It would be helpful if the authors could discuss potential applications of their approach beyond image classification.\n\nFinally, the authors could improve the clarity of their writing in a few areas. For example, on page 4, there is a sentence that reads \"Further studies revealed that when the fraction value is above 0.5, the performance of networks start to decrease with the increasing of fraction value.\" This sentence is somewhat difficult to understand and could be rewritten to be more concise and clear.\n\nOverall, \"Training deep neural-networks using a noise adaptation layer\" presents an interesting and valuable approach to training deep neural networks. The authors provide compelling experimental evidence that their approach is effective, and the paper is well written and organized. However, the authors could improve their presentation in a few areas, as noted above.","model":"chatGPT","source":"peerread","label":1,"id":2088}
{"text":"The paper titled \"Training deep neural-networks using a noise adaptation layer\" is an interesting contribution to the field of deep neural networks, particularly with the incorporation of a noise adaptation layer. The authors aim to address the problem of overfitting in deep neural networks, which often occurs due to high complexity models and relatively small datasets.\n\nOverall, the paper is well-written and the novelty of the proposed noise adaptation layer is well explained. The authors have successfully demonstrated that the proposed noise adaptation layer can improve the performance of deep neural networks by reducing overfitting. They have also compared the performance of the proposed approach with other state-of-the-art methods and achieved promising results.\n\nOne of the key strengths of the paper is the thorough experimentation and analysis conducted to examine the impact of the noise adaptation layer on different deep neural network configurations. The authors have used multiple datasets to test the effectiveness of their approach, which adds credibility to their findings. \n\nHowever, there are a few areas of weakness in the paper. Firstly, the authors could have provided more details of the experimental setup, such as the choice of hyperparameters, to enable reproducibility and better comparison with other approaches. Secondly, the authors could have discussed the potential computational costs of incorporating the Noise Adaptation Layer in the training process.\n\nOverall, the paper presents an innovative approach to improving the performance of deep neural networks using a noise adaptation layer, and the results are encouraging. The research can be extended by analyzing the behavior of the noise adaptation layer with a range of complex networks and datasets.","model":"chatGPT","source":"peerread","label":1,"id":2089}
{"text":"Peer Review:\n\nThe paper \"Training deep neural-networks using a noise adaptation layer\" presents an interesting approach for handling noisy labels in classification problems using deep neural networks. The authors introduce a noise adaptation layer, modeled by an additional softmax layer, which connects the correct labels to the noisy ones. This layer explicitly models the noise process and allows the EM algorithm to be applied to both estimate the parameters of the network and the noise and to estimate the correct label.\n\nThe work is well motivated and addresses an important problem in machine learning. The paper is well-written and clearly presents the proposed approach. The experiments conducted demonstrate that the proposed method outperforms previous methods in handling noisy labels in classification tasks. The authors should be commended for their thorough evaluation of the approach on multiple datasets.\n\nOverall, this paper makes a significant contribution to the field of deep learning for noisy label problems. The approach is both novel and effective and has the potential for wide applicability. The paper is well-organized and clearly presents the proposed method and experimental results. I would recommend this paper for publication in a reputable conference or journal.","model":"chatGPT","source":"peerread","label":1,"id":2090}
{"text":"The paper, \"Training deep neural-networks using a noise adaptation layer\" addresses the problem of noisy labels in classification problems. Specifically, inaccurate class labels are known to deteriorate the performance of even the best classifiers in a broad range of classification problems. This study proposes a neural-network approach that explicitly models the noise created by inaccurate class labels by utilizing an additional softmax layer that connects correct labels to noisy ones. \n\nThe main strength of the paper is the proposed approach that incorporates a noise adaptation layer to correct for the effects of noisy labels, which outperforms previous methods. The study extends the approach to the case where noisy labels are dependent on the features in addition to the correct labels. Additionally, the paper provides a thorough and clear explanation of the approach and its applications. \n\nOne weakness of the paper is that there is limited discussion on the datasets used in the experiments. This can limit the generalizability of the approach to other datasets with different characteristics. Additionally, the paper lacks a detailed analysis of the computational complexity and cost of the proposed approach, which could be important for practical applications. Lastly, the paper could benefit from more rigorous analyses and comparisons with other noisy label correction approaches in the literature. \n\nOverall, the paper presents an interesting and relevant approach to tackle the problem of noisy labels in the context of classification problems, and provides experimental evidence to support its effectiveness.","model":"chatGPT","source":"peerread","label":1,"id":2091}
{"text":"Overall, I thought this was a well-written and interesting paper exploring the use of reinforcement learning (RL) to help computers learn how to compose words into coherent sentences. The authors clearly articulated a research question and provided a thorough literature review to contextualize their work within the broader field of natural language processing (NLP).\n\nThe authors\u2019 use of two different RL algorithms for training their model, Q-learning and policy gradient, highlighted the versatility of RL approaches for NLP tasks. And the choice to use BLEU scores as a metric for evaluating the model\u2019s performance helped demonstrate the usefulness of RL for improving the quality of text generation. \n\nHowever, while the authors\u2019 experimental results showed improvement in sentence composition metrics, I felt the paper could have provided more context as to the actual usability of this approach in real-world applications. Additionally, the paper would have benefited from more clarity in terms of how the authors addressed typical issues encountered in RL approaches, such as the exploration-exploitation tradeoff.\n\nOverall, I believe this paper makes a valuable contribution to the field of NLP and has important implications for improving the automation of natural language generation.","model":"chatGPT","source":"peerread","label":1,"id":2092}
{"text":"The paper \"Learning to Compose Words into Sentences with Reinforcement Learning\" addresses the problem of composing sentences from a fixed set of words, which is a fundamental task in natural language processing. Unlike traditional approaches where sentence composition is done using complex language models, this paper presents a novel approach using reinforcement learning.\n\nThe main strength of this paper is their proposed approach, which is based on the actor-critic model using a policy gradient algorithm. The authors demonstrate that their model outperforms traditional methods such as recurrent neural networks, which have been the standard in this field.\n\nAnother strength is that the authors provide a comprehensive experimental evaluation of their proposed approach. The results show significant improvements in sentence composition accuracy compared to other state-of-the-art models.\n\nHowever, there are a few limitations of the paper that need to be addressed. The authors do not provide any analysis or discussion on how their approach can be extended to scenarios where the vocabulary is not fixed or when the composition process involves generating new words. Additionally, the authors use a synthetic dataset to evaluate their model, which raises questions about how well their approach would perform on real-world datasets.\n\nIn conclusion, the paper \"Learning to Compose Words into Sentences with Reinforcement Learning\" presents an innovative approach to sentence composition using reinforcement learning. Despite some limitations, the proposed approach shows promising results and has the potential to advance the field of natural language processing.","model":"chatGPT","source":"peerread","label":1,"id":2093}
{"text":"The paper \u201cLearning to Compose Words into Sentences with Reinforcement Learning\u201d presents a novel approach to learning tree-structured neural networks for computing representations of natural language sentences using reinforcement learning. The paper builds on prior work on tree-structured models, where tree structures were either provided as input or predicted using supervision from explicit treebank annotations. In contrast, this work optimizes the tree structures to improve performance on a downstream task.\n\nThe paper provides a thorough evaluation of the proposed approach using several experiments. The results demonstrate that learning task-specific composition orders outperforms both sequential encoders and recursive encoders based on treebank annotations. Additionally, the paper analyzes the induced trees and shows that while they discover some linguistically intuitive structures, they differ from conventional English syntactic structures.\n\nOverall, the paper presents a solid contribution to the field of natural language processing with a clear problem statement, methodology, and evaluation. The experiments are well-executed and demonstrate the effectiveness of the proposed approach. The analysis of the induced trees is interesting and provides valuable insights into the behavior of the model. However, the paper could be improved by addressing some minor issues. For example, the contribution of the paper could be better highlighted and discussed in the introduction and conclusion sections. Additionally, a comparison with related work could be included to better situate the paper within the existing literature.\n\nIn summary, the paper presents a promising approach for learning tree-structured models for natural language processing and makes a significant contribution to this field. The proposed approach outperforms existing methods based on treebank annotations and has the potential to discover novel and useful structures for a wide range of downstream tasks.","model":"chatGPT","source":"peerread","label":1,"id":2094}
{"text":"The paper \"Learning to Compose Words into Sentences with Reinforcement Learning\" focuses on the use of reinforcement learning to learn tree-structured neural networks for computing sentence representations. The main contribution of this work is that it optimizes tree structures to improve performance on a downstream task. The authors show that their model outperforms both sequential encoders and recursive encoders based on treebank annotations. \n\nThe paper's strength lies in its innovative approach of using reinforcement learning to learn task-specific composition orders for sentence representations. The experiments demonstrate the effectiveness of this approach in outperforming existing methods. The analysis of the induced trees is also informative, as it reveals how the neural networks discover linguistically intuitive structures within the sentences.\n\nHowever, the paper's weaknesses lie in the lack of discussion around the implications of the differences between the induced trees and conventional English syntactic structures. While the authors state that the induced trees are different, no further discussion or analysis is provided to show how these differences affect the learned representations. Additionally, the language used in the paper is not always clear, making it difficult for readers unfamiliar with reinforcement learning to understand the methodology.\n\nOverall, this paper presents an innovative approach to learning sentence representations using reinforcement learning. While some areas could be improved, the paper's contribution is significant and warrants further exploration.","model":"chatGPT","source":"peerread","label":1,"id":2095}
{"text":"Review of \"Delving into Transferable Adversarial Examples and Black-box Attacks\"\n\nThe paper \"Delving into Transferable Adversarial Examples and Black-box Attacks\" investigates the phenomenon of adversarial attacks in deep learning models in both white-box and black-box settings. The authors propose a new algorithm for generating transferable adversarial examples that can be used to attack different models and show that this approach is highly effective even in black-box scenarios.\n\nOverall, the paper presents a well-written and thorough investigation of the topic. The authors provide a comprehensive literature review to contextualize their work and design experiments to evaluate the performance of their model against state-of-the-art adversarial attacks. They also provide detailed analysis and discussion of the results, which helps readers to understand the implications of their findings.\n\nOne strength of this paper is the proposed algorithm for generating transferable adversarial examples. The authors demonstrate that their approach outperforms previous methods in terms of transferability and attack success rate. This is an important contribution, as transferability is a key factor in real-world attacks where the attacker may not have access to the target model's architecture or parameters.\n\nHowever, there are a few areas where the paper could be improved. Firstly, the authors could provide more detail about their experimental setup, such as the choice of hyperparameters and the specific models used for evaluation. This would allow readers to replicate the experiments and compare the results with their own work. Secondly, although the paper considers a range of deep learning models, it is unclear if the results generalize to other domains or datasets. More experiments on different datasets and models would help to establish the robustness and generalizability of their approach.\n\nFinally, the paper could benefit from a more thorough discussion of the limitations and implications of the proposed algorithm. For example, it would be interesting to explore potential defense strategies against transferable adversarial examples and their impact on real-world applications.\n\nIn conclusion, \"Delving into Transferable Adversarial Examples and Black-box Attacks\" is a solid contribution to the field of adversarial attacks in deep learning models. The proposed algorithm for generating transferable adversarial examples is effective and has the potential for practical use in real-world attacks.","model":"chatGPT","source":"peerread","label":1,"id":2096}
{"text":"The paper \"Delving into Transferable Adversarial Examples and Black-box Attacks\" addresses the problem of transferability of adversarial examples between different machine learning models and the usage of black-box attacks in this context. The paper aims to explore the mechanisms behind the transferability of adversarial examples and investigate the performance and impact of black-box attacks in the context of transferable adversarial examples.\n\nThe strengths of the paper include a clear and concise introduction, a well-organized presentation of the research, and a comprehensive analysis of previous work in the field. The authors come up with a novel approach to creating transferable adversarial examples that shows impressive performance across different models, datasets, and attack strengths. The experiments conducted are thorough and highlight the effectiveness and potential of black-box attacks.\n\nHowever, there are also some weaknesses that need to be addressed. The paper does not provide enough explanation or justification for the key mathematical concepts used, which makes it difficult for readers to follow the methodology used for transferable adversarial example creation. Additionally, the paper could benefit from more detailed explanations of the experimental results, especially in the case of black-box attacks, which sometimes rely on a more intuitive understanding.\n\nOverall, the paper contributes to the field of adversarial machine learning and transferability, and the novel approach for creating transferable adversarial examples shows great promise. The weaknesses are relatively minor and can be addressed in future revisions. I would recommend this paper for publication with minor revisions.","model":"chatGPT","source":"peerread","label":1,"id":2097}
{"text":"The paper \"Delving into Transferable Adversarial Examples and Black-box Attacks\" presents an extensive study on the transferability of adversarial examples in neural networks. Specifically, the authors focus on the transferability of targeted adversarial examples with their target labels, which has not been widely studied in previous works.\n\nThe authors make a significant contribution by conducting experiments on large models and a large-scale dataset, providing more comprehensive results than previous studies. The paper presents novel ensemble-based approaches to generate transferable adversarial examples, which are shown to have high success rates in transferring targeted adversarial examples with their target labels. Furthermore, the authors provide geometric studies to help understand the transferable adversarial examples, which add more insights to the experimental results.\n\nThe paper is well-organized and clearly presents the research objectives, methodology, and results. The writing is clear and concise, making it easy for the reader to follow the authors' arguments. Additionally, the authors show the practical significance of their findings by demonstrating that the adversarial examples generated using the proposed ensemble-based approach can successfully attack a black-box image classification system, Clarifai.com.\n\nA potential limitation of the paper is that it focuses on attack scenarios, but does not discuss defense strategies against adversarial attacks. It would be interesting to see if the proposed ensemble-based approach can be used to generate robust adversarial examples that can fool defense mechanisms.\n\nIn summary, the paper \"Delving into Transferable Adversarial Examples and Black-box Attacks\" is a well-written and significant contribution to the field of adversarial attacks in neural networks. The authors present novel approaches to generate transferable adversarial examples and provide comprehensive experimental results. The paper is recommended for publication in its current form.","model":"chatGPT","source":"peerread","label":1,"id":2098}
{"text":"The paper, \"Delving into Transferable Adversarial Examples and Black-box Attacks\", addresses the problem of transferable adversarial examples that can hinder deep neural network-based applications. The authors conduct an extensive study on the transferability of both non-targeted and targeted adversarial examples over large models and a large-scale dataset. They propose novel ensemble-based approaches to generating transferable adversarial examples and show that targeted adversarial examples generated using existing approaches almost never transfer with their target labels.\n\nStrengths:\nThe authors conduct an extensive study on the transferability of adversarial examples using a large scale dataset and large models, which is a significant contribution to the field. The novel ensemble-based approaches proposed in the paper are effective in generating transferable targeted adversarial examples that can transfer with their target labels. The geometric studies presented in the paper provide a better understanding of the transferable adversarial examples.\n\nWeaknesses:\nThe paper could have provided more details on the specific characteristics of the large-scale dataset used for the study. Additionally, while the authors show that the proposed approaches can successfully attack Clarifai.com, they could have discussed the effectiveness of these approaches on a wider range of black-box image classification systems. Furthermore, the paper could have discussed the ethical implications of generating adversarial examples that can successfully attack black-box systems.\n\nOverall, this paper makes a valuable contribution to the field of adversarial attacks and transferability of adversarial examples. The novel ensemble-based approaches proposed in the paper can be useful in generating transferable targeted adversarial examples. I recommend this paper for publication after addressing the weakness noted above.","model":"chatGPT","source":"peerread","label":1,"id":2099}
{"text":"Title: \"Identity Matters in Deep Learning: A Comprehensive Review\"\n\nThe paper presents a comprehensive review of the role of identity in deep learning. The authors have analyzed various aspects of the role of identity in machine learning, including the need for comprehensive identity data, the importance of effective preprocessing of data, and the use of deep architectures in identity recognition.\n\nOverall, the paper is well-organized and clearly written. The authors have provided a detailed and thorough analysis of the role of identity in deep learning, and have discussed the challenges involved in implementing effective identity recognition systems.\n\nOne of the strengths of the paper is the authors' approach to the topic. They bring together insights from a wide range of areas, including computer vision, natural language processing, and speech recognition, to provide a comprehensive analysis of the role of identity in machine learning. This approach enables the authors to provide a more nuanced and comprehensive picture of the issues at hand.\n\nIn terms of areas for improvement, the paper could have included more detailed discussion of the limitations of current identity recognition systems. While the authors briefly mention some of the challenges involved in achieving accurate identity recognition, they do not provide a detailed analysis of these limitations. Future research in this area could benefit from a more nuanced understanding of the limitations of current identity recognition systems.\n\nOverall, the paper represents a valuable contribution to the field of deep learning, and provides insights that will be of interest to researchers and practitioners working in this area. I would recommend this paper for publication in a reputable journal.","model":"chatGPT","source":"peerread","label":1,"id":2100}
{"text":"This paper titled \"Identity Matters in Deep Learning\" addresses the problem of how the deep learning models are influenced by the identity and social backgrounds of their creators. The paper focuses on the importance of diversity in creating deep learning models that are fair and unbiased towards all demographics. The study also highlights the impact of imbalanced datasets on the model's prediction accuracy, especially for underrepresented communities.\n\nThe paper's strength lies in its in-depth analysis of the role of identity in deep learning models, and how it can influence their predictions. The authors have reviewed and analyzed several studies that highlight the impact of bias on deep learning models, which strengthens the argument made in this paper.\n\nHowever, the paper lacks clarity in its methodology, making it difficult for readers to replicate the study. The authors have used a case study to demonstrate the impact of identity on deep learning models. Still, they have not detailed the data collection and analysis process, which could raise questions regarding the validity of the study.\n\nFurthermore, the paper has failed to quantify the impact of identity on deep learning models systematically. As such, the authors provide suggestions for future research to address this issue, but a more in-depth and conclusive analysis would help validate the study's findings.\n\nOverall, Identity Matters in Deep Learning is a well-written and engaging paper that addresses an essential issue in artificial intelligence. The paper could benefit from more clarity in the methodology and a more quantitative approach to demonstrating the impact of identity on deep learning models.","model":"chatGPT","source":"peerread","label":1,"id":2101}
{"text":"The paper \"Identity Matters in Deep Learning\" provides a detailed analysis of the principle of identity parameterization and its impact on deep learning. The authors argue that each layer of a deep artificial neural network should be able to express the identity transformation easily. The idea has led to the development of various normalization techniques such as batch normalization, which have played a key role in the success of residual networks.\n\nThe authors present a straightforward proof that arbitrarily deep linear residual networks have no spurious local optima. However, the same result for feed-forward networks in their standard parameterization is considerably more complex. This finding is an essential theoretical contribution to the field.\n\nIn addition, the authors show that residual networks with ReLu activations have universal finite-sample expressivity. They demonstrate that the network can represent any function of its sample when the model has more parameters than the sample size. This finding highlights the significance of the principle of identity parameterization in deep learning architectures.\n\nInspired by their theoretical results, the authors propose a radically simple residual architecture that consists of only residual convolutional layers and ReLu activations, without batch normalization, dropout, or max pool. The proposed model shows a significant improvement over previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks. \n\nThe paper makes several significant contributions to the field of deep learning by establishing a solid theoretical footing to the principle of identity parameterization in deep learning networks. The experiments and results presented provide concrete evidence of the effectiveness of the proposed architecture. The paper is well-written and clearly presented, and its contributions will be of interest to researchers in the field.","model":"chatGPT","source":"peerread","label":1,"id":2102}
{"text":"The paper \"Identity Matters in Deep Learning\" addresses the problem of designing deep artificial neural networks that can easily express the identity transformation in each layer. This design principle has been found to be important in an emerging trend in deep learning and has led to the success of residual networks. The authors aim to develop a more solid theoretical basis for this design principle and provide empirical progress on this front.\n\nOne of the strengths of this paper is the strikingly simple proof given that deep linear residual networks have no spurious local optima. The authors show that the same result is substantially more delicate for feed-forward networks in their standard parameterization. This is an important theoretical result that sheds light on the effectiveness of residual networks and provides a better understanding of the optimal design of deep neural networks.\n\nAnother strength of the paper is its experimental results. The authors experiment with a residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Their model improves significantly on previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks. This shows that their theory can inspire the design of practical neural network architectures that are both simple and effective.\n\nOne weakness of the paper is that the theoretical and experimental results are focused on residual networks with ReLu activations. It would be interesting to see the applicability of their results to other commonly used activation functions.\n\nOverall, this paper makes an important contribution to the theoretical and practical development of deep neural networks. The results provide a better understanding of the design principles that underlie the success of residual networks and can inspire the creation of simpler and more effective neural network architectures. The paper is well-written and clearly presents its main points and findings, making it a valuable addition to the literature on deep learning.","model":"chatGPT","source":"peerread","label":1,"id":2103}
{"text":"Title: \u201cAdversarial Feature Learning for Neural Text Generation\u201d\n\nAuthors: Xinyu Hua, Lu Wang, Mohit Bansal\n\nOverview:\nThe paper discusses a novel approach called Adversarial Feature Learning (AFL) to improve the performance of neural text generation models. The authors propose a new objective function that enables the generator to learn features that capture the stylistic and content-related features of the input data. They show that AFL improves the quality of the generated text, as well as enables the model to generate more diverse and realistic text.\n\nStrengths:\nThe paper provides a clear and detailed explanation of the proposed method and its underlying principles. The authors show that AFL outperforms state-of-the-art text generation models in terms of text quality and diversity. The paper also presents a thorough evaluation of the proposed method, including a comprehensive set of experiments and analysis.\n\nWeaknesses:\nThe paper could benefit from further clarity and detail in certain sections. For instance, the authors could provide more explanation on how the discriminator distinguishes between generated and human-written text, and how it is trained. Additionally, more discussion and comparison with related works would help establish the novelty of the proposed method.\n\nOverall:\nThe paper presents a promising method, AFL, for improving neural text generation. The proposed approach is based on sound principles and is supported by a thorough evaluation. The authors could strengthen their arguments by addressing some of the weaknesses mentioned above. Overall, I think the paper is a valuable contribution to the field of NLP and text generation.","model":"chatGPT","source":"peerread","label":1,"id":2104}
{"text":"The paper \"Adversarial Feature Learning\" investigates the effectiveness of adversarial feature learning in improving the robustness of machine learning models against adversarial attacks. The authors propose a method for incorporating adversarial examples into the training process, which involves generating adversarial perturbations on the training data, and using a two-part training procedure to ensure that the model learns to distinguish these perturbed examples from the original data.\n\nThe strengths of this paper include its clear and concise presentation of the proposed method and its experimental results. The authors provide a thorough explanation of the adversarial feature learning procedure and its motivation, which makes it accessible even to readers with limited prior knowledge of adversarial attacks. The experimental results demonstrate the effectiveness of the method in improving the robustness of models against various types of attacks.\n\nHowever, there are also some weaknesses in the paper. One potential weakness is that the experimental results are limited to a relatively small number of datasets and models. Moreover, the authors do not provide a detailed analysis of the computational complexity involved in the training procedure and its scalability to larger datasets.\n\nIn summary, \"Adversarial Feature Learning\" is a well-presented paper that provides a promising approach for improving the robustness of machine learning models against adversarial attacks. While there are some limitations to the experimental results, the proposed method and its potential applications should motivate further research in this area.","model":"chatGPT","source":"peerread","label":1,"id":2105}
{"text":"Review of \"Adversarial Feature Learning\" by [authors]\n\nThis paper presents the Bidirectional Generative Adversarial Networks (BiGANs) that provide a solution for learning the inverse mapping between the data space and the latent space in the Generative Adversarial Networks (GANs) framework. The authors demonstrate that this technique can learn useful feature representations for auxiliary supervised discrimination tasks and is competitive with traditional unsupervised and self-supervised feature learning approaches.\n\nOverall, this paper presents a well-thought-out solution to a significant limitation of the GANs framework. The experimental results on various datasets validate the efficacy of BiGANs for learning a feature representation that can be useful for downstream tasks. The proposed approach is clearly described and easy to understand, and the evaluations are thorough and rigorous.\n\nHowever, the paper could benefit from more detailed explanations of the intuition behind the proposed method. Also, the authors could provide more comparison with recent studies that show similar results. Additionally, it would be beneficial to have a discussion of the limitations of the proposed approach and directions for further exploration.\n\nIn summary, the proposed method is innovative, and the paper is well-presented and convincing. The authors have successfully illustrated the effectiveness of BiGANs for learning feature representations and provided a compelling alternative to unsupervised and self-supervised learning approaches. I recommend this paper for publication in its current form.","model":"chatGPT","source":"peerread","label":1,"id":2106}
{"text":"The paper \"Adversarial Feature Learning\" proposes a novel framework, Bidirectional Generative Adversarial Networks (BiGANs), to learn the inverse mapping from data space to the latent space. The authors argue that the latent space of Generative Adversarial Networks (GANs) may serve as useful feature representations for auxiliary problems, but existing GANs have no means of learning the inverse mapping. Therefore, they propose BiGANs, which not only learn the generative model mapping from the latent space to the data space but also learn the inverse mapping. The authors demonstrate that the learned feature representation with BiGANs is useful for auxiliary supervised discrimination tasks and competitive with existing unsupervised and self-supervised feature learning methods.\n\nStrengths:\n- The proposed BiGANs framework is novel and addresses an important limitation of existing GANs.\n- The authors provide a clear and thorough explanation of the proposed framework, making it easy to follow and understand the technical details.\n- The experimental evaluation of BiGANs is thorough and includes comparisons with several existing unsupervised and self-supervised feature learning methods, demonstrating the competitiveness of the proposed framework.\n\nWeaknesses:\n- The paper is relatively technical, and it may be challenging for readers without sufficient knowledge of GANs and deep learning to follow the details.\n- While the proposed BiGANs framework is novel, it may not be entirely clear how it may be applied to real-world problems, limiting its practicality.\n\nOverall, this paper makes a significant contribution to the field of deep learning by proposing a novel framework to address the limitations of existing GANs. The proposed framework, BiGANs, is backed up by a thorough experimental evaluation, and the learned feature representation is competitive with existing unsupervised and self-supervised feature learning methods. While the paper may be relatively technical, and the practical applications of the proposed framework may not be immediately clear, these issues do not diminish the importance of the contribution.","model":"chatGPT","source":"peerread","label":1,"id":2107}
{"text":"Title: Towards the Limit of Network Quantization\n\nAuthors: John Doe, Jane Smith\n\nAbstract:\nThis paper proposes a novel method for network quantization - a technique used to reduce the size of neural networks by reducing the precision of weights and activations. In contrast to existing methods which rely on uniform precision reduction, our approach uses layer-dependent precision to achieve better results. We evaluate our method on several benchmark datasets and show that it can achieve state-of-the-art performance with significant reductions in model size and computation cost.\n\nOverall Evaluation:\nI have reviewed the paper \"Towards the Limit of Network Quantization\" submitted by John Doe and Jane Smith. In general, the paper is well-written and presents a new approach to network quantization that achieves state-of-the-art performance while reducing model size and computation cost. The experiments demonstrate the effectiveness of the approach and the comparison with existing methods is comprehensive.\n\nHowever, there are several areas that could be improved:\n\n1. Novelty: The paper claims to propose a novel method for network quantization. However, the concept of layer-dependent precision has been explored in previous work such as \"Learning both Weights and Connections for Efficient Neural Networks\" by Han et al. (2016). The authors need to clarify how their approach differs from these previous works, and what unique contributions it brings.\n\n2. Technical Details: The authors do not provide enough technical detail regarding the implementation of their method. Specifically, how do they determine the optimal layer-dependent precision for each layer? What is the impact of the different layer-dependent precisions on the accuracy and complexity of the network?\n\n3. Experiments: The experimental section is informative, but it would be beneficial to have more detailed comparisons between the proposed method and existing state-of-the-art methods. In particular, the authors should provide a more comprehensive comparison between their method and the method presented in Han et al. (2016).\n\n4. Conclusion: The conclusion is rather brief and does not provide a clear summary of the contributions of the paper. The authors need to highlight the main benefits and applications of their method and how it compares to existing methods.\n\nOverall, I think this paper presents a novel approach to network quantization that could be valuable for practitioners and researchers in the field. With the suggested improvements, the paper has the potential to make a significant contribution to the field of deep learning.","model":"chatGPT","source":"peerread","label":1,"id":2108}
{"text":"The paper \"Towards the Limit of Network Quantization\" addresses the problem of reducing the computational complexity and memory requirements for deep neural networks by quantizing their weights and activations. The authors propose a novel approach that can approximate a floating-point neural network with a low-precision quantized network without significant loss in accuracy.\n\nOne of the strengths of the paper is that the proposed approach achieves state-of-the-art results on several benchmark datasets, including CIFAR-10, ImageNet, and COCO. The authors also provide a thorough experimental evaluation of their method, comparing it to previous network quantization methods and analyzing the performance across different parameter settings.\n\nAnother strength of the paper is the thoroughness of the technical description of the proposed approach. The authors provide clear and concise explanations of the mathematical and algorithmic aspects of their method, as well as detailed implementation and training details.\n\nHowever, there are also some weaknesses that need to be addressed. Firstly, the paper could benefit from a more detailed analysis and discussion of the trade-off between accuracy and complexity in the proposed approach. Secondly, the authors did not compare their method against some of the most recent state-of-the-art methods in the field of network quantization, which could limit the generalizability of their findings.\n\nOverall, the paper \"Towards the Limit of Network Quantization\" presents an interesting and effective approach to the problem of network quantization, with strong experimental results and a robust technical description. However, some improvements would help to further strengthen the paper's impact and relevance to the broader NLP and machine learning communities.","model":"chatGPT","source":"peerread","label":1,"id":2109}
{"text":"This paper titled \"Towards the Limit of Network Quantization\" focuses on the problem of deep neural network compression through network quantization. This technique reduces the number of distinct network parameter values by quantization in order to save the storage for them. The authors aim to propose a network quantization scheme that minimizes the performance loss due to quantization given a compression ratio constraint. \n\nThe authors provide a thorough analysis of the relationship between the quantization errors and the neural network loss function. They identify that the Hessian-weighted distortion measure is locally the right objective function for the optimization of network quantization. They propose a Hessian-weighted k-means clustering algorithm for quantization, which is shown to minimize the performance loss. The authors also explore the use of optimal variable-length binary codes, such as Huffman codes, for further compression. They derive that the network quantization problem can be related to the entropy-constrained scalar quantization (ECSQ) problem in information theory and propose two solutions of ECSQ for network quantization, i.e., uniform quantization and an iterative solution similar to Lloyd's algorithm.\n\nFinally, the authors perform experiments on LeNet, 32-layer ResNet, and AlexNet to demonstrate the effectiveness of their proposed quantization scheme. Using simple uniform quantization followed by Huffman coding, the authors achieve compression ratios of 51.25, 22.17 and 40.65 for LeNet, 32-layer ResNet and AlexNet, respectively.\n\nOverall, the paper presents a well-motivated study with solid experimentation results. The proposed scheme is well-formulated and mathematically sound with practical implications. The experiments demonstrate the effectiveness of the proposed quantization scheme on different network architectures, which indicates the viability of the proposed method in different applications. The paper is well-written and accessible. The authors have highlighted the limitations of their proposed approach and provided direction for future research, which further strengthens the paper.","model":"chatGPT","source":"peerread","label":1,"id":2110}
{"text":"The paper, \"Towards the Limit of Network Quantization,\" addresses the problem of reducing the redundancy of deep neural networks through network quantization, wherein the number of distinct network parameter values is reduced to save storage. The main objective of the paper is to design network quantization schemes that minimize performance loss due to quantization while adhering to a compression ratio constraint. To achieve this, the paper proposes using the Hessian-weighted distortion measure as the right objective function for the optimization of network quantization. The paper then proposes two solutions for entropy-constrained scalar quantization (ECSQ) for network quantization, namely uniform quantization and an iterative solution similar to Lloyd's algorithm.\n\nThe paper provides several strengths, such as its clear description of the problem and an excellent overview of the related work. The paper also proposes new solutions for network quantization, namely Hessian-weighted k-means clustering for clustering network parameters to quantize, and two solutions for ECSQ for network quantization. Additionally, the paper presents experiments using simple uniform quantization and Huffman coding, which show that the compression ratios of 51.25, 22.17, and 40.65 are achievable for LeNet, 32-layer ResNet, and AlexNet, respectively.\n\nHowever, the paper also has several weaknesses. While the paper does provide experimental results, it could benefit from more detailed evaluations and comparisons with existing methods. Additionally, the paper does not explore how the proposed solutions could be applied to other types of neural networks or different optimization objectives. Finally, the paper lacks details on how the proposed solutions could be integrated with existing compression techniques.\n\nIn conclusion, the paper contributes new solutions for network quantization, but it could benefit from additional evaluations and discussions on how the proposed solutions can be applied to different types of neural networks and integrated with existing compression techniques.","model":"chatGPT","source":"peerread","label":1,"id":2111}
{"text":"Title: Faster CNNs with Direct Sparse Convolutions and Guided Pruning\n\nOverview:\nThe paper proposes a method for increasing the speed of convolutional neural networks (CNNs) by directly performing sparse convolutions and using guided pruning. The authors achieve better results compared to existing methods, by reducing the amount of computation needed during inference, while maintaining high accuracy.\n\nStrengths:\n- The paper tackles an important problem of increasing the speed of CNNs, which are widely used in various applications, such as image classification and object detection.\n- The proposed method achieves impressive results, in terms of speed and accuracy, compared to existing methods.\n- The paper provides a detailed description of the proposed method, including the algorithmic and implementation details, which can be useful for researchers and practitioners who aim to reproduce the results or build on this work.\n\nWeaknesses:\n- The paper could benefit from including more experimental results or comparisons with other state-of-the-art methods to demonstrate the robustness of the proposed method.\n- The authors could provide more insights or analysis on the behavior or characteristics of the sparsity pattern that they used, and how it impacts the performance of the CNNs.\n- Some of the technical details, such as the pruning strategy or the choice of hyperparameters, are not clearly explained, which could make it challenging for readers to fully understand and replicate the experiments.\n\nConclusion:\nIn summary, the paper presents a promising approach to improve the efficiency of CNNs by leveraging direct sparse convolutions and guided pruning. While the paper could have included more experimental results and analysis, the proposed method provides a good starting point for further research in this direction. Overall, the paper is well-written and provides sufficient details to enable other researchers to validate or extend the results.","model":"chatGPT","source":"peerread","label":1,"id":2112}
{"text":"The paper that I have reviewed is titled \"Faster CNNs with Direct Sparse Convolutions and Guided Pruning\" by authors Liu, Ding, Liao, Cheng, and Qian. The paper addresses the problem of accelerating the training and inference of convolutional neural networks (CNNs) by introducing two novel techniques: direct sparse convolutions (DSC) and guided pruning.\n\nIn terms of strengths, the paper presents a significant improvement in the speed and efficiency of CNNs. The proposed DSC method reduces the computation complexity of the convolution operation by directly computing sparse convolution, which leads to a reduction in the number of computations and memory consumption. Moreover, the guided pruning method achieves a higher compression rate in terms of the number of parameters while maintaining accuracy levels similar to those obtained by non-pruned models.\n\nHowever, the paper fails to provide sufficient detail on some critical aspects of their approach. For instance, it is unclear how the authors decide on the pruning threshold, which is critical for setting the appropriate sparsity level. Additionally, the paper needs to provide more details about the effect of pruning and sparsity on the generalization and fine-tuning of the model. Another important limitation is the evaluation of the proposed approach on more diverse datasets and problems.\n\nOverall, the paper presents an innovative approach to accelerate and compress CNNs, and the results obtained are promising. However, to improve the quality of the paper, some aspects of their approach should be clarified, and more evaluation and analysis need to be conducted.","model":"chatGPT","source":"peerread","label":1,"id":2113}
{"text":"The paper \"Faster CNNs with Direct Sparse Convolutions and Guided Pruning\" presents a novel approach to speed up convolutional neural networks (CNNs) by pruning them to reduce their size while maintaining their accuracy. The authors introduce an efficient and general sparse-with-dense matrix multiplication implementation that is applicable to convolution of feature maps with kernels of arbitrary sparsity patterns. They complement this implementation by developing a performance model that predicts the sweet spots of sparsity levels for different layers and computer architectures.\n\nThe paper is well-written and the methodology is sound. The authors provide a comprehensive explanation of their approach, and their experimental results demonstrate significant speedups over dense convolution in AlexNet on a wide range of computer architectures, from mobile devices to supercomputers. The paper also discusses the limitations and future research directions.\n\nHowever, there are a few points that need to be addressed. Firstly, the authors could provide more details on how the general sparse-with-dense matrix multiplication implementation works and how it is different from existing methods. Secondly, it would be interesting to see how their approach compares to other recent works on pruning CNNs. Lastly, the authors could further elaborate on the implications of their approach for real-world applications and how it can be extended beyond AlexNet.\n\nOverall, this paper presents a valuable contribution to the field of CNN pruning and acceleration, and it is recommended for publication after addressing the above-mentioned points.","model":"chatGPT","source":"peerread","label":1,"id":2114}
{"text":"This paper addresses the important problem of reducing the size and increasing the speed of convolutional neural networks (CNNs) through pruning. Specifically, the authors propose a method that combines an efficient general sparse-with-dense matrix multiplication implementation with a performance model that predicts optimized sparsity levels for different layers and on different computer architectures. They demonstrate significant speedups in convolution performance over dense convolution in AlexNet on a range of processors, from mobile devices to supercomputers.\n\nOne of the strengths of this paper is its clear and thorough explanation of their proposed method and the results of their experiments. The authors provide detailed analysis of the performance of their method compared to dense convolutions and other methods, and provide evidence to support the effectiveness of their method across multiple architectures.\n\nAnother strength of this paper is its relevance to practical applications of deep learning, as reducing the size and increasing the speed of CNNs is critical for their deployment on low-power mobile and edge devices, as well as in resource-constrained data centers and supercomputers.\n\nHowever, one potential weakness of this paper is that it focuses on a specific CNN architecture (AlexNet) and may not generalize to other architectures. Additionally, although the authors claim to achieve significant speedups with their method, it is unclear how much of these speedups can be attributed to the proposed pruning scheme versus the improved sparse matrix multiplication implementation or other optimizations.\n\nOverall, this paper presents a promising approach to pruning CNNs to reduce their size and increase their speed, and provides important insights into the relationship between sparsity patterns, convolution performance, and computer architecture. However, further research is needed to determine the generalizability of this approach to other CNN architectures and to fully understand the contributions of different optimization techniques to overall performance improvements.","model":"chatGPT","source":"peerread","label":1,"id":2115}
{"text":"Title: Stick-Breaking Variational Autoencoders\n\nAuthors: John Smith, Jane Doe, and Michael Johnson\n\nAbstract:\nThe paper introduces a novel approach to Variational Autoencoders (VAEs) by utilizing the stick-breaking process. The proposed method, Stick-Breaking Variational Autoencoders (SB-VAEs), achieves improved performance in terms of likelihood, reconstruction, and unsupervised clustering tasks. The experiments performed on various datasets show the effectiveness of the proposed method in comparison to standard VAEs and other state-of-the-art techniques.\n\nReview:\n\nThe paper presents an interesting application of the stick-breaking process to improve the performance of Variational Autoencoders. The proposed method, SB-VAEs, seems to be promising because it outperforms standard VAEs and other state-of-the-art techniques in terms of likelihood, reconstruction, and unsupervised clustering tasks, as demonstrated by the experiments on several datasets.\n\nThe authors provide a clear explanation of the stick-breaking process and how it is incorporated into the framework of VAEs. They also provide an articulate discussion of the difference between the proposed method and the existing methods. \n\nHowever, there are certain areas that could be improved in the paper. Firstly, the paper could benefit from a more detailed explanation of the experimental setup and the hyperparameters used. The authors only briefly mentioned the selection process of hyperparameters for the SB-VAE, and further detail would enhance the reproducibility of the experiments.\n\nSecondly, although the authors discuss the advantages of SB-VAE over VAE in terms of computational efficiency, the speed of the proposed method should be quantified and compared with that of other state-of-the-art techniques.\n\nLastly, there could be some more clarity on the unsupervised clustering tasks application of the SB-VAE. It would be helpful if the authors could elaborate on how the unsupervised clustering tasks were conducted in their experiments and how the SB-VAE approach improves the outcomes.\n\nOverall, the paper presents a valuable contribution to the area of autoencoding by introducing a novel approach that effectively improves upon the standard method. With a more detailed explanation of the methods and experiments, the paper can be useful for researchers working in the fields of deep learning, unsupervised clustering, and pattern recognition.","model":"chatGPT","source":"peerread","label":1,"id":2116}
{"text":"Problem\/Question addressed: \n\nThe paper \"Stick-Breaking Variational Autoencoders\" by Chenhao Wang, Ruiyi Zhang, Liqun Chen, Zhe Gan, Changyou Chen, and Jingjing Liu addresses the problem of unsupervised learning of latent variable models. Specifically, the paper introduces a new model called Stick-Breaking Variational Autoencoder (SB-VAE) that employs efficient inference techniques and can capture complex posterior distributions. \n\nStrengths: \n\nThe paper provides a comprehensive overview of the proposed model, as well as an analysis of its performance on several benchmark datasets. The authors propose a new method for generating samples from the SB-VAE model and demonstrate its effectiveness in capturing the true data distribution. Additionally, they provide an extensive comparison of the SB-VAE model with other existing models, including Variational autoencoders (VAEs) and Generative Adversarial Networks (GANs), demonstrating that the SB-VAE model outperforms them on various metrics.\n\nWeaknesses:\n\nOne potential weakness of the paper is that the experimental evaluation is solely focused on image datasets, without including any other type of data sources. Another potential concern is that the paper could have been clearer about the differences between the proposed model and other related models like VAEs and GANs. Specifically, the authors could have provided a more in-depth discussion about how the SB-VAE model relates to other generative models in terms of its expressiveness, scalability, and computational complexity. Overall, the paper provides a valuable contribution to the field of unsupervised learning and latent variable models, but additional work is needed to explore the potential of this model in different settings and applications. \n\nRecommendation:\n\nI would recommend this paper for publication, with minor revisions. Specifically, the authors could address the concerns mentioned above by providing additional experimental results and providing further clarification in the theoretical analysis of the proposed model. Overall, the paper makes a valuable contribution to the field and provides a novel approach to unsupervised learning of latent variable models that can be of interest to researchers in machine learning and related areas.","model":"chatGPT","source":"peerread","label":1,"id":2117}
{"text":"Review of \"Stick-Breaking Variational Autoencoders\"\n\nIn this paper, the authors propose an extension to Stochastic Gradient Variational Bayes that allows for posterior inference for the weights of Stick-Breaking processes. With this development, the authors propose a Stick-Breaking Variational Autoencoder (SB-VAE), which is a Bayesian nonparametric version of the variational autoencoder with a latent representation that has stochastic dimensionality.\n\nThe paper is well-organized and clearly written. The authors provide a good background and introduction to the topic, and explain their proposed extension and its implications clearly. \n\nThe experiments presented in the paper demonstrate the effectiveness of the SB-VAE and its semi-supervised variant for learning highly discriminative latent representations. The results show that the SB-VAE models perform better than Gaussian VAEs in several benchmark datasets, which is an important contribution to the field. \n\nThe use of Stick-Breaking processes for Bayesian nonparametric modeling is becoming increasingly popular, and the proposed extension to Stochastic Gradient Variational Bayes provides a useful tool for exploring these models. The authors have done a good job of clearly demonstrating the effectiveness of their proposed method, and this paper will likely be of interest to researchers working on Bayesian nonparametric models and variational autoencoders.\n\nOverall, I believe that this is a well-written and well-executed paper, and I recommend it for publication.","model":"chatGPT","source":"peerread","label":1,"id":2118}
{"text":"The paper \"Stick-Breaking Variational Autoencoders\" addresses the problem of Bayesian nonparametric inference for the weights of stick-breaking processes in the context of Variational Autoencoders (VAEs). The authors propose a novel algorithm for posterior inference for the weights of these processes, allowing for the construction of a Bayesian non-parametric version of the VAE. The proposed method is demonstrated to be highly discriminative, effectively handling high-dimensional data.\n\nOne of the main strengths of this paper is its novelty; the authors propose a new approach for Bayesian non-parametric inference that is specifically designed for VAEs. This approach allows for the modelling of complicated posterior distributions with a high degree of accuracy, leading to highly discriminative latent representations. The authors also provide comprehensive experimental results, demonstrating the effectiveness of the proposed approach across a range of different datasets.\n\nHowever, there are a few potential weaknesses to the paper that should be addressed. The first is that the paper could benefit from more theoretical analysis; while the experimental results are quite convincing, it would be helpful to see more theoretical results or proofs regarding the convergence and stability of the proposed algorithm. Additionally, while the paper does include some comparisons to existing methods, more exhaustive comparisons to a wider range of current state-of-the-art VAE models would be beneficial.\n\nOverall, the paper presents a compelling approach to Bayesian non-parametric inference for the weights of stick-breaking processes in the context of VAEs. The proposed algorithm shows promising results in terms of discriminative performance and has the potential to be applied to a broad range of machine learning problems in the future.","model":"chatGPT","source":"peerread","label":1,"id":2119}
{"text":"Title: Support Regularized Sparse Coding and Its Fast Encoder\n\nOverall, this paper presents a novel approach to sparse coding with the additional support regularization structure. The authors build upon previous work in the field to develop an algorithm that is not only efficient but also accurate. The paper also includes experimental results that demonstrate the effectiveness of the proposed method compared to other state-of-the-art techniques.\n\nStrengths:\n1. The proposed method uses support regularization, which is a novel contribution to the sparse coding literature. The authors clearly explain how the method works and illustrate its benefits over other approaches.\n2. The fast encoder that is proposed in this paper is a significant improvement over previous methods in terms of computational efficiency. This is an important contribution that will make sparse coding more accessible to researchers and practitioners.\n3. The experimental results support the effectiveness of the proposed method, and the authors provide a thorough analysis of the results. The experimental setup is well-designed, and the comparison with other state-of-the-art methods is appropriate.\n\nWeaknesses:\n1. The paper could benefit from a clearer introduction that motivates the need for support regularization and explains why other approaches are insufficient. While the authors touch on this in the literature review section, a more explicit discussion would help readers understand the importance of the work.\n2. The theoretical analysis of the proposed method could also be expanded. While the authors provide some intuition for why support regularization is effective, a more formal analysis could enhance the paper's contributions.\n3. The authors use only one dataset in their experiments, which limits their ability to draw broad conclusions about the effectiveness of the proposed method. Including additional datasets in future work would help to validate the approach more broadly.\n\nOverall, the proposed approach to sparse coding with support regularization and fast encoder is a valuable contribution to the field. The novel method addresses a key limitation of current sparse coding techniques and provides a practical solution that can be used in real-world applications. The authors have presented the work in a clear and accessible way, and the experimental results demonstrate the effectiveness of the approach.","model":"chatGPT","source":"peerread","label":1,"id":2120}
{"text":"The paper \"Support Regularized Sparse Coding and Its Fast Encoder\" addresses the problem of sparse coding, which is a widely used technique in many fields, such as signal processing, computer vision, and machine learning. The authors propose a new method for sparse coding called Support Regularized Sparse Coding (SRSC) that expands on the widely used L1-regularized sparse coding by adding an additional support regularization term. This new method aims to improve the sparsity of the coding while preserving the discriminative information in the data.\n\nThe paper has several strengths. First, the experimental results show that the proposed method outperforms several state-of-the-art sparse coding methods in various tasks, including image classification and face recognition. Second, the authors provide a thorough explanation of the proposed method and the motivation behind it, making the paper accessible to readers with varying levels of expertise in the field. Finally, the fast encoder algorithm presented in this paper is computationally efficient, thus making it suitable for real-world applications.\n\nHowever, there are some weaknesses to the paper. While the experimental results are impressive, they could have been further validated on larger datasets or in more challenging scenarios. Additionally, a more thorough comparison with other state-of-the-art sparse coding methods could have been provided.\n\nOverall, \"Support Regularized Sparse Coding and Its Fast Encoder\" is a well-written and informative paper that introduces a new method to address the well-known problem of sparse coding. The proposed method shows promising results in various applications and has the potential to further advance the field of sparse coding.","model":"chatGPT","source":"peerread","label":1,"id":2121}
{"text":"The paper \"Support Regularized Sparse Coding and Its Fast Encoder\" proposes a novel approach to sparse coding that incorporates the manifold structure of the data. The authors present the Support Regularized Sparse Coding (SRSC) algorithm, which encourages nearby data points in the manifold to choose similar dictionary atoms, thus improving the quality of the sparse codes obtained. Additionally, the authors present a Fast Encoder algorithm called Deep Support Regularized Sparse Coding (Deep-SRSC), which is an approximation of the sparse codes generated by SRSC.\n\nThe paper is technically sound, and the authors provide a comprehensive introduction to sparse coding and its limitations. The presentation of the SRSC algorithm is clear and well-motivated, providing ample description of the intuition behind the algorithm, and the optimization algorithm is well-explained with theoretical guarantees. Moreover, the Deep-SRSC algorithm is well-described and appears to be computationally efficient while still providing good performance.\n\nThe results presented in the experimental section are impressive, showing that the proposed SRSC algorithm outperforms existing sparse coding approaches on several benchmark datasets. The experiments also demonstrate that the Deep-SRSC approach is computationally efficient while still providing good performance.\n\nOverall, the paper presents a well-motivated and technically sound approach to improving sparse coding by incorporating the manifold structure of the data. The proposed algorithms appear to be computationally efficient and provide good performance on benchmark datasets. I recommend this submission for publication with minor revisions to improve the clarity of some parts of the paper.","model":"chatGPT","source":"peerread","label":1,"id":2122}
{"text":"The paper \"Support Regularized Sparse Coding and Its Fast Encoder\" by the authors proposes a new method called Support Regularized Sparse Coding (SRSC) to obtain sparse codes that take into account the geometric information and manifold structure of the entire data, which makes it more robust to noise. Additionally, the authors propose a feed-forward neural network called Deep Support Regularized Sparse Coding (Deep-SRSC) as a fast encoder, which is used to approximate the sparse codes generated by SRSC. The paper presents an optimization algorithm for SRSC with theoretical guarantees for optimizing the sparse codes, and provides extensive experimental results to demonstrate the effectiveness of SRSC and Deep-SRSC.\n\nThe strengths of this paper are as follows. Firstly, the paper addresses an important problem in machine learning, which is to obtain sparse codes that capture the underlying structure of the data and are robust to noise. Secondly, the proposed SRSC method is theoretically grounded and based on a sound mathematical framework. Thirdly, the paper proposes a fast encoder in the form of Deep-SRSC to approximate sparse codes generated by SRSC, which can potentially speed up the computation time in practical applications. Finally, the authors provide extensive experimental results to show the effectiveness of SRSC and Deep-SRSC.\n\nHowever, there are some weaknesses that should be addressed in future work. Firstly, the paper does not provide a detailed comparison with other state-of-the-art methods for sparse coding. Secondly, the paper does not discuss the limitations of the proposed method, or provide a thorough analysis of the sensitivity of the method to different parameters. Finally, the paper could benefit from providing more insights into the interpretability of the learned dictionaries and how they relate to the underlying structure of the data.\n\nIn summary, the proposed Support Regularized Sparse Coding (SRSC) and its fast encoder, Deep Support Regularized Sparse Coding (Deep-SRSC), represent a promising new approach to obtain sparse codes that take into account the manifold structure of the data and are robust to noise. The paper provides a sound theoretical basis and extensive experimental results to support their claims. However, further work is needed to provide a better understanding of the limitations and sensitivities of the method as well as its interpretability.","model":"chatGPT","source":"peerread","label":1,"id":2123}
{"text":"Title: Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\n\nAuthors: Wang Ling, Tiago Lu\u00eds, Lu\u00eds Marujo and Chris Dyer\n\nSummary:\nThe paper presents a framework for learning word embeddings (word vectors) and word classifiers jointly for language modeling. The proposed framework ties the weight matrices of the word embeddings and the classifier layer, and uses a novel loss function to optimize them. The authors validate the proposed approach on standard language modeling benchmarks, showing that it outperforms several state-of-the-art methods.\n\nOverall Assessment:\nThe paper presents a well-motivated and clearly explained approach to language modeling, which is an important task in natural language processing. The authors provide a clear theoretical foundation for their method, and show empirical results that demonstrate its effectiveness.\n\nStrengths:\nThe paper provides a thorough explanation of the proposed method and how it relates to existing methods. The authors also provide empirical results that demonstrate the effectiveness of their approach. Moreover, they conduct a thorough analysis of their results and compare them to other state-of-the-art methods.\n\nWeaknesses:\nThe paper could provide more detail on the limitations of their approach and potential areas for future work. Additionally, the evaluation could be expanded beyond language modeling to include other benchmark tasks.\n\nRecommendations:\nThe authors could provide more detail on the practical implications of their approach, including how it may be used in real-world applications. Additionally, they could expand their evaluation to other tasks beyond language modeling to demonstrate the versatility of their approach. Finally, the authors should more clearly address the limitations of their approach and potential areas for future work.","model":"chatGPT","source":"peerread","label":1,"id":2124}
{"text":"The paper \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" proposes a novel framework for language modeling that offers a solution to the problem of gradient vanishing and overfitting that commonly occurs in language models. The paper addresses the central question of whether word vectors and word classifiers can be tied together in a loss function to effectively train language models while overcoming these common obstacles.\n\nStrengths:\n- The paper presents a new framework that effectively ties word vectors and classifiers together in a loss function, which yields significant improvements in language modeling performance.\n- The paper provides a rigorous analysis of the proposed method, demonstrating its effectiveness in overcoming common issues such as gradient vanishing, and overfitting.\n- The paper includes a wide range of experimental evaluations that demonstrate the method's effectiveness on various benchmark datasets, showing significant improvements over existing state-of-the-art models.\n\nWeaknesses:\n- The paper assumes a certain level of technical understanding of deep learning concepts, which may make the content challenging for some readers to follow.\n- While the paper addresses the issue of overfitting, it may not fully explore the potential impact of underfitting.\n- The paper does not provide practical guidelines on how to best implement the proposed framework, making it difficult for researchers to apply the results to their own work.\n\nOverall, \"Tying Word Vectors and Word Classifiers\" presents a novel framework for language modeling that effectively overcomes common challenges such as gradient vanishing and overfitting, offering significant improvements over existing state-of-the-art models. While there are some challenges in following the technical content and implementation guidelines, these are relatively minor in comparison to the overall strengths of the paper. Thus, I would recommend this paper to researchers and practitioners working in the fields of natural language processing and deep learning.","model":"chatGPT","source":"peerread","label":1,"id":2125}
{"text":"The paper, \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,\" proposes a novel theoretical framework that improves learning in language modeling, leading to state-of-the-art performance on the Penn Treebank with a variety of network models. The authors present a convincing argument for why the conventional classification framework used in recurrent neural networks for language modeling is inefficient in terms of utilizing all the available information and using the minimal number of parameters. They then present their framework, which ties together the input embedding and output projection matrices, leading to a significant reduction in the number of trainable variables.\n\nThe paper is well written and easy to follow. I appreciated the authors' clear explanation of the conventional classification framework and the challenges that arise from using it in language modeling. The introduction of their novel framework is well-motivated and clearly explained. The experiments performed on the Penn Treebank are thorough and convincing. The authors show that their framework leads to better performance than previous state-of-the-art models, despite using fewer parameters.\n\nOverall, the paper presents a valuable contribution to the field of language modeling, specifically in terms of reducing the number of trainable parameters required for neural networks. The results are convincing and suggest that this framework has the potential for even greater performance gains in future work. However, further experiments could be done to evaluate the performance of the proposed framework on other datasets and in other language-related tasks.","model":"chatGPT","source":"peerread","label":1,"id":2126}
{"text":"The paper \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" addresses the problem of inefficiencies in learning in language modeling when using recurrent neural networks. The authors propose a novel theoretical framework that ties together the input embedding and output projection matrices, reducing the number of trainable variables and allowing for better learning.\n\nStrengths:\n- The authors introduce a novel theoretical framework that reduces the number of trainable variables and leads to state of the art performance on the Penn Treebank with a variety of network models.\n- The paper clearly explains the problem with the conventional classification framework and the benefits of their proposed loss framework.\n- The experiments conducted are rigorous and include comparisons to previous state of the art models.\n\nWeaknesses:\n- The paper can be challenging to follow for those without a strong background in neural networks and language modeling.\n- The authors could have provided more explanation and detail on the intuition behind their proposed framework.\n- The experiments could have included other datasets besides the Penn Treebank to test the effectiveness of the proposed framework in a broader range of language modeling tasks.\n\nOverall, the paper presents a novel framework for language modeling that reduces the number of trainable variables and achieves state of the art results. It is a valuable contribution to the field of natural language processing, although some improvements on the presentation and explanation of the framework could be made.","model":"chatGPT","source":"peerread","label":1,"id":2127}
{"text":"The paper \"Towards Deep Interpretability (MUS-ROVER II): Learning Hierarchical Representations of Tonal Music\" presents an interesting approach to represent tonal music in a hierarchical manner, allowing for more interpretable and meaningful representations to be used for various tasks in music analysis.\n\nThe authors do a good job in explaining the motivation behind their approach and how it differs from previous methods. The use of a hierarchical approach allows for more nuanced representations of tonal music that capture both local and global features, making it better suited for music analysis tasks.\n\nAdditionally, the authors provide thorough details on the model architecture, hyperparameters, and training process, allowing for reproducibility and transparency.\n\nThe experimental evaluation of the methodology on a tonal music dataset shows promising results, with the hierarchical representations outperforming previous state-of-the-art methods.\n\nOne suggestion for improvement would be to provide more detailed explanations and examples of how the hierarchical representations can be used for specific music analysis tasks. This would help readers better understand the potential applications of the methodology and how it can be used in practice.\n\nOverall, the paper presents an innovative approach to represent tonal music in a hierarchical manner, with promising results for music analysis tasks.","model":"chatGPT","source":"peerread","label":1,"id":2128}
{"text":"The paper \"Towards Deep Interpretability (MUS-ROVER II): Learning Hierarchical Representations of Tonal Music\" addresses the problem of learning hierarchical representations of tonal music through deep learning methods. Specifically, it attempts to provide a framework for analyzing tonal music using deep learning approaches that can provide interpretable insights.\n\nOne of the strengths of this paper is its clear and concise explanations of the deep learning techniques used in the experiments. The paper also presents a novel approach to analyze tonal music which can be useful to researchers in the music domain. Additionally, the paper provides thorough experimentation with different methods of representation learning and evaluation metrics, making it easy for readers to verify the findings.\n\nHowever, one of the weaknesses of this paper is the limited size of the dataset used for experimentation, which could be a potential cause of issues related to the generalizability of the results. Additionally, the paper doesn't provide much detail on the scope of interpretability achieved by the deep learning models and how the insights obtained can be leveraged. \n\nOverall, the paper presents a potentially valuable contribution to the field of music analysis using deep learning methods but requires further investigation to establish practical utility.","model":"chatGPT","source":"peerread","label":1,"id":2129}
{"text":"The paper \"Towards Deep Interpretability (MUS-ROVER II): Learning Hierarchical Representations of Tonal Music\" presents a continuation of the authors' previous work on automatic theorists in music. Specifically, the paper focuses on the ability of the new rover, MUS-ROVER II, to learn hierarchical representations of tonal music and explore its potential applications in music theory education, style recognition, and synthesis.\n\nThe paper provides a clear overview of the background and motivation of the study, explaining the importance of music theory in capturing underlying concepts in music styles and composers' decisions. The authors describe their previous work on algorithmic concept learning for tonal music, which extracted interpretable rules for composition from symbolic music scores. The paper then introduces MUS-ROVER II, which builds on the previous work by incorporating a deeper learning hierarchy and an adaptive memory selection mechanism to enhance interpretability.\n\nThe evaluation methodology and results are well-described and demonstrate the effectiveness of MUS-ROVER II in generating customizable syllabi for composition learning. The paper also discusses the potential applications of MUS-ROVER II beyond music theory education, such as style recognition and synthesis, and notes that the rover can be adapted to other domains.\n\nOverall, the paper makes a valuable contribution to the field of music theory and machine learning. The study presented in the paper is original and relevant to the field. The methodology, evaluation, and results are robust and well-presented. The paper concludes with insights on future directions where this technology can be applied. The only major criticisms are that the paper could have provided a more detailed description of the technical aspects of the proposed methodology and a discussion on the limitations of the presented work. However, these minor issues do not detract significantly from the paper's overall quality. Therefore, I recommend the acceptance of this paper for publication.","model":"chatGPT","source":"peerread","label":1,"id":2130}
{"text":"The paper \"Towards Deep Interpretability (MUS-ROVER II): Learning Hierarchical Representations of Tonal Music\" presents a novel approach to building automatic theorists, or rovers, which can learn and represent music concepts in a hierarchy to better facilitate human interpretation and education. The paper builds upon previous work in algorithmic concept learning of tonal music and aims to address limitations in representation hierarchy and adaptive memory selection. The authors train a new rover, MUS-ROVER II, on Bach's chorales and demonstrate its capability in producing customizable syllabi for learning compositional rules.\n\nThe paper's strengths lie in its approach to the problem of automatic music theory learning. The authors make an interesting contribution to the field by introducing a hierarchical representation of music concepts and integrating adaptive memory selection mechanisms that enable the rover to learn dynamically. The paper presents a methodological approach that is well-supported by empirical evidence, as demonstrated by the results of the study. The authors also highlight the potential value of their rover in style recognition and synthesis, as well as applications beyond music, which adds to the paper's significance.\n\nHowever, there are several weaknesses in the paper. Firstly, the paper could benefit from a more comprehensive literature review that situates the research in the broader context of automatic music theory learning. This would allow for a clearer identification of the paper's contributions in relation to competing approaches. Additionally, the paper could provide more detailed explanations of the technical aspects of the methodology, particularly with regards to the hierarchical representation of concepts and the use of adaptive memory selection. Lastly, the paper's claims regarding the potential applications of the rover beyond music could benefit from further development and evidence.\n\nOverall, \"Towards Deep Interpretability (MUS-ROVER II): Learning Hierarchical Representations of Tonal Music\" presents an innovative approach to building automatic music theorists that addresses limitations in previous approaches. While the paper could benefit from improvements in certain areas, it presents a promising avenue for future research in automatic music theory learning.","model":"chatGPT","source":"peerread","label":1,"id":2131}
{"text":"Title: Discrete Variational Autoencoders\n\nAuthors: John Doe, Jane Smith, and Sarah Lee\n\nSummary: \n\nThe paper proposes a novel approach to the traditional Variational Autoencoder (VAE) by introducing a discrete version called Discrete Variational Autoencoder (DVAE). The authors demonstrate that DVAE can be used effectively in data generation tasks such as image generation from binary codes, sequence generation, and text generation. The proposed method is evaluated against traditional VAEs and other state-of-the-art models through qualitative and quantitative experiments.\n\nReview: \n\nOverall, the paper presents a thorough and well-written study on the proposed DVAE method. The authors clearly explain the motivation behind their approach and provide a detailed explanation of its key components. The experiments conducted are well designed and the results are convincing. The use of quantitative metrics such as log-likelihood, BLEU and FID scores as well as qualitative evaluations add credibility to the proposed method.\n\nOne strength of the paper is the clear description of the experiments and their results. The authors provide plenty of details, including code and data availability, which makes it easy to reproduce their work. Additionally, the comparison of the proposed method with several state-of-the-art models enhances the value of the study.\n\nOne area for improvement is the discussion of the limitations of the proposed method. The authors briefly mention some potential drawbacks related to the high demand for computation resources and the non-differentiability of the discrete distribution. However, these limitations could be further developed in the discussion section to provide more insight into the applicability of the proposed method in different scenarios.\n\nAnother suggestion for improvement is the presentation of the results. The paper uses tables and plots to report the results. However, some of them are difficult to read due to the small font size and the complexity of the information presented. These figures could be simplified or better annotated to make them more accessible to readers.\n\nIn summary, the paper presents an interesting contribution to the field of Variational Autoencoders by introducing a discrete version called DVAE. The proposed method is thoroughly evaluated through experiments and compared to state-of-the-art models. The study is well-written and provides valuable insights into the applicability of DVAE in data generation tasks.","model":"chatGPT","source":"peerread","label":1,"id":2132}
{"text":"The paper \"Discrete Variational Autoencoders\" addresses the problem of developing a generative model for discrete data such as text and music. The authors introduce the concept of a discrete variational autoencoder (DVAE), which uses a discrete latent variable to model categorical data. The DVAE is trained using a variational lower bound objective function, which enables the model to capture the underlying structure of the data and generate new samples.\n\nStrengths:\n- The authors provide a clear and concise introduction to the problem domain and the proposed solution.\n- The paper provides a thorough description of the DVAE model, including its architecture and training procedure.\n- The authors conduct experiments on multiple datasets and demonstrate that the DVAE outperforms existing state-of-the-art models for generating discrete data.\n- The paper includes detailed analysis of the results, which sheds light on the strengths and limitations of the DVAE.\n\nWeaknesses:\n- The paper is fairly technical and may be difficult for non-experts to understand.\n- Although the experiments are thorough, the paper does not provide a comparison of the computational efficiency of the DVAE compared to existing models.\n- The paper does not provide a clear explanation of how the DVAE can be used in real-world applications.\n\nOverall, the paper presents a significant contribution to the field of generative modeling for discrete data. The DVAE model is well-described and the experiments demonstrate its effectiveness. However, further research is needed to fully understand its potential for real-world applications, as well as its limitations in terms of computational efficiency.","model":"chatGPT","source":"peerread","label":1,"id":2133}
{"text":"The paper \"Discrete Variational Autoencoders\" proposes a novel method to train probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The authors show that this approach outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.\n\nThe paper is well-organized and well-written. The motivation and context of the study are clearly explained at the outset, and the research question is well stated. The authors provide good background information on the existing literature in the field, which helps to contextualize the study. \n\nThe novel method presented in this paper is technically rigorous and well-explained, and the authors provide ample detail to enable the replication of the experiments by other researchers. The results presented in the paper are impressive, with the proposed method outperforming state-of-the-art methods on the evaluation datasets. \n\nA potential weakness of the study is that the proposed approach may be limited to the specific datasets studied (i.e. MNIST, Omniglot, and Caltech-101 Silhouettes), which are relatively small and present a specific type of problem. Further investigation is needed to determine the generalizability of the approach to other datasets and problem domains. \n\nOverall, I believe that this paper represents a significant contribution to the field of probabilistic modeling with discrete latent variables, and I would recommend it for publication in a peer-reviewed journal.","model":"chatGPT","source":"peerread","label":1,"id":2134}
{"text":"This paper, titled \"Discrete Variational Autoencoders,\" addresses the problem of efficiently training probabilistic models with discrete latent variables. While such models are well-suited for datasets composed of discrete classes, they are challenging to train using traditional methods due to difficulties with backpropagation through discrete variables. The authors present a novel method for overcoming this challenge by utilizing the variational autoencoder framework, which allows for backpropagation through the discrete latent variables.\n\nThe paper includes a detailed explanation of the associated class of probabilistic models, which consists of an undirected discrete component and a directed hierarchical continuous component. The authors argue that this framework efficiently learns both the class of objects in an image and their specific realization in pixels from unsupervised data, outperforming state-of-the-art methods on several datasets.\n\nOne strength of this paper is the thoroughness of the explanations and derivations provided. The authors provide a clear and detailed explanation of the method used for training the probabilistic models, which is essential for other researchers to replicate and build upon the work presented here. Additionally, the authors demonstrate the effectiveness of this method on several well-known datasets, making it easier for others to compare their results with those presented in this paper.\n\nOne weakness of this paper is the lack of discussion about the potential limitations of this method. While the authors acknowledge that challenges remain in training probabilistic models with discrete latent variables, they do not explicitly address any specific concerns about their approach. It would be helpful for future work in this area to include a more comprehensive discussion of the method's limitations and potential areas for improvement.\n\nOverall, this paper provides a valuable contribution to the field of probabilistic modeling with discrete latent variables. The method presented here has the potential to improve the efficiency and effectiveness of training such models, which could have a significant impact in fields such as computer vision and natural language processing.","model":"chatGPT","source":"peerread","label":1,"id":2135}
{"text":"Title: Do Deep Convolutional Nets Really Need to be Deep and Convolutional?\n\nAuthors: Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun\n\nThe paper titled \"Do Deep Convolutional Nets Really Need to be Deep and Convolutional?\" investigates the hypothesis that deep convolutional neural networks, which have shown remarkable success in image classification tasks, may not necessarily require both depth and convolutional layers to achieve high levels of accuracy. The authors propose a new architecture, SqueezeNet, which reduces the number of parameters without sacrificing performance.\n\nThe paper provides a clear and well-structured introduction to the topic and an excellent overview of the related work done in the area. The authors have presented the motivation and goals of their research, which are clearly identified, and the experimental evaluation has been conducted rigorously using standard practices.\n\nThe authors have clearly explained the architecture of the proposed model and the specific techniques used to refine the model. The experimental results have demonstrated that the proposed network does indeed achieve excellent performance with significantly fewer parameters compared to traditional deep convolutional models. However, the paper lacks discussion on the limitations and drawbacks of the proposed method, which could have been informative to the reader.\n\nThe paper is written in a clear and concise manner, making it easy to follow, and the authors have provided insightful interpretations of the results obtained. The paper is well-organized, and the research intention was adequately achieved. However, it would have been beneficial to include more comparisons with other state-of-the-art image recognition models and datasets to support the claims made in the paper.\n\nOverall, the paper presents an interesting and worthwhile approach to explore alternate architectures for convolutional neural networks, and its results are highly persuasive. The authors should be commended for their excellent work on this paper, and I would highly recommend it to be accepted for publication.","model":"chatGPT","source":"peerread","label":1,"id":2136}
{"text":"The paper \"Do Deep Convolutional Nets Really Need to be Deep and Convolutional?\" addresses the fundamental question whether deep and convolutional neural networks are always necessary for high-performance image classification tasks. The authors propose a new architecture that combines convolutional and fully connected layers into a shallow neural network and show that it can achieve similar accuracy to deeper and more convolutional models on benchmark datasets.\n\nOne strength of this paper is its clear and concise presentation of a well-defined research question. The authors systematically compare the performance of their shallow architecture against traditional deep convolutional networks and provide a critique of the widespread belief that depth and convolutions are essential for achieving high accuracy. The experimental results are rigorous and include detailed analysis of the effects of hyperparameters on the performance of each model.\n\nHowever, one weakness of this paper is that it only evaluates the performance of the proposed shallow architecture on image classification tasks. It would be interesting to see how the model generalizes to other computer vision tasks, such as object detection or semantic segmentation. In addition, the authors do not provide much insight into the underlying mechanisms that drive the performance of their shallow architecture. A more detailed analysis of the model's internal workings could help validate their claim that convolutional layers may not always be essential for achieving high accuracy.\n\nOverall, this paper provides a compelling argument for the need to question long-held assumptions in the field of deep learning and provides empirical evidence that shallow architectures may be a viable alternative to deeper and more complex models for certain tasks.","model":"chatGPT","source":"peerread","label":1,"id":2137}
{"text":"Peer review for \u201cDo Deep Convolutional Nets Really Need to be Deep and Convolutional?\u201d\n\nThis paper provides important empirical evidence to address a long-standing question in deep learning research: Do deep convolutional neural networks (CNNs) really need to be both deep and convolutional to perform well on image classification tasks? While previous research has suggested that it may be possible for shallow neural networks to perform comparably to deep neural networks, this paper takes a closer look at the use of distillation techniques to train shallow models and finds that multiple layers of convolution are necessary for accurate classification.\n\nThe study is well-designed and thorough, using multiple experiments with different network architectures and optimization methods to demonstrate the necessity of depth and convolution. The authors clearly describe their methodology and results, making the paper easy to follow and understand for a wide range of audiences.\n\nOne strength of the paper is that it provides a clear rationale for its experiments and hypotheses. Specifically, the authors speculate that convolutional layers are necessary for image recognition tasks because they can capture spatial relationships between pixels. They support this argument with experimental evidence, showing that shallow models that lack convolutional layers are not able to capture these important spatial relationships as well as deeper CNNs.\n\nOverall, the paper is well-written and provides timely and valuable insights for deep learning practitioners and researchers. The authors should be commended for their careful experimentation and clear presentation of results. I recommend this paper for publication in a high-quality machine learning or computer vision conference or journal.","model":"chatGPT","source":"peerread","label":1,"id":2138}
{"text":"Peer Review: \"Do Deep Convolutional Nets Really Need to be Deep and Convolutional?\"\n\nThe paper under review addresses the significant question of whether deep convolutional models need to be both deep and convolutional to achieve high accuracy in training. The authors aim to provide an empirical demonstration of the importance of depth and convolution in deep convolutional models, even when trained with techniques like distillation.\n\nThe main strength of the paper is its experimental approach, which involves a detailed analysis of the performance of student models with various depths and convolutional layers on the CIFAR-10 dataset. The authors also provide a comprehensive evaluation of distillation techniques for training shallow models with high accuracy. Overall, the paper convincingly demonstrates that deep convolutional models indeed require both depth and convolutional layers to achieve comparable accuracy as deep convolutional teacher models.\n\nHowever, there are a few areas where the paper could be improved. Firstly, while the authors provide a thorough explanation of their experiments, they could have elaborated further on why the results of their investigations are significant. Specifically, it would have been helpful to connect the implications of their findings to the broader concept of deep learning and how this could shape future research in this area.\n\nSecondly, in terms of the scope of the paper, the authors only consider the CIFAR-10 dataset. It would be interesting to see if their conclusions could be extended to other image classification datasets. Additionally, the authors could have included a more in-depth comparison of their results with those of prior studies in the same domain.\n\nIn conclusion, the paper provides a compelling empirical demonstration that both depth and convolutional layers are essential for achieving high accuracy in deep convolutional models, even when trained with distillation techniques. While there is room for improvement, the authors make a significant contribution to the field of deep learning by shedding light on the importance of depth and convolution in deep convolutional models.","model":"chatGPT","source":"peerread","label":1,"id":2139}
{"text":"Title: Reasoning with Memory Augmented Neural Networks for Language Comprehension\n\nAuthors: [list of authors]\n\nJournal: [journal name]\n\nIn this paper, the authors propose a neural network model for language comprehension that includes a memory module to support reasoning. The authors claim that this model performs better than existing models that do not incorporate such memory modules.\n\nOverall, the paper is well-written, and the authors begin by providing helpful background information on previous work in natural language processing (NLP) and neural network models. The authors then clearly explain the structure and functioning of their proposed model, providing technical details and diagrams to support their explanations.\n\nThe results of the experiments presented in this paper are promising. The authors compare the performance of their model to existing models on several datasets and show that their model outperforms these models on a language understanding task. The authors also conduct an analysis of their model\u2019s attention mechanism, providing further insight into how their model is able to support reasoning.\n\nHowever, I have several concerns with this paper. Firstly, while the authors provide insight into the attention mechanism of their model, I think they could give more detail on how the memory module specifically supports reasoning. Additionally, the authors only test their model on two datasets, and it would be helpful if they could include more datasets to further validate their claims.\n\nI would recommend this paper to researchers in the NLP field, especially those interested in neural network models for language comprehension. Overall, the proposed model and results show promise, but further work is necessary to fully validate the effectiveness of the memory module for reasoning.","model":"chatGPT","source":"peerread","label":1,"id":2140}
{"text":"The paper \"Reasoning with Memory Augmented Neural Networks for Language Comprehension\" addresses the issue of language comprehension using memory augmented neural networks. Specifically, the paper introduces an approach that utilizes a novel neural network architecture with a memory component, named Neural Turing Machine (NTM), to improve performance on language comprehension tasks.\n\nThe paper's strength lies in its novel approach to language comprehension using the NTM. The paper provides a detailed description of the model and its components, which allows readers to understand the model's inner workings. Additionally, the paper provides empirical evidence of the model's performance on several benchmark datasets, demonstrating its effectiveness in improving language comprehension.\n\nHowever, there are some weaknesses in the paper that need to be addressed. One key weakness is the lack of thorough comparison with other state-of-the-art models. It is unclear from the paper how well the model performs compared to other existing models for language comprehension. Furthermore, the paper could benefit from an in-depth analysis of the model's limitations, such as identifying scenarios where the model may not perform well, and how to overcome these limitations.\n\nOverall, this paper presents a novel approach to language comprehension using a memory augmented neural network architecture, which shows promising results. However, additional experiments and comparative analysis with other models are needed to validate the effectiveness of the proposed approach.","model":"chatGPT","source":"peerread","label":1,"id":2141}
{"text":"Overall, this paper presents a novel and interesting approach for hypothesis testing using memory augmented neural networks. The experimental results demonstrate the effectiveness of the proposed approach in the language comprehension task, achieving state-of-the-art performance on standard benchmark datasets.\n\nThe paper's introduction provides a clear motivation and context for the proposed approach. The presentation of the methodology and the hypothesis testing loop is well-written and detailed, providing sufficient information for readers to understand the approach. The use of Neural Semantic Encoder models is a suitable choice for the language comprehension task, and the authors provide a clear explanation of their implementation.\n\nAdditionally, the experimental results are presented in a clear and concise manner, showing the superiority of the proposed approach over previously published results. The authors also provide a detailed analysis of the model's performance and demonstrate the consistency of the results.\n\nThe paper could benefit from further discussion and analysis of the limitations of the approach, as well as potential directions for future research. Additionally, the authors could consider discussing the implications of their findings in relation to natural language processing and cognitive science.\n\nOverall, this paper presents a compelling approach for hypothesis testing in language comprehension, and the experimental results support the effectiveness of the proposed approach. This paper has the potential to contribute to the field of natural language processing, and I would recommend it for publication.","model":"chatGPT","source":"peerread","label":1,"id":2142}
{"text":"Paper Title: Reasoning with Memory Augmented Neural Networks for Language Comprehension\n\nProblem\/Question Addressed:\nThis paper addresses the problem of how to improve language comprehension tasks by introducing a novel computational hypothesis testing approach based on memory augmented neural networks. The authors aim to refine the previously formed hypothesis and generate new ones for testing, supported by this approach.\n\nStrengths:\nThe paper presents a novel computational approach to hypothesis testing based on neural networks to improve language comprehension. The proposed approach is innovative, and it addresses an essential cognitive process that supports human reasoning. The authors apply their approach to the evaluation of the Neural Semantic Encoders (NSE) models, which is an interesting perspective, and the use of NSE models for hypothesis generation and testing performance is impressive. The experimental results and comparisons with the previous state-of-the-art models in the domain of machine comprehension benchmarks such as the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets are impressive, which demonstrates that their approach is effective in improving the performance of language comprehension systems.\n\nWeaknesses:\nOne potential issue with the paper is that it lacks a detailed description of the potential limitations of the proposed hypothesis testing approach. Although the authors show significant improvements over previous state-of-the-art models, the paper does not explore some possible areas for improvement. Besides, the paper would benefit from providing a more comprehensive and detailed explanation of the entire hypothesis testing loop, including how the system updates the current hypothesis in every loop and how it decides when to stop. Moreover, the authors could also provide more intuition for their approach to make it more understandable to the general audience.\n\nOverall, the paper is well-written, and the proposed approach is innovative and useful. Despite some potential limitations, the paper's performance improvements over previous state-of-the-art models demonstrate the usefulness of the proposed approach in addressing the problem of language comprehension.","model":"chatGPT","source":"peerread","label":1,"id":2143}
{"text":"Title: Learning Recurrent Representations for Hierarchical Behavior Modeling\n\nAuthors: X. Zhang, Y. Pan, and Z. Wang\n\nJournal: IEEE Transactions on Neural Networks and Learning Systems\n\nIntroduction:\nThe authors present a novel approach for hierarchical behavior modeling using recurrent neural networks. The paper highlights the importance of modeling temporal dependencies in hierarchical behavior modeling and proposes a new architecture, named Hierarchical Recurrent Neural Networks (HRNN), for this purpose.\n\nStrengths:\nThe paper provides a well-structured and clear explanation of the proposed method, including the HRNN architecture and the training methodology. The authors also demonstrate the effectiveness of the proposed approach through experiments conducted on synthetic and real-world datasets. The experiments show significant improvements in both predictive accuracy and efficiency.\n\nWeaknesses:\nThe paper lacks a thorough analysis of the capacity and limitations of the proposed method. The authors could benefit from exploring the sensitivity of the HRNN model to its hyperparameters, such as the number of hidden nodes and variable lengths of time intervals. Additionally, the authors could also provide a more comprehensive comparison with existing approaches for hierarchical behavior modeling, including both classical and modern machine learning methods.\n\nOverall Evaluation:\nThe proposed method presents a significant contribution to the field of behavior modeling by proposing a novel approach for recurrent hierarchical modeling of complex temporal dependencies. The paper is well-written, and the experiments are well designed to demonstrate the effectiveness of the proposed methodology.  However, given the increased interest in neural network-based approaches for behavior modeling, further investigations on the proposed approach's limitations and comparison with existing approaches would make this paper more impactful.","model":"chatGPT","source":"peerread","label":1,"id":2144}
{"text":"The paper \"Learning Recurrent Representations for Hierarchical Behavior Modeling\" addresses the problem of modeling human behavior in a hierarchical way using a machine learning approach. The authors present a novel method that trains a recurrent neural network to learn a latent representation of behavior that captures both temporal and hierarchical structure. This method is evaluated on two datasets, and the results are compared to other state-of-the-art methods.\n\nOne strength of this paper is the clarity and conciseness of its presentation. The authors provide a comprehensive overview of related work and explain their own method in detail, making it easy for readers to follow their approach. Additionally, the experiments are well-designed and demonstrate the effectiveness of the proposed method in capturing both the temporal and hierarchical structure of human behavior.\n\nHowever, there are also some weaknesses in the paper. First, the authors do not provide a clear justification for why their method is superior to other state-of-the-art methods, apart from the experimental results. A detailed analysis of the strengths and limitations of each method would have provided more insights into the proposed approach. Second, the authors do not discuss the interpretability of the learned representations. While the proposed method achieves higher accuracy, it would be interesting to know how the recurrent representations look like and if they can help to understand the hierarchical structure of human behavior.\n\nOverall, the paper \"Learning Recurrent Representations for Hierarchical Behavior Modeling\" presents a novel method for modeling hierarchical human behavior that performs well on two datasets. Despite some weaknesses, this paper contributes to the research on machine learning-based models for behavior analysis, and the proposed approach could be useful in various applications such as robotics, human-robot interaction, and activity recognition.","model":"chatGPT","source":"peerread","label":1,"id":2145}
{"text":"Paper Review:\n\nThe paper proposes a novel framework for detecting action patterns from motion sequences and modeling the sensory-motor relationship of animals, using a generative recurrent neural network. The authors demonstrate the effectiveness of their proposed framework by applying it to two types of tracking data, fruit fly behavior and online handwriting.\n\nThe authors' approach of using a generative model to predict future motion and detecting actions is interesting and innovative. The utilization of unlabeled data for motion prediction in the generative part of the proposed network is noteworthy, as it shows that the network can perform well with scarce training labels.\n\nThe authors also demonstrate that the proposed network can learn to represent high-level phenomena such as writer identity and fly gender without supervision. This is an important aspect, as unsupervised learning can significantly reduce the need for manual annotation and lead to better generalization.\n\nThe generated motion trajectories presented in the paper look realistic and can be used to qualitatively evaluate the model's learning of generative control rules. This is a crucial aspect, as it indicates that the proposed network has learnt meaningful representations of the underlying dynamics of motion data.\n\nThe evaluation of the proposed framework on two different datasets demonstrates the robustness of the proposed approach. However, one limitation of the paper is that the authors do not compare their approach with other state-of-the-art techniques in the field.\n\nOverall, the paper presents a unique approach to detect action patterns from motion sequences using a generative recurrent neural network. The authors provide compelling evidence showing that their proposed framework is effective in modeling the underlying sensory-motor relationship of animals.","model":"chatGPT","source":"peerread","label":1,"id":2146}
{"text":"The paper titled \"Learning Recurrent Representations for Hierarchical Behavior Modeling\" proposes a new framework that detects action patterns by modeling the sensory-motor relationship of animals, using a generative recurrent neural network. The authors test their framework on two types of tracking data, fruit fly behavior and online handwriting. Their results show three key findings: 1) taking advantage of unlabeled sequences improves action detection performance when training labels are scarce; 2) the network learns to represent high-level phenomena such as writer identity and fly gender, without supervision; and 3) simulated motion trajectories generated by treating motion prediction as input to the network look realistic and may be used to qualitatively evaluate the model's generative control rules.\n\nOverall, the paper has several strengths. Firstly, the authors present a valuable contribution to the field of behavior modeling by proposing a framework that detects action patterns using a generative recurrent neural network. Secondly, the use of unlabeled sequences to improve action detection performance is an innovative idea that could help overcome the scarcity of labeled training data. Finally, the paper provides evidence that the framework performs well on two different types of tracking data, which supports the generalizability of the approach.\n\nHowever, there are also some weaknesses that need to be addressed. Firstly, the paper may benefit from a stronger motivation for the problem of behavior modeling and action pattern detection, which is lacking in the introduction. Secondly, the authors do not compare their approach with other existing methods, which limits the understanding of how innovative their framework is compared to the state-of-the-art techniques. Furthermore, it would be valuable to provide more details about the architecture and training of the model, especially in terms of the hyperparameters used. Lastly, some of the claims made by the authors, such as the network's ability to represent high-level phenomena like fly gender and writer identity, require further justification and validation.  \n\nIn summary, the paper makes a valuable contribution to behavior modeling by proposing a new framework that detects action patterns using a generative recurrent neural network. The results presented are promising, but the paper could be further improved by addressing the weaknesses mentioned above.","model":"chatGPT","source":"peerread","label":1,"id":2147}
{"text":"Title: Adversarial Machine Learning at Scale: Evaluating and Mitigating Risks of Adversarial Attacks in Online Learning Systems \n\nAuthors: John Smith, Jane Doe, and Sarah Lee \n\nIn this paper, the authors present a comprehensive study on the risks and mitigation techniques of adversarial attacks in online learning systems. The paper predominantly focuses on evaluating the effectiveness of different adversarial examples generated from various techniques against specific machine learning models.\n\nThe authors have conducted extensive experiments using multiple datasets, including the MNIST digit classification dataset, the breast cancer classification dataset, and the ImageNet dataset, to analyze the performance of different adversarial attacks. The authors have also proposed a new method called the Proportional Adversarial Training (PAT), which they show provides a robust defense against adversarial attacks.\n\nThe paper is well-structured and clearly describes the objectives, hypotheses, experiments, and findings of the study. The authors have provided a comprehensive literature review, which highlights the current state of research on adversarial machine learning. They have also explained the different types of adversarial attacks in detail and provided an overview of the potential risks and threats they pose to online learning systems.\n\nThe experiments conducted by the authors are thorough, and their findings are significant. The authors have shown that different adversarial attacks can significantly affect the accuracy of different learning models, and that traditional defense mechanisms are often insufficient in mitigating these attacks. The authors\u2019 proposed approach, PAT, is demonstrated to be a promising technique for defending against adversarial attacks.\n\nOverall, the study presented in this paper is an important and insightful contribution to the field of adversarial machine learning. The authors\u2019 proposed method, PAT, has the potential to offer a robust solution to the problem of adversarial attacks in online learning systems, and further research is warranted to explore its efficacy in different real-world scenarios.","model":"chatGPT","source":"peerread","label":1,"id":2148}
{"text":"The paper \"Adversarial Machine Learning at Scale\" addresses the problem of adversarial attacks on machine learning systems at a large scale. The authors aim to conduct a comprehensive study of the impact of different adversarial attacks on machine learning models, exploring the effectiveness of various defenses and their scalability.\n\nThe strengths of this paper lie in its thoroughness and extensive experimentation. The authors have provided a detailed analysis of the impact of different types of attacks such as FGSM, PGD, and Carlini-Wagner attacks on image classification models such as AlexNet, Inception-v3, and ResNet-50. They have also proposed a defense called DefenseGAN to address adversarial attacks on generative models such as Deep Convolutional GANs (DCGANs), which is a novel contribution. The paper also examines the scalability of the proposed defenses, providing insights into their efficacy on large datasets.\n\nHowever, the paper's weaknesses lie in its lack of novelty in some of its proposed solutions. Some of the defense mechanisms proposed, such as adversarial training and ensemble methods, have been previously studied and presented in literature. Additionally, the paper could have provided more information on the specific hyperparameters used in the experiments to ensure reproducibility.\n\nIn conclusion, \"Adversarial Machine Learning at Scale\" is a valuable paper for the machine learning community, providing a comprehensive study of adversarial attacks and defense mechanisms at a large scale. While some of the proposed solutions may not be entirely novel, the paper's extensive experimentation and evaluations make up for it.","model":"chatGPT","source":"peerread","label":1,"id":2149}
{"text":"Title: Peer Review for Adversarial Machine Learning at Scale\n\nThe paper discusses the crucial issue of adversarial examples and adversarial training, which have become crucial in the field of machine learning. The authors' primary focus is on applying adversarial training to ImageNet, and they make important contributions, such as offering recommendations on scaling adversarial training to large models, observing the effect adversarial training has on single-step attack methods, observing the transferability of attacks, and resolving the \"label leaking\" effect that causes adversarially trained models to perform better on adversarial examples than clean ones.\n\nThe paper is well-written, and the authors are clear and concise, making it understandable for an audience with varying degrees of knowledge about the topic. The paper's organization is also commendable, with each section appropriately flowing into the next. Moreover, the authors provide sufficient details for their method's implementation and evaluation, making their approach reproducible.\n\nHowever, there is a significant limitation to the study. While the authors acknowledge the issue of black-box attacks, their study does not address it, and this is a significant limitation of the paper. To further the applicability of the study in a more comprehensive manner, the authors could investigate more against black-box attacks to have a well-rounded approach.\n\nOverall, the paper is an important contribution to the field of adversarial machine learning. Even though the study has a limitation regarding black-box attacks, it provides valuable recommendations for scaling adversarial training and is an essential step towards making adversarial training applicable to larger datasets and models.","model":"chatGPT","source":"peerread","label":1,"id":2150}
{"text":"The paper \"Adversarial Machine Learning at Scale\" addresses the problem of adversarial attacks on machine learning models, specifically the transferability of adversarial examples between models and the vulnerability of these models to black-box attacks. The paper primarily focuses on the application of adversarial training to ImageNet to create a more robust and accurate model.\n\nOne of the strengths of this paper is its clear and concise presentation of the research findings. The authors provide a thorough overview of adversarial training, the challenges of scaling it to large datasets, and the potential benefits of applying it to ImageNet. The paper provides valuable insights into the effectiveness of adversarial training for improving the robustness of machine learning models.\n\nAnother strength of the paper is the authors' recommendations for successfully scaling adversarial training to large models and datasets. Their research findings suggest that multi-step attack methods are less transferable than single-step attack methods, which provides critical information on how attackers can best mount black-box attacks.\n\nOne potential weakness of the paper is that it mainly focuses on adversarial examples in the context of image classification. It would be interesting to see how the findings in this paper could be applied to other applications of machine learning, such as natural language processing or speech recognition.\n\nIn conclusion, \"Adversarial Machine Learning at Scale\" provides valuable insights into the application of adversarial training on large datasets such as ImageNet. It highlights the importance of taking adversarial examples seriously in machine learning security and presents practical recommendations for successfully scaling adversarial training to improve model robustness.","model":"chatGPT","source":"peerread","label":1,"id":2151}
{"text":"Introduction:\n\nThe paper presents a novel approach for spatiotemporal visual attention using a Recurrent Mixture Density Network. The authors suggest that the proposed method provides an efficient method for attention modeling in dynamic visual scenes.\n\nStrengths:\n\nThe paper includes a thorough literature survey detailing various approaches to spatiotemporal attention modeling. The authors clearly demonstrate the necessity for models capable of capturing the dynamics of visual scenes.\n\nThe proposed model incorporates a recurrent neural network for temporal modeling and a mixture density network for spatiotemporal attention modeling. The authors demonstrate that the proposed model successfully captures visual attention across multiple frames of a dynamic scene.\n\nThe experimental results demonstrate the efficacy of the proposed approach. The authors successfully demonstrate that the proposed model outperforms several state-of-the-art approaches on a comprehensive benchmark dataset.\n\nWeaknesses:\n\nThe authors do not compare their approach to other recurrent attention models. It would be beneficial to know how the proposed model compares to other approaches that utilize recurrent neural networks for attention modeling.\n\nThe paper does not discuss the interpretability of the model. While the proposed model successfully models spatiotemporal attention, it is unclear how well the model can be interpreted, and what aspects of the model are most crucial for attention modeling.\n\nConclusion:\n\nOverall, the paper presents a novel approach for spatiotemporal attention modeling that successfully captures the dynamics of visual scenes. The experimental results demonstrate that the proposed approach outperforms several state-of-the-art approaches. However, some issues related to the interpretability of the model and a lack of comparison to other recurrent attention models should be addressed in future work.","model":"chatGPT","source":"peerread","label":1,"id":2152}
{"text":"The paper \"Recurrent Mixture Density Network for Spatiotemporal Visual Attention\" addresses the problem of spatiotemporal visual attention in natural scenes. The authors propose a recurrent mixture density network architecture to predict the spatial locations of objects in video frames. The method is evaluated on a dataset of natural videos, and the results demonstrate that the proposed method outperforms current state-of-the-art methods for spatiotemporal visual attention prediction.\n\nStrengths:\n1. The proposed method is based on a recurrent mixture density network, which is a state-of-the-art technique for modeling complex distributions. The architecture is well-suited to the problem of spatiotemporal visual attention prediction.\n\n2. The authors conducted a thorough experimental evaluation of the proposed method on a large-scale dataset. The results show that their approach outperforms current state-of-the-art methods.\n\n3. The proposed method is generalizable and can be applied to various spatiotemporal attention prediction tasks.\n\nWeaknesses:\n1. The paper lacks a detailed discussion on the limitations and potential drawbacks of the method. Further insights into the challenges and limitations of the method could be helpful to readers.\n\n2. Although the experimental evaluation is comprehensive, the authors could have also included a more detailed comparison with other state-of-the-art models. \n\n3. The paper could have provided more insights on how the proposed model can help in real-world applications, in addition to its applicability in predicting visual attention in natural scenes.\n\nOverall, the paper \"Recurrent Mixture Density Network for Spatiotemporal Visual Attention\" presents an innovative and effective approach for spatiotemporal visual attention prediction. The authors have provided a detailed experimental evaluation of the method, and the results are impressive. The paper could have benefited from a deeper discussion of potential limitations and a fuller related work comparison.","model":"chatGPT","source":"peerread","label":1,"id":2153}
{"text":"The paper titled \"Recurrent Mixture Density Network for Spatiotemporal Visual Attention\" proposes a novel spatiotemporal attentional model that learns to focus on relevant parts of an image or video using human fixation data. The authors model visual attention with a mixture of Gaussians at each frame, which is used to express the probability of saliency for each pixel. The model's time consistency in videos is modeled hierarchically using deep 3D convolutional features and a long short-term memory network, which aggregates the clip-level representation of sequential clips to expand the temporal domain from a few frames to seconds. \n\nThe paper's main contribution is an attentional model that achieves state-of-the-art performance on saliency prediction for video. Specifically, the authors demonstrate that their proposed model's attentional mechanism can be leveraged to improve action classification accuracy on both Hollywood2 and UCF101 datasets. \n\nThe paper is well written and provides a comprehensive explanation of the proposed model. The authors demonstrate their model's effectiveness in tackling the problem of saliency prediction for videos, which is a challenging task in the field of computer vision. However, there are a few areas that could be improved:\n\n1. Although the authors provide a detailed explanation of their proposed model, it would be helpful to include more information about the dataset and evaluation metrics used in their experiments. This would make it easier for readers to replicate their experiments and compare their results with other methods.\n\n2. The paper would benefit from a more rigorous comparison with other state-of-the-art methods for saliency prediction in videos. Although the authors claim that their proposed model achieves state-of-the-art performance, a more detailed comparison with other methods would reinforce this claim.\n\n3. The paper does not provide a in-depth discussion of the limitations of the proposed model. It would be helpful if the authors discussed the limitations of their model and potential directions for future research.\n\nOverall, the paper presents a novel spatiotemporal attentional model that achieves state-of-the-art performance on saliency prediction for videos. The proposed model has potential applications in various computer vision tasks and can be leveraged to improve action classification accuracy on both Hollywood2 and UCF101 datasets.","model":"chatGPT","source":"peerread","label":1,"id":2154}
{"text":"Peer review:\n\nThis paper proposes a spatiotemporal attentional model to learn where to look in a video directly from human fixation data. The relevant information to solve the problem at hand is often mixed with irrelevant pixels, so the proposed model models visual attention with a mixture of Gaussians at each frame to express the probability of saliency for each pixel. The model learns time consistency in videos through deep 3D convolutional features and a long short-term memory network. The results are demonstrated on Hollywood2 and UCF101 datasets by showing state-of-the-art performance on saliency prediction and an improvement in action classification accuracy.\n\nStrengths:\nThe paper addresses an important problem in computer vision by proposing an attentional model that can dynamically focus on parts of images or videos that are salient, and the results are demonstrated on two benchmark datasets. The model learns directly from human fixation data, which is a more natural way of learning to attend compared to other methods that require extensive manual annotation. The approach is also generalizable to other datasets, as demonstrated on UCF101.\n\nWeaknesses:\nThe paper could provide more details on the experimental setup, such as the size of the training and test datasets, hyperparameter settings, and the evaluation metrics used. It would also be useful to compare the proposed model with other state-of-the-art approaches in terms of performance and computational cost. Additionally, it would be interesting to know how well the proposed model generalizes to other types of videos beyond those considered in this work. Finally, there could be more thorough analysis of the results, such as visualizing the saliency maps for different videos and comparing them with ground-truth saliency maps.","model":"chatGPT","source":"peerread","label":1,"id":2155}
{"text":"The paper \"Efficient Representation of Low-Dimensional Manifolds using Deep Networks\" presents a machine learning approach for efficient representation of low-dimensional manifolds using deep neural networks. The authors propose a novel framework that combines manifold learning and deep embedding techniques to capture and represent the underlying structure of the data in a more efficient way.\n\nOverall, this is a well-written and technically sound paper that presents an important contribution to the field of machine learning and representation learning. The authors have clearly identified the research gaps in the existing literature and proposed a practical solution that is computationally efficient and can enable better feature extraction and representation for low-dimensional manifolds.\n\nThe paper presents a thorough review of the relevant literature and adequately contextualizes the proposed approach within the existing research landscape. The authors have provided a detailed explanation of the proposed methodology, including the formulation of the optimization problem, the description of the algorithm, and the evaluation metrics. The experiments are well-designed and the evaluations are thorough and convincing, demonstrating the effectiveness of the proposed approach in terms of accuracy and computational efficiency.\n\nThere are a few minor issues that could be addressed to strengthen the paper. Specifically, it would be helpful if the authors provided a more detailed discussion of the limitations of their approach and future directions for research in this area. Moreover, it would be beneficial to have a more rigorous evaluation of the proposed method's ability to scale with increasing dataset size, as this is an important consideration in real-world applications.\n\nIn conclusion, \"Efficient Representation of Low-Dimensional Manifolds using Deep Networks\" presents a novel and practical approach to representation learning for low-dimensional manifolds that can significantly improve efficiency and accuracy. The paper is well written and the methodology is thoroughly explained and evaluated, making it a valuable contribution to the field of machine learning.","model":"chatGPT","source":"peerread","label":1,"id":2156}
{"text":"The paper \"Efficient Representation of Low-Dimensional Manifolds using Deep Networks\" addresses the problem of efficiently representing low-dimensional manifolds using deep neural networks. The authors have proposed a novel method that leverages the power of deep networks to create a compressed and efficient representation of such manifolds. \n\nOne of the key strengths of the paper is the clear and detailed explanation of the proposed method. The authors have provided step-by-step guidance along with extensive mathematical formulations to explain the approach. They have also presented experimental results to demonstrate the effectiveness of their approach for various applications, including image and video compression, and dimensionality reduction.\n\nAnother notable strength of the paper is the comparison with existing state-of-the-art techniques. The authors have effectively compared their approach to other popular deep learning and manifold learning methods, such as auto-encoders and t-SNE, and demonstrated superior performance in terms of computational efficiency and accuracy.\n\nHowever, there are a few weaknesses that need to be addressed in the paper. Firstly, the authors have not discussed the limitations or constraints of their proposed method. It is important to explain the cases where their approach may not be suitable or may fail to provide optimal results.\n\nSecondly, the authors have not provided a detailed analysis of the interpretability of their approach. While the proposed method yields compressed representations of manifolds, it is unclear how researchers and practitioners can interpret and understand the features represented in the compressed form.\n\nOverall, the paper \"Efficient Representation of Low-Dimensional Manifolds using Deep Networks\" presents a promising and creative approach for efficiently representing low-dimensional manifolds using deep networks. The authors have provided convincing experimental results and effective comparisons with other state-of-the-art techniques. However, the authors should address the above-mentioned weaknesses to make their work more comprehensive and impactful.","model":"chatGPT","source":"peerread","label":1,"id":2157}
{"text":"Overall, the paper \u201cEfficient Representation of Low-Dimensional Manifolds using Deep Networks\u201d presents a novel approach for efficiently representing high-dimensional data that lies near a low-dimensional manifold using deep neural networks. The paper offers notable contributions that advance the existing state-of-the-art in manifold learning.\n\nThe paper is well-structured, and clearly lays out the problem, challenges, and proposed solutions. The introduction offers a good overview of the motivation behind the study, and effectively conveys the significance of the problem under consideration. The related works section is comprehensive and critically assesses the strengths and weaknesses of the existing literature in this area.\n\nOne of the main contributions of the paper is that it shows how the first two layers of a deep neural network can precisely embed the points on a monotonic chain onto a low-dimensional Euclidean space. The paper also shows that this network can project nearby points onto the manifold and embed them with minimal error. The evaluation of the approach is well-designed and performed effectively, demonstrating the efficiency of the proposed algorithm.\n\nThe paper is written clearly and concisely, and presents the technical details in an accessible manner. However, there are a few minor additions that could improve the clarity of the presentation. Firstly, the authors may want to include more formal definitions and explanations of key terms early on in the paper, especially for readers who are not familiar with the topic. Secondly, the discussion section could benefit from more details on potential future research directions and open issues in this field. Finally, the results should be presented in more detail, including more comprehensive visualizations and tables to support the finer points of the argument.\n\nOverall, the quality of the paper is commendable, and I would recommend this paper for acceptance at the conference. The proposed approach exhibits significant potential for manifold learning and offers potential avenues for further research.","model":"chatGPT","source":"peerread","label":1,"id":2158}
{"text":"Peer Review:\n\nProblem Statement and Contribution:\n\nThe paper \"Efficient Representation of Low-Dimensional Manifolds using Deep Networks\" addresses the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space. The paper shows that deep neural networks can efficiently extract the intrinsic coordinates of the data and carry out low-dimensional embeddings. The main contribution of this paper is the demonstration that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space.\n\nStrengths:\n\nThe authors demonstrate that their proposed technique can effectively represent and encode low-dimensional manifolds using deep neural networks. Their technique allows for an almost optimal number of parameters to be used while being able to accurately perform low-dimensional embeddings of the input data. The experiments conducted to validate the proposed technique are thorough and well designed. The experimental results show that the proposed technique performs well using stochastic gradient descent.\n\nWeaknesses:\n\nThe study is limited to the modeling of only a specific class of low-dimensional manifolds (i.e., monotonic chains). The effectiveness of the proposed technique on other types of manifolds remains to be studied. The authors have not shown the generalization capability of the proposed technique to handle other types of data, such as noisy or incomplete data, or data having multiple interconnected manifolds. Furthermore, the authors have not provided a systematic comparison with other state-of-the-art techniques applied to the same dataset, which could help validate the effectiveness of the proposed approach.\n\nConclusion:\n\nThe paper efficiently deals with the problem of representing low-dimensional manifolds using deep neural networks. The proposed technique has some limitations, but it is a useful contribution to the literature, and it opens up new avenues for future research. Addressing some of the weaknesses mentioned above could further enhance the impact of the proposed technique in practical applications. Overall, this paper is worth reading and is of interest to researchers working on low-dimensional embedding and manifold learning.","model":"chatGPT","source":"peerread","label":1,"id":2159}
{"text":"The paper \"Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks\" proposes a novel approach for sparsifying the connectivity structure of neural networks to reduce the computational complexity and memory requirements of deep learning models. The authors aim to enable efficient hardware implementation of deep neural networks on VLSI (Very Large-Scale Integration) chips.\n\nOverall, the paper provides a well-structured and coherent description of the proposed method, including the background, motivation, design principles, and experimental results. The authors present a comprehensive theoretical analysis of the sparsity-based approach, which highlights the benefits of reducing network connectivity for power reduction, computational efficiency, and memory management.\n\nThe experimental results demonstrate the effectiveness of the proposed method in reducing the computational and memory requirements of deep neural networks. The authors show that the sparsified neural network achieves comparable accuracy to the original fully connected model, while reducing the number of connections and activations by several orders of magnitude. The authors also provide a detailed analysis of the hardware implementation of the sparsified network, including the power consumption, area overhead, and runtime performance.\n\nDespite the strengths of the proposed method and the soundness of the experimental setup, there are some minor issues with the presentation and clarity of the paper. Firstly, the authors could improve the clarity of the figures and tables, making them more readable and easier to interpret. Secondly, some parts of the text could be rephrased or expanded to provide more context and explanations for readers who are less familiar with the field.\n\nOverall, the paper represents a solid contribution to the field of deep learning and VLSI design. The proposed method could have significant practical applications in enabling efficient hardware implementation of deep neural networks for resource-constrained devices or low-power applications. I would, therefore, recommend accepting the paper subject to minor revisions.","model":"chatGPT","source":"peerread","label":1,"id":2160}
{"text":"In this paper, the authors explore the potential of sparsely-connected neural networks for efficient VLSI implementation of deep neural networks.\n\nOne of the strengths of this paper is the clarity in which the problem and motivation are articulated. The authors do an excellent job of clearly articulating the challenges associated with implementing deep neural networks in hardware and how sparsity could potentially alleviate the issue. Furthermore, the authors provide helpful context by reviewing prior work on sparsity in deep neural networks.\n\nAnother strength of this paper is the experimental results section. The authors provide a thorough analysis of the performance of the sparsely-connected neural network using a variety of different datasets. The analysis is backed up by a comprehensive set of experiments, which includes comparisons with a fully-connected neural network that demonstrate the effectiveness of the proposed method.\n\nOne potential weakness of the paper is the lack of a detailed discussion on how to optimize the sparsity patterns for different types of datasets. While some general guidelines are mentioned, the authors could have gone into greater details on how to select the appropriate sparsity level and pattern for different types of datasets. \n\nAnother weakness of the paper is the limited discussion on the impact of sparsity on the accuracy of the neural network. While the authors do mention that the sparsely-connected neural network was able to achieve comparable accuracy to the fully-connected network, they could have provided further discussion on how accuracy is impacted by sparsity.\n\nOverall, this is a well-written paper that presents an interesting approach to addressing the challenges associated with implementing deep neural networks in hardware. The authors provide a clear motivation for their approach and back up their claims with convincing experimental results.","model":"chatGPT","source":"peerread","label":1,"id":2161}
{"text":"Dear authors,\n\nI have reviewed your paper titled \"Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks\" and found it to be well-written and presented. The paper proposes a novel technique to reduce the number of connections in fully-connected neural networks by up to 90%, while improving the accuracy performance on three popular datasets, namely MNIST, CIFAR10, and SVHN. The authors propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Furthermore, the authors show that the proposed method can lead to a reduction of up to 84% in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.\n\nOverall, the paper presents an interesting idea and results that are useful in practical implementations of deep neural networks on hardware. The authors have well-motivated their approach and presented a convincing argument for the effectiveness of the proposed technique. The experiments are well-designed and thorough, and the results presented are significant and relevant.\n\nI do, however, have a few minor suggestions that could improve the clarity and strength of the paper:\n\n1. The introduction section can be more focused and concise. The authors can quickly introduce the topic and go straight to the problem statement, followed by the proposed solution.\n\n2. The authors can explain how the sparsely-connected networks can lead to improved accuracy on the datasets used in the experiment. The authors can elaborate on the factors contributed to the improvement and provide more details on the comparison with previous work.\n\n3. The paper lacks a discussion of the limitations of the proposed approach. The authors can discuss the factors that could limit the effectiveness of the approach in other scenarios, such as more complex networks or larger datasets.\n\nOverall, I believe that this paper's contribution to the field of efficient hardware implementation of deep neural networks is significant and promising. With the suggested improvements, this paper can become an even stronger and clearer contribution to the field.\n\nBest regards,\n[Your Name]","model":"chatGPT","source":"peerread","label":1,"id":2162}
{"text":"The paper \u201cSparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks\u201d addresses the issue of the high silicon area overhead and high power consumption of fully-connected and convolutional neural networks. The authors propose a solution to this problem by introducing sparsely-connected networks that can reduce the number of connections in fully-connected networks by up to 90% while still maintaining accuracy performance. Furthermore, they present an efficient hardware architecture that reduces the memory requirements of the proposed networks by up to 90%. The proposed architecture results in up to 84% reduction in energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of a fully-connected neural network. \n\nThe strengths of this paper lie in the proposed solution to an important problem in deep neural networks. The paper is clear and well-organized, making it easy for the audience to understand the proposed algorithm and architecture. The experiments conducted on three popular datasets (MNIST, CIFAR10, and SVHN) demonstrate that the sparsely-connected network has similar accuracy performance compared to fully-connected networks while reducing the memory requirements and energy consumption. The paper includes a comprehensive evaluation and comparison with other related work, further validating the authors' contribution. The proposed approach is straightforward and potentially can be applied to various deep neural network architectures.\n\nHowever, there are a few weaknesses. Although the authors presented a promising approach to improving the efficiency of deep neural networks, the paper lacks some detailed analysis of the characteristics of the proposed algorithm and architecture. Additionally, the paper did not provide a comprehensive comparison of the proposed approach with state-of-the-art approaches specific to the hardware implementation of deep neural networks. \n\nOverall, this paper provides a new perspective to improving the efficiency of deep neural networks. While further evaluation and comparison with state-of-the-art approaches are needed, the proposed approach has the potential to be adopted for VLSI implementation of deep neural networks.","model":"chatGPT","source":"peerread","label":1,"id":2163}
{"text":"Introduction:\nThe authors present an interesting approach to improve the performance of semi-supervised text classification methods by using adversarial training techniques. They introduce a framework that leverages pre-trained models to generate adversarial examples and improve generalization of the model on the unlabeled data.\n\nStrengths:\nThe paper provides a comprehensive literature review of previous works on semi-supervised text classification and adversarial training. The proposed framework is well-designed and the experimental results show an improvement in classification accuracy compared to previous works. The authors also analyze the impact of different parameters on the performance of the model.\n\nWeaknesses:\nThere are some issues that need to be addressed. First, the authors did not compare their framework to state-of-the-art models on the datasets used in the experiments. Second, there is no clear explanation on how the pre-trained models are selected and trained. Third, the paper lacks a detailed analysis of how the use of adversarial examples leads to an improvement of the model's performance.\n\nSuggestions for Improvement:\nTo improve the paper, the authors should compare and evaluate their model on a larger set of benchmark datasets and compare their results to state-of-the-art models. They should also provide a more detailed explanation of how the pre-trained models were selected and trained. Additionally, they should perform a deeper analysis of how the use of adversarial examples contributes to the performance improvements.\n\nConclusion:\nOverall, the paper presents an interesting approach to semi-supervised text classification using adversarial training techniques. However, some improvements are needed, particularly in terms of comparison with state-of-the-art models, a more detailed explanation of the pre-trained models selection and training, and a more comprehensive analysis of the effect of adversarial examples on the performance improvements.","model":"chatGPT","source":"peerread","label":1,"id":2164}
{"text":"The paper \"Adversarial Training Methods for Semi-Supervised Text Classification\" addresses the issue of semi-supervised text classification, which is an important topic in Natural Language Processing (NLP). Specifically, the paper addresses the challenge of improving the accuracy of text classification models when there is limited labeled data available.\n\nOne of the major strengths of this paper is its innovative approach in incorporating an adversarial training method into the text classification task. The authors show that this method not only improves the accuracy of the model, but also enhances its robustness to adversarial attacks. The paper presents a thorough experimental evaluation on several benchmark datasets, which demonstrates the effectiveness of the proposed method compared to several baseline methods and existing state-of-the-art approaches. \n\nHowever, there are some weaknesses in the paper that need to be addressed. Firstly, the authors do not provide a detailed explanation of the adversarial training method used in the model architecture. Although some related works are mentioned, it may be difficult for readers to fully understand the method without more detailed explanations or mathematical formulations of the training procedure. Additionally, the paper lacks a thorough analysis of the computational complexity of the proposed method, which is an important consideration for practical deployment of the model. \n\nIn general, the paper successfully proposes an innovative approach to tackle the problem of semi-supervised text classification. By introducing adversarial training methods, the proposed model not only achieves higher accuracy, but also improves robustness to adversarial attacks. However, further work needs to be done to provide more detailed explanations of the proposed approach and to evaluate its computational complexity.","model":"chatGPT","source":"peerread","label":1,"id":2165}
{"text":"The paper \"Adversarial Training Methods for Semi-Supervised Text Classification\" presents a novel method to extend adversarial and virtual adversarial training to the text domain. The authors propose applying perturbations to the word embeddings in a recurrent neural network instead of the original input, which is more appropriate for sparse high-dimensional inputs such as one-hot word representations. They demonstrate that the proposed method achieves state-of-the-art results on several benchmark semi-supervised and purely supervised tasks, and provide visualizations and analysis to show that the learned word embeddings are of higher quality and that the model is less prone to overfitting during training.\n\nOverall, the paper is well written and the approach proposed is interesting and potentially impactful. The authors provide detailed descriptions of the methodology and experimental setup, and consistently compare the proposed approach to several state-of-the-art methods on different datasets, providing evidence for the superiority of their method. The visualizations and analysis are also helpful in demonstrating the effectiveness of the approach.\n\nHowever, there are a few areas where the paper could benefit from further clarification or expansion. Firstly, it would be helpful to provide more details on the specific architectures and hyperparameters used in the experiments. For example, how were the models initialized and trained? Were there any modifications to the standard RNN architecture used? Additionally, the paper states that \"our model used a state-of-the-art language model pre-trained on a large text corpus\" but does not provide any details on the pre-trained model used. It would be helpful to provide more information on the pre-training process and the specific pre-trained model used.\n\nSecondly, the paper could benefit from a more comprehensive discussion of the limitations of the proposed approach. While the authors briefly mention that their method may not perform well on datasets with very few labeled examples, it would be helpful to discuss other potential limitations or challenges of the approach. For example, are there any trade-offs in terms of computational efficiency or generalization ability when applying perturbations to word embeddings instead of the input vector itself?\n\nFinally, the paper could benefit from a more in-depth analysis of the learned word embeddings. The authors provide some visualizations and analysis of the embeddings, but additional experiments or analysis could provide more insights into the properties of the embeddings and how they contribute to the improved performance of the model.\n\nOverall, the paper presents an interesting and well-executed approach to extending adversarial and virtual adversarial training to the text domain. With some clarification and expansion in certain areas, the paper has the potential to make an important contribution to the field of semi-supervised text classification.","model":"chatGPT","source":"peerread","label":1,"id":2166}
{"text":"The paper, \"Adversarial Training Methods for Semi-Supervised Text Classification,\" aims to extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings rather than the original input in order to address the issue of inappropriate perturbations for sparse high-dimensional inputs. The paper presents a new method for adversarial and virtual adversarial training in the text domain, which achieves state-of-the-art results on multiple benchmark semi-supervised and purely supervised tasks.\n\nOverall, the paper provides a valuable contribution to the field by proposing a method for applying adversarial and virtual adversarial training to text inputs, which has not been previously explored in-depth. The paper's method achieves state-of-the-art results on several datasets, demonstrating its effectiveness for semi-supervised and purely supervised tasks. The paper's visualizations and analysis further demonstrate the improvements in quality of the learned word embeddings and the model's reduced overfitting.\n\nHowever, there are some weaknesses in the paper that should be addressed. Firstly, the paper does not provide in-depth comparison with other state-of-the-art algorithms for text classification. Secondly, the paper lacks discussion on the effectiveness of the proposed method in smaller data sets. Thirdly, the paper places more emphasis on the results than on the methodology, making it difficult for a reader to replicate and extend the proposed method.\n\nOverall, the paper presents an innovative method for applying adversarial and virtual adversarial training to text inputs, thereby improving the quality of the learned word embeddings and reducing overfitting. There are some weaknesses, particularly in the lack of in-depth comparison with other state-of-the-art algorithms and lack of discussion on the performance of the method in smaller datasets. However, the paper's contributions are valuable to the field and have potential for future work.","model":"chatGPT","source":"peerread","label":1,"id":2167}
{"text":"General comments:\n\nThe paper presents a well-motivated approach for deep fine-grained analysis of sentence embeddings using auxiliary prediction tasks. The authors argue that such a fine-grained analysis helps to better understand the semantic properties captured by sentence embeddings. The paper is well-written and the experimental results demonstrate the effectiveness of their approach. However, there are some areas that need to be clarified before accepting the paper for publication.\n\nSpecific comments:\n\n1. The paper mentions that it uses standard pre-trained models for generating sentence embeddings. However, it is not clear which pre-trained models have been used. The authors should provide clarification on the same.\n\n2. The paper mentions using a \"dropout rate of 0.3\". It would be helpful to know how was this value chosen and what impact it has on the results.\n\n3. While the authors have provided a justification for choosing the prediction tasks, it would be helpful to provide a comparison with other prediction tasks, such as part-of-speech tagging and named entity recognition, to demonstrate the effectiveness of their approach.\n\n4. The authors have used the MLP (Multi-Layer Perceptron) approach for the auxiliary prediction tasks. This approach has been compared with other models, such as SVMs and Random Forests. It would be helpful to know the reasons for choosing MLP over the other models and what impact it has on the results.\n\n5. The paper mentions that the word embeddings used in the auxiliary prediction task were also pre-trained embeddings. However, it is not clear which pre-trained embeddings were used. The authors should provide clarification on the same.\n\n6. In Table 3, the results for the paraphrase detection task are significantly lower than the other tasks. The authors should provide a discussion on why this is the case and what improvements can be made for this task.\n\n7. It would be helpful to know how much time was taken for training and testing the models used in the experiments.\n\n8. The paper mentions using a dataset of 21,578 sentences. It would be helpful to know how these sentences were selected and what was the criteria for selecting them.\n\nOverall, the paper presents a valuable approach for deep fine-grained analysis of sentence embeddings. However, the authors should provide more details and clarifications on some aspects of their approach and results.","model":"chatGPT","source":"peerread","label":1,"id":2168}
{"text":"Problem and Question:\nThe paper \"Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks\" addresses the problem of improving the interpretability and quality of sentence embeddings. The authors attempt to answer this question by exploring how auxiliary prediction tasks can be used alongside traditional learning objectives to improve sentence embeddings. The auxiliary prediction tasks include sentence-level classification and word-level similarity prediction.\n\nStrengths:\nThe paper provides a comprehensive study of the effectiveness of auxiliary prediction tasks in improving the interpretability and quality of sentence embeddings. The authors use a wide range of benchmarks to evaluate the sentence embeddings, and they show that the combination of traditional learning objectives and auxiliary prediction tasks can lead to substantial improvements in the quality of sentence embeddings. The authors also provide a detailed analysis of the sentence embeddings produced by their approach, providing insight into how the different tasks work together to improve the embeddings.\n\nWeaknesses:\nOne potential weakness of the paper is that the authors did not compare their approach with other state-of-the-art sentence embedding models to evaluate its performance. Additionally, while the authors' approach improves the quality of the embeddings, it is not clear how scalable the approach is to large datasets or how it compares to other methods in terms of computational efficiency. The evaluation of the classification task is also not fully explained, leaving open questions on the confounds and possible biases in the data set.\n\nOverall, the paper presents a useful and informative study on how auxiliary prediction tasks can be applied to improve sentence embeddings. The authors' approach provides a substantial improvement in the quality of sentence embeddings, and the study provides a valuable contribution to the field of natural language processing.","model":"chatGPT","source":"peerread","label":1,"id":2169}
{"text":"The authors proposed a framework to understand the language information captured by sentence representations based on fixed-length vectors. The proposed framework focuses on three aspects of sentence structure: sentence length, word content, and word order, and uses prediction tasks based on these aspects to score sentence representations. The authors analyzed different sentence embedding mechanisms and provided insights into the relative strengths of these methods with respect to the proposed low-level prediction tasks.\n\nThe paper is well-written and the findings are presented in a clear and concise manner. The proposed framework provides a useful method to evaluate the quality of sentence representations and better understand the properties that are encoded in these representations. The experiments carried out in the paper are thorough, and the results are insightful.\n\nOne potential weakness of the paper is that the authors only used a limited set of prediction tasks, which may not capture all aspects of sentence structure. Future work could explore additional prediction tasks that would allow for a more comprehensive analysis of sentence representations.\n\nOverall, this paper makes a substantial contribution to the field of NLP and provides a valuable framework that can be used to evaluate and compare sentence embedding methods. The findings should be of interest to both researchers and practitioners working in the field of NLP.","model":"chatGPT","source":"peerread","label":1,"id":2170}
{"text":"This paper presents a framework that aims to provide better understanding of the properties encoded in sentence representations. The authors focus on two popular methods for encoding variable length sentences into fixed length vectors, and propose three auxiliary prediction tasks to evaluate the quality of the sentence embeddings. The three prediction tasks are sentence length prediction, word content prediction, and word order prediction. The authors use these task scores to compare different sentence embedding methods and to investigate the effect of embedding dimensionality.\n\nOne strength of this paper is the comprehensive evaluation of various sentence embedding methods, including averaging word vectors and LSTM-based methods. The authors use multiple datasets to validate the performance of these methods on the auxiliary prediction tasks. Another strength is the clear explanation of the proposed prediction tasks, which enable reproducibility and further research in this area.\n\nHowever, one weakness of this paper is the lack of discussion on the relevance of the auxiliary prediction tasks to downstream NLP tasks. While the authors mention that the primary purpose of sentence embeddings is to provide input features for subsequent machine learning tasks, it would be helpful to know if the embeddings that perform better on the prediction tasks also provide better performance on these downstream tasks. Another weakness is the limited discussion on the limitations of the proposed framework. For example, the authors only evaluate a fixed set of auxiliary prediction tasks, and it is unclear if these tasks are representative of all possible ways to evaluate sentence embeddings.\n\nOverall, this paper offers a valuable contribution to the field of sentence embeddings, and the proposed framework can serve as a useful tool for future research in this area. However, further investigation is necessary to determine the validity and generalizability of the proposed approach.","model":"chatGPT","source":"peerread","label":1,"id":2171}
{"text":"I have reviewed the paper on \"Pointer Sentinel Mixture Models\" and I must say that it provides a significant contribution to the field of natural language processing (NLP).\n\nThe authors present a novel approach to sequence-to-sequence (Seq2Seq) modeling for NLP tasks, which combines three important elements: pointer networks, sentinel vectors, and mixture models. Specifically, the paper proposes a new neural architecture that enables attention mechanism to choose between generating new words or copying from the input sequence. \n\nOne of the strengths of this work is its thorough experimental evaluation. The paper includes experiments on several benchmark datasets, including translation, text summarization, and table-to-text generation. The results demonstrate that the proposed model outperforms the state-of-the-art methods in most of the cases, achieving a new state-of-the-art on many of the datasets.\n\nThe paper also provides a detailed analysis of the model, which is crucial for gaining insights into the underlying mechanisms and for potential future improvements. Specifically, the authors analyze the impact of different components of the model, such as the pointer mechanism and the mixture model, on the final performance.\n\nHowever, there are some aspects of the paper that could be further improved. For instance, the paper would benefit from a more detailed explanation of the architecture and a clearer description of the training process. In addition, the paper would benefit from providing more thorough comparison with related work, especially those that use attention mechanisms.\n\nOverall, I highly recommend this paper for publication in a leading NLP conference. The proposed Pointer Sentinel Mixture Models approach has the potential to advance the state-of-the-art in various NLP tasks and is expected to have a strong impact on the research community.","model":"chatGPT","source":"peerread","label":1,"id":2172}
{"text":"Title: Pointer Sentinel Mixture Models\n\nProblem or question addressed:\nThe paper \"Pointer Sentinel Mixture Models\" proposes a novel neural network model that combines ideas from sequence-to-sequence models and mixture of experts architectures to solve the problem of generating an output sequence based on an input sequence that may contain unknown or out-of-vocabulary words. The authors motivate the approach by noting that this problem has typically been addressed in two ways: methods that copy tokens from the source sequence, and methods that rely on a fixed dictionary of possible tokens. However, the authors argue that copy mechanisms do not allow for generation of novel words, and dictionary-based methods require significant prior knowledge about the domain and may not generalize well to new data. The proposed model, called Pointer Sentinel Mixture Model (PSMM), aims to overcome these limitations by combining the ability to copy words directly and to generate new words based on a mixture of expert models.\n\nStrengths:\nThe proposed model represents a novel contribution to the field of neural machine translation and natural language processing. The paper presents a detailed and clear explanation of the proposed model, including key mathematical formulations and architecture diagrams, making it accessible to the broader research community. The model achieves significant improvements in performance compared to existing models on several benchmark NLP datasets, demonstrating the effectiveness of the approach. The paper also provides a detailed analysis of the model's behavior, including visualization of attention weights, which helps to better understand how the model functions.\n\nWeaknesses:\nOne potential limitation of the PSMM is that it requires the use of a pre-defined vocabulary, which may not be ideal for some applications where the space of possible words is very large or constantly changing. Additionally, while the authors provide a detailed analysis of the model's performance, more detailed analysis of the individual components of the model, such as the mixture of experts, would be helpful for understanding its behavior in different scenarios. Finally, although the model achieves improved performance on several benchmark datasets, it would be valuable to evaluate its performance on more diverse domains and tasks to assess its generalizability beyond NLP.\n\nOverall, the paper \"Pointer Sentinel Mixture Models\" presents a promising approach for addressing the challenge of generating output sequences based on input sequences containing unknown words. The paper is well-written and clearly presents the proposed model, including mathematical formulations and detailed analysis of results. While the model has some limitations, it represents an important contribution to the field of NLP and opens up opportunities for further research.","model":"chatGPT","source":"peerread","label":1,"id":2173}
{"text":"Peer Review for \"Pointer Sentinel Mixture Models\"\n\nThe paper presents an innovative neural sequence model called \"Pointer Sentinel Mixture Models\" for language modeling by addressing the challenge of predicting rare or unseen words, even when the context makes the prediction unambiguous. The proposed model achieves state-of-the-art performance in language modeling while using significantly fewer parameters than a standard softmax LSTM model. \n\nThe paper is well-written, and the authors have presented the technical details of the model in a clear and concise manner. The model is based on a pointer mechanism to predict rare or unseen words and employs a mixture of softmax and pointer mechanism to produce the output. The authors have conducted experiments on the Penn Treebank and WikiText corpus and have shown that the proposed model outperforms the existing state-of-the-art models in terms of perplexity. \n\nHowever, there are some points that need to be addressed. The authors have not provided a comprehensive analysis of the proposed model. For instance, the experiment results are presented for only one corpus, and it is unclear how the model performs on other language modeling tasks. Additionally, there is a lack of analysis of how the model handles the rare or unseen words, which is the main focus of the paper. Therefore, the authors should conduct more experiments to provide a comprehensive evaluation of the proposed model. \n\nFurthermore, the paper lacks comparison with other pointer-based language models. Since pointer-based models are widely used in sequence modeling tasks, it would be useful to compare the proposed approach with other pointer-based models to identify its unique features and limitations. Additionally, the authors could conduct an analysis to identify the limitations of the proposed approach.\n\nOverall, the paper presents a novel approach to address the challenge of predicting rare or unseen words in language modeling tasks. The proposed model shows promising results and has the potential to be applied to various other sequence modeling tasks. However, to make the paper more informative and comprehensive, the authors should provide more analysis of the proposed approach, comparison with similar models, and identify its limitations.","model":"chatGPT","source":"peerread","label":1,"id":2174}
{"text":"Paper: Pointer Sentinel Mixture Models\n\nProblem\/Question:\nThis paper addresses the problem of predicting rare or unseen words in language modeling even when the context makes the prediction unambiguous. The authors argue that recent neural network sequence models with softmax classifiers have struggled to achieve optimal language modeling performance due to large hidden states and larger vocabularies. They propose a novel architecture, called the pointer sentinel mixture model, that can either reproduce a word from the recent context or produce a word from a standard softmax classifier.\n\nStrengths:\nThe paper's greatest strength is its novel approach, the pointer sentinel mixture model, which achieves state-of-the-art performance on the Penn Treebank dataset. This model uses fewer parameters than a standard softmax LSTM model and can deal with more realistic vocabularies and corpora effectively. The freely available WikiText corpus introduced by the authors is another strength, as it provides researchers with a new tool for language model evaluation.\n\nWeaknesses:\nThe paper is limited in its evaluation of the proposed model, as it only uses a single corpus, the Penn Treebank, to evaluate performance. Additionally, the authors only provide limited analysis of the results they obtain. The lack of comparative analysis with other state-of-the-art language models in terms of training time, size, and performance is another weakness.\n\nOverall, the paper presents a novel approach to language modeling that achieves state-of-the-art performance on the Penn Treebank dataset. However, more work needs to be done to evaluate the model's performance on other datasets and to conduct comparative analysis with other state-of-the-art language models.","model":"chatGPT","source":"peerread","label":1,"id":2175}
{"text":"Title: Tighter bounds lead to improved classifiers\n\nSummary:\nThe paper presents a novel approach for improving the bounds of classifiers by analyzing the relationship between the margin, the lowest confidence level, and the expected error. The authors provide theoretical proofs and empirical results that demonstrate the effectiveness of their approach in achieving better classification performance in various datasets.\n\nStrengths:\nThe paper is well-written and easy to follow. The approach proposed by the authors is innovative and presents a significant contribution to the field of classification. The authors provide a clear and thorough explanation of their methodology along with theoretical proofs and empirical results supporting the effectiveness of their approach. The empirical evaluation is comprehensive and the datasets used for experimentation are diverse, making the results generalizable.\n\nWeaknesses:\nOne potential weakness of the paper lies in the evaluation metrics selected for the experimental evaluation. The authors focus on accuracy, which may not always be the most informative or relevant metric for evaluating classifiers in all scenarios. Other important metrics such as precision, recall, F1-score, etc., should also have been included to provide a more comprehensive evaluation of the proposed approach.\n\nAnother potential weakness is the lack of discussion on the computational complexity of the approach. It would have been helpful for readers to understand the time and space complexity of the proposed approach and how it would scale to larger datasets.\n\nOverall, this is a well-written and well-researched paper that presents an innovative approach to improving classification performance. The authors provide strong theoretical and empirical evidence to support their claims. To improve the paper, the authors should consider incorporating a more comprehensive evaluation metric and addressing the computational complexity of their approach.","model":"chatGPT","source":"peerread","label":1,"id":2176}
{"text":"In this paper, the authors address the problem of improving the performance of classifiers by deriving tighter upper bounds on various measures of generalization error. Specifically, they explore how tighter bounds on the Rademacher and Gaussian complexities of a classifier can be used to improve its classification accuracy.\n\nOne of the strengths of the paper is the clarity with which the authors present their findings. The introduction provides a clear motivation for the study, and the subsequent sections are organized in a logical and easy-to-follow manner. Additionally, the authors provide a thorough literature review and explain how their work builds upon previous studies.\n\nAnother strength of the paper is the thoroughness of the experimental evaluation. The authors conduct experiments on several real-world datasets and demonstrate that their proposed method of using tighter bounds leads to improved classification accuracy. They also perform sensitivity analysis on various hyperparameters and provide insightful discussion on the implications of their results.\n\nOne weakness of the paper is the lack of discussion on the computational complexity of the proposed method. The authors briefly mention that the method may be computationally expensive for large datasets, but they do not provide any details on the actual runtime performance of their algorithm. It would be helpful for future work to investigate the computational feasibility of this method on larger datasets.\n\nAdditionally, while the authors provide a thorough explanation of the mathematical concepts underlying their work, the paper could benefit from additional clarification for readers who may not have a strong background in machine learning theory. Specifically, the authors could provide more intuitive explanations of how their proposed method leads to improved generalization performance.\n\nOverall, the paper presents a promising approach to improving classification accuracy through the use of tighter upper bounds on generalization error measures. The authors' thorough experimental evaluation and clear presentation of results make this paper a valuable contribution to the field of machine learning.","model":"chatGPT","source":"peerread","label":1,"id":2177}
{"text":"Title: Peer Review for \"Tighter bounds lead to improved classifiers\"\n\nGeneral Comments:\nThe paper presents a modification to the standard approach of supervised classification that results in improved classification rates. The proposed modification involves updating the upper bound during the optimization and linking the classifier's performance with that of the whole system. The experimental results demonstrate the effectiveness of the proposed method.\n\nStrengths:\n- The paper provides a clear and concise description of the proposed approach.\n- The experimental results provide evidence of the effectiveness of the proposed method.\n- The proposed approach is motivated by a practical problem where classifiers are used as part of a larger system, increasing its relevance to real-world applications.\n- The paper is well-organized and easy to follow.\n\nWeaknesses:\n- The paper does not provide a comprehensive overview of related work on this topic, which could help readers understand the novelty and significance of the proposed approach.\n- The paper does not provide a detailed analysis of the computational cost of the proposed method, which could be relevant for its scalability to larger datasets.\n\nConclusion:\nOverall, the paper presents a useful modification to the standard supervised classification approach that can result in improved classification rates, as demonstrated by the experimental results. Future work could focus on further analyzing the computational cost of the proposed method and expanding the discussion of related work to provide a more comprehensive overview of the field.","model":"chatGPT","source":"peerread","label":1,"id":2178}
{"text":"The paper \"Tighter bounds lead to improved classifiers\" addresses the problem of the standard approach to supervised classification where the minimization of a log-loss is used as an upper bound to the classification error. Although this approach is initially a tight bound, it places too much emphasis on incorrectly classified examples that are far from the decision boundary. As such, the paper proposes updating the upper bound during optimization as a means of improving classification rates and transforming learning into a sequence of minimization problems.\n\nOne of the strengths of this paper is its clear and concise writing style, which makes the complex technical concepts accessible to a broad audience. The authors also provide a thorough review of related work, which allows readers to understand the novelty and significance of the proposed approach.\n\nAnother strength is the experimental evaluation conducted in the paper, which shows that the proposed approach leads to improved classification performance over several benchmark datasets. The authors compare their approach to state-of-the-art methods and show that it achieves better results on most of the datasets.\n\nOn the other hand, one weakness of the paper is that it does not provide a detailed analysis of the computational complexity of the proposed approach. Although the authors mention that their approach requires solving a sequence of minimization problems, they do not provide a rigorous analysis of the computational complexity.\n\nAnother weakness is that the discussion on the implications of this approach for linking the performance of the classifier to that of the whole system is relatively brief. This could be an important topic for future research and could be explored in more depth.\n\nOverall, this paper presents a promising approach for improving classification performance by updating the upper bound during the optimization. The experimental evaluation conducted in the paper is convincing and demonstrates the effectiveness of the proposed approach. However, future work could delve deeper into the computational complexity of the method and explore its potential implications for linking the performance of the classifier to that of the whole system.","model":"chatGPT","source":"peerread","label":1,"id":2179}
{"text":"Title: HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving\n\nSummary:\nThe paper describes the HolStep dataset, which is a collection of 9,416 higher-order logic (HOL) theorems and proofs, grouped into 61 domains. The dataset is intended for use in machine learning systems for automated theorem proving in HOL.\n\nStrengths:\n- The paper provides a detailed description of the dataset, including its size, structure, and contents. This information is crucial for researchers who wish to use the dataset for their own experiments.\n- The authors have taken care to ensure that the dataset is diverse and representative of a wide range of mathematical concepts and problem types.\n- The paper provides a thorough analysis of the performance of several state-of-the-art automated theorem provers on the dataset, which serves as a useful baseline for future research.\n- The authors have made the dataset freely available to the research community, which is a valuable contribution to the field.\n\nWeaknesses:\n- While the authors have conducted a thorough analysis of the performance of several automated theorem provers, they do not provide any insights into why certain approaches perform better than others on the dataset. Some discussion of the strengths and weaknesses of the different approaches would be beneficial.\n- It is not entirely clear from the paper how the dataset was constructed. Some additional information on the process used to select the theorems and proofs, and how they were grouped into domains, would be helpful.\n\nOverall, I believe that this is a valuable contribution to the field of automated theorem proving, and I would recommend the paper for publication.","model":"chatGPT","source":"peerread","label":1,"id":2180}
{"text":"The paper titled \"HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving\" presents a novel dataset for conducting automated theorem proving in higher-order logic using machine learning techniques. The paper primarily addresses the challenge of developing machine learning-based approaches for theorem proving in the context of higher-order logic, a highly expressive and powerful logical system that is widely used in formal verification, program synthesis, and type theory.\n\nOne of the key strengths of this paper is the comprehensive approach to data preparation and cleaning that the authors took in generating the HolStep dataset. The authors applied a rigorous curation process that resulted in a highly diverse and challenging dataset, consisting of a large number of proved and unproved theorems covering a broad range of domains, from algebraic structures to real analysis. \n\nThe paper also includes thorough documentation of the dataset, including detailed information about the syntax and semantics of the HOL logic system, the annotation system used to represent theorems and proof attempts, and the types of errors typically encountered in automated theorem proving.\n\nDespite its strengths, the paper does have some weaknesses. One of the primary limitations is that the evaluation of the dataset is mainly focused on the performance of deep learning approaches, without extensive comparison to other non-machine learning methods. Moreover, the assessment of the machine learning algorithms' performance was mainly based on metrics of accuracy and F1 score, wherein further evaluation could explore other metrics' impact.\n\nAll in all, this paper presents an exciting contribution to the field of automated theorem proving in higher-order logic. HolStep will be of great value to researchers in machine learning, artificial intelligence, and theorem proving and will help push the field forward towards more efficient and intelligent theorem proving.","model":"chatGPT","source":"peerread","label":1,"id":2181}
{"text":"Peer Review of \"HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving\"\n\nThe paper presents a new dataset for developing machine learning-based theorem-proving strategies in the context of higher-order logic (HOL) proofs. The authors propose various tasks that can be performed on this dataset and discuss their potential significance for theorem proving. They also benchmark baseline machine learning models for the tasks and provide promising results that demonstrate the validity of applying machine learning to HOL theorem proving.\n\nOverall, the paper is well-written and presents innovative ideas that can inspire further research in this area. Providing a publicly available dataset is an invaluable contribution to the community, and the authors should be commended for that. The paper's rigor and novelty are confirmed by the benchmarked results, which suggest that machine learning may have a significant impact on the automation of theorem proving tasks.\n\nHowever, there are some areas that the authors could further clarify. First, the authors might explain in greater detail how they designed the dataset and how it differs from other existing datasets. Considering the relevance of existing datasets, the authors could provide more comparisons and arguments to highlight why HolStep is a valuable new resource.\n\nSecond, the authors could discuss the limitations and future extensions of the proposed dataset and models. For example, it would be interesting to explore the feasibility of the proposed models on other theorem-proving problems (e.g., first-order logic, automated reasoning) and compare their performance with existing methods.\n\nThird, the paper would benefit from more detailed discussions and explanations of the proposed machine learning methods. While the benchmark results suggest that these methods are promising, more detailed descriptions of the models and their parameters can help other researchers better understand their architecture and provide opportunities for further improvements.\n\nIn conclusion, \"HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving\" presents a novel dataset and benchmark results for machine learning-based theorem proving in HOL logic. The paper's contributions could inspire further research in this field and provide valuable resources to the community.","model":"chatGPT","source":"peerread","label":1,"id":2182}
{"text":"The paper \"HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving\" aims to address the challenge of developing new machine learning-based strategies for theorem proving by introducing a new dataset of Higher-Order Logic (HOL) proofs. The authors make this dataset available under the BSD license and propose tasks that can be performed to demonstrate the potential impact of machine learning on HOL theorem proving. The paper also benchmarks a set of baseline machine learning models that prove to be promising for the said task.\n\nOne of the strengths of the paper is the novelty of the proposed dataset, which can enable researchers to explore new approaches to theorem proving with machine learning techniques. The paper's clarity and structure are noteworthy, as it instills an understanding of the dataset's usage and the task performed. In addition to its exploratory goal, the paper effectively benchmarks the performance of various machine learning models that can be used for the HOL dataset.\n\nHowever, this paper has some weaknesses that could be improved upon. The benchmarks of simple machine learning models have limitations as they may not represent the depth and complexity of the problem, which may require more advanced models to obtain better results. The authors could have explored other machine learning techniques in addition to the traditional supervised learning methods used in this study. Moreover, it would be beneficial to understand how these models bring value to practical applications and real-world systems.\n\nIn conclusion, the paper \"HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving\" is an interesting contribution to the field of theorem proving, and it opens up avenues for future research. Despite some weaknesses, it provides important insights into how machine learning could be used in theorem proving and the aspiration for more advanced machine learning models in the future for smarter and optimized theorem proving.","model":"chatGPT","source":"peerread","label":1,"id":2183}
{"text":"Peer Review for \"Why Deep Neural Networks for Function Approximation?\"\n\nSummary:\n\nThis paper aims to explore the reasons behind the effectiveness of deep neural networks (DNNs) in function approximation tasks. The authors begin by providing an overview of several studies that demonstrate the superiority of DNNs compared to other machine learning models in various domains. They then provide theoretical justifications for the effectiveness of DNNs in approximating complex non-linear functions through the use of deep architectures and non-linear activation functions. The authors conclude by highlighting the potential benefits of DNNs and the importance of continued research in this field.\n\nStrengths:\n\nThe paper provides a clear and concise overview of the current state of research on the effectiveness of DNNs in function approximation tasks. The theoretical explanations offered for the success of DNNs are well-founded and supported by relevant research studies. The paper is well-organized, well-written, and easily accessible to a broad range of audiences.\n\nWeaknesses:\n\nThe paper focuses primarily on theoretical explanations for the effectiveness of DNNs, without offering much practical insight into the considerations and decisions involved in the application of DNNs in function approximation tasks. Further discussion and analysis of real-world applications of DNNs in function approximation tasks would enhance the practical value of the paper.\n\nAdditionally, while the paper acknowledges some of the limitations and challenges associated with the use of DNNs, it does not address some of the pressing ethical considerations relating to bias and discrimination that can arise from the use of these models, particularly in high-stakes decision-making applications.\n\nSuggestions for Improvement:\n\nTo enhance the practical value of the paper, the authors can provide additional insights and analysis of real-world examples of DNNs in function approximation tasks. This can include discussion of the trade-offs involved in selecting appropriate architectures and hyperparameter settings to achieve optimal performance, as well as considerations for implementing DNNs in resource-constrained environments.\n\nFurthermore, to ensure greater inclusivity and ethical considerations, the paper can include a discussion of the potential biases and ethical implications associated with the use of DNNs in function approximation tasks. The authors can also suggest future research directions aimed at addressing these challenges to enhance the usability and trustworthiness of DNNs in real-world applications.","model":"chatGPT","source":"peerread","label":1,"id":2184}
{"text":"The paper \"Why Deep Neural Networks for Function Approximation?\" by Luiz O. Pessoa Jr., Renato L. Silva and Andre L.D. Rossi addresses the problem of using deep neural networks (DNNs) for function approximation. The authors aim to provide a theoretical explanation of why DNNs have been successful in approximating complex functions.\n\nStrengths:\nThe paper provides a comprehensive review of the literature on function approximation and neural networks, which is useful for readers who are not familiar with the topic. The authors effectively explain the underlying mathematical principles of DNNs, which help the reader to understand why DNNs are more effective in approximating functions compared to shallow neural networks. The authors have also provided empirical evidence to support their claims, which adds credibility to their argument.\n\nWeaknesses:\nOne possible weakness of the paper is that it focuses primarily on orthogonal polynomials and Fourier functions. The authors could have explored other functions to make the paper more comprehensive. Another possible weakness of the paper is the lack of clarity in some sections, which may make it difficult for some readers to follow.\n\nOverall, the paper makes a valuable contribution to the field by providing a theoretical explanation of why DNNs are effective in approximating complex functions. While there are some weaknesses, the paper provides valuable insights for researchers working in the field of neural networks and function approximation.","model":"chatGPT","source":"peerread","label":1,"id":2185}
{"text":"Review of \u201cWhy Deep Neural Networks for Function Approximation?\u201d\n\nThe paper provides interesting insights into why deep neural networks are preferred over shallow networks for a large class of piecewise smooth functions. The authors show that in order to achieve a given degree of function approximation, deep networks require exponentially fewer neurons than shallow networks. Specifically, the authors demonstrate that for univariate functions on a bounded interval, shallow networks require at least $\\Omega(\\text{poly}(1\/\\varepsilon))$ neurons, while deep networks require only $\\mathcal{O}(\\text{polylog}(1\/\\varepsilon))$ neurons. The authors further extend these results to certain classes of important multivariate functions.\n\nThe paper is well-written, and the analysis is clear and sound. The authors provide a simple yet powerful observation that multiplication of two bits can be represented by a rectifier linear unit (ReLU), which they use in their analysis. The results are derived for neural networks that use a combination of ReLUs and binary step units, which are popular types of activation functions.\n\nOverall, the paper presents significant results that provide a better understanding of the power of deep neural networks for function approximation. The paper is clearly written, builds on a simple observation, and provides important insights that could be useful in the design and optimization of neural networks for a wide range of applications.","model":"chatGPT","source":"peerread","label":1,"id":2186}
{"text":"In \u201cWhy Deep Neural Networks for Function Approximation?\u201d the authors address the question of why deep neural networks are more effective for certain types of function approximation than shallower networks. They show that the number of neurons required by shallow networks to achieve a desired level of approximation error is exponentially larger than the corresponding number of neurons required by deep networks, for a large class of piecewise smooth functions, both for univariate functions on a bounded interval and for certain classes of multivariate functions.\n\nThe strengths of this paper are that it addresses an important and widely discussed topic in deep learning, provides a rigorous mathematical analysis, and supports the findings with empirical results. The authors\u2019 use of simple observation of the multiplication of two bits represented by a ReLU is a clever and insightful insight that provides an elegant solution to an interesting problem.\n\nHowever, the paper also has some weaknesses. First, the scope of the paper is limited to a specific type of activation functions (ReLUs and binary step units). The results may not generalize to other activation functions or network architectures. Second, the discussion of the empirical results is somewhat brief and would benefit from more detailed analysis and interpretation. Additionally, the paper could benefit from further discussion on the implications of these findings for deep learning practice, and whether these results also hold true for other types of optimization techniques beyond gradient descent.\n\nOverall, \u201cWhy Deep Neural Networks for Function Approximation?\u201d is a well-written and interesting paper that addresses an important question in deep learning. While there are some limitations in its scope and discussion of results, the paper\u2019s contribution is valuable to the field and provides a basis for further research.","model":"chatGPT","source":"peerread","label":1,"id":2187}
{"text":"Title: HyperNetworks: Deep Learning with Dynamic Computation Graphs\n\nThe paper presents a new architecture called HyperNetworks, which utilizes dynamic computation graphs for deep learning. The authors argue that this architecture enables better generalization and reduces the number of parameters required in the model. The proposed approach is evaluated on image classification and language modeling tasks, and the results show promising outcomes in terms of speed, accuracy, and parameter reduction.\n\nOverall, the paper is well-written and structured. The introduction provides a clear motivation and background to the proposed approach, and the authors do a good job of explaining their methodology and implementation details. The experiments are carefully designed, and the results are reported in a clear and concise manner. The comparison with existing state-of-the-art methods is appropriate and helps to demonstrate the effectiveness of the HyperNetworks architecture.\n\nIn terms of technical contributions, the paper proposes a novel approach that can be applicable to a broad range of deep learning problems. The use of dynamic computation graphs in HyperNetworks enables efficient training and better generalization properties, which can improve the overall performance of deep learning models. Additionally, the parameter reduction achieved by HyperNetworks is impressive, which can be particularly useful in scenarios where computational resources are limited.\n\nHowever, one area for improvement is the analysis of the HyperNetworks architecture. Although the paper provides insights into how the architecture works, the authors could have delved deeper into the intuition behind the dynamic computation graphs and how they enable better generalization. Additionally, it would have been helpful to see a more in-depth discussion of the limitations and drawbacks of their approach.\n\nIn summary, the paper presents a novel and promising approach for deep learning that achieves good results on image classification and language modeling tasks. The results suggest that HyperNetworks can reduce the number of parameters required in the model while maintaining or improving the accuracy, which can be particularly useful in resource-constrained environments. However, further analysis and discussion of the HyperNetworks architecture would help enhance the paper's technical contributions.","model":"chatGPT","source":"peerread","label":1,"id":2188}
{"text":"This paper, \"HyperNetworks\" by Ha et al., addresses the problem of training large neural networks with limited computational resources. Specifically, the authors propose a method for parameter sharing across tasks by using a \"hypernetwork\" to generate the weights for several smaller task-specific networks. \n\nThe main strength of this paper is its contribution to the field of neural architecture search. By using a hypernetwork to generate task-specific weights, this method allows for more efficient training and sharing of information across tasks. The paper provides a clear explanation of the technical details and implementation of the method, along with experimental results that demonstrate its effectiveness on several benchmark datasets. \n\nOne weakness of this paper is the limited comparison to other state-of-the-art methods. While the authors do compare their method to a few baseline models, it would be helpful to see a more comprehensive comparison to other popular neural architecture search methods. Additionally, it would be interesting to see how this method performs on larger, more complex datasets.\n\nOverall, this paper provides a valuable contribution to the field of neural architecture search, and the use of hypernetworks presents a promising approach for efficiently training large neural networks with limited computational resources.","model":"chatGPT","source":"peerread","label":1,"id":2189}
{"text":"Overall, this paper presents an interesting approach towards generating non-shared weights for recurrent neural networks using hypernetworks. The authors provide a clear and concise explanation of the concept of hypernetworks and how they can be applied to recurrent networks for generating adaptive weights. The experiments conducted to evaluate the proposed method are comprehensive and well-designed, and the authors compare their results to existing state-of-the-art models on several sequence modeling tasks.\n\nThe authors also provide a detailed evaluation of the limitations and potential drawbacks of the hypernetwork approach, which provides valuable insight for further improvements and development of the method.\n\nOne potential area for improvement is the clarity of the description of the hypernetwork architecture. While the authors provide a clear explanation of the concept of the hypernetwork, some readers may benefit from a more detailed representation of the architecture used in the experiments, such as diagrams or schematics.\n\nOverall, the paper presents a compelling approach for generating non-shared weights for recurrent networks using hypernetworks, and the results achieved in the experiments demonstrate the potential of this method. The authors provide a thorough evaluation and analysis of the proposed approach, and the paper has the potential to make a significant contribution to the field of deep learning.","model":"chatGPT","source":"peerread","label":1,"id":2190}
{"text":"The paper \"HyperNetworks\" explores the application of hypernetworks, a novel approach to using one network to generate the weights for another network, with the specific application of generating adaptive weights for recurrent neural networks. The main strength of this work is its ability to challenge the weight-sharing paradigm for recurrent networks by demonstrating that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation, and neural machine translation. This is a significant contribution to the field of neural networks and has the potential to advance research in many areas, including speech recognition and natural language processing.\n\nOne weakness of the paper is that it lacks a clear and detailed description of the architecture of the hypernetworks and their implementation. Additionally, it would be helpful to have more information on how hypernetworks compare to other approaches to generating non-shared weights, such as evolutionary strategies or gradient descent.\n\nOverall, this paper presents an innovative approach to generating non-shared weights for recurrent neural networks using hypernetworks. It challenges the traditional weight-sharing paradigm and demonstrates impressive results on a range of sequence modelling tasks. However, additional details on the hypernetworks' architecture and a more thorough comparison with other approaches would strengthen this work.","model":"chatGPT","source":"peerread","label":1,"id":2191}
{"text":"Introduction:\nThe paper \"Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses\" proposes an automatic evaluation model for assessing whether a dialogue response is human-like or not. The authors argue that the current metrics used for evaluating dialogue responses fail to capture the complexity of human linguistic performance, and therefore propose a deep learning-based model that utilizes sentence-level features, semantic similarity, and corresponding attributes to effectively evaluate dialogue responses. Overall, the paper presents an interesting and relevant approach to automating the Turing Test, which has relevance to both the field of natural language processing and artificial intelligence.\n\nStrengths:\nThe paper presented a novel approach for automatic evaluation of dialogue responses that incorporated several aspects of language, including semantics and syntax, to create a more comprehensive evaluation metric. The authors have done a good job of clearly presenting the methodology, including the different features and attributes used in their model, as well as the various evaluation metrics that were considered in their experiments. The authors have also provided a well-structured overview of the related work in the field, which serves to highlight the novelty of their proposed approach.\n\nWeaknesses:\nThe authors could benefit from providing a more extensive discussion of the limitations of their proposed model. Although the experiments show that their approach is effective, it is important to consider some of the potential limitations that their approach poses, including how their model may perform with a different domain, audience or language that it wasn't trained on, and how it may perform compared to human evaluations.\n\nConclusion: \nOverall, this paper presents an innovative approach to automating the Turing Test for evaluating dialogue responses. The authors have demonstrated the effectiveness of their proposed model, which incorporates semantic and syntax-based features and attributes, and can be used to assess whether a dialogue response is human-like or not. While the study of its limitations may need to be explored further, this approach stands to facilitate advancements in the field of artificial intelligence and natural language processing. I would, therefore, recommend this paper for publication with minor revisions.","model":"chatGPT","source":"peerread","label":1,"id":2192}
{"text":"This paper titled \"Towards an automatic Turing test: learning to evaluate dialogue responses\" addresses the problem of developing an automated system to evaluate the quality of dialogue responses, thus making the Turing test automated. The authors argue that to make automated dialogue evaluation more reliable, it is necessary to move beyond traditional rule-based techniques and employ machine learning-based methods trained on large-scale human-labeled data. \n\nStrengths: \n\nThe authors make a valuable contribution by proposing a novel approach to evaluating dialogue quality where the proposed method takes into account various aspects of dialogue quality such as coherency, relevance, and fluency. They achieve this by training a machine learning classifier using a large dataset of human-labeled dialogue responses. Consequently, the paper allows for automated dialogue evaluation, which is an essential part of Natural Language Processing (NLP). \n\nThe authors also employ diverse evaluation metric techniques such as precision, recall, and F1 in the study to assess the effectiveness of the proposed approach. Additionally, the authors provide sufficient details on the dataset collection and annotation process, which provides essential information to enable replication of the study.\n\nWeaknesses: \n\nOne of the weaknesses identified in this paper is that the dataset used for training the machine learning model was sourced from Amazon Mechanical Turk, and may not be representative of the general population. Hence, the lack of diversity may affect the generalizability of the proposed approach. Also, the paper did not thoroughly explain how the classifiers operate, which assumes a level of familiarity with the classification models from the readers; as such, there could be difficulties in understanding the model's nuances.\n\nAnother weakness of this paper is that the authors could have provided more comparisons and insights into other similar studies in the area of automated dialogue evaluation.\n\nOverall, the paper makes a valuable contribution to the field by proposing a novel approach to automated dialogue evaluation. However, there is more room for improvement in the dataset collection process and provision of more detailed information on the classifier components.","model":"chatGPT","source":"peerread","label":1,"id":2193}
{"text":"Summary\n\nThe paper presents an automatic dialogue evaluation model, known as ADEM, that learns to predict human-like scores to input responses for unstructured domains. The authors argue that existing evaluation metrics tend to be biased and fail to correlate well with human judgments of response quality. Therefore, ADEM represents an important contribution to dialogue research, as it enables rapid prototyping and testing of new models with fewer expensive human evaluations. The authors show that the ADEM model achieves significantly higher correlations with human judgments compared to conventional metrics such as BLEU, and can also generalize to evaluating dialogue models that were unseen during training.\n\nStrengths\n\nThe paper addresses an important problem in dialogue research and provides a novel approach to automatic evaluation. The authors justify their approach using empirical evidence, and highlight the limitations of existing metrics. The experimental design is well-executed, with the authors using appropriate statistical tests to demonstrate the superiority of their approach. The authors also discuss the implications of their work and suggest possible directions for future research.\n\nWeaknesses\n\nWhile the authors present compelling evidence for the superiority of the ADEM model, more could be done to explain how the model works in practice. For example, it is not clear how the model extracts meaningful representations of input responses, or how it processes the human response scores used for training. More details on the model architecture and training process would be helpful. Additionally, more discussion could be provided on the limitations and potential biases of the new dataset of human response scores used for training ADEM.\n\nOverall, the paper represents a strong contribution to dialogue research and presents a promising approach to automatic evaluation. The authors are commended for their well-executed experimental design and empirical validation of the ADEM model's performance. However, further improvements can be made by providing more details on the inner workings of the model and acknowledging potential limitations of the training data.","model":"chatGPT","source":"peerread","label":1,"id":2194}
{"text":"The paper \"Towards an automatic Turing test: Learning to evaluate dialogue responses\" addresses the challenge of automatically evaluating the quality of dialogue responses in unstructured domains. The authors highlight that existing evaluation metrics are biased and have poor correlation with human judgments of response quality. To address this problem, the authors propose a new model called ADEM, which formulates automatic dialogue evaluation as a learning problem.\n\nStrengths:\n1. The paper addresses an important problem in the field of natural language processing, and proposes a novel approach to tackle it.\n2. The authors present a new dataset of human response scores, which can be used to train and evaluate dialogue systems.\n3. The ADEM model is evaluated using multiple metrics and experimental settings, and the results show that it outperforms existing metrics in predicting human judgments of response quality for both utterance and system-level evaluation.\n4. The authors show that the ADEM model can generalize to evaluating dialogue models that were not seen during training, which demonstrates its potential usefulness for rapid prototyping and testing of new models.\n\nWeaknesses:\n1. The paper does not thoroughly discuss the limitations of the proposed approach. For instance, it would be interesting to know how the ADEM model could be affected by different sources of noise or how well it performs in different domains.\n2. The paper does not discuss potential ethical issues related to the use of automated evaluation procedures, such as potential biases in the dataset or the impact of such methods on the dialogue participants.\n3. While the authors mention that the proposed dataset has high inter-annotator agreement, it is not clear whether such scores are reliable or repeatable across different evaluations.\n\nOverall, this paper is a valuable contribution to the field of dialogue evaluation. While some limitations and ethical concerns should be further discussed, the proposed approach shows great promise for automating the evaluation of dialogue systems.","model":"chatGPT","source":"peerread","label":1,"id":2195}
{"text":"Title: Gated Multimodal Units for Information Fusion\n\nAuthors: [names of authors]\n\nOverall evaluation:\nThe paper presents an interesting approach to information fusion using gated multimodal units. The idea of combining different types of features in a neural network model and using gating mechanisms to control the flow of information through the units is promising. The experiments conducted demonstrate the effectiveness of the proposed approach in several tasks, including sentiment analysis and visual question answering.\n\nStrengths:\n- The paper presents a clear and well-motivated problem statement, describing the limitations of current approaches to information fusion and the need for more effective solutions.\n- The proposed approach is innovative and based on a straightforward yet effective idea of gating the flow of information through multimodal units.\n- The experiments conducted are thorough, and the results show that the proposed approach outperforms baselines on all tasks, with significant improvements in some cases.\n- The paper is well-structured, with clear sections and sub-sections, making it easy to navigate and understand.\n\nWeaknesses:\n- Some technical details of the model architecture, such as the number and size of the layers and the hyperparameters used, are not provided. Providing this information would help in reproducibility and comparability with other works.\n- The experiments lack a comprehensive ablation study to show how different aspects of the proposed model contribute to its effectiveness. This analysis would help in better understanding the strengths and limitations of the approach.\n- The paper could benefit from a more in-depth analysis and discussion of the obtained results, including insights into the underlying factors that may have contributed to the observed improvements.\n\nOverall, the paper presents a promising approach to information fusion that is well-motivated, innovative, and backed by thorough experimental evaluations. However, some minor details could be addressed to improve the overall quality of the work.","model":"chatGPT","source":"peerread","label":1,"id":2196}
{"text":"The paper \"Gated Multimodal Units for Information Fusion\" addresses the problem of integrating different modalities of information in natural language processing systems. The paper proposes a new method for unifying textual and visual representations of data using gated multimodal units.\n\nThe primary strength of the paper is its novel approach to addressing the problem of information integration. The proposed method demonstrates significant improvements in performance over existing methods in several benchmark datasets. Additionally, the paper provides a clear and accessible explanation of the proposed method, making it easy for researchers to replicate and build upon.\n\nOne weakness of the paper is the limited evaluation of the proposed method. While the authors compare their method to several baselines, it would have been beneficial to see more in-depth analysis and comparison to other state-of-the-art methods. Additionally, the paper mainly focused on textual and visual modalities and did not explore the applicability of the proposed method to other modalities, such as audio or sensor data.\n\nOverall, the paper \"Gated Multimodal Units for Information Fusion\" brings a new perspective to the problem of information integration in NLP systems. The proposed method is promising and provides significant improvements to existing methods. However, there is room for further evaluation and exploration of the proposed method on different modalities.","model":"chatGPT","source":"peerread","label":1,"id":2197}
{"text":"The paper \u201cGated Multimodal Units for Information Fusion\u201d presents a novel model that employs gated neural networks for multimodal learning. This model, called the Gated Multimodal Unit (GMU), learns to combine information from multiple modalities to find an intermediate representation. The paper presents an evaluation of GMU on the task of genre classification of movies using plot and poster information. Additionally, the paper introduces the MM-IMDb dataset, which appears to be the largest publicly available multimodal dataset for genre prediction on movies. \n\nOverall, I found the paper to be well-written, clear, and informative. The paper provides a good context for the problem being addressed and clearly describes the proposed approach. The experiments appear to be thorough and well-designed, and the results suggest that the GMU model outperforms other fusion strategies and improves over single-modality approaches.\n\nOne point that I found unclear was the justification for the choice of the MM-IMDb dataset. Although the paper notes that it is the largest publicly available multimodal dataset for genre prediction on movies, it is not clear why other datasets were not considered or why this dataset was selected. It would be useful for the authors to provide more information on this to aid in the reproducibility of the experiments.\n\nAnother point that could be further elaborated is the interpretability of the GMU model. The paper notes that the multiplicative gates allow the model to learn how modalities influence the unit\u2019s activation. However, it is not entirely clear how the model is making these decisions. More information on the interpretability of the model may be useful for readers interested in applying the model to other tasks.\n\nOverall, I think that the paper presents a good contribution to the field of multimodal learning, and the results suggest that the GMU model may be a useful approach for combining information from multiple modalities. I recommend this paper for publication.","model":"chatGPT","source":"peerread","label":1,"id":2198}
{"text":"The problem that this paper addresses is the challenge of multimodal learning, specifically how to combine data from different sources for improved performance in classification tasks. The paper proposes a novel approach, Gated Multimodal Units (GMU), which uses multiplicative gates to determine how data from different modalities influence the activation of the unit. The evaluation is performed on a multilabel genre classification task using the MM-IMDb dataset, which is the largest publicly available multimodal dataset for genre prediction on movies. \n\nOne of the strengths of this paper is its novelty in proposing a new model for multimodal learning. The authors provide a detailed explanation of the GMU model and its components, making it easy to understand and replicate. The evaluation of the GMU on the multilabel genre classification task is thorough and convincing, as the GMU outperformed single-modality approaches and other fusion strategies. The release of the MM-IMDb dataset is also a significant contribution to the field of multimodal learning.\n\nHowever, the paper does have some weaknesses. The paper lacks a clear discussion of the limitations of the proposed model, such as its potential weaknesses in handling large volumes of data or dealing with sparse or missing data. Additionally, the use of only one dataset for evaluation limits the generalizability of the GMU to other multimodal datasets. Finally, the paper could benefit from a more detailed comparison with existing multimodal learning approaches.\n\nOverall, this paper presents a novel approach for multimodal learning using GMUs, which outperforms other fusion strategies on a genre classification task. Although there are some limitations and weaknesses, the paper represents a valuable contribution to the field of multimodal learning.","model":"chatGPT","source":"peerread","label":1,"id":2199}
{"text":"Title: Unsupervised Perceptual Rewards for Imitation Learning\n\nAuthors: [names of authors]\n\nThe paper presented an interesting approach for creating unsupervised perceptual rewards for imitation learning. The authors presented a framework that uses unsupervised learning to create a set of reward functions that can be used in imitation learning algorithms. The framework is designed to exploit the perceptual structure of the task, allowing it to generate appropriate rewards for the task without the need for human intervention.\n\nThe authors demonstrate the effectiveness of their approach by applying it to a set of benchmark tasks in robotics. The results show that their approach outperforms both standard supervised and unsupervised imitation learning algorithms. The authors provide insightful analysis of why this is the case, and they explain how the unsupervised approach is able to capture the underlying structure of the task.\n\nOverall, the paper is well-written and clearly presents the authors\u2019 approach and results. However, I would suggest that the authors provide more details on the benchmarks tasks they used, including how they were selected and what the specific tasks were. Additionally, some of the figures could benefit from labels and more detailed captions to help readers better understand the results.\n\nIn conclusion, this paper presents a novel approach to creating unsupervised perceptual rewards for imitation learning. The results demonstrate its effectiveness, and the insights provided by the authors are valuable. However, some details could be further elaborated upon to improve the clarity of the paper. I would recommend this paper for publication in the field of machine learning and robotics.","model":"chatGPT","source":"peerread","label":1,"id":2200}
{"text":"Problem\/Question:\nThe paper \"Unsupervised Perceptual Rewards for Imitation Learning\" aims to address the problem of how to design effective reward functions for imitation learning tasks. Specifically, the authors propose a novel unsupervised method for generating perceptual rewards which requires minimal human intervention.\n\nStrengths:\nThis paper presents a novel approach to solving the problem of designing effective reward functions for imitation learning, which has the potential to significantly reduce the need for human involvement in the process. The unsupervised nature of the proposed method makes it particularly appealing, as it removes the need for costly and time-consuming manual annotation. The authors provide a thorough description of the proposed approach, including a detailed explanation of how it can be integrated into different learning frameworks. \n\nWeaknesses:\nOne potential weakness of the paper is the relatively limited experimental evaluation. While the authors demonstrate the effectiveness of their approach on several benchmark tasks, it would be beneficial to see additional experiments on more complex and realistic scenarios. Furthermore, the paper does not provide a clear comparison with existing methods for reward function design, so it is difficult to assess the strengths and weaknesses of the proposed method relative to the state-of-the-art in the field. Finally, the paper could benefit from additional clarification and intuition on some of the more technical aspects of the proposed approach, which could make it more accessible to readers unfamiliar with the relevant literature.\n\nOverall, the paper \"Unsupervised Perceptual Rewards for Imitation Learning\" presents an interesting and potentially impactful approach to solving the problem of reward function design in imitation learning, although further experimentation and comparison to existing methods are needed to fully evaluate its effectiveness.","model":"chatGPT","source":"peerread","label":1,"id":2201}
{"text":"Review of \u201cUnsupervised Perceptual Rewards for Imitation Learning\u201d \n\nThe paper by authors highlights the challenge of reward function design as one of the main obstacles in the deployment of reinforcement learning (RL) agents in the real world. The authors propose a novel approach to infer perceptual reward functions quickly from small numbers of demonstrations by leveraging the abstraction power of intermediate visual representations learned by deep models. The key novelty of their approach is its ability to identify the key intermediate steps of a task automatically and identify the most discriminative features that can identify these steps without using any explicit sub-goal supervision. They demonstrate the efficacy of their approach by presenting qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. Further, they show that their approach can be applied to learn a complex real-world door opening skill using a real robot from a video of a human performing the task. \n\nOverall, the paper presents a compelling idea, and the results obtained in the experiments demonstrate the effectiveness of the proposed approach. The authors provide a clear and concise description of their method, and the technical details are explained in a precise and comprehensible manner. The authors have also made the code of their method available online, which strengthens the reproducibility of their research. \n\nHowever, there are a few points that the authors could have elaborated on. Firstly, the authors could have stated the limitations of their approach or possible situations where their method might not work well. Secondly, the evaluation of their approach could be further strengthened by benchmarking against other state-of-the-art methods for unsupervised reward learning. Finally, the high computational cost of their method and the requirement of pre-trained deep models could affect its practical application. It would be beneficial if the authors discussed the efficiency of their approach compared to other state-of-the-art methods.\n\nIn summary, the paper proposes a novel approach that addresses the challenge of reward function design and exploration time in RL. The authors demonstrate the effectiveness of their approach through experiments on real-world tasks and a real robot. The paper is well written, and the technical details are explained lucidly. Nonetheless, the authors could have studied and discussed the limitations of their approach, benchmarked against other methods, and elaborated on the efficiency of their approach.","model":"chatGPT","source":"peerread","label":1,"id":2202}
{"text":"The paper titled \"Unsupervised Perceptual Rewards for Imitation Learning\" aims to address the challenge of designing rewarding functions in reinforcement learning (RL), which is one of the significant obstacles to deployment of RL agents in the real world. The paper proposes a method that leverages the power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from a small number of demonstrations. The approach identifies the key intermediate steps of a task from limited demonstration sequences without any explicit sub-goal supervision. The resulting reward functions are dense, smooth and can be used by an RL agent to learn to perform tasks in real-world settings. The paper evaluates the method on two real-world tasks and presents a quantitative evaluation of the learned reward functions against a human-designed reward function. In addition, the paper demonstrates that the approach can be used to learn a complex real-world door opening skill using a real robot.\n\nThe strengths of this paper include its novelty, as it presents the first results that show that complex robotic manipulation skills can be learned without supervised labels from a video of human performing the task. The approach is scalable and addresses one of the significant challenges in the RL field, which is creating a suitable reward function. Moreover, the paper's evaluation is comprehensive, and it demonstrates the effectiveness of the proposed method on real-world tasks.\n\nHowever, one potential weakness is that the paper did not provide a thorough discussion on the limitations of the proposed approach. The paper could have discussed potential scenarios that may lead to suboptimal results using the proposed method or the extent to which the approach generalizes to unseen tasks. Furthermore, the paper did not present quantitative evaluation on diverse tasks, which could provide better insights into the performance of the proposed approach.\n\nIn conclusion, the proposed method in the paper is innovative and tackles a challenging problem in the RL field. The paper's evaluation is comprehensive and demonstrates the effectiveness of the approach on real-world tasks. Nevertheless, the paper could benefit from a more thorough discussion on its limitations and future directions.","model":"chatGPT","source":"peerread","label":1,"id":2203}
{"text":"Summary: \nThe paper proposes a novel approach for automated generation of multilingual clusters for the evaluation of distributed representations. The proposed approach is compared to the conventional approach for evaluation of distributed representations, which involves manual selection of similar words from different languages. The evaluation results show that the proposed approach is efficient and yields better results as compared to the conventional approach.\n\nStrengths:\n1. The proposed approach is novel and has the potential to significantly improve the evaluation of distributed representations.\n2. The experiments conducted in the paper are comprehensive, and the results obtained are statistically significant.\n3. The paper is well-written and explains the proposed approach in detail.\n\nWeaknesses:\n1. Although the paper claims that the proposed approach is efficient, it would be helpful to include some computational complexity analysis to support this claim.\n2. The paper only evaluates the proposed approach on a limited number of languages. Future work could include evaluation of the approach on additional languages.\n\nOverall, this is a well-written and informative paper that proposes a novel and promising approach for evaluation of distributed representations.","model":"chatGPT","source":"peerread","label":1,"id":2204}
{"text":"The paper \"Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations\" presents a methodology for automatically generating multilingual lexical clusters for the evaluation of distributed representations. The problem under consideration is the evaluation of distributed representations, which are widely used in the field of Natural Language Processing for tasks such as machine translation, sentiment analysis, named entity recognition, and others. The evaluation of distributed representations is a crucial step in determining their quality and effectiveness for use in various NLP tasks.\n\nStrengths:\n\n1. The paper provides a novel methodology for generating multilingual lexical clusters for the evaluation of distributed representations.\n\n2. The authors make use of a large multilingual corpus, which is based on Wikipedia, to generate the lexical clusters. This ensures that the clusters cover a wide range of languages and are representative of real-world language use.\n\n3. The paper provides extensive experimental results that demonstrate the effectiveness of the proposed methodology in evaluating distributed representations across multiple languages and tasks.\n\nWeaknesses:\n\n1. The paper lacks a clear motivation for why the proposed methodology is better than existing approaches in the area of evaluating distributed representations.\n\n2. The paper does not provide a detailed analysis of the limitations of the proposed methodology or potential areas for improvement.\n\n3. The paper could benefit from a more thorough discussion and interpretation of the experimental results, including any unexpected or surprising findings.\n\nOverall, the paper \"Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations\" presents a promising methodology for evaluating distributed representations across multiple languages and tasks, but could benefit from further refinement and analysis.","model":"chatGPT","source":"peerread","label":1,"id":2205}
{"text":"The paper \"Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations\" presents an interesting approach to evaluate word embeddings' performance in detecting outliers by generating sets of semantically similar clusters of entities and outliers automatically in a language-agnostic way. The authors applied their methodology to create a well-curated dataset called WikiSem500, which they used to evaluate several popular embedding methods. Furthermore, they also demonstrated that the performance on this dataset correlates with performance on the sentiment analysis task.\n\nThe authors provide a clear explanation of their methodology and dataset creation process, which enables reproducibility and easier adoption by the research community. The results obtained with their approach are significant, and the relationship between the outlier detection task and sentiment analysis is noteworthy.\n\nHowever, there are a few minor issues with the paper that need to be addressed. First, the authors could improve the clarity of their methodology section by providing more context on how they generated their datasets. Second, the authors should discuss the limitations of their approach, such as the dependence on the quality of the input dataset used for generating clusters and outliers.\n\nOverall, this paper presents an interesting and useful contribution to the evaluation of word embeddings, and I recommend it for publication after addressing the minor issues mentioned above.","model":"chatGPT","source":"peerread","label":1,"id":2206}
{"text":"This paper proposes a solution to the problem of evaluating distributed representations of words by developing a language-agnostic way of automatically generating clusters of semantic similarity. Specifically, the authors used their methodology to generate a gold-standard dataset for the intrinsic evaluation of word embeddings, which they tested using various state-of-the-art embeddings. They also demonstrated a correlation between the performance on their dataset and the performance on sentiment analysis. \n\nOverall, the paper makes a valuable contribution to the field of natural language processing by providing a new way of generating clusters and evaluating word embeddings. The methodology is innovative and appears to be effective in detecting outliers, which is an important aspect of evaluating distributed representations. The use of a standardized dataset, along with the evaluation of multiple embeddings, adds rigor and credibility to the results.\n\nOne strength of the paper is the thorough description of the methodology for generating the WikiSem500 dataset. The authors provided detailed explanations of the algorithms used and the parameters chosen, along with examples, which make it easy for researchers to replicate their work. Additionally, the results obtained from their experiments are relevant and useful, as they show a clear correlation between performance on the WikiSem500 dataset and performance on sentiment analysis tasks.\n\nHowever, there are also some weaknesses in the paper that need to be addressed. First, the authors did not provide a comprehensive analysis of the limitations of their methodology. This is important because, without an understanding of its limitations, it may not be clear when the methodology is appropriate to use. Second, the authors did not provide a detailed discussion of the implications of their findings. In particular, it would be interesting to know how the results may impact future work on distributed representations or sentiment analysis.\n\nOverall, the paper presents an interesting and novel approach to generate clusters and evaluate distributed representations. The methodology and results are well presented and provide valuable insights into the performance of different embeddings. However, more in-depth analysis is required to identify the limitations and implications of this research.","model":"chatGPT","source":"peerread","label":1,"id":2207}
{"text":"Title: Peer Review for \"Shift Aggregate Extract Networks\"\n\nThe paper, \"Shift Aggregate Extract Networks\" proposes a novel method for text classification using a combination of shift, aggregate, and extract operations on embedding matrices of text data. The proposed method is evaluated on a number of standard text classification datasets and compared with several baseline models.\n\nOverall, the paper is clearly written and well-organized. The authors have provided a detailed explanation of the proposed method and the experimental setup. The paper presents novel contributions that are likely to be of interest to the research community. However, there are certain areas where the paper could be improved.\n\nFirstly, the authors could provide more insight into the intuition behind their approach. Although the paper explains the theory behind shift, aggregate, and extract operations in detail, it is not entirely clear why these operations are effective in text classification tasks. The authors could provide more experimental evidence and analysis to support their intuition.\n\nSecondly, the authors could provide more details on the hyperparameter selection process. The paper only mentions the range of hyperparameters used for the experiments, but it would be helpful to know how these values were chosen. Providing more details on the hyperparameter tuning process would help readers better understand how the model performed in comparison to the baseline models.\n\nFinally, the authors could provide more discussion on the limitations of the proposed method. Although the paper discusses some limitations, such as the vague boundaries between the shift, aggregate, and extract operations, it would be helpful to have a more detailed discussion on the challenges and constraints of the proposed approach.\n\nOverall, I would recommend this paper for publication with minor revisions. The proposed method is well-motivated and well-presented, and the experimental results suggest that it outperforms several baseline models on various text classification datasets. With the suggested revisions, this paper has the potential to make a valuable contribution to the field of natural language processing.","model":"chatGPT","source":"peerread","label":1,"id":2208}
{"text":"The paper \"Shift Aggregate Extract Networks\" presents a new methodology for natural language processing (NLP) that combines sentence parsing and entity recognition to create a more efficient and effective approach for text synthesis. The problem that this paper addresses is the difficulty in summarizing, analyzing, and understanding large quantities of text data. Namely, there is a need for robust and scalable methods that can identify relevant entities and their relationships in a given text and synthesize this information into a coherent summary.\n\nOne of the strengths of this paper is the novelty of the approach. The authors propose a new method that leverages existing NLP techniques, such as parsing and entity recognition, but combines them in a novel approach. This methodology, Shift Aggregate Extract Networks (SAEN), integrates parsing and entity recognition by aggregating over word embeddings corresponding to parsing nodes and mentions of entities, which allows the system to handle long-text input in a real-time setting. This approach has the potential to provide a more efficient and effective approach to text analysis and summarization, which could be highly impactful in a variety of research domains.\n\nAnother strength of this paper is the evaluation of SAEN. The authors provide experimental results that demonstrate that SAEN outperforms existing state-of-the-art methods for text summarization, as measured by several standard evaluation metrics. The authors also provide a detailed analysis of the performance of the system in terms of accuracy, recall, and precision.\n\nOne weakness of this paper is the lack of a comparison to alternative techniques for parsing and entity recognition. While the proposed methodology is shown to be effective, it is unclear how it compares to other approaches that rely on different underlying techniques. It would be beneficial to have a more comprehensive comparison with related methods to fully appreciate the strengths of the SAEN approach and its potential limitations.\n\nOverall, this paper presents an innovative and promising approach to NLP that could have a significant impact on text summarization and analysis. The authors did an excellent job of demonstrating the effectiveness of their system, but further research is needed to fully understand the strengths and weaknesses of this approach compared to other state-of-the-art techniques.","model":"chatGPT","source":"peerread","label":1,"id":2209}
{"text":"Review of \"Shift Aggregate Extract Networks\":\n\nThe paper proposes a novel architecture called Shift Aggregate Extract Network (SAEN) that is designed for learning representations on social network data. The authors claim that SAEN can achieve state-of-the-art results on real-world social network datasets. \n\nThe overall structure of the paper is well-organized, with clear descriptions of the problem and proposed method. The theoretical underpinnings of the method are supported with examples, which help to clarify the technical details of the model. In addition, the paper provides an extensive experimental evaluation of the proposed algorithm, showing how their approach compares favorably to existing methods. \n\nOne of the key strengths of the proposed algorithm is its ability to decompose input graphs into hierarchies made of multiple strata of objects, and to learn vector representations of each object by applying shift, aggregate and extract operations on the vector representations of its parts. In addition, the paper provides an algorithm for domain compression that takes advantage of symmetries in hierarchical decompositions to reduce the memory usage and obtain significant speedups.\n\nAlthough the paper is well-written and well-organized, there are several areas that could be improved. Firstly, the authors could have provided more insights into the underlying principles of the shift, aggregate, and extract operations. This would have helped readers to understand how these operations work together to build the hierarchical representations. In addition, while the paper provides an extensive experimental evaluation of the proposed algorithm, the authors could have included more detailed analysis of the results to help readers understand the strengths and limitations of the proposed approach.\n\nOverall, I believe that the proposed SAEN method is a promising approach for learning representations on social network data. The technical details of the method are well-explained, and the experimental evaluation demonstrates the effectiveness of the proposed approach. However, more explanation of the theoretical underpinnings of the operations would be beneficial, and further analysis of the results would help readers understand the strengths and limitations of the proposed algorithm.","model":"chatGPT","source":"peerread","label":1,"id":2210}
{"text":"This paper proposes an architecture referred to as Shift Aggregate Extract Network (SAEN) for learning representations on social network data. The objective of the paper is to design an algorithm that can efficiently learn vector representations of each object by decomposing input graphs into hierarchies made of multiple strata of objects. The authors further claim that SAEN can reduce the memory usage and obtain significant speedups through the proposed domain compression algorithm that takes advantage of symmetries in hierarchical decompositions. The paper is concluded with an empirical evaluation of SAEN on a set of real-world social network datasets.\n\nThe strengths of this paper are that it addresses an important problem in the field of social network analysis - how to optimize learning representations of objects in graph hierarchies. The paper presents a well-structured methodology that is easy to follow, and the empirical evaluation provides convincing evidence that the proposed algorithm outperforms the current state of the art.\n\nHowever, there are some potential weaknesses in this paper worth noting. The paper does not provide any theoretical analysis to support the proposed approach. There is also no detailed discussion about the trade-offs between computational efficiency and accuracy. Additionally, the empirical evaluation does not provide a discussion of the limitations of the proposed algorithm and how it can be extended or improved.\n\nDespite these limitations, the paper makes a valuable contribution to the field of social network analysis by offering a novel architecture for learning representations of objects in social network data. The paper's empirical evaluation demonstrates that the proposed algorithm is a competitive alternative to the current state of the art.","model":"chatGPT","source":"peerread","label":1,"id":2211}
{"text":"Title: Programming With a Differentiable Forth Interpreter\n\nAuthors: John Smith and Lisa Jones\n\nIn this paper, John Smith and Lisa Jones explore the use of a differentiable Forth interpreter as a tool for programming. The authors show how this new approach can allow programmers to easily create differentiable programs that can be optimized through gradient descent.\n\nOverall, the paper presents an interesting and well-researched idea. The authors provide a thorough explanation of the Forth interpreter and how it can be used for differentiable programming. The paper also includes several examples that demonstrate the effectiveness of their approach.\n\nOne of the strengths of this paper is the way that the authors frame their work within the broader context of programming as a whole. They provide a clear explanation of the challenges that traditional programming approaches face when dealing with optimization, and how their approach presents a solution. The authors also highlight the potential practical applications of this approach, such as in machine learning and robotics.\n\nHowever, while the paper is well-written and easy to follow, there are a few areas where I think it could be improved. Firstly, the paper would benefit from a more detailed discussion of the limitations of their approach. While the examples provided illustrate the effectiveness of their Forth interpreter, it is unclear to what extent this approach could be used in more general programming contexts.\n\nAdditionally, I would have liked to see a more detailed explanation of the implementation and experimental setup for their approach. While the authors do provide some information, more detail would be helpful in understanding how their approach works in practice.\n\nOverall, I think this paper presents a valuable contribution to the field of programming and could help to open up new avenues for research in differentiable programming. However, in order to fully evaluate the effectiveness of their approach, more detailed experiments and analysis would be necessary.","model":"chatGPT","source":"peerread","label":1,"id":2212}
{"text":"The paper titled \"Programming With a Differentiable Forth Interpreter\" proposes a novel approach to programming through a differentiable Forth interpreter. The authors argue that traditional programming languages are not designed to learn from data and lack the flexibility to adapt to changing scenarios, while their approach enables programs to be optimized in real-time, and they can learn from data. \n\nThe strength of the paper is its unique approach towards programming and machine learning, by demonstrating how differentiable programming with the Forth interpreter can be used to train and optimize programs on-the-fly. The authors explain the benefits of their approach and provide several case studies to demonstrate the performance of their method. Moreover, the paper is well-structured, clearly written, and provides detailed explanations of the proposed method.\n\nHowever, the paper has some weaknesses. First, the authors do not provide details about how their approach compares to existing methods in programming and machine learning. Moreover, the paper lacks rigorous experimental evaluation, and it is unclear how effective the proposed approach would be on large-scale, real-world problems. Another potential weakness of the paper is that the proposed method may be difficult to implement and understand for non-experts.\n\nIn conclusion, the paper provides a novel approach to programming through a differentiable Forth interpreter, which has the potential to revolutionize the field of machine learning. While it has some weaknesses, it opens up new directions for research and inspires future work on differentiable programming. Overall, the paper is a valuable addition to the field and should be of interest to researchers and practitioners interested in machine learning and optimization.","model":"chatGPT","source":"peerread","label":1,"id":2213}
{"text":"Title: Peer Review for \"Programming With a Differentiable Forth Interpreter\"\n\nThis paper proposes a differentiable interpreter for the programming language Forth. It is designed to incorporate procedural prior knowledge into a model for learning that provides better generalization over problem sizes, and this is achieved even with limited training data. The authors demonstrate the effectiveness of the interpreter in performing complex tasks such as sequence sorting and addition. The paper is well written with an excellent explanation of the implementation details. The novelty and significance of the interpreter are demonstrated through experimental results.\n\nStrengths:\n\n1. The paper presents an innovative approach for incorporating procedural prior knowledge into a model. The design of the interpreter is elegantly executed by leveraging a neural implementation of the dual stack machine which underlies Forth.\n\n2. The results reported are impressive and provide evidence for the effectiveness of the interpreter in performing complex tasks such as sequence sorting and addition. The experimental results also show that the interpreter performs better than other models when faced with limited amounts of training data.\n\n3. The integration of the program with existing neural computation graphs provides an added functionality that increases the scope for using the interpreter in practical applications.\n\n4. The paper is very well written, and the implementation details are explained with clarity.\n\nWeaknesses:\n\n1. While the authors have provided a comprehensive experimental evaluation, further experiments could be carried out to demonstrate the robustness of the interpreter. The paper does not explain how performance is affected when the training data is varied, or when the structure of the input\/output data is changed.\n\n2. It would have been useful to provide a theoretical explanation for the effectiveness of the interpreter. Without such an explanation, the interpretation of the observed results is left to the reader's subjective judgment.\n\n3. The authors assume prior knowledge of the programming language Forth. It would be helpful to provide an introduction to the language or explain the key features in detail for readers with limited background knowledge.\n\nOverall, the paper presents an innovative approach to solving a critical problem in the field of neural programming, i.e., the incorporation of procedural prior knowledge. The results reported are encouraging and demonstrate the effectiveness of the proposed model. With some further experimental work and theoretical explanation, this paper could become an essential reference for researchers working in the field of neural programming.","model":"chatGPT","source":"peerread","label":1,"id":2214}
{"text":"The paper, \"Programming With a Differentiable Forth Interpreter,\" addresses the problem of incorporating prior procedural knowledge, such as knowing the overall recursive structure of a sequence transduction program or the fact that a program will likely use arithmetic operations on real numbers to solve a task, into neural networks that can learn to compute any function given sufficient training data. The authors present a differentiable interpreter for the programming language Forth, through which programmers can write program sketches with slots that can be filled with behavior trained from program input-output data. The authors show that their interpreter is able to effectively leverage different levels of prior program structure and learn complex transduction tasks such as sequence sorting or addition with substantially less data and better generalization over problem sizes.\n\nThe main strength of this paper is its innovative approach to incorporating procedural knowledge into neural network models. The authors present a novel method for integrating program structure and behavior, which has the potential to significantly reduce the amount of training data required for effective learning. Additionally, the use of symbolic computation and parallel branching optimizations provide significant speed improvements that further enhance the feasibility of this approach.\n\nOne weakness of the paper is its limited evaluation of the proposed method. While the authors demonstrate the effectiveness of their approach on a few selected tasks, it remains to be seen how it performs on other types of problems, particularly those that require more complex program structures or with more diverse input and output data. Additionally, the authors do not compare their method to alternative approaches, leaving the reader uncertain as to how this method compares in terms of effectiveness and efficiency.\n\nOverall, this paper presents an interesting and innovative approach to incorporating prior knowledge into neural networks. While further evaluation is necessary to fully assess the effectiveness and efficiency of this method, the results presented suggest that this approach could contribute to improving the performance of neural networks on a variety of tasks.","model":"chatGPT","source":"peerread","label":1,"id":2215}
{"text":"Overall, I found the paper \"Recursive Regression with Neural Networks: Approximating the HJI PDE Solution\" to be well-written and informative. The authors effectively present a novel approach to solving the Hamilton-Jacobi-Isaacs (HJI) equation using neural networks.\n\nThe authors provide a clear and concise description of the HJI equation and its importance in control theory. They also provide a thorough overview of existing methods used to solve the HJI equation and their limitations.\n\nThe proposed method, recursive regression with neural networks, is presented in a logical and understandable manner. The authors provide a detailed description of the algorithm and its implementation, including the use of backpropagation and regularization techniques.\n\nThe experiments presented in the paper are well-designed and provide clear evidence of the effectiveness of the proposed method. The authors compare their method to existing approaches using several numerical examples and demonstrate that their method is able to provide accurate solutions with significantly reduced computational time.\n\nThere are a few minor points that could be clarified or expanded upon. For example, the authors mention the use of standard libraries for deep learning, but do not provide specific details or citations. Additionally, it would be helpful to provide more information on the hyperparameters used in the experiments, such as the learning rate and regularization strength.\n\nOverall, I believe that this paper makes a valuable contribution to the field of control theory and neural network-based PDE solvers. The proposed method is well-motivated, clearly described, and supported by compelling experimental results. I would recommend this paper to researchers interested in either control theory or neural networks.","model":"chatGPT","source":"peerread","label":1,"id":2216}
{"text":"The paper \"Recursive Regression with Neural Networks: Approximating the HJI PDE Solution\" addresses the problem of approximating the solution to the Hamilton-Jacobi-Isaacs (HJI) equation using neural networks. This equation is used in many areas, such as robotics and autonomous systems, to compute a safe set of control inputs for a given system. However, solving the HJI equation can be computationally expensive, making it challenging to use in real-time applications. \n\nThe paper proposes an approach based on recursive regression using neural networks to approximate the HJI solution. The method iteratively improves the approximation by fitting the difference between the true solution and the current approximation, thus reducing the error over time.\n\nOne strength of the paper is the clear and concise presentation of the proposed method, which makes it easy to understand the steps involved in using recursive regression with neural networks. The authors provide detailed explanations of the mathematical foundations of the HJI equation, as well as the specific neural network architecture and training process used. \n\nAnother strength is the experimental evaluation of the method on several benchmark problems in robotics and autonomous systems. The authors show that their approach achieves comparable or better results than state-of-the-art methods, while being significantly faster.\n\nOne weakness of the paper is the lack of discussion of the limitations of the proposed method. While the authors acknowledge that their approach may not work well for highly nonlinear systems, they do not provide examples or insights into these limitations. Additionally, the paper does not address the implementation details of the method, such as the required computational resources or the potential challenges in training the neural network.\n\nOverall, \"Recursive Regression with Neural Networks: Approximating the HJI PDE Solution\" presents an interesting and promising approach to approximating the solution to the HJI equation using neural networks. The paper is well-written, with clear explanations and experimental evidence to support the proposed method. However, additional discussion of the limitations and implementation details would be valuable to help readers better understand the practical implications of the approach.","model":"chatGPT","source":"peerread","label":1,"id":2217}
{"text":"Review of \"Recursive Regression with Neural Networks: Approximating the HJI PDE Solution\"\n\nThe paper presents an algorithm that approximates the solution to the Hamilton-Jacobi-Isaacs (HJI) partial differential equation (PDE) using neural networks. The HJI PDE arises in the fields of control theory and robotics and is crucial for ensuring safe operation of critical systems. The proposed algorithm frames the problem of approximating the solution to the HJI PDE as a regression problem, even when pairs of inputs and outputs are not available. \n\nThe paper is well-written with clear explanations of the technical details. The approach taken by the authors is novel and innovative, and the results of the experiments demonstrate the effectiveness of the proposed algorithm. However, there are some areas where the paper could be improved.\n\nFirst, it would be helpful to provide more details about the experimental setup, such as the number of data samples used and how they were generated. This would allow readers to better understand the scope of the experiments and the limitations of the proposed algorithm. \n\nSecond, it would be useful to compare the proposed algorithm to other state-of-the-art approaches for approximating the solution to the HJI PDE. This would provide a more complete picture of the performance of the proposed algorithm, and help readers understand how the algorithm compares to other methods. \n\nFinally, the authors could provide more insights into the limitations of the proposed approach. For example, are there any particular scenarios where the algorithm may not work well? This would help readers understand the potential practical applications of the algorithm and avoid potential pitfalls in its use.\n\nOverall, this is an interesting and promising paper that could benefit from additional empirical comparisons and a more detailed exploration of the limitations of the proposed approach.","model":"chatGPT","source":"peerread","label":1,"id":2218}
{"text":"Peer Review:\n\nThe paper \"Recursive Regression with Neural Networks: Approximating the HJI PDE Solution\" presents a novel algorithm for approximating the solution to the Hamilton-Jacobi-Isaacs (HJI) partial differential equation using neural networks. The paper starts by highlighting the importance of solving the HJI PDE, which is of particular relevance in control theory and robotics for ensuring the safety and performance of critical systems. The authors then present the proposed algorithm and a comparative analysis with current state-of-the-art solutions.\n\nOverall, the paper presents an interesting and important problem and proposes a novel solution that can potentially have practical implications. The paper is well-structured, with a clear introduction, methodology, experimental setup, results, and conclusions. The authors use rigorous mathematical derivations that are adequately explained, making the paper accessible to a wide range of readers, including researchers in NLP and machine learning.\n\nThe main strength of the paper is the proposed algorithm, which can approximate the solution to the HJI PDE when data pairs are unavailable. The authors demonstrate the effectiveness of the algorithm through empirical experiments and provide a comparative analysis with other state-of-the-art solutions. The experimental results show that the proposed algorithm outperforms other solutions in terms of accuracy and efficiency, which further corroborate the potential practical usefulness of the algorithm.\n\nHowever, there are a few weaknesses that need to be addressed. One is that the paper does not evaluate the robustness of the proposed algorithm under different scenarios, such as when the input space changes, or when the underlying problem shifts. It would be interesting to see how the proposed algorithm compares to other approaches under such conditions.\n\nAnother weakness is that the paper does not discuss the limitations and potential shortcomings of the proposed algorithm. It would be helpful to have a section dedicated to this, where the authors discuss scenarios in which the algorithm may not be effective. Additionally, the authors should also provide insights into future work that could further improve the proposed algorithm.\n\nOverall, the paper provides an interesting contribution to the field of machine learning and control theory, and the proposed algorithm could have practical implications in ensuring the safety and performance of critical systems. The paper is well-written and presents clear and rigorous mathematical derivations. The weaknesses I mentioned earlier could be addressed to make the paper even stronger.","model":"chatGPT","source":"peerread","label":1,"id":2219}
{"text":"Overall, I found this paper on Nonparametrically Learning Activation Functions in Deep Neural Nets to be well-written and informative. The authors present a novel approach to learn activation functions in deep neural networks without requiring specific parameterization or prior knowledge.\n\nThe experimental results presented in the paper are impressive and convincing. The authors provide a comprehensive comparison of their nonparametric approach with several state-of-the-art parametric and nonparametric methods, demonstrating the superiority of their method in both classification and regression tasks. Furthermore, the authors provide detailed analysis and explanation of their results, making it easy for readers to understand the performance gains achieved by their method.\n\nOne area that could be improved is the paper's formalization and explanation of the proposed method. While the authors do a good job summarizing the key ideas, there are moments where the explanation could have been clearer. In particular, the authors could have provided more details on the design of their algorithm and how it works within the context of deep neural networks.\n\nOverall, I believe that this paper makes a valuable contribution to the field of deep learning and will be of interest to researchers working in this area. I recommend this paper for publication, subject to minor revisions to improve the clarity of the explanation of the proposed method.","model":"chatGPT","source":"peerread","label":1,"id":2220}
{"text":"Problem\/Question Addressed:\n\nThe paper \"Nonparametrically Learning Activation Functions in Deep Neural Nets\" proposes a nonparametric approach to learn activation functions for deep neural networks without relying on any particular distributional assumptions. The problem that the paper addresses is that while there exist many activation functions to choose from, finding the optimal activation function that can maximize the performance of a neural network still remains a challenge. The authors propose a new approach to address this issue by incorporating nonparametric methods to learn the activation functions from the data.\n\nStrengths:\n\nThe paper presents an innovative and novel approach to the problem of selecting the optimal activation function for deep neural networks. The nonparametric approach of learning activation functions directly from the data is a promising direction that could be useful in many real-world applications. The experiments presented in the paper demonstrate that the proposed methodology outperforms existing methods for selecting the activation function in neural networks, and this is a significant contribution to the field.\n\nWeaknesses:\n\nThe paper does not provide a thorough comparison of the proposed approach with other state-of-the-art methods for activation function selection. In addition, while the experimental results are promising, they only demonstrate the effectiveness of the proposed method on a limited number of datasets. A more extensive evaluation of the proposed approach on a wider range of datasets could strengthen the findings of the paper.\n\nOverall, the paper provides a promising approach to the problem of activation function selection in deep neural networks. The nonparametric approach is innovative and the experimental results are promising, but the comparison with existing methods could be further developed, and the results could be extended to a wider range of datasets.","model":"chatGPT","source":"peerread","label":1,"id":2221}
{"text":"The paper titled \"Nonparametrically Learning Activation Functions in Deep Neural Nets\" presents a novel approach for nonparametrically learning activation functions in deep neural networks. The authors address the current limitation of treating the choice of activation function as a hyperparameter before training, by proposing a framework that allows activation functions to be estimated as part of the training procedure.\n\nThe paper provides a clear and concise description of the proposed framework. The authors provide a theoretical justification for their choice of nonparametric activation functions, and they demonstrate that networks with the nonparametric activation functions generalize well. The experimental results in image recognition datasets are impressive, as the proposed technique achieved up to a 15% relative increase in test performance compared to the baseline. \n\nThe paper is well written, and the results are adequately presented. The authors have carefully addressed the research question and provided a high-level of technical rigor to support their claims. The approach presented in this paper can have a significant impact on the development of deep learning algorithms by expanding the class of functions that each node in the network can learn.\n\nOverall, I recommend this paper for publication after minor revisions, primarily in providing a clear discussion of possible limitations of their framework, providing more information on how the hyperparameters of the method are chosen based on the experimentation they conducted, and what implications the proposed method has for practical applications beyond image recognition.","model":"chatGPT","source":"peerread","label":1,"id":2222}
{"text":"This paper titled \"Nonparametrically Learning Activation Functions in Deep Neural Nets\" aims to provide a principled framework for learning activation functions in deep neural networks in a nonparametric manner. The authors argue that currently, these functions are treated as hyper-parameters before training, limiting the class of functions that each node in the network can learn. By estimating activation functions as part of the training procedure, the authors suggest that they expand the class of functions that nodes in the network can learn. The authors provide a theoretical justification for their nonparametric activation functions and also demonstrate that networks with nonparametric activation functions generalize well. To evaluate their proposed techniques, the authors test them on image recognition datasets and report up to a 15% relative increase in test performance compared to the baseline.\n\nThe paper's strength lies in its novel idea of nonparametrically learning activation functions, and the authors demonstrate its potency through their experiments. The authors provide an insightful theoretical justification for their nonparametric approach, complemented by results demonstrating the enhanced performance on image recognition tasks. Additionally, the authors take a principled approach in problem definition, experimental setup, and evaluation.\n\nHowever, there are some weaknesses. For instance, the paper could benefit from a more detailed presentation of the dataset characteristics, including preprocessing techniques and hyperparameters set during training. Also, the authors did not explore the impact of the size of the dataset and the architecture of the network, which could play a critical role in the performance of their techniques. Lastly, the paper could benefit from more extensive experiments on more diverse datasets to further validate their approach's effectiveness on other domains.\n\nIn conclusion, the paper's strength lies in its novel approach for nonparametrically learning activation functions in deep neural networks and its theoretical justification. The authors also provide convincing experimental results that support their proposed techniques' effectiveness on image recognition tasks. Nonetheless, a more detailed presentation of datasets' characteristics, exploring the impact of the size of the dataset and network architecture, and more extensive experiments on different datasets could further strengthen the paper.","model":"chatGPT","source":"peerread","label":1,"id":2223}
{"text":"Title: Generalizable Features From Unsupervised Learning\n\nThe paper titled \"Generalizable Features From Unsupervised Learning\" presents an interesting study on the use of unsupervised learning for extracting generalizable features from a given data set. The authors make a strong argument that most of the current approaches for feature extraction are limited by their dependence on labeled data, which is often expensive, time-consuming, and biased towards specific domains or tasks. In this regard, the study aimed to explore the potential of unsupervised learning algorithms such as Principal Component Analysis (PCA), t-SNE, and Autoencoder to extract generalizable features that can be applied to various domains and tasks.\n\nThe paper presents a comprehensive review of related studies on feature extraction and highlights the limitations of supervised learning approaches. Moreover, the authors provide a detailed description of each unsupervised algorithm used in their study and explain their strengths and limitations. The methodology employed in the study is well-designed and clearly explained, making it easy to follow and replicate. The authors also provide a detailed analysis of the performance of each unsupervised algorithm and their ability to extract generalizable features.\n\nThe results of the study are impressive, as the authors show that unsupervised algorithms can achieve comparable, if not better results than supervised approaches in feature extraction. They demonstrate that the features learned by unsupervised algorithms can be applied to various machine learning tasks, including image classification, text classification, and clustering. The authors also show that the generalizable features learned by unsupervised algorithms can outperform those of supervised learning algorithms in some cases.\n\nDespite the many strengths of this paper, there are some limitations that need to be addressed. Firstly, the authors propose the use of only a limited set of unsupervised algorithms for feature extraction. Including other unsupervised algorithms such as Restricted Boltzmann Machines (RBMs) or Deep Belief Networks (DBNs) might enhance the study's depth and make it more comprehensive. Secondly, the paper does not provide a benchmark for measuring the performance of unsupervised algorithms, which would further demonstrate their effectiveness in feature extraction.\n\nOverall, the paper provides a well-written, well-researched, and convincing argument on the potential of unsupervised learning algorithms to extract generalizable features. It is a valuable contribution to the field of feature extraction and machine learning in general. I would recommend this paper for publication in its current form, with the suggested improvements outlined above.","model":"chatGPT","source":"peerread","label":1,"id":2224}
{"text":"The paper \"Generalizable Features from Unsupervised Learning\" by Dusek et al. presents a method for extracting generalizable features from unsupervised learning that can be used across different tasks and domains. The authors argue that existing methods for feature extraction typically rely on either hand-crafted features or task-specific learning and are not able to generalize well to new tasks or domains.\n\nStrengths:\n- The authors provide a well-motivated and clearly formulated research question.\n- The proposed method is based on a sound theoretical framework and builds on recent advances in unsupervised learning.\n- The experimental results demonstrate that the method is able to extract generalizable features that outperform state-of-the-art methods on several benchmark datasets.\n- The authors provide a detailed analysis of the features extracted by their method, which can be helpful for understanding the inner workings of the model.\n\nWeaknesses:\n- The paper lacks a discussion of potential shortcomings or limitations of the proposed method.\n- The authors do not provide a detailed comparison of their method with existing approaches, which could make it easier to assess the novelty of their contributions.\n- The paper does not address the computational cost of the proposed method, which could be a concern for applications with limited resources.\n\nOverall, I believe this is a solid paper that presents an interesting and potentially valuable contribution to the field of unsupervised learning. However, some changes and improvements could be made to provide a more comprehensive and convincing argument for the proposed method.","model":"chatGPT","source":"peerread","label":1,"id":2225}
{"text":"The paper titled \"Generalizable Features From Unsupervised Learning\" discusses the potential of unsupervised learning to extract features that enhance model generalization outside the supervised training distribution. The authors focus on physical intuition and explore the ability of unsupervised learning to find features that can predict the stability of towers of square blocks.\n\nThe paper is well-written and provides insightful research into the ability of unsupervised learning to identify features that support extrapolation beyond trained scenarios. The authors have effectively demonstrated the possibility of using unsupervised learning to solve tasks where the labeled data is difficult or impossible to collect. The proposed approach of unsupervised learning and feature extraction can be applied to various other physical tasks, which is a strong contribution to the field.\n\nThe authors have also provided a clear and detailed experimental setup to demonstrate the effectiveness of their approach. The results are thorough and convincingly show that unsupervised learning can indeed extract generalizable features. However, there is a lack of comparison with supervised learning approaches, which could potentially provide a deeper understanding of the benefits of unsupervised learning and the limitations of the proposed approach. Moreover, the authors could consider addressing the limitations of their approach in future work.\n\nOverall, the paper successfully contributes to the research on generalization in machine learning and proposes a promising approach to solve tasks with limited labeled data. The paper, therefore, is recommended for publication in high-quality NLP and machine learning conferences or journals.","model":"chatGPT","source":"peerread","label":1,"id":2226}
{"text":"Peer Review:\n\nThis paper titled \"Generalizable Features From Unsupervised Learning\" presents an approach to predicting the stability of tower configurations using unsupervised learning. The main focus of the paper is to explore the potential of unsupervised learning to identify features that lead to better generalization beyond the training distribution.\n\nThe paper addresses an important research question concerning the generalizability of predictive models. Humans can easily learn predictive models and generalize their knowledge to unseen scenarios, but machines have been less successful in achieving this ability. The paper aims to bridge this gap by using unsupervised learning to extract features that promote generalization.\n\nOne of the strengths of this paper is the clarity of its presentation. The authors provide a clear and concise description of their research approach, methodology, and findings. The results presented in the paper are also impressive, suggesting that unsupervised learning can yield features that improve generalization beyond the training distribution. This finding has important implications for the development of predictive models that can be used in real-world scenarios where the distribution of data may change over time.\n\nHowever, the paper also has some limitations. For example, the authors only tested their approach on a task of predicting block stability, and it is unclear whether the same approach would be effective for other tasks. Additionally, the authors did not compare their approach to other state-of-the-art methods, which could have provided additional insights into the effectiveness of their approach.\n\nOverall, this paper makes a valuable contribution to the field of unsupervised learning and generalization. The findings of this paper could potentially lead to the development of more robust predictive models that can perform well in settings beyond the training distribution.","model":"chatGPT","source":"peerread","label":1,"id":2227}
{"text":"Peer Review for \"Deep Learning with Sets and Point Clouds\"\n\nThe paper \"Deep Learning with Sets and Point Clouds\" presents an overview of recent advancements in deep learning techniques applied to set and point cloud data. It provides a comprehensive review of various deep learning architectures suitable for learning on sets and point cloud data, as well as their applications to different domains.\n\nOverall, the paper is well-written and organized, and the authors demonstrate a deep understanding of the subject matter. The literature review is comprehensive and covers a wide range of relevant studies. The authors have done an excellent job of synthesizing the information into a cohesive narrative, and the paper is highly informative.\n\nOne of the key strengths of this paper is its thoroughness. The authors have provided an overview of a variety of deep learning methods and their applications to sets and point clouds, including recent developments in the field. Furthermore, they have identified the limitations of current methods and provided suggestions for future research directions.\n\nIn particular, I appreciate the section on \"Set Transformer\", which proposes a new method of learning permutation invariant functions. This section provides a detailed discussion of the motivation for the approach, its architecture, and its applications.\n\nAnother strong point of the paper is the authors' clear understanding of the importance of the problem of invariance and equivariance in deep learning with sets and point clouds. The authors have discussed the significance of these concepts and how they are addressed in some of the deep learning architectures.\n\nOn the other hand, I suggest that the authors should provide more concrete examples of their applications to various domains, rather than just providing a general overview. This would make the paper more accessible to readers who may not be familiar with the specific applications of these techniques.\n\nOverall, I believe this paper provides a valuable contribution to the field of deep learning with sets and point clouds. The authors have provided a comprehensive overview of various deep learning methods for these types of data and their applications, and have identified current limitations and opportunities for future research.","model":"chatGPT","source":"peerread","label":1,"id":2228}
{"text":"The paper \"Deep Learning with Sets and Point Clouds\" addresses the problem of training deep learning models with unordered and variable-length input sets or point clouds. The authors propose a new neural network architecture, called PointNet, which is able to process such input types directly and learn discriminative representations without the need for hand-engineering features or process the input using a sequence of steps.\n\nOne of the main strengths of this paper is the effectiveness of the proposed PointNet architecture. The authors provide comprehensive experiments on various benchmarks and demonstrate that the proposed architecture outperforms the state-of-the-art methods that use hand-engineered features or multi-step workflows to process unordered sets and point clouds. The paper presents a clear and intuitive explanation of the architecture and how it works, which enhances its accessibility to the audience.\n\nAnother strength of this paper is the authors' ability to connect the proposed PointNet architecture to previous works in deep learning, thus showing how existing techniques can be adapted for processing sets and point clouds.\n\nHowever, one limitation of this work is that the PointNet architecture learns feature representations that are invariant to any permutation of the input set, which may not be always desired. In some cases, the permutation of the input set may contain important information that should not be ignored. Furthermore, while the PointNet architecture works well for segmentation tasks on individual point clouds, it may not scale well for processing large sets of point clouds or point clouds with different resolutions.\n\nOverall, this paper provides a strong contribution to the deep learning community and offers a promising approach to processing sets and point clouds directly. The paper's strengths outweigh its weaknesses, and I would recommend this paper for publication.","model":"chatGPT","source":"peerread","label":1,"id":2229}
{"text":"This paper titled \"Deep Learning with Sets and Point Clouds\" presents a novel approach to deep learning with set structures. The paper introduces a new type of permutation equivariant layer obtained through parameter-sharing, which has a straightforward implementation and linear-time complexity. The authors demonstrate the usefulness of this layer in point cloud classification, MNIST digit summation, set-outlier detection, and semi-supervised learning with clustering side-information.\n\nOverall, the paper is well-written and presents interesting research. The authors clearly describe the motivation behind their work, which is to enable deep learning with set-structured data, an important problem in the field of machine learning. The simplified approach presented here is a valuable contribution to this research area.\n\nThe experiments conducted in the paper to evaluate the proposed method are thorough and provide ample evidence of its effectiveness. The authors show that the new permutation equivariant layer outperforms previous methods in point cloud classification and produces excellent results in MNIST digit summation. The application of the proposed method to set-outlier detection and semi-supervised learning with clustering side-information is also an essential contribution.\n\nHowever, there are a few areas where the authors could improve the paper. First, the paper contains several typos and grammatical errors that should be addressed. Second, the motivation behind the proposed approach could be better explained. Specifically, it could be more helpful if the authors could highlight why existing approaches to deep learning with set-structured data are insufficient or impractical. Finally, more detailed explanations of the experiments, including the choice of hyperparameters, would make the paper more useful to readers in reproducing the results.\n\nOverall, the paper presents a valuable contribution to the field of deep learning with set-structured data. The proposed method is simple and effective and should be of great interest to those working in this area. With some minor revisions, this paper could be an excellent addition to the literature.","model":"chatGPT","source":"peerread","label":1,"id":2230}
{"text":"The paper \"Deep Learning with Sets and Point Clouds\" introduces a simple permutation equivariant layer for deep learning with set structure. The main goal of this work is to use deep permutation-invariant networks to perform point-cloud classification and MNIST digit summation where the output is invariant to permutations of the input. Furthermore, the authors also demonstrate the usefulness of this type of layer in set-outlier detection and semi-supervised learning with clustering side-information.\n\nStrengths:\nOne significant strength of the paper is that it introduces an effective and efficient way to perform deep learning with set structures. The parameter-sharing strategy significantly reduces the number of weights required for deep learning models with set structures, making the proposed model more computationally efficient. As the authors pointed out, the proposed model has linear-time complexity in the size of each set, which is a huge computational advantage compared to other models. Another strength of the paper is that it provides a comprehensive experimental evaluation of the proposed model's performance in various tasks, including point-cloud classification, digit summation, set-outlier detection, and semi-supervised learning.\n\nWeakness:\nOne weakness of the paper is the lack of a thorough comparison with other state-of-the-art deep learning models. While the proposed model is shown to achieve excellent results in various tasks, it would have been more informative to see how it compares with existing models. Another issue is that the paper does not provide much theoretical grounding for the proposed model. Although the paper shows that the model works well in practice, it would have been better if it had a more robust theoretical foundation. \n\nOverall, the paper \"Deep Learning with Sets and Point Clouds\" introduces a simple and efficient way to perform deep learning with set structures. The authors\u2019 experiments show that the proposed model delivers promising results in many tasks. The authors could improve the paper by providing more theoretical background knowledge and comparing the proposed model with existing models.","model":"chatGPT","source":"peerread","label":1,"id":2231}
{"text":"Title: Exponential Machines: Transfer Learning between Text and Sequences for Sentiment Analysis\n\nGeneral Comments:\n\nThe paper explores the use of Exponential Machines (EMs), a type of machine learning algorithm, for transfer learning between text and sequences in the domain of sentiment analysis. The paper is well-structured and clearly written, making it easy to follow the methodology and results.\n\nStrengths:\n\n1. Novelty: the paper introduces EMs, a relatively new machine learning algorithm, for the domain of sentiment analysis.\n\n2. Transfer learning: This paper demonstrates that EMs can be used effectively for transfer learning between text and sequences, which can be useful for many applications in natural language processing.\n\n3. Experimental Setup: The paper provides a detailed experimental setup, including the datasets used, evaluation metrics, and comparison with state-of-the-art methods.\n\n4. Results: The paper reports promising results, showing that EMs outperform several baseline methods, including some state-of-the-art methods.\n\nWeakness:\n\n1. More experiments: Although the paper provides a good range of experiments, it would be helpful to see additional experiments related to different variations of EMs, such as different hyperparameters or different types of architectures.\n\n2. Comparison: The paper would be strengthened by comparing the proposed method with other popular transfer learning approaches for sentiment analysis such as BERT.\n\n3. Lack of discussion: The discussion section is relatively brief and would benefit from further exploration of the implications of the results for the field of NLP.\n\nOverall, the paper makes a valuable contribution to the field of NLP and presents promising results for using EMs for transfer learning in sentiment analysis. With some improvements, this paper could be a solid addition to the literature.","model":"chatGPT","source":"peerread","label":1,"id":2232}
{"text":"Paper: Exponential Machines\n\nThe Exponential Machines paper addresses a significant problem of improving the performance of machine learning algorithms by introducing a novel non-linear approach based on exponentiation. The paper aims to demonstrate that the new approach can outperform traditional machine learning models on various benchmark datasets, such as Cifar10 and MNIST.\n\nThe main strength of the Exponential Machines paper is the introduction of a new idea for non-linear models that could result in higher accuracy than traditional models. The authors provide a clear and detailed explanation of the technical aspects of the approach and justify the usage of exponentiation by linking it to the function composition theory. The paper contains a sufficient amount of experimental results with detailed analysis and comparisons to various models, demonstrating superior results.\n\nOne weakness of the paper is the extensive use of jargon and technical language, which could be hard for readers unfamiliar with the field to understand. Additionally, while the authors provide a thorough analysis of the results, they do not thoroughly discuss the limitations or potential drawbacks of the method, which could have been explored in greater detail.\n\nOverall, the Exponential Machines paper is a well-written and well-researched paper, introducing a novel approach to non-linear models that could have implications for improving the accuracy of machine learning algorithms. The paper would benefit from additional discussion of the potential limitations of the method and could be made more accessible to a wider range of readers by avoiding extensive jargon.","model":"chatGPT","source":"peerread","label":1,"id":2233}
{"text":"Peer Review for \"Exponential Machines: A Tensor Factorization Approach for Learning High-order Interactions in Recommender Systems\"\n\nThe paper introduces Exponential Machines (ExM), a novel predictor that models all interactions of every order through a factorized format, which employs the tensor train (TT) decomposition. The authors demonstrate how the TT format can be used to regularize the model and control the number of underlying parameters. The proposed method is evaluated on synthetic data with high-order interactions and on a widely used recommender system dataset MovieLens 100K, and the authors show it achieves state-of-the-art performance in both scenarios.\n\nThe paper is technically sound, and the methodology appears to be well designed for modeling interactions of every order. The authors provide a clear explanation of the ExM's architecture and the TT format. The approach is innovative and provides a new perspective to modeling high-order interactions in machine learning.\n\nThe paper also provides sufficient empirical evidence to support the authors' claims. The authors show that ExM achieves state-of-the-art performance on both synthetic data and in the MovieLens 100K dataset. This provides a strong indication of the method's potential to outperform existing high-order factorization models.\n\nHowever, a few areas could benefit from elaboration. The paper lacks a comparison with other state-of-the-art methods such as deep learning models designed to model complex interactions, which could further confirm the superiority of ExM in real-world applications. Additionally, the paper could provide more details on the proposed stochastic Riemannian optimization procedure, which is crucial for fitting tensors with 2^160 entries.\n\nIn conclusion, the paper presents an innovative and effective approach for modeling high-order interactions in machine learning, and the empirical results indicate that it has great potential. The methodology could benefit from being compared with other state-of-the-art methods, and more details on the proposed stochastic Riemannian optimization procedure would improve the paper's technical details.","model":"chatGPT","source":"peerread","label":1,"id":2234}
{"text":"This paper proposes Exponential Machines (ExM), a predictor that models all possible interactions of every order to improve the performance of machine learning solutions in various domains. The authors introduce the concept of representing an exponentially large tensor of parameters in a factorized Tensor Train (TT) format, which not only regularizes the model but also allows control over the number of underlying parameters. To train the model, the authors develop a stochastic Riemannian optimization procedure capable of fitting tensors with up to 2^160 entries. The authors demonstrate the effectiveness of ExM by comparing its performance on synthetic data with high-order interactions to that of other models and showing that ExM performs equally well as high-order factorization machines on the MovieLens 100K dataset.\n\nThe paper presents a novel approach to modeling interactions between features, which is a currently active area of research in machine learning. The authors provide an extensive theoretical background on the Tensor Train format and the stochastic Riemannian optimization method to demonstrate the soundness and comprehensiveness of their work. The paper also presents a detailed experimental analysis of ExM's performance on synthetic and real-world datasets, which helps understand the strengths and weaknesses of the proposed approach.\n\nOne of the strengths of this paper is its clear presentation of the proposed approach and its theoretical underpinnings. The authors provide a detailed explanation of the Tensor Train format, which is complex, to make it more accessible to readers. Additionally, the authors conduct experiments on both synthetic and real-world datasets, which allows the readers to understand the applicability of ExM across various domains.\n\nOne of the weaknesses of this paper is that it might not be easy to implement for practitioners without the necessary theoretical background in the Tensor Train format and the stochastic Riemannian optimization method. Although the authors provide a detailed explanation of the methods, more detailed guidelines on implementation could be provided to help practitioners use ExM. Also, it would have been beneficial to see more experiments on various real-world datasets to show how ExM performs compared to other state-of-the-art models.\n\nOverall, this paper presents a novel approach to modeling interactions between features using Exponential Machines, with a comprehensive theoretical background and rigorous experimental analysis. The proposed approach has the potential to improve the performance of machine learning solutions in various domains.","model":"chatGPT","source":"peerread","label":1,"id":2235}
{"text":"Title: Recurrent Normalization Propagation\n\nAuthors: [Name of authors]\n\nSummary:\n\nThe paper proposes a new variant of normalization called \u201cRecurrent Normalization Propagation\u201d (RNP) for deep recurrent neural networks. RNP is an extension of Layer Normalization, which is a technique that normalizes the inputs at each layer of a neural network to have zero mean and unit variance. This normalization technique prevents internal covariate shift, which can cause problems during training.\n\nThe authors propose to apply standard layer normalization across the recurrent dimension of a sequence of hidden states rather than across the feature dimension of each hidden state. The paper claims that RNP improves the performance of recurrent neural networks (RNNs) on several benchmark datasets.\n\nStrengths:\n\nThe paper presents a new normalization technique for RNNs that is based on layer normalization. The authors provide a clear and concise explanation of the RNP technique and demonstrate its effectiveness on various benchmark datasets. The paper also includes thorough experiments and ablation studies that help understand the effect of different hyperparameters and components of the algorithm. The authors also provide comparisons of RNP with other normalization techniques such as Batch Normalization and Layer Normalization. \n\nWeaknesses:\n\nWhile the authors provide a comprehensive evaluation of the new RNP technique, the experiments could be further improved by providing more extensive comparisons with other state-of-the-art RNN models. Additionally, it would be helpful to examine the effectiveness of RNP on more complex tasks such as language modeling and machine translation. Finally, the paper could benefit from more in-depth discussion of the limitations of the proposed method and possible avenues for future research.\n\nConclusion:\n\nOverall, the paper presents a new technique for normalizing RNNs called Recurrent Normalization Propagation (RNP) which builds on Layer Normalization. The authors conducted thorough experiments that demonstrate the effectiveness of RNP on a variety of benchmark datasets. While there are some areas for improvement, the paper presents valuable insights into developing effective normalization techniques for recurrent neural networks. The paper is well-written, easy to understand, and provides useful comparisons with other normalization techniques. This paper is worth considering for publication in a conference that focuses on neural network research.","model":"chatGPT","source":"peerread","label":1,"id":2236}
{"text":"The paper \"Recurrent Normalization Propagation\" proposes a novel technique that applies a form of normalization to recurrent neural networks (RNNs) to improve their training and generalization capabilities. The normalization technique, called Recurrent Normalization Propagation (RNP), involves the incorporation of a normalization layer within the RNN architecture.\n\nOne of the strengths of the paper is its clear and concise description of the normalization technique and its effectiveness compared to other normalization techniques. The authors provide a thorough analysis of the proposed technique and demonstrate its superior performance in a variety of tasks, including natural language processing and image classification.\n\nAnother strength of the paper is the experiments conducted to validate the RNP approach. The authors provide a comprehensive evaluation of their technique, comparing it with other state-of-the-art RNN models. They show that RNP improves the performance of RNNs in a number of applications, including language modeling, speech recognition, and image classification.\n\nHowever, there are also some weaknesses in the paper. One concern is the lack of a detailed discussion on when RNP may not be effective, or what limitations and constraints it may face. Additionally, the authors could have included more detailed analysis of the computational cost of RNP compared to other normalization techniques.\n\nOverall, \"Recurrent Normalization Propagation\" presents a novel and effective approach to normalize recurrent neural networks that can improve their performance in a wide range of applications. The paper presents a promising direction for further research, and the proposed technique has the potential to be widely applied in the field of deep learning.","model":"chatGPT","source":"peerread","label":1,"id":2237}
{"text":"General comments:\n\nThe paper proposes a novel parameterization called Recurrent Normalization Propagation (RNP) for LSTM networks, which aims to preserve the means and variances of the hidden state and cell state across time without requiring the estimation of statistics at each time step. In particular, the paper compares the proposed RNP with Recurrent Batch Normalization (RBN) and Layer Normalization (LN) for both language modeling and image generative modeling tasks. Empirical results show that RNP outperforms the two existing approaches in terms of training speed and accuracy.\n\nStrengths:\n\n1. The proposed RNP is an interesting and novel approach for normalizing recurrent networks, which has the advantage of preserving semantic consistency over time without the need to compute statistics at each time step, thus reducing computational overheads.\n\n2. The paper provides a detailed analysis of the proposed approach's gradient flow, which is essential for understanding how RNP performs compared to RBN and LN.\n\n3. The proposed initialization method is straightforward, and it efficiently helps training to converge faster.\n\nWeaknesses:\n\n1. The paper lacks a comprehensive ablation study on the effectiveness of the different components of the proposed RNP architecture, which would provide more insights into the impact of each component.\n\n2. It would be useful to see some analysis of the effect of different batch sizes or sequence lengths on the performance of the proposed approach.\n\n3. It would be helpful if the paper could discuss any possible limitations of the proposed approach.\n\n4. The paper did not discuss any potential impact on the generalization performance of RNP, and it would be useful if the paper included experiments that demonstrate generalization performance on the datasets tested.\n\nMinor comments:\n\n1. In Section 2, it would be useful to have a more detailed explanation of how RNP achieves semantic consistency over time compared to RBN or LN.\n\n2. The paper would benefit from adding an explicit comparison with other normalization approaches proposed recently, e.g., feedback normalization, which has shown excellent results for some tasks.\n\nIn summary, the paper presents an interesting and novel approach for normalizing recurrent networks. The experiments demonstrate that the proposed RNP architecture outperforms RBN and LN for both language modeling and image generative modeling tasks, and it is faster to execute. In general, the paper is well written and easy to understand, although some modifications and additional experiments are needed to address the issues raised above.","model":"chatGPT","source":"peerread","label":1,"id":2238}
{"text":"The paper proposes a new LSTM parametrization called Recurrent Normalization Propagation (RNP) that aims to preserve means and variances of hidden states and memory cells across time. The goal is to achieve training benefits similar to those of Recurrent Batch Normalization and Layer Normalization, but without the need to estimate statistics at each time step. Furthermore, the paper investigates the impact of the proposed parametrization on gradient flows and presents a method for parameter initialization.\n\nStrengths:\nThe paper's proposed method, RNP, addresses a key challenge in recurrent normalization approaches by offering an effective solution that preserves means and variances of hidden states and memory cells across time while requiring fewer computations overall. The proposed method's ability to reduce the number of computations needed to estimate statistics at each time step is a significant strength, as it can improve the efficiency of learning recurrent neural networks (RNNs). The paper presents a thorough evaluation of the RNP on language modelling and image generative modelling tasks, demonstrating that it performs as well as or better than other recurrent normalization approaches while being faster to execute.\n\nWeaknesses:\nThe paper could benefit from additional explanation and clarification in some areas. The significance of the initialization method proposed in the paper could be better explained, and the technical details of how the weights are initialized could be elaborated upon. Additionally, while the paper provides empirical results on various tasks to demonstrate the effectiveness of RNP, more analysis could be done to better understand why the proposed method performs well. Finally, while the authors mention the importance of accounting for small batch sizes when using batch normalization techniques, more could be done to explain how RNP can address this issue. \n\nConclusion:\nOverall, the paper presents a novel approach to recurrent normalization, introducing a new method for preserving means and variances of hidden states and memory cells across time. The method is validated through thorough evaluation across different tasks, and the paper presents a compelling argument that the method is faster and requires fewer computations overall than other recurrent normalization techniques. Despite some weaknesses, the paper's contributions are significant, and it offers a valuable advancement to the field of RNNs. Therefore, I recommend this paper for publication in a peer-reviewed journal.","model":"chatGPT","source":"peerread","label":1,"id":2239}
{"text":"Title: \"Generating Interpretable Images with Controllable Structure\"\n\nAuthors: [redacted]\n\nJournal: [redacted]\n\nIn this paper, the authors propose a novel generative model for generating interpretable images with controllable structure. The model comprises of a generative adversarial network (GAN) with a hierarchical latent space network, which allows for controlling and manipulating different aspects of the generated images. The authors demonstrate the effectiveness of their approach through experiments on MNIST and CelebA datasets.\n\nOverall, the paper presents a well-written and well-organized approach for generating interpretable images with controllable structure. The authors provide a clear motivation for their work and a comprehensive overview of previous work in the field. Furthermore, the authors provide a detailed explanation of the proposed model and its implementation, which will be helpful for researchers looking to replicate or extend their work.\n\nThe experimental evaluation conducted by the authors is both thorough and convincing. The authors show that their model generates high-quality images that are interpretable and easily controllable. The comparison with previous work is appropriate, and the results of these comparisons are clearly reported and analyzed.\n\nHowever, there are a few areas where the authors could improve the paper. Firstly, the authors could provide more insight into the choice of the evaluation metrics used in their experiments. Secondly, the authors could consider experimenting with more diverse datasets beyond MNIST and CelebA, to further demonstrate the robustness of their model. Finally, the authors could provide more discussion about the limitations of their approach and potential avenues for future work.\n\nOverall, the paper presents an interesting and potentially impactful approach for generating interpretable images. I would recommend this paper for publication in a peer-reviewed journal, pending minor revisions.","model":"chatGPT","source":"peerread","label":1,"id":2240}
{"text":"This paper titled \"Generating Interpretable Images with Controllable Structure\" addresses the problem of generating images with controllable structures that can be interpreted by humans. The authors propose an approach that utilizes Gaussian mixture model to learn a disentangled representation of images that can be manipulated by adjusting the mixture weights. The goal is to generate interpretable images that allow users to control various aspects of the image structure such as the pose, shape, and texture.\n\nThe paper has several strengths. Firstly, the proposed approach is quite novel and employs a rigorous methodology that is well grounded in machine learning theory. The Gaussian mixture model used in the approach allows for the generation of disentangled representations that can be interpreted easily by humans. Secondly, the authors present a comprehensive validation of the approach both quantitatively and qualitatively. The quantitative evaluation shows that the generated images have high perceptual quality and are competitive with state-of-the-art techniques. The qualitative evaluation, on the other hand, shows that the generated images represent interpretable structures that can be manipulated by users to achieve desired results. Lastly, the authors provide a thorough analysis of the results and discuss the implications for future research.\n\nHowever, there are also some weaknesses that need to be addressed. Firstly, the approach requires a relatively large amount of data to train the model which limits its applicability to smaller datasets. Secondly, the interpretability of the generated images is still somewhat subjective and needs more research to establish clear guidelines for image interpretation. Also, the approach relies heavily on the information provided in the input image, and thus it has limited flexibility when it comes to generating completely new structures that are not seen in the training data.\n\nIn summary, the paper \"Generating Interpretable Images with Controllable Structure\" presents a promising approach for generating images with controllable structures that can be interpreted by humans. The approach is well grounded in machine learning theory, and the results demonstrate high perceptual quality and interpretability of the generated images. However, there is a need to address some of the limitations such as the interpretability of the images and the reliance on large datasets. Overall, this paper makes a significant contribution to the field, and it is recommended for publication after addressing the weaknesses discussed.","model":"chatGPT","source":"peerread","label":1,"id":2241}
{"text":"Title: Peer Review for \"Generating Interpretable Images with Controllable Structure\"\n\nIntroduction:\nThe paper \"Generating Interpretable Images with Controllable Structure\" presented a new method for text-to-image synthesis. The authors have extended Pixel Convolutional Neural Networks (PixelCNN) to generate images with controllable object locations by conditioning on the text, part keypoints, and segmentation masks.\n\nSummary:\nThe authors have presented an approach to generate images from the given text description by training a model end-to-end via maximum likelihood. The proposed approach uses character-level text encoder and PixelCNN to generate images. The authors have shown that the proposed method outperforms the existing methods in terms of pixel log-likelihood on the three data sets: Caltech-UCSD Birds, MPII Human Pose, and Common Objects in Context.\n\nStrengths:\nThe authors have presented a compelling method for text-to-image synthesis. The proposed approach uses PixelCNN, which is an effective method for modeling the image generation process. In addition, the authors have extended PixelCNN to generate images with controllable object locations by conditioning on the text, part keypoints, and segmentation masks. The proposed method has achieved state-of-the-art performance on the three data sets.\n\nWeaknesses:\nThe authors provided a limited analysis of the generated images. The quality of the generated images is subjective and not evaluated quantitatively. It would be beneficial to evaluate the generated images with human annotators or use automatic metrics such as FID or IS. Additionally, the paper lacks a comparison with other state-of-the-art methods in the field.\n\nConclusion:\nIn conclusion, the paper \"Generating Interpretable Images with Controllable Structure\" presents a new approach for text-to-image synthesis, which extends PixelCNN to generate images with controllable object locations by conditioning on the text, part keypoints, and segmentation masks. The proposed method has demonstrated state-of-the-art performance on three data sets. However, the authors should provide a more in-depth analysis of the generated images and compare their method with other state-of-the-art methods in the field.","model":"chatGPT","source":"peerread","label":1,"id":2242}
{"text":"The paper \"Generating Interpretable Images with Controllable Structure\" addresses the problem of text-to-image synthesis, specifically the generation of images with controllable object locations through an extension of Pixel Convolutional Neural Networks. Additionally, the paper explores how the model can generate images conditioned on part keypoints and segmentation masks, and the character-level text encoder and image generation network are jointly trained end-to-end via maximum likelihood. To evaluate the model's performance, the paper establishes quantitative baselines in terms of text and structure-conditional pixel log-likelihood for three data sets: Caltech-UCSD Birds (CUB), MPII Human Pose (MHP), and Common Objects in Context (MS-COCO).\n\nThe paper's strength lies in its innovative approach to solving the problem of text-to-image synthesis by incorporating part keypoints and segmentation masks to generate images with controllable object locations. The end-to-end training approach also enhances the model's performance, as it ensures the optimization of the entire system for the task at hand. The establishment of quantitative baselines for three different datasets is commendable and provides a measure for the model's performance in generating interpretable images.\n\nHowever, the paper seems to have some weaknesses that require attention. First, the paper did not adequately address the limitations of the proposed model, which is essential to understand the broader implications of the study. Additionally, the paper did not provide information on the computational requirements of the model or the time taken to generate images. The authors could also provide more information through qualitative analysis such as human evaluation of the output images, which will further enhance the study's credibility.\n\nIn conclusion, the paper demonstrates an innovative approach to solving the problem of text-to-image synthesis, and the experimental results obtained are promising. Nevertheless, further studies are required to address the identified weaknesses of the paper.","model":"chatGPT","source":"peerread","label":1,"id":2243}
{"text":"Overall, I found the \"Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech\" paper to be well-structured, clear and insightful. The authors focus on enhancing the performance of a Convolutional Neural Network (CNN) by integrating a Fast Chirplet Transform (FChT) algorithm as a pre-processing step for audio analysis in animal calls and human speech.\n\nThe paper provides a detailed background on the state-of-the-art techniques for audio signal processing and how the FChT can be used to extract the chirp signal that is commonly present in animal calls and speech. The authors then proceed to explain how they integrated the FChT algorithm with the CNN model and tested the performance of the model on two datasets - one for animal calls and one for speech signals - to validate their approach.\n\nExperimental results showed that integrating FChT with CNN model can significantly improve the accuracy of classification compared to using CNN alone. The authors also conducted a comprehensive evaluation of their results and provide a detailed analysis which enhances the value of the paper. \n\nThe paper is well-written and clearly presents the idea and their validation approach for classification of animal calls and human speech. However, there are a few areas that could benefit from further elaboration. For instance, the authors could provide more detail on how they selected the hyperparameters of the FChT for this particular application. Additionally, it would be good to gain insights into how well the proposed approach generalizes on novel datasets outside of the ones used in the study.\n\nOverall, I found the \"Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech\" paper to be insightful and well-executed. The paper presents a new method for audio processing and can be of valuable relevance for other researchers working in the field of audio classification, especially for bioacoustic research applications.","model":"chatGPT","source":"peerread","label":1,"id":2244}
{"text":"The paper \"Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech\" addresses the problem of improving the accuracy of Convolutional Neural Networks (CNNs) in recognizing audio signals. The authors have proposed the use of Fast Chirplet Transform (FChT) for audio signal processing and then integrating it with CNNs for enhanced audio recognition.\n\nOne of the strengths of this paper is the thorough explanation of the FChT method, which is a relatively new approach in audio signal processing. The authors have presented several experiments comparing the performance of plain CNNs and CNNs integrated with FChT on both animal call and speech recognition tasks, demonstrating the superiority of the FChT-enhanced CNN.\n\nA weakness of this paper is the lack of comparison with other state-of-the-art methods in audio signal processing, such as Mel-frequency cepstral coefficients (MFCC), which are commonly used in speech recognition research. The authors have also not discussed limitations of the FChT method, such as sensitivity to noise and computational complexity, which could be important factors to consider in real-world applications.\n\nOverall, this paper presents a promising approach to improving audio recognition accuracy, but further work is needed to fully validate the effectiveness of FChT-enhanced CNNs, including comparison to existing approaches and exploration of the limitations of the method.","model":"chatGPT","source":"peerread","label":1,"id":2245}
{"text":"The paper titled \"Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech\" provides a novel approach to pretrain Convolutional Neural Network (CNN) using the efficient Q constant bioacoustic representation of Chirplet kernel. The authors demonstrate that the proposed Fast Chirplet Transform (FCT) can pretrain low-level layers of CNN and thereby reduced the required amount of training data to learn its kernels. \n\nThe paper is written and organized well, and the authors provide a clear and concise explanation of the motivation, methods, and results of their research. The paper is easy to follow, and the figures provided are of high quality and add to the clarity of the presented results.\n\nThe authors have also provided detailed information about the datasets used in their experiments, and the results obtained through FCT pretraining are compared with those obtained through raw audio CNN. The authors demonstrate that FCT pretraining improves the accuracy of the classifiers significantly and reduces the training duration. Additionally, the authors expand on the possibilities of using FCT for deep machine listening and inter-species bioacoustic transfer learning to generalize the representation of animal communication systems.\n\nOverall, the paper presents a comprehensive and well-executed study that offers a novel approach to pretraining CNN, thereby reducing the required training data and improving the accuracy and training time of machine listening classifiers. It offers a practical and efficient contribution to the field of bioacoustic signal processing and machine listening.","model":"chatGPT","source":"peerread","label":1,"id":2246}
{"text":"This paper proposed a method of using Chirplet kernel representation to pre-train Convolutional Neural Nets (CNN) by introducing a Fast Chirplet Transform (FCT) algorithm. The main strength of this work lies in the use of chirplet-based representations that are efficient in terms of capturing bioacoustic features important for machine listening with CNNs. This is demonstrated by the validation of FCT on a variety of environmental and speech datasets, showing that pre-training CNNs with chirplets reduces training time and improves accuracy.\n\nThe advantage of FCT is that it is computationally efficient and can be used for large datasets like months of Orca recordings and thousands of bird species. This is a significant improvement over current state-of-the-art methods which are limited in terms of processing time.\n\nHowever, a potential weakness of this work is that the experimental validation is not very extensive. It would have been better to demonstrate the performance of FCT using other speech datasets and different bioacoustic sounds. Furthermore, the authors did not provide concrete explanations of how the chirplet-based representations improve the performance of Convolutional Neural Networks.\n\nDespite these concerns, this research provides a valuable contribution towards improving the processing of large and complex bioacoustic datasets. The proposed method of using FCT to pre-train CNNs offers a significant reduction in training time and a boost of accuracy. This research has important implications in many domains that use machine listening, such as bird classification, animal communication, and speech recognition. Overall, this paper is well-written, well-organized, and offers a novel contribution to the field of bioacoustic signal processing with machine learning.","model":"chatGPT","source":"peerread","label":1,"id":2247}
{"text":"This paper presents a convergence analysis of a two-layered neural network with rectified linear unit (ReLU) nonlinearity in the hidden layer. The main contribution of the paper is the proof of the symmetry-breaking phenomenon that occurs in these types of networks, that is, the network converges to a solution that breaks the symmetry of the weights initialized with the same values. \n\nThe paper is well-structured and the mathematical derivations are clear and concise. The authors provide a good overview of the previous works in the area and explain the relevance of their contribution. Additionally, the authors provide rigorous mathematical proofs to support their claims.\n\nHowever, the paper lacks experimental results to validate the theoretical findings. Although the analytic results are important, it would be helpful to demonstrate this in practice. Additionally, the paper does not provide a detailed discussion on the implications of the symmetry-breaking phenomenon for practical applications of deep neural networks.\n\nOverall, the paper presents a valuable contribution to the convergence analysis of two-layered neural networks with ReLU nonlinearity. However, to make the paper more substantial, the authors could consider adding experimental results and further discussion on the practical implications of this phenomenon.","model":"chatGPT","source":"peerread","label":1,"id":2248}
{"text":"The paper entitled \"Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity\" deals with the convergence analysis of certain two-layered neural networks employing the rectified linear unit (ReLU) activation function. Specifically, the paper tackles the problem of symmetry-breaking in these networks, and examines how this property affects the convergence rate of their optimization algorithms.\n\nOne of the strengths of this paper is that it presents a novel analysis of the convergence behavior of ReLU-based neural networks that takes into account the symmetry-breaking property. The results obtained by the authors indicate that, under certain conditions, the symmetry-breaking property can indeed accelerate convergence, which is an important contribution to the field of neural network optimization. Additionally, the paper provides a clear and concise illustration of the mathematical techniques utilized in the analysis, making it accessible to a wide range of readers.\n\nHowever, one weakness of the paper is that it assumes a certain level of familiarity with the mathematical tools used in the analysis, which may narrow its target audience. Additionally, while the analysis presented in the paper is novel, its practical relevance may be limited, as the specific network architectures studied are somewhat specialized in nature.\n\nOverall, the paper makes a notable contribution to the analysis of ReLU-based neural networks and should be of interest to researchers in the field of machine learning and optimization.","model":"chatGPT","source":"peerread","label":1,"id":2249}
{"text":"Summary:\n\nThis paper presents a convergence analysis of different two-layer neural networks with ReLU nonlinearity that are trained using gradient descent. The authors use dynamical system techniques to analyze the weight dynamics of these networks and make some important assumptions such as the input follows the Gaussian distribution. They also verify the empirical practices for random weight initializations. The paper contributes to the literature by proving that global convergence is achievable for a certain symmetry-breaking weight initialization.\n\nStrengths:\n\n1. The paper is well-motivated and emphasizes the importance of the problem it addresses.\n\n2. The paper offers a novel proof of global convergence for two-layer neural networks with ReLU nonlinearity, which breaks the symmetry in weight initialization.\n\n3. The authors have used rigorous mathematical analysis to support their claims.\n\n4. The experiments conducted in the paper are healthy additions to the theoretical intuition.\n\nWeaknesses:\n\n1. The paper assumes that the input follows the Gaussian distribution and also restricts the form of the teacher network. This is a strong assumption and needs to be justified with appropriate validation.\n\n2. Although the authors provide a thorough discussion of the problem, the paper might be difficult to understand for the readers who are not familiar with dynamical systems and nonlinear neural networks.\n\n3. The study is limited to two-layer networks and does not provide any generalization to deep neural networks.\n\n4. The paper does not provide a detailed discussion of the limitations of the proposed approach.\n\nSuggestions:\n\n1. The authors should illustrate the validity of the Gaussian input assumption and discuss any implications it may have on the practical use of their method.\n\n2. The paper should include more detailed explanations and discussions to make it more accessible to the non-specialist reader.\n\n3. Future research should explore the possibility of extending this convergence analysis to deep neural networks.\n\n4. The authors should consider discussing the limitations of the proposed approach in terms of the assumptions, applicability, and scalability.","model":"chatGPT","source":"peerread","label":1,"id":2250}
{"text":"The paper \"Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity\" addresses the problem of analyzing the convergence of two-layered neural networks with ReLU nonlinearity. The paper proposes a dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free networks, and to prove the convergence of the network to the teacher network.\n\nOne of the major strengths of this paper is that it presents a new proof that shows global convergence in a nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, the paper also gives a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with L2 loss.\n\nAnother strength of the paper is that it includes simulation results that verify the theoretical analysis. The paper also provides clear exposition of the analysis steps and formulas, making it easy to follow and understand.\n\nHowever, there are also some weaknesses in the paper. One weakness is that the paper focuses only on the convergence analysis of two-layered neural networks with ReLU nonlinearity, and does not make any comparison or connection to existing related work on this topic. Another weakness is that the paper assumes that the input follows a Gaussian distribution, which may not always be the case in real-world scenarios.\n\nOverall, the paper provides a novel and rigorous analysis of the convergence of two-layered ReLU networks, which is a significant contribution to the field of deep learning. The paper presents clear and well-explained proofs and simulation results that support the analysis. However, it could be strengthened if the authors could make connections to related work and address some of the assumptions made in their analysis.","model":"chatGPT","source":"peerread","label":1,"id":2251}
{"text":"Title: Efficient Softmax Approximation for GPUs\n\nAuthors: [Insert Authors' Names]\n\nJournal\/Conference: [Insert Name of Journal\/Conference]\n\nOverview:\nThe paper presents an efficient softmax approximation algorithm for GPUs. The authors claim that their proposed method reduces computation time while also maintaining accuracy compared to the traditional softmax. The paper provides a detailed analysis of the proposed method, including mathematical derivations, experimental results, and comparisons with state-of-the-art methods.\n\nStrengths:\n- The paper tackles an important problem in the field of deep learning, i.e., improving the computational efficiency of softmax for GPUs.\n- The proposed method is well-designed and supported by rigorous mathematical derivations.\n- The paper presents a thorough experimental evaluation conducted using standard datasets, demonstrating the effectiveness and superiority of the proposed method over the state-of-the-art methods.\n- The paper is well-written, organized, and easy to follow.\n\nWeaknesses:\n- Although the experimental evaluation is thorough, it would be beneficial to conduct more extensive experiments to investigate the generalization ability and robustness of the proposed method.\n- The paper lacks a discussion of the limitations and potential future directions of the proposed method.\n\nConclusion:\nOverall, the paper presents a well-designed and effective softmax approximation algorithm for GPUs, which makes a significant contribution to the field of deep learning. The paper's experimental evaluation provides conclusive evidence of the proposed method's superiority over the state-of-the-art methods. However, the paper could benefit from further experiments to address the potential limitations of the proposed method.","model":"chatGPT","source":"peerread","label":1,"id":2252}
{"text":"The paper \"Efficient Softmax Approximation for GPUs\" addresses the problem of efficiently computing the softmax function, which is a commonly used activation function in neural networks. The softmax function takes a vector of scores as input and maps it to a probability distribution over different classes. The softmax function is computationally complex and requires a large number of computations, which makes it difficult to scale to large dataset sizes. \n\nThe paper proposes a novel approximate algorithm for the softmax function that makes use of graphics processing units (GPUs) to accelerate computation. The algorithm is based on a two-step procedure that first approximates the exponentiation function using a piecewise linear function, and then computes the softmax function using a fast lookup table. The authors show that their algorithm provides an accurate approximation of the original softmax function while reducing the computational cost by a factor of 6.3.\n\nOne of the main strengths of this paper is the successful demonstration of an efficient softmax approximation algorithm that can be implemented on a GPU, which is a key requirement for modern deep learning applications. The experimental results presented in the paper show that the proposed algorithm achieves significant speedup compared to the baseline softmax function while maintaining reasonable accuracy. Additionally, the algorithm is simple and easy to implement, making it a practical solution for real-world problems.\n\nHowever, there are some potential weaknesses of the paper. One possible limitation is the lack of comparison with other existing softmax approximation algorithms. It would be useful to see how the proposed algorithm compares to other state-of-the-art solutions. Additionally, the authors could have provided more detailed analysis of the performance and scalability of the algorithm with respect to different network architectures and datasets.\n\nOverall, the paper \"Efficient Softmax Approximation for GPUs\" presents an innovative and practical solution for the problem of efficiently computing the softmax function. The paper's strengths include accurate and fast results, a simple algorithm, and a demonstration of its potential applications for deep learning. However, to further strengthen the paper, additional experimental analysis should be performed, including comparing the proposed algorithm with other existing solutions.","model":"chatGPT","source":"peerread","label":1,"id":2253}
{"text":"Overall, the paper \u201cEfficient Softmax Approximation for GPUs\u201d presents an interesting and effective approach to training neural network language models over very large vocabularies. The proposed method, called adaptive softmax, appears to be well-motivated in its use of clusters to minimize computational complexity and its exploitation of modern architectures and matrix-matrix vector operations to further reduce computational cost. \n\nThe authors thoroughly evaluate the adaptive softmax approach on two standard benchmarks, EuroParl and One Billion Word, and show that their method is significantly more efficient than standard approximations while still maintaining accuracy comparable to that of the full softmax.\n\nOne concern I have is that the authors do not compare their method to other approaches for handling large vocabularies, such as the hierarchical softmax. It would be interesting to see if the adaptive softmax outperforms other methods in terms of efficiency and accuracy.\n\nFurthermore, the paper lacks a thorough explanation of the computational complexity of the adaptive softmax method compared to the full softmax and standard approximations. The authors briefly mention that their approach reduces computational complexity, but it would be helpful for readers to have a more detailed understanding of this trade-off.\n\nOverall, I think this paper is a valuable contribution to the field of natural language processing and neural network language modeling. The proposed approach appears to be highly efficient and effective, and the results presented in the paper are impressive. However, I would suggest the authors address the concerns mentioned above in future revisions to strengthen the paper further.","model":"chatGPT","source":"peerread","label":1,"id":2254}
{"text":"The paper \"Efficient Softmax Approximation for GPUs\" presents a strategy to address one of the key challenges in training neural networks based on large vocabularies. Specifically, the paper proposes an approach called adaptive softmax that leverages the unbalanced word distribution in language models to form clusters, thereby minimizing the computational complexity required for training. The paper highlights the effectiveness of the approach by showcasing experiments conducted on standard benchmarks, such as EuroParl and One Billion Word, which demonstrate significant gains in efficiency without compromising on the accuracy of the full softmax.\n\nThe paper's strength lies in its clear presentation of the problem and the proposed approach. The authors provided relevant background information and explained the motivation behind the research. They also presented the technical details in a manner that is easy to follow and understand. Additionally, the result section is well-structured, with comprehensive experiments conducted to validate the proposed approach.\n\nHowever, there are a few weaknesses worth noting. Firstly, the authors could have addressed the limitations of the proposed approach more thoroughly. While they mentioned the potential drawbacks, such as the reliability of clustering based on the variation in training sets, the paper does not provide any comprehensive analysis of such limitations. Secondly, the authors could have compared their proposed approach against alternative methods, such as hierarchical softmax or negative sampling. Such a comparison would have provided readers with a broader perspective on the efficiency and effectiveness of the proposed approach.\n\nOverall, the paper is a valuable contribution to the field of NLP, providing a promising direction for improving the efficiency of training neural network-based language models. Therefore, I believe the paper deserves publication given its strengths outweigh the weaknesses.","model":"chatGPT","source":"peerread","label":1,"id":2255}
{"text":"Title: Dataset Augmentation in Feature Space\n\nAbstract:\n\nThe authors suggest a novel approach to dataset augmentation that operates in feature space rather than on raw data. They propose using an unsupervised method to generate synthetic features that would add new variations to the dataset. The authors also empirically demonstrate the effectiveness of their method on three different datasets with three different models. The results indicate that their approach improves the performance of a classifier trained on augmented data.\n\nOverall assessment:\n\nThe paper presents an interesting approach to dataset augmentation by generating synthetic features in feature space. The proposed method is well-motivated and described in detail, including the mathematical formulation of the unsupervised method. The experiments conducted are comprehensive and demonstrate the potential effectiveness of the proposed method. The clarity and coherence of writing is good, and the figures and tables are effective in communicating the results.\n\nHowever, there are several aspects of the paper that could be improved. Firstly, it would be useful to compare the proposed method with existing techniques for dataset augmentation to highlight the advantages of feature space augmentation. Secondly, it would be valuable to provide more insight into the interpretability of the synthetic features generated in feature space. Finally, the authors could provide more discussion on the limitations of their approach and future directions for research in this area.\n\nIn conclusion, the paper presents a promising method for dataset augmentation in feature space, which is supported by empirical results. However, further work is needed to fully understand the potential advantages and limitations of this approach. Overall, I recommend the paper for publication with minor revisions.","model":"chatGPT","source":"peerread","label":1,"id":2256}
{"text":"Overview:\nThe paper titled \"Dataset Augmentation in Feature Space\" proposes a novel method of synthesizing augmented data samples in the feature space for improving the performance of machine learning models. The authors aim to address the limitation of traditional data augmentation techniques that rely on geometric and intensity transformations. The proposed technique is based on generating synthetic samples in the feature space that interpolates between existing samples.\n\nStrengths:\nThe paper presents a significant approach towards addressing an important issue in machine learning- improving the performance of models by augmenting the data samples. The authors have proposed a new method of data augmentation in the feature space that outperforms existing techniques in terms of accuracy and robustness. The authors have provided a comprehensive comparison of their approach with counterpart methods, which significantly enhances the validity of the proposed method.\n\nWeaknesses:\nWhile the paper provides a good explanation of the proposed method, it lacks details on the types of datasets on which this technique is applicable. The authors have not provided any insight into the size of datasets used for experimentation, which could enhance the reproducibility and generalizability of the proposed method. The section on evaluation could be expanded to provide more details of the experiments performed, which could provide better insights into the capabilities of the proposed approach. Additionally, the paper could benefit from further discussion on practical applications of this approach in different machine learning domains.\n\nConclusion:\nThe paper \"Dataset Augmentation in Feature Space\" presents a novel approach of data augmentation that addresses the limitations of traditional data augmentation techniques. While the authors have demonstrated the effectiveness of their proposed approach, few details have been shared that might affect the reproducibility and general applicability of this approach. Ultimately, the paper provides a significant contribution to the field, and it is recommended for publication with minor revisions.","model":"chatGPT","source":"peerread","label":1,"id":2257}
{"text":"Review of the paper \"Dataset Augmentation in Feature Space\"\n\nThe paper presents a domain-agnostic approach to dataset augmentation by applying simple transformations on existing data points in a learned feature space. The authors conducted experiments on static and sequential data using context vectors generated by sequence-to-sequence models to demonstrate the effectiveness of this approach.\n\nThe paper is well-written, easy to follow and presents a novel idea. The authors provided a clear description of the motivation behind their work, and how their approach differs from existing dataset augmentation techniques. The experiments conducted by the authors are thorough and provide strong evidence to support their claims.\n\nHowever, there are a few areas where the paper can be improved. The authors could provide a more detailed explanation of the feature space they used in their experiments, and how it relates to the input space. Additionally, a more detailed analysis of the limitations and potential drawbacks of the proposed method would be helpful. Finally, while the paper focuses on sequence-to-sequence models, it would be interesting to see how the proposed method performs on other types of models.\n\nOverall, the paper makes an interesting contribution to the field of dataset augmentation, and suggests a promising approach that could be applied to a variety of domains. I would recommend this paper for publication, with minor revisions as mentioned above.","model":"chatGPT","source":"peerread","label":1,"id":2258}
{"text":"This paper titled \"Dataset Augmentation in Feature Space\" addresses the problem of dataset augmentation in supervised learning. The authors' proposed approach is to apply simple transformations such as adding noise, interpolating, or extrapolating between existing data points in a learned feature space instead of in input space, making it more effective and domain-agnostic. The paper's main focus is demonstrating the effectiveness of their proposed technique through empirical testing in the context of context vectors generated by sequence-to-sequence models for both static and sequential data.\n\nOne of the paper's strengths is its clear and concise presentation of the proposed methodology, making it easy to understand the authors' approach to dataset augmentation. The authors' choice of focusing on the learned feature space is a crucial strength since it offers an alternative to the current practice of designing domain-specific transformations that may not be easily transferable across different domains. Additionally, the use of context vectors generated by sequence-to-sequence models demonstrates the generality of the proposed approach on a range of different data types.\n\nHowever, some weaknesses can be identified in this paper. First, the authors do not provide any theoretical justification or proof of their approach's effectiveness in a learned feature space. While the empirical experiments show the high-performance results of the proposed approach, a theoretical explanation will be helpful in understanding better the reasons behind its effectiveness. Second, the experiments presented in this paper were limited to only one domain, which constrains the generalizability of the proposed approach across different domains.\n\nIn conclusion, the proposed approach in this paper offers a domain-agnostic alternative to dataset augmentation, with the potential for a more transferable and efficient approach to augmenting training data. While the lack of theoretical justification and experimentation across multiple domains limit its generality, this paper's empirical results are promising and represent a significant contribution to the field of dataset augmentation in supervised learning.","model":"chatGPT","source":"peerread","label":1,"id":2259}
{"text":"The paper \"Online Structure Learning for Sum-Product Networks with Gaussian Leaves\" presents an interesting approach to learning the structure of Sum-Product Networks (SPNs) in an online setting, where new data points are received continuously. The paper focuses on the use of Gaussian leaf nodes in SPNs, which is a commonly used approach in modeling continuous data.\n\nThe authors have provided a thorough introduction to the motivation and background of the work, which helps in situating the contribution of the paper in the larger context of the field. The proposed approach, which is based on a heuristic search algorithm for learning the structure of SPNs, is described in detail and supported by appropriate mathematical formulations.\n\nThe experimental evaluation is detailed and provides evidence to support the effectiveness of the proposed approach. The authors have compared their method with other state-of-the-art methods, and the results show that their approach results in better performance in terms of prediction accuracy and convergence speed.\n\nOne area where the paper could be improved is in providing more detailed information about the implementation of the algorithm. Specifically, it would be helpful to know what libraries or software were used, as well as any specific programming languages.\n\nOverall, the paper presents a well-motivated and well-executed approach to learning the structure of SPNs in an online setting, and it makes a novel contribution to the field. The authors have provided clear explanations and have backed up their claims with solid experimental results. I would recommend this paper for publication in a reputable NLP conference or journal.","model":"chatGPT","source":"peerread","label":1,"id":2260}
{"text":"The paper \"Online Structure Learning for Sum-Product Networks with Gaussian Leaves\" addresses the problem of learning the structure of Sum-Product Networks (SPNs) with Gaussian leaves online. The study aims to develop an algorithm that can learn an SPN's structure incrementally based on streaming data.\n\nOne strength of the paper is that the authors provided a comprehensive overview of the related work on online structure learning for SPNs. This discussion helps to contextualize the paper's contribution and demonstrates the novelty of the proposed algorithm. The experimental results presented in the paper show that the proposed method performs comparably with existing state-of-the-art methods.\n\nHowever, there are some weaknesses within the paper that must be addressed. Firstly, the authors only presented an experimental evaluation of their algorithm on synthetic datasets. The use of real-world datasets and comparison with other state-of-the-art methods on real-world datasets would strengthen the validity of the presented results.\n\nSecondly, the paper lacks a thorough discussion of the algorithm's limitations, potential extensions, and practical implications. A study of the algorithm's sensitivity to hyperparameters and the potential impact of the algorithm on practical applications could provide valuable insights.\n\nOverall, the paper presents a novel and promising algorithm for online structure learning for SPNs with Gaussian leaves. However, further research is needed to validate the algorithm on real-world datasets and to explore its limitations and potential extensions.","model":"chatGPT","source":"peerread","label":1,"id":2261}
{"text":"Review of \"Online Structure Learning for Sum-Product Networks with Gaussian Leaves\"\n\nThe paper under review presents an online structure learning technique for continuous Sum-Product Networks (SPNs) with Gaussian leaves. The authors have introduced a new parameter learning technique that accompanies the structure learning technique. The authors have demonstrated the effectiveness of the proposed approach through experiments. The paper is well written and the contributions are important for the field of probabilistic graphical models.\n\nThe paper starts with a brief overview of the Sum-Product Networks and their properties. The authors then discuss the limitations of specifying a valid SPN by hand due to the conditions required for completeness and decomposability. The authors then present the proposed online structure learning technique and show how it can be used to learn the structure of a continuous SPN with Gaussian leaves. They also describe the accompanying parameter learning technique in detail.\n\nThe experimental evaluation of the proposed approach includes simulation experiments on synthetic datasets, and a comparison with other state-of-the-art approaches. The results show that the proposed online structure learning technique is effective in learning the structure of a continuous SPN and the accompanying parameter learning technique improves the performance of the overall approach.\n\nOverall, the paper presents an important contribution to the field of probabilistic graphical models by introducing an online structure learning technique for continuous SPNs with Gaussian leaves. The experimental evaluation demonstrates the effectiveness of the proposed approach. However, some areas of the paper need improvement. For instance, the authors do not compare the time and space complexity of their proposed approach with the other state-of-the-art approaches. Additionally, the limitations of the proposed approach and future directions for research are not discussed in sufficient detail.\n\nIn summary, the paper is well written, the proposed approach is important for the field of probabilistic graphical models, and the experimental evaluation demonstrates the effectiveness of the proposed approach. However, the authors should address the issues mentioned above in their revision.","model":"chatGPT","source":"peerread","label":1,"id":2262}
{"text":"The paper \"Online Structure Learning for Sum-Product Networks with Gaussian Leaves\" addresses the problem of learning the structure of sum-product networks (SPNs) with Gaussian leaves in an online setting. Sum-product networks have attracted interest in recent years because of their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model whose inference is always tractable. However, specifying a valid sum-product network by hand is difficult, and structure learning techniques are necessary in practice. \n\nThe paper proposes an online structure learning technique for continuous SPNs with Gaussian leaves, which can be applied to continuous data streams. The proposed technique uses streaming data to update the network structure incrementally. The accompanying parameter learning technique is also introduced, which extends the maximum likelihood estimation to the online setting, and is shown to be well-behaved in simulation experiments.\n\nThe paper's strengths lie in its novelty and potential to solve an important problem. The paper's description of an online structure learning technique for continuous SPNs with Gaussian leaves is new and has significant practical implications. The experiments conducted on synthetic and real-world datasets demonstrated encouraging results and illustrate the utility of the approach. \n\nHowever, the paper does have some weaknesses that need to be addressed. The authors do not provide a comparison of their approach with other state-of-the-art methods in the field, which would strengthen the manuscript's contribution. In addition, the paper does not provide a discussion of the possible limitations and challenges of the proposed method, and they do not provide any thorough analysis of the robustness of the proposed method against outliers and noise. \n\nIn summary, the paper \"Online Structure Learning for Sum-Product Networks with Gaussian Leaves\" introduces a significant contribution to the field of sum-product networks. However, to improve the manuscript, the authors should address the weaknesses mentioned above, including a comparison with other methods and a thorough analysis of limitations and robustness.","model":"chatGPT","source":"peerread","label":1,"id":2263}
{"text":"Title: Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks\n\nAuthor: Unknown\n\nIntroduction:\n\nThe paper introduces the concept of Neural Taylor Approximations, which is a way to approximate the behaviour of a neural network using a Taylor expansion. The paper claims that this approximation can help in convergence and exploration in rectifier networks.\n\nMethodology:\n\nThe paper presents a mathematical framework for Neural Taylor Approximations and derives the optimal approximation given a set of previous input-output pairs. The paper then discusses the convergence properties of the algorithm and presents an exploration algorithm based on the approximation.\n\nResults:\n\nThe paper presents experiments on several toy datasets, demonstrating that the Neural Taylor Approximation algorithm can outperform other state-of-the-art algorithms in terms of convergence and exploration. \n\nDiscussion:\n\nThe concept of Neural Taylor Approximations is interesting and potentially useful, as it provides a way to approximate the behaviour of neural networks which can be hard to analyze analytically. The results presented in the paper are promising, but additional experiments on more complex, real-world datasets would be needed to validate the claims made in the paper. \n\nAdditionally, the paper could have presented a more detailed discussion of the limitations and potential drawbacks of the proposed approach. \n\nOverall, this paper provides an interesting approach to improving the convergence and exploration abilities of rectifier networks using Neural Taylor Approximations. Further research is needed to fully assess the potential of this method, but the preliminary results are promising. \n\nRecommendation:\n\nOverall, I recommend this paper for publication, with minor revisions to address the limitations and potential drawbacks of the proposed approach.","model":"chatGPT","source":"peerread","label":1,"id":2264}
{"text":"The paper \"Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks\" addresses the problem of finding a more efficient and effective way to compute second-order derivatives for neural networks using Taylor series expansion. The authors propose a method for computing second-order derivatives using Taylor approximations and demonstrate its effectiveness in the context of rectifier networks.\n\nStrengths:\nOne major strength of this paper is its clear and concise writing style, which makes it easy to follow the proposed method and its applications. Additionally, the paper provides a thorough analysis of the proposed method, exploring both its theoretical convergence properties and its practical implications. The empirical evaluations also suggest that the method is effective in improving accuracy in network optimization tasks, highlighting its potential practical value.\n\nWeaknesses:\nOne potential weakness of the paper is that it relies heavily on mathematical notation and techniques, which may make it challenging for readers without a strong mathematical background to understand. Furthermore, the paper focuses mainly on the application of the proposed method to rectifier networks, limiting the scope of the study.\n\nOverall, \"Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks\" presents a valuable contribution to the field of neural network optimization. However, it might benefit from more detailed explanations of mathematical concepts for the benefit of a wider audience, and further evaluations with other network architectures to prove its effectiveness in more diverse contexts.","model":"chatGPT","source":"peerread","label":1,"id":2265}
{"text":"Peer Review for \"Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks\"\n\nThe paper presents a new approach using the neural Taylor approximation technique to analyze the convergence and exploration properties of modern convolutional neural networks with rectifiers and max-pooling layers. The authors provide the first convergence guarantee for modern convnets, which matches a lower bound for convex nonsmooth functions. They investigate the hypothesis that adaptive optimizers such as RMSProp and Adam can overcome the main difficulty in training rectifier networks, i.e., the shattering of gradients, by exploring the space of activation configurations more thoroughly.\n\nThe paper is technically sound and well-written, with detailed explanations of the proposed methods and results. The authors provide a comprehensive set of experiments on various optimizers, layers, and tasks to support their claims. The convergence guarantee is a significant contribution to the field, as it establishes a theoretical basis for the use of gradient-based optimization algorithms in modern convnets. The investigation of the shattering of gradients and the hypothesis that adaptive optimizers can mitigate this issue is also novel and interesting.\n\nHowever, there are a few issues that need to be addressed. Firstly, the authors did not provide a clear definition of what they mean by \"exploration\" in the context of using adaptive optimizers to explore the space of activation configurations. What specific mechanism or metric are they using to measure exploration? This should be clarified to avoid confusion.\n\nSecondly, the experiments seem to focus mainly on demonstrating the correctness of the convergence guarantee and the ability of adaptive optimizers to find better solutions in rectifier networks. It would be useful to see more practical benefits of the proposed approach, such as faster convergence or improved accuracy compared to existing methods.\n\nLastly, the authors did not provide a comprehensive discussion of the limitations and future directions of the proposed approach. For instance, what are the potential drawbacks of using the Taylor approximation technique in practice, and how can these be addressed? What are the potential applications of the convergence guarantee beyond modern convnets? These issues should be addressed in the paper to provide a more comprehensive view of the proposed approach.\n\nOverall, this is a well-written and innovative paper that provides useful insights into the convergence and exploration properties of modern convolutional neural networks. With some minor revisions to address the issues mentioned above, it could be a valuable contribution to the field of deep learning.","model":"chatGPT","source":"peerread","label":1,"id":2266}
{"text":"This paper aims to address the challenge of modern convolutional networks, which are comprised of rectifiers and max-pooling, which are neither smooth nor convex. The authors provide the first convergence guarantee applicable to modern convolutional networks, which matches a lower bound for convex nonsmooth functions. The neural Taylor approximation is proposed as the key technical tool that is associated with the Taylor loss. \n\nThe strength of this paper lies in its ability to provide the first convergence guarantee applicable to modern convolutional networks, which matches a lower bound for convex nonsmooth functions. The proposal of the neural Taylor approximation is valuable as it is a straightforward application of Taylor expansions to neural networks which demonstrates how the dynamics of neural optimization can accurately capture the behavior of optimizers, layers, and tasks.\n\nOne weakness of the paper is that it does not provide sufficient empirical evidence to support the claims made in the second half of the paper. Although the exploration of the space of activation configurations more thoroughly may be a viable solution for the shattering of gradients, it is critical to supplement this hypothesis with empirical experiments. In addition, the paper could benefit from a larger range of optimizers, layers, and tasks to expand its scope and validity of the results.\n\nOverall, the paper presents information that is highly relevant and valuable to the field of deep learning optimization. The proposal of the neural Taylor approximation provides a clear technical solution for capturing the dynamics of neural optimization, whilst the exploration of the space of activation configurations provides new insights into the training of rectifier nets. The paper could be improved with further empirical evidence to support its claims, so that its findings can be more confidently applied to future research in the field.","model":"chatGPT","source":"peerread","label":1,"id":2267}
{"text":"Title: Multiplicative LSTM for sequence modelling\nAuthors: [Names of authors]\n\nSummary:\nThis paper proposes a novel variant of LSTM, called Multiplicative LSTM, which aims to enhance the modelling of long-term dependencies in sequence data. The proposed model replaces the traditional additive gates of LSTM with a set of multiplicative gates that are more effective in capturing nonlinear relationships between different inputs. The authors report significant improvements in various benchmark datasets by the proposed method over the traditional LSTM.\n\nStrengths:\nThe paper presents an interesting idea to modify the LSTM architecture to improve its capability in modelling long-term dependencies. The authors provide a clear description of the proposed model, and they demonstrate its effectiveness through a thorough evaluation on different datasets. The proposed method is based on a sound and theoretically grounded motivation, as the multiplicative interaction among input features is an essential component of many nonlinear models.\n\nWeaknesses:\nOne possible issue with the paper is that the authors do not provide a detailed comparison with other recently proposed LSTM variants. Although the evaluation results of the proposed model are impressive, it would be helpful to see how it performs against other state-of-the-art LSTM variants that tackle the same problem. Moreover, the authors do not provide any insights into the relative strengths and weaknesses of the proposed method compared to the original LSTM architecture, which makes it challenging to understand the advantages of the proposed approach.\n\nConclusion:\nOverall, the proposed Multiplicative LSTM method is a significant contribution to the field of sequence modelling. The paper presents an interesting and effective approach to improve long-term dependency modelling by modifying the traditional LSTM architecture. However, some improvements can be made in the paper's evaluation and comparison to other LSTM variants. Nonetheless, the proposed method has the potential to be used in various real-world applications involving sequential data, and the paper provides a clear exposition of its operation and effectiveness.","model":"chatGPT","source":"peerread","label":1,"id":2268}
{"text":"The paper \"Multiplicative LSTM for sequence modelling\" addresses the problem of effectively modeling sequential data with long-term dependencies using the Long Short-Term Memory (LSTM) architecture. Specifically, the authors propose a novel variant of LSTM called Multiplicative LSTM that employs multiplicative interactions between the input and forget gates to enhance the gate-dependent information flow and improve the model's ability to capture long-term dependencies.\n\nOne of the notable strengths of this paper is the rigorous experimentation to demonstrate the effectiveness of the proposed Multiplicative LSTM architecture. The authors conducted extensive experiments on several benchmark datasets, including text classification, language modeling, and speech recognition, and compared the performance of various LSTM variants. The results indicate that Multiplicative LSTM outperforms other LSTM variants on most tasks, especially on long-term dependency modeling.\n\nAnother strength of the paper is the clear and concise description of the proposed architecture and the experimental setup. The authors provide a detailed illustration of the multiplicative interactions between the input and forget gates and the mathematical formulation of the model. They also describe the experimental setup, including the datasets, evaluation metrics, and hyperparameters, in a clear and reproducible manner.\n\nHowever, there are also some weaknesses that need to be addressed. Firstly, the paper lacks a comparative analysis of Multiplicative LSTM with state-of-the-art sequence modeling models outside the LSTM family, such as Transformer and WaveNet. Secondly, although the paper extensively evaluates the proposed model's effectiveness on various benchmark datasets, it falls short on analyzing the model's efficiency and scalability, which is crucial for real-world applications.\n\nIn conclusion, the paper \"Multiplicative LSTM for sequence modeling\" proposes a novel variant of the LSTM architecture that enhances the model's ability to capture long-term dependencies in sequential data. The paper is well-written, clearly presented, and thoroughly evaluated. However, it could benefit from a comparative analysis with other state-of-the-art sequence modeling models and a more comprehensive analysis of the model's efficiency and scalability.","model":"chatGPT","source":"peerread","label":1,"id":2269}
{"text":"Peer Review for \u201cMultiplicative LSTM for sequence modelling\u201d\n\nThe paper presents a novel sequence modelling approach called multiplicative LSTM (mLSTM), which combines the long short-term memory (LSTM) and multiplicative recurrent neural network architectures. The paper highlights the advantages of mLSTM over the traditional LSTM models, including the ability to have different recurrent transition functions for each possible input, making it more expressive for autoregressive density estimation. \n\nThe authors present empirical evidence to show that mLSTM outperforms the standard LSTM and its deep variants for a range of character level modelling tasks. Moreover, the paper demonstrates that the improvement of mLSTM increases with the complexity of the task. The experiment shows that mLSTM achieves a test error of 1.19 bits\/character on the last 4 million characters of the Hutter prize dataset when combined with dynamic evaluation. \n\nThe paper is well written, and the technical presentation is clear and concise. The main contributions of the paper include presenting a novel sequence modelling approach, and providing evidence of its improved performance over traditional LSTM models. The experiments are well designed, and the results are convincing, indicating that the mLSTM model has better performance for character level modelling tasks than other existing models.\n\nHowever, there are a few minor issues, including:\n\n1. The paper only demonstrates the efficacy of the proposed approach for character level modelling tasks. It could have been more beneficial to have compared mLSTM with traditional LSTMs for other types of sequences (besides character-level datasets). \n\n2. Although the paper provides empirical validation of the proposed approach for sequence modelling, it would have been beneficial to have a more substantial discussion of the theoretical underpinnings of why mLSTM appears to be more effective than traditional LSTM.\n\nOverall, this paper is well written, and the proposed approach shows the potential for further development in sequence modelling. The experimental evaluation is comprehensive, and the results are convincing. The authors have shown that the proposed mLSTM model has clear advantages over traditional LSTMs on a variety of character level modelling tasks.","model":"chatGPT","source":"peerread","label":1,"id":2270}
{"text":"In this paper, the authors introduce a novel architecture for sequence modelling called the multiplicative LSTM (mLSTM). They argue that mLSTM is more expressive for autoregressive density estimation because it allows for different recurrent transition functions for each possible input. They compare the performance of mLSTM to standard LSTM and its deep variants on a range of character-level modelling tasks and show that mLSTM outperforms these architectures, with improvement increasing as the complexity of the task increases. \n\nStrengths:\n- The paper clearly explains the motivation behind developing the mLSTM architecture and provides a detailed description of the architecture itself.\n- The authors provide empirical evidence to support their claim that mLSTM outperforms other state-of-the-art models on character-level modelling tasks.\n- The paper includes a thorough evaluation of mLSTM on the Hutter prize dataset, which provides a real-world example of the model's performance.\n\nWeaknesses:\n- While the paper provides evidence that mLSTM outperforms standard LSTM and its deep variants, it does not compare mLSTM to other state-of-the-art models for sequence modelling, which would provide a more complete picture of its performance.\n- The paper could benefit from a more detailed explanation of how the mLSTM architecture works, especially for readers who may be unfamiliar with the LSTM or multiplicative RNN architectures.\n- The paper lacks a discussion of potential drawbacks or limitations of the mLSTM architecture, which would provide a more balanced perspective on its usefulness for sequence modelling.\n\nOverall, this paper provides a valuable contribution to the field of sequence modelling with its development of the multiplicative LSTM architecture and its demonstration of its superior performance on character-level modelling tasks. However, further evaluation and discussion of the mLSTM architecture's strengths and weaknesses would enhance the paper's impact and applicability to other areas of research.","model":"chatGPT","source":"peerread","label":1,"id":2271}
{"text":"Title: Neural Functional Programming \n\nAuthors: John Smith, Jane Doe, and Robert Johnson \n\nAbstract: The paper introduces a novel approach to program synthesis called Neural Functional Programming (NFP). NFP combines the power of neural networks with functional programming techniques to generate computer programs from natural language inputs. The authors demonstrate the effectiveness of their approach through extensive experiments on several benchmark datasets.\n\nPeer Review:\n\nThe paper presents an innovative approach to program synthesis that attempts to combine neural networks with functional programming techniques. This is a timely and interesting idea, and the authors provide a convincing argument for the advantages that their approach offers over existing techniques. The paper is well-written and structured, and the authors present their ideas in a clear and concise manner.\n\nThe experimental section is particularly impressive, and the authors demonstrate the superiority of their approach over existing methods for program synthesis. This provides strong evidence for the effectiveness of NFP in solving real-world problems. However, there are some areas that require improvement before the paper can be considered for publication:\n\n1. One major concern is that the dataset used for experimentation is limited to a few benchmark problems. While this provides a good initial indication of the potential of NFP, it would be more convincing if the authors could demonstrate its applicability to a wider range of problems.\n\n2. The authors could also provide more details on the techniques used to train and optimize the neural network. This would help other researchers to replicate their experiments and evaluate the effectiveness of NFP in different use cases.\n\n3. The paper could benefit from a comparison with other recent neural-based program synthesis methods. This would help to place NFP in a broader context and highlight its unique contributions.\n\nOverall, the paper presents a compelling argument for the use of Neural Functional Programming as a promising approach to program synthesis. With some minor improvements, this paper has the potential to make a valuable contribution to the field of NLP and program synthesis.","model":"chatGPT","source":"peerread","label":1,"id":2272}
{"text":"Description of problem\/question addressed:\n\nThe paper \"Neural Functional Programming\" presents a new approach to combining neural networks with symbolic reasoning, specifically for the task of program synthesis. The authors aim to address the problem of synthesizing programs that satisfy a given specification, using a combination of deep learning and program synthesis techniques. The proposed approach involves representing programs in a functional programming language, with input-output examples as the only form of supervision. The authors evaluate the proposed approach on several benchmarks and show its effectiveness in synthesizing correct programs.\n\nStrengths:\n\nOne of the key strengths of this paper is its innovative approach to combining neural networks with symbolic reasoning. The paper presents a clear and detailed explanation of the proposed approach, including the representation of programs as functions and the use of neural networks for program synthesis. The authors provide a thorough evaluation of the proposed approach on several benchmarks, demonstrating its effectiveness in synthesizing correct programs. The paper also includes a comparison with existing program synthesis approaches, highlighting the advantages of the proposed approach.\n\nWeaknesses:\n\nOne weakness of this paper is that it may be difficult for readers without a background in functional programming to fully understand the proposed approach. The paper assumes some level of familiarity with functional programming concepts and does not provide a detailed explanation of them. Another weakness is that the evaluation of the proposed approach is limited to synthetic benchmarks and may not generalize to real-world scenarios. The authors acknowledge this limitation but do not provide a clear path for addressing it in future work.\n\nOverall, the paper \"Neural Functional Programming\" presents an innovative approach to combining neural networks with symbolic reasoning for program synthesis. The proposed approach is well-motivated and well-explained, and the evaluation results are encouraging. However, the paper could benefit from providing more detail on the functional programming concepts used and addressing the limitations of the evaluation in future work.","model":"chatGPT","source":"peerread","label":1,"id":2273}
{"text":"Overall, this paper provides a valuable contribution to the field of end-to-end differentiable programming language, specifically in the area of learning programs from input-output examples. The authors do a good job of outlining the considerations that arise when constructing such a language, exploring the impact of different modeling choices on the success rate of learning algorithms. \n\nThe paper is well-structured, with a clear introduction that provides necessary background and context, followed by a detailed methodology section that outlines the steps taken to develop the range of models explored in the study. The authors clearly explain their experimental setup, allowing for reproducibility of the results. \n\nThe empirical evaluation of the differentiable functional programming language is particularly noteworthy, as it demonstrates that this language allows for the learning of a far greater number of programs than existing baselines. The paper provides evidence of the success of the proposed approach and adds to the growing body of literature on end-to-end differentiable programming language.\n\nOne area of improvement for the paper is that the authors could provide more detailed explanations of certain concepts, particularly for readers who may not be as familiar with programming language research. Additionally, the paper could benefit from a more thorough discussion of potential challenges or limitations that may arise when using this language in practice.\n\nIn conclusion, this paper offers valuable insights into the design and implementation of a differentiable functional programming language for program learning from input-output examples. The experimental results provide evidence of the superiority of this language over existing baselines, and the methodology and experimental setup are detailed and well explained.","model":"chatGPT","source":"peerread","label":1,"id":2274}
{"text":"The paper \"Neural Functional Programming\" addresses the challenge of constructing an end-to-end differentiable programming language that is suitable for learning programs from input-output examples. The authors study a range of modeling choices, including memory allocation schemes, immutable data, type systems, and built-in control-flow structures, and they evaluate the success rate of learning algorithms.\n\nOverall, the paper has several strengths. Firstly, the authors provide a comprehensive overview of the various choices that need to be made while constructing a differentiable programming language. The discussion on memory allocation schemes, immutable data, type systems, and built-in control-flow structures is particularly enlightening. Secondly, the authors build on existing research in programming languages and draw inspiration from it to propose a range of models leading up to a simple differentiable functional programming language. Finally, the empirical evaluation demonstrates that the proposed language outperforms existing baselines in terms of learning a greater number of programs from input-output examples.\n\nHowever, there are also some weaknesses in the paper. Firstly, the authors could have provided more details on the choice of evaluation metrics used to compare the proposed language with existing baselines. Secondly, the empirical evaluation is limited to synthetic datasets and may not generalize well to real-world scenarios. Finally, the paper does not discuss the scalability of the proposed language with respect to the size and complexity of input-output examples.\n\nOverall, \"Neural Functional Programming\" is a well-written and insightful paper that makes a valuable contribution to the field of differentiable programming languages.","model":"chatGPT","source":"peerread","label":1,"id":2275}
{"text":"Peer Review for \"Tuning Recurrent Neural Networks with Reinforcement Learning\"\n\nThis paper presents a novel approach to optimizing the hyperparameters of recurrent neural networks (RNNs) using reinforcement learning (RL) techniques. The authors argue that the traditional method of hyperparameter tuning, such as grid search or random search, can be highly inefficient and require too much computation time. Therefore, they propose a new hyperparameter optimization framework that employs a reward-based RL algorithm, where the goal is to maximize the validation accuracy of the RNN.\n\nThe paper is technically sound, and the methodology is well-motivated and clearly explained. Throughout the paper, the authors provide detailed explanations of their proposed framework and how it can be applied to optimize the RNN hyperparameters. The experiments are thorough and have been conducted on multiple benchmark datasets, showing the effectiveness of the proposed approach in improving the performance of RNNs, especially on complex datasets.\n\nHowever, a few things could be improved in the paper. First, the authors do not provide any comparisons with state-of-the-art hyperparameter optimization methods. Despite the improved results obtained with their framework, it remains unclear whether other methods could perform even better. Second, the reward function used in the RL algorithm deserves more attention. The authors describe the function only briefly, and it is not clear how they have chosen the specific weights for different terms in this function. Moreover, it would be helpful if the authors could provide some insights into the generalization ability of the proposed approach.\n\nOverall, the paper presents a compelling approach to optimize the hyperparameters of RNNs that could potentially improve the performance of many NLP tasks. However, the authors need to address the above-mentioned concerns to make their contribution more convincing.","model":"chatGPT","source":"peerread","label":1,"id":2276}
{"text":"The paper \"Tuning Recurrent Neural Networks with Reinforcement Learning\" presents a novel approach for tuning the hyperparameters of recurrent neural networks (RNNs) using the reinforcement learning (RL) framework. The authors argue that most current methods for hyperparameter tuning do not make optimal use of available computational resources and that the RL approach can improve the overall performance of RNNs.\n\nOne of the main strengths of this paper is its novelty. RL-based hyperparameter tuning for RNNs is an area with limited research, and the authors' approach provides a new and potentially valuable method for addressing this challenge. Additionally, the authors conducted extensive experiments on various datasets and compare their RL approach with several other state-of-the-art methods, demonstrating its effectiveness in terms of speed and accuracy.\n\nOne potential weakness of this paper is that the authors do not provide a detailed explanation of the RL algorithm used for hyperparameter tuning. Given the complexity of the RL framework, readers may have difficulty understanding the specifics of the algorithm without additional explanation. Additionally, the authors did not conduct experiments using larger, more complex datasets, which would have provided a better understanding of the scalability and limitations of their approach.\n\nOverall, the paper \"Tuning Recurrent Neural Networks with Reinforcement Learning\" provides a valuable contribution to the field of hyperparameter tuning for RNNs. The authors' RL approach has the potential to improve the performance of RNNs and the paper's extensive experiments provide evidence for its effectiveness. However, additional explanation of the RL algorithm used and experiments with larger datasets would further strengthen the paper\u2019s contributions.","model":"chatGPT","source":"peerread","label":1,"id":2277}
{"text":"Summary:\n\nThis paper proposes a novel approach to sequence learning by using a pre-trained RNN to supply part of the reward value in an RL model. The paper explores the proposed approach within the context of music generation, where an LSTM is trained on a large corpus of songs to predict the next note in a musical sequence, and is then refined using the proposed method and rules of music theory. The authors show that by combining maximum likelihood (ML) and RL in this way, they can not only produce more pleasing melodies but can also significantly reduce unwanted behaviors and failure modes of the RNN while maintaining information learned from data.\n\nStrengths:\n\n- The paper presents a novel approach to sequence learning that addresses known failure modes of current methods.\n- The use of a pre-trained RNN to supply part of the reward value in an RL model is an interesting and potentially valuable contribution to the field.\n- The paper provides a clear explanation of the proposed approach and the underlying algorithms used.\n- The authors provide a thorough evaluation of the proposed approach on a real-world application (music generation) and demonstrate its effectiveness through both qualitative and quantitative analysis.\n\nWeaknesses:\n\n- The paper could benefit from a more detailed discussion of related work and how the proposed approach relates to and builds upon existing methods.\n- The evaluation could be further strengthened by comparing the proposed method with other state-of-the-art methods for music generation.\n- The paper would benefit from additional explanation and motivation for the specific reward functions chosen and how they relate to the desired properties of the generated melodies.\n\nOverall, this paper presents an interesting and potentially valuable contribution to the field of sequence learning using RL. The proposed approach demonstrates promising results within the context of music generation, and the paper is well-written and clearly presented. However, additional discussion of related work and the specific reward functions used would further strengthen the paper.","model":"chatGPT","source":"peerread","label":1,"id":2278}
{"text":"The paper proposes a new approach to training sequence models by using reinforcement learning (RL) in conjunction with a pre-trained Recurrent Neural Network (RNN) as a way to address known failure modes associated with supervised learning and next-step prediction. Specifically, in the proposed approach, a part of the reward value is supplied by the pre-trained RNN which is then used to optimize for imposed reward functions in the RL model. The proposed approach is demonstrated in the context of music generation, where an LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. The authors show that by combining maximum likelihood (ML) and RL in this way, they can produce more pleasing melodies and significantly reduce unwanted behaviors and failure modes of the RNN, while still maintaining information learned from data.\n\nStrengths:\nThe paper presents a novel approach to address known failure modes of supervised learning and next-step prediction. The addition of RL for sequence prediction is an innovative approach that shows great promise for different applications such as music generation. The approach is based on modifying the reward mechanism by supplementing the reward from the RNN model. The proposed approach is applied to the generation of pleasing melodies that significantly reduce unwanted behaviors in the RNN model while maintaining information learned from data. The paper is well-structured, and the experiments are convincingly presented.\n\nWeaknesses:\nThe paper lacks a comprehensive analysis of the performance of the proposed algorithm compared to state-of-the-art models for music generation. Specifically, it would have been informative to see how the proposed method compares to other recent approaches proposed for music generation. Additionally, the paper does not provide details regarding the exact metrics used to measure the performance of the proposed algorithm.\n\nOverall, the paper presents a novel and interesting approach for sequence prediction by combining supervised and reinforcement learning. The application of the proposed algorithm for music generation is a compelling example. However, more comprehensive analyses and comparative evaluations against state-of-the-art models would have made the paper stronger.","model":"chatGPT","source":"peerread","label":1,"id":2279}
{"text":"Title: Discovering objects and their relations from entangled scene representations\n\nAuthors: [Names of authors]\n\nJournal: [Journal name]\n\nAs an NLP expert, I have reviewed the paper titled \"Discovering objects and their relations from entangled scene representations\". I must say that the paper presents a valuable contribution to the field of computer vision by proposing an effective method to discover objects and their relations from scene representations.\n\nThe paper discusses a novel approach that enables the discovery of the relationship between objects and scenes by simultaneously extracting different aspects of the scene using pre-trained convolutional neural networks (CNNs) and a specially designed generative model. The results obtained in the experiments indicate the feasibility of the proposed method and its high potential for a broad range of applications.\n\nOne of the remarkable strengths of this work is the innovative use of generative models for extracting information from scene representations. The proposed approach not only allows for the detection of individual objects but also discovers implicit relationships between objects in the scene.\n\nDespite the strengths, there are some limitations that need to be addressed in further studies. Particularly, the paper has provided experiments on limited datasets, and the scalability of the proposed approach to datasets with different characteristics remains to be evaluated. Also, the proposed model involves multiple layers of CNNs and requires significant computational resources, and it would be interesting to investigate ways to reduce the model complexity and improve the computational efficiency.\n\nIn conclusion, this paper offers a highly innovative and valuable contribution to the field of computer vision, providing a knowledge discovery approach that can facilitate broader understanding of the relation between objects in images. Therefore, I highly recommend this paper for publication in the respected journal.","model":"chatGPT","source":"peerread","label":1,"id":2280}
{"text":"The paper titled \"Discovering objects and their relations from entangled scene representations\" addresses the problem of representing and extracting object-level relationships in the context of AI vision systems. The authors propose a novel approach based on deep learning techniques for learning object representations and discovering their relationships from visual input.\n\nThe strengths of the paper include the use of a novel and effective approach to the problem by leveraging entangled scene representations, which allows for learning about objects and their relationships jointly. The use of a deep neural network with multiple layers for feature extraction and learning also demonstrates the viability of the proposed approach. The experimental results presented in the paper show that the proposed method achieves state-of-the-art performance for multiple visual reasoning tasks.\n\nHowever, there are some weaknesses in the paper that can be addressed. The authors could further elaborate on the limitations of their approach and discuss the scenarios where it may not work as expected. More detailed discussion about the complexity and computational requirements of the proposed system could also be beneficial.\n\nOverall, the proposed approach appears to be a promising method for discovering object-level relationships in AI vision systems. The authors provide solid experimental evidence to back up their claims, and the novelty of their approach makes the paper a worthwhile contribution to the field.","model":"chatGPT","source":"peerread","label":1,"id":2281}
{"text":"The paper \"Discovering objects and their relations from entangled scene representations\" presents a novel neural network architecture called Relation Networks (RNs). The focus of the paper is on the ability of RNs to learn object relations from scene description data and to extract information from distributed deep representations of scene images provided by a variational autoencoder. The paper also explores the use of RNs in conjunction with differentiable memory mechanisms for implicit relation discovery in one-shot learning tasks.\n\nOverall, the paper presents a well-written and organized study that tackles an important problem in computer vision and artificial intelligence. The authors provide a clear and concise description of the RN architecture and its potential applications. The experiments and results are presented in a logical and structured way, with appropriate quantitative and qualitative evaluations.\n\nOne strength of the paper is its exploration of different datasets to test the RN architecture. The authors employ the Sort-of-CLEVR dataset and the Visual Genome dataset to demonstrate the effectiveness of the RN architecture in multiple settings. Additionally, the authors provide detailed analyses of the learned representations and the input-output behavior of the RN architecture, giving insight into the inner workings of the model.\n\nOne weakness of the paper is the limited discussion on related work in the field of object relation reasoning. The authors briefly introduce some related works, such as the Relational Networks for Visual Reasoning paper. However, more in-depth discussions and comparisons with related works would have been beneficial in understanding the true contributions and novelty of the RN architecture.\n\nIn conclusion, the paper \"Discovering objects and their relations from entangled scene representations\" presents an interesting and promising neural network architecture for object relation reasoning. The authors provide compelling experimental results, demonstrating the effectiveness and potential of the proposed RN architecture. While there are some areas for improvement, overall, this paper makes a valuable contribution to the field of computer vision and artificial intelligence.","model":"chatGPT","source":"peerread","label":1,"id":2282}
{"text":"This paper introduces the concept of relation networks (RNs), a neural network architecture designed to facilitate object-relation reasoning as a means of learning and understanding the underlying structure of structured scenes of objects and their relations. The authors demonstrate how RNs can be used to explore correlated features such as position, function, and shape, which are essential for solving a range of tasks that require object relation reasoning. The paper also shows that RNs can be used to learn object relations from scene description data, which allows them to act as a bottleneck for inducing factorization of objects from entangled scene description inputs. Additionally, the authors demonstrate how RNs can be combined with differentiable memory mechanisms for implicit relation discovery in one-shot learning tasks.\n\nThe strength of this paper lies in the novel concept of RNs and its applicability to solving problems that require object relation reasoning. The authors demonstrate the usefulness of RNs in tackling various tasks, which shows the versatility of the architecture. The paper provides clear and concise explanations of how the RNs work and how they can be employed in various scenarios, making it easy for readers to understand the material.\n\nOne potential weakness of the paper is that it can be difficult to decipher without a strong background in neural networks and machine learning, which may limit its accessibility to readers who are not experts in the field. Furthermore, while the paper provides results that demonstrate the effectiveness of RNs, it does not make a comparative analysis between RNs and other architectures for object relation reasoning, which would provide additional insights into the strengths and limitations of RNs in solving object relation reasoning problems.\n\nOverall, this paper presents a valuable contribution to the field of object relation reasoning, introducing the concept of RNs and demonstrating its effectiveness in solving a range of tasks that require object relation reasoning. The paper is well-structured and provides detailed explanations of how RNs work and can be used in different scenarios, making it a valuable resource for researchers and practitioners in the field of machine learning and data analysis.","model":"chatGPT","source":"peerread","label":1,"id":2283}
{"text":"Title: Perception Updating Networks: On architectural constraints for interpretable video generative models\n\nThe paper presents a novel way to design interpretable video generative models using Perception Updating Networks (punets) architecture. The study aims to overcome the limitations of existing generative models and improve interpretability by introducing architectural constraints that promote a structure similar to human perception.\n\nThe authors provide a comprehensive literature review of existing generative models and demonstrate the limitations of these models in terms of interpretability. They provide a clear and concise explanation of the proposed punishment architecture, and the study results show that it effectively improves the interpretability of video generative models.\n\nThe authors have shown that the complexity of video data requires more advanced and sophisticated models, and their approach can handle the temporal dependencies in video data. The study tests the proposed architecture on several datasets and shows its strengths and limitations.\n\nThe paper is well written, and the concept is novel and interesting. The authors provide good illustrations and explanations that make it easy for readers to understand the proposed architecture. However, there are some limitations to the study. Firstly, the study does not compare the proposed architecture with other state-of-the-art models in terms of quantitative results. Secondly, the authors do not provide any insights into the practical implications of using their architecture in real-world applications.\n\nIn summary, the paper presents a promising approach to designing interpretable video generative models, and the concept seems to be innovative and novel. However, further studies are needed to investigate the practical applications and compare the proposed architecture with other state-of-the-art models. I recommend this paper for publication with minor revisions.","model":"chatGPT","source":"peerread","label":1,"id":2284}
{"text":"The paper \"Perception Updating Networks: On architectural constraints for interpretable video generative models\" deals with the problem of generating interpretable video models through the application of architectural constraints on perception updating networks (PUNs). The authors address the issue of recent approaches to video generation models that often lack interpretability, which can hinder their ability to perform in real-world applications where explainability is critical.\n\nStrengths:\n1. The authors propose a unique approach to video generation that emphasizes interpretability, which is critical when working with sensitive applications such as in healthcare or criminal justice.\n2. The paper demonstrates how architectural constraints on perception updating networks can help achieve interpretability in video generative models.\n3. The authors provide a clear description of the proposed approach and demonstrate its effectiveness through various experiments and evaluations.\n\nWeaknesses:\n1. The paper could benefit from a more in-depth discussion on the limitations and trade-offs of the proposed approach, particularly in terms of its computational complexity and limitations in modeling complex temporal relationships.\n2. The paper lacks a comparison with other state-of-the-art interpretable video generative models, which would help to evaluate the significance of the proposed approach.\n3. It would have been better if the paper had provided more examples to showcase how their proposed approach can be implemented for real-world scenarios.\n\nOverall, this paper provides a valuable contribution to the field of interpretable video generative models, with its focus on interpretability through perception update networks (PUNs). The authors demonstrate the effectiveness of their proposed approach through various experiments and evaluations. However, additional discussion and comparative analysis could help build the significance of their findings.","model":"chatGPT","source":"peerread","label":1,"id":2285}
{"text":"The paper titled \"Perception Updating Networks: On architectural constraints for interpretable video generative models\" presents an interesting approach towards modeling frames in videos using computer graphics inspired principles. The authors propose a neural network architecture and statistical framework that explicitly represents \"sprites\" or percepts inferred from maximum likelihood of the scene, and infers the movement of these \"sprites\" independently of their content. \n\nThe paper presents a detailed analysis of the proposed approach and discusses the various architectural constraints that are imposed on the network. In particular, the authors impose constraints that force the resulting architecture to behave as a recurrent what-where prediction network. The authors further evaluate the proposed approach on various video datasets and demonstrate that it achieves state-of-the-art performance on a number of benchmarks.\n\nOverall, the paper is well-written and presents a novel approach towards modeling frames in videos. The authors present a thorough analysis of the proposed approach and its underlying architectural constraints. However, there are a few areas where the paper could be improved. \n\nFirst, the authors could provide more details on the limitations and potential drawbacks of the proposed approach. Additionally, while the paper presents results on various video datasets, it would have been useful to see a more detailed comparison with other state-of-the-art methods. Finally, the authors could provide more insights and interpretations based on the results obtained.\n\nOverall, I believe the proposed approach is promising and has the potential to lead to interesting future research directions in the field of video generative models. Therefore, I would recommend this paper for publication after addressing the aforementioned concerns.","model":"chatGPT","source":"peerread","label":1,"id":2286}
{"text":"The paper \"Perception Updating Networks: On Architectural Constraints for Interpretable Video Generative Models\" addresses the problem of creating interpretable video generative models through the use of a neural network architecture and statistical framework inspired by computer graphics pipelines. The proposed architecture uses \"sprites\" or percepts inferred from the maximum likelihood of the scene, and is able to infer their movement independently of their contents. The paper imposes architectural constraints that enforce the resulting architecture to function as a recurrent what-where prediction network.\n\nOne of the main strengths of the paper is its exploration of a novel approach to creating video generative models that are more interpretable. The use of \"sprites\" as percepts provides a structured way of representing scene elements that can be easily understood by humans, and the ability to infer their movement independently of their contents allows for more nuanced and accurate representations of video.\n\nAnother strength is the paper's clear explanation of the proposed architecture and the principles behind it. The authors are able to effectively communicate the technical details of the model, making it accessible to researchers in the field of NLP.\n\nHowever, one potential weakness of the paper is that it lacks a thorough empirical evaluation of the proposed model. While the authors describe several experiments conducted to show the effectiveness of the architecture, the evaluation is limited in scope and does not fully explore the capabilities and limitations of the model.\n\nAnother potential weakness is that the architecture is limited to modeling videos with relatively simple animations, and may not be suitable for more complex scenes. The paper acknowledges this limitation, but it may be an area of concern for researchers considering implementing the proposed model in their own work.\n\nOverall, \"Perception Updating Networks: On Architectural Constraints for Interpretable Video Generative Models\" is a well-written and innovative paper that provides valuable insights into creating interpretable video generative models with neural networks. However, additional empirical evaluation and exploration of the model's limitations would further strengthen the work.","model":"chatGPT","source":"peerread","label":1,"id":2287}
{"text":"Overall assessment:\nThe paper \"Neural Data Filter for Bootstrapping Stochastic Gradient Descent\" presents an interesting approach for improving the training process of deep neural networks by filtering out data samples that produce noisy gradients during training. The authors propose a novel algorithm called Neural Data Filter (NDF) that utilizes the statistical properties of the gradients to identify and remove data samples that could lead to suboptimal training. The experiments presented in the paper demonstrate the efficacy of the proposed approach in improving the convergence rate and reducing the generalization error of various deep learning models.\n\nStrengths:\nThe proposed NDF algorithm is based on a solid statistical framework and provides a principled method for identifying and removing noisy data samples during training. The authors provide a thorough analysis of the algorithm's performance on various datasets and deep learning models, demonstrating its effectiveness in improving the convergence rate and reducing the generalization error. The paper is well-organized and clearly presents the motivation, methodology, and experimental results.\n\nWeaknesses:\nAlthough the authors provide some discussion on the computational complexity of the NDF algorithm, it would be helpful to include more detailed analysis and discussion of its scalability and potential limitations when dealing with larger datasets or more complex models. Additionally, the authors should provide a more in-depth analysis of the factors that contribute to the effectiveness of the NDF algorithm, such as the choice of hyperparameters and how they affect the performance of the algorithm in different settings. Finally, it would also be beneficial to compare the proposed approach with similar techniques that aim to address the problem of noisy gradients during training.\n\nRecommendations:\nOverall, the paper presents a novel and promising approach for improving the training process of deep neural networks. However, to make the paper more impactful, the authors should consider addressing the above-mentioned weaknesses by providing a more detailed analysis of the algorithm's computational complexity, scalability, and potential limitations, analyzing the sensitivity of the algorithm to hyperparameters, and comparing the approach with similar techniques in the literature. Additionally, the authors could also consider discussing potential future directions and applications of the proposed approach that would help establish its relevance to the broader deep learning community.","model":"chatGPT","source":"peerread","label":1,"id":2288}
{"text":"The paper \"Neural Data Filter for Bootstrapping Stochastic Gradient Descent\" addresses the problem of efficiently training neural networks on large datasets using Stochastic Gradient Descent (SGD). Specifically, the authors propose a novel technique called Neural Data Filter that aims to reduce the computational cost of SGD by filtering out irrelevant data points during the training process.\n\nThe authors present a clear and well-structured paper, which is easy to follow. The idea behind the Neural Data Filter is well-motivated, and the experimental results demonstrate that the proposed method can lead to significant improvements in the speed of training deep neural networks on large-scale datasets. The authors conduct thorough experiments on several benchmark datasets, and the results are presented in detail with clear visualizations, making it easy to understand the findings.\n\nHowever, the paper has some weaknesses that need to be addressed. First, the authors did not provide enough details on some key components of the proposed method, such as the hyperparameters used in the experimental evaluation. Additionally, the authors could benefit from a more thorough discussion of the limitations and potential drawbacks of their proposed method. Finally, the authors should consider comparing their proposed method with other state-of-the-art methods for improving the efficiency of SGD.\n\nIn conclusion, this paper presents a promising technique for improving the efficiency of SGD by filtering out irrelevant data. The proposed method is well-motivated, and the experimental results are promising. However, there are some minor weaknesses that need to be addressed, and further comparisons with state-of-the-art methods are suggested.","model":"chatGPT","source":"peerread","label":1,"id":2289}
{"text":"This paper proposes a method for improving the efficiency of mini-batch based Stochastic Gradient Descent (SGD) in neural network training by designing a framework that automatically selects and adapts training data based on a deep reinforcement network, which guides the filtering of data in sequentially arrived mini-batches. The experiments show that the Neural Data Filter (NDF) bootstraps the SGD training of various neural network models for a variety of tasks while achieving comparable accuracy as vanilla SGD trained on the full dataset.\n\nThe proposed method is innovative and addresses a frequently encountered issue in neural network training, namely, how to effectively filter noisy and irrelevant data during SGD training. The suggested approach of using a reinforcement network to monitor and supervise the training process is well motivated, and the experiments convincingly demonstrate that NDF achieves better training efficiency while maintaining accuracy. The results indicate that NDF is a promising direction for enhancing SGD training in deep neural networks.\n\nHowever, there are also several possible points for improvement. First, the authors could have provided more details about the experimental settings, such as the network architectures, mini-batch size, hyperparameters, and initialization methods. This would help readers better understand the robustness and generalizability of the proposed method. Second, the choice of benchmark tasks is limited, and it would be interesting to see how NDF performs on more challenging and diversified datasets in the future work. Finally, it would be beneficial if the authors could provide a deeper analysis of the inner workings of NDF, such as the policies learned by the reinforcement network, and how they relate to the characteristics of the training data and network structure.\n\nIn summary, this paper presents a novel approach for enhancing the training efficiency of SGD in deep neural networks by using a neural data filter, which is monitored by a deep reinforcement network. The experiments indicate that NDF achieves comparable accuracy while using up to 90% fewer samples than vanilla SGD. Despite some minor drawbacks, the proposed method shows great potential for boosting the performance and scalability of neural network training. Therefore, I recommend this paper for acceptance.","model":"chatGPT","source":"peerread","label":1,"id":2290}
{"text":"The paper \u201cNeural Data Filter for Bootstrapping Stochastic Gradient Descent\u201d presents a general framework called Neural Data Filter (NDF) for automatically and adaptively selecting training data for Stochastic Gradient Descent (SGD). The paper takes the approach of using a deep reinforcement learning network to supervise and monitor the entire training process of an original neural network. The aim of this is to optimize the selection of mini-batches for training, with a view to minimizing the amount of data required for convergence to a reasonable accuracy level. The authors demonstrate the efficacy of the NDF algorithm across a range of neural network models, including Multi Layer Perception Network and Recurrent Neural Network trained on image classification and text understanding tasks.\n\nThe paper has a number of strengths. First and foremost, the authors\u2019 approach is creative and innovative, and will be of interest to both experts in the field and researchers looking to develop new methodologies for training deep neural networks. The experimental results also demonstrate how the NDF effectively reduces the amount of training data required, while achieving comparable accuracy to that of standard SGD trained on the full dataset. This is especially important for applications that involve large amounts of data and computational resources, allowing for faster convergence and more efficient training.\n\nHowever, there are also some limitations to the paper. The authors could have provided a more detailed mathematical analysis of the proposed algorithm to better understand its convergence properties and optimization guarantees. Additionally, while the experimental results were quite promising, these experiments were primarily conducted on relatively simple datasets, and it remains to be seen whether NDF will be able to achieve similar results on more complex real-world datasets.\n\nOverall, this paper makes an important contribution to the field of neural network training and optimization, and the proposed Neural Data Filter algorithm is a significant step forward in improving the efficiency and effectiveness of Stochastic Gradient Descent. The limitations mentioned above notwithstanding, the paper should be of interest to researchers and practitioners involved in deep learning and neural network optimization.","model":"chatGPT","source":"peerread","label":1,"id":2291}
{"text":"Title: Learning in Implicit Generative Models\n\nAuthors: John Doe and Jane Smith\n\nSummary: The paper proposes a novel approach to learning in implicit generative models. The approach is based on an amortized inference scheme and a variational bound optimization technique. The authors demonstrate the effectiveness of their approach on several benchmark datasets using both binary and continuous data.\n\nOverall Assessment: \n\nThe paper presents a valuable contribution to the field of implicit generative models. The proposed approach is innovative and helps to address some of the issues associated with current techniques. The use of amortized inference as well as variational bound optimization is a novel and practical approach, and the authors provide a comprehensive evaluation of their model on diverse datasets.\n\nOne of the strengths of the paper is the discussion of the potential limitations of the proposed approach. The authors provide a thorough analysis of the challenges and potential drawbacks of their model, which is important for guiding future research in the field.\n\nThe paper\u2019s use of practical experiments and illustrative examples was a great asset in demonstrating the effectiveness of the proposed approach. The authors\u2019 capacity to show the performance of their approach on different datasets and various data types, both binary and continuous, demonstrates the robustness of the technique and its utility across multiple domains.\n\nOne area where the paper could have been improved is in the discussion of related work. While the authors provide some context by introducing a few papers on generative models, they could have included a more comprehensive literature review of the field. One suggestion could be to provide a detailed comparison of their proposed approach with other techniques that have been proposed in recent years for implicit generative models.\n\nIn conclusion, the paper provides a useful contribution to the field of implicit generative models. It offers an innovative solution to some of the limitations that have been found in current methods. The manuscript has been well written and presented; therefore, I recommend that it should be accepted for publication without any major revisions.","model":"chatGPT","source":"peerread","label":1,"id":2292}
{"text":"This paper, Learning in Implicit Generative Models, proposes a new approach to unsupervised learning via implicit generative models. The authors specifically address the challenge of learning generative models that can effectively capture the underlying distribution of complex data without compromising privacy.\n\nOne strength of this paper is the comprehensive and well-structured presentation of the proposed approach. The authors provide a clear explanation of the various components involved in building an implicit generative model and provide an insightful discussion on the different tradeoffs involved.\n\nAnother strength of the paper is the analysis of the proposed approach's performance on different datasets. The authors demonstrate that their approach outperforms several well-known and related models in terms of both the overall predictive accuracy and the level of privacy preserving.\n\nOne weakness of this paper is that it could have provided more in-depth analysis of the different parameters involved in training the model. For instance, the paper only scratches the surface of the hyperparameters used in the experiments and the potential effects of varying them.\n\nAnother potential weakness of this paper is that the model might not be able to handle specific types of data distributions. This has not been adequately addressed in the paper, and future research could explore how the model could be tweaked to accommodate such datasets.\n\nOverall, this paper makes a significant contribution to the field of unsupervised learning and demonstrates the potential effectiveness of implicit generative models. The strengths of the paper outweigh the weaknesses, and it provides a good starting point for research in this area.","model":"chatGPT","source":"peerread","label":1,"id":2293}
{"text":"Summary:\nThe paper under review provides an extensive review of Generative Adversarial Networks (GANs) by framing them within the wider landscape of algorithms for learning in implicit generative models. The authors discuss approaches for density ratio estimation include divergence minimisation, moment matching, and classifiers, with a focus on hypothesis testing as a principle for learning in implicit generative models. A synthesis of these approaches is also provided along with opportunities for cross-pollination and future exploration.\n\nStrengths:\nThe authors have provided a comprehensive and clear overview of GANs and their relationship to other algorithms for learning in implicit generative models. The review covers many technical details, while still being accessible for a general audience.\n\nWeaknesses:\nWhile this paper has provided an extensive overview of GANs and the broader field of implicit generative models, it does not provide any novel contributions. Also, although the paper has touched upon different approaches to density ratio estimation, some of the technical details of these methods have been omitted.\n\nOverall, this paper serves as an excellent resource for researchers and students interested in GANs and their relationship to other algorithms for learning in implicit generative models. The review is written in such a way that it could serve as a starting point for someone with no prior knowledge of GANs, while still providing relevant and detailed information for experts in the field.","model":"chatGPT","source":"peerread","label":1,"id":2294}
{"text":"The paper \"Learning in Implicit Generative Models\" attempts to provide a comprehensive analysis of generative adversarial networks (GANs) and their relationship with other algorithms used for learning in implicit generative models, with a focus on density ratio estimation. The paper is well-written, and the authors provide a clear explanation of their ideas and methodology.\n\nThe strength of the paper lies in its ability to provide a thorough understanding of GANs, the objectives used by them, and the density ratio estimation problem. The authors have explored different approaches for density ratio estimation and have synthesized them to provide a theoretical understanding that can guide future research. Moreover, the authors' focus on the testing viewpoint as a principle for learning in implicit generative models, and their use of hypothesis testing, is a novel perspective that can contribute to further research in this area.\n\nHowever, there are some weaknesses in the paper that the authors need to address. The authors have discussed various objectives used by GANs but have not provided a clear comparison of their effectiveness. The paper lacks empirical validation to support their theoretical claims. The authors could have provided more clarity when discussing the challenges faced in density ratio estimation and could have explained how their proposed solution addresses these challenges.\n\nIn conclusion, the paper provides a comprehensive understanding of GANs and density ratio estimation, as well as a novel perspective on testing as a principle for learning in implicit generative models. However, the authors need to address some weaknesses in their arguments and empirical validation to strengthen their claims. Overall, this paper can significantly contribute to further research in this area.","model":"chatGPT","source":"peerread","label":1,"id":2295}
{"text":"Title: Adaptive Feature Abstraction for Translating Video to Language\n\nAuthors: John Doe, Jane Smith\n\nSummary:\nThe authors propose a new method for translating video content to natural language descriptions, using an adaptive feature abstraction approach that learns to selectively integrate information from different video frames depending on their semantic relevance to the desired sentence. The proposed model is evaluated on two benchmark datasets and achieves state-of-the-art results.\n\nOverall Assessment:\nThe paper presents a novel approach for generating natural language descriptions from video content, which addresses some of the limitations of existing methods, particularly in terms of their ability to capture complex and dynamic visual scenes. The proposed adaptive feature abstraction method is well motivated and designed, and the experimental evaluation demonstrates significant improvements over baselines on both datasets, which confirms the effectiveness of the approach.\n\nSpecific comments:\n- The introduction provides a good overview of the problem and the motivation for the proposed approach, but it could be strengthened by more clearly highlighting the limitations of existing methods and how the proposed method addresses them.\n- The related work section is somewhat brief and could be expanded to provide a more comprehensive overview of the literature on video-to-language translation, and how the proposed method relates to it.\n- The technical description of the proposed method is clear and well-structured, but it could benefit from more concrete examples to illustrate how the adaptive feature abstraction works in practice, especially for readers who are not familiar with the specific techniques used (e.g. LSTM).\n- The experimental evaluation is well designed and the results are impressive. However, it would be useful to have more detailed analysis of the results, such as error analysis or ablation studies, to better understand the strengths and weaknesses of the proposed method.\n- The paper is generally well written and organized, but there are some minor typos and grammatical errors that could be corrected.\n\nOverall, I recommend this paper for publication, as it presents a significant contribution to the field of video-to-language translation and provides a promising direction for future research.","model":"chatGPT","source":"peerread","label":1,"id":2296}
{"text":"The paper \"Adaptive Feature Abstraction for Translating Video to Language\" addresses the problem of translating videos to natural language descriptions. Specifically, the authors propose a new approach that uses an adaptive feature abstraction mechanism to learn meaningful features from source video and language data. The objective of this approach is to generate a text description that is more accurate and semantically meaningful than existing video description methods.\n\nOne of the strengths of this paper is its clear and concise writing style, which effectively communicates the authors' research objectives and contributions to the field. Additionally, the authors provide a clear explanation of the technical details of their proposed method, which will enable other researchers to replicate and extend their work.\n\nHowever, there are also some weaknesses to this paper. Firstly, the authors do not provide a comprehensive comparison with existing state-of-the-art models for video description. Although they do compare their approach to several baselines, it would be beneficial to see a more thorough comparison to better understand the relative strengths and weaknesses of the proposed method. Additionally, while the authors do perform experiments on multiple datasets, the sample size is relatively small and limited only to specific domains such as sports and cooking.\n\nOverall, this paper presents an interesting approach to the problem of video to language translation, and its adaptive feature abstraction mechanism could potentially lead to more accurate and semantically meaningful video descriptions. However, more experiments and comparisons with state-of-the-art methods are needed to thoroughly evaluate its performance.","model":"chatGPT","source":"peerread","label":1,"id":2297}
{"text":"The paper titled \"Adaptive Feature Abstraction for Translating Video to Language\" explores a novel approach for generating adaptive spatiotemporal representations of videos for the task of video captioning. The authors propose using attention mechanisms with spatiotemporal alignment for adaptively and sequentially focusing on different layers of CNN features and local spatiotemporal regions of the feature maps at each layer. The experiments were conducted on three benchmark datasets YouTube2Text, M-VAD, and MSR-VTT, and the results are presented using both qualitative and quantitative analysis.\n\nThe paper is very well written, and the research work is well-motivated, clearly articulated, and well-executed. The authors have presented an excellent literature review that builds up to the proposal of the new approach for adaptive feature abstraction. The proposed approach is original and very promising. The authors provide evidence of its effectiveness through experiments conducted on three benchmarks datasets, which demonstrate that their approach outperforms state-of-the-art methods in video captioning tasks.\n\nThe paper is well-structured and easy to follow. The authors provide relevant practical details in describing their methodology and present the results in a clear and comprehensive manner. However, some additional details could be provided about the implementation of the model, particularly in the choice of hyperparameters and the optimization algorithm.\n\nIn summary, I recommend this paper for acceptance. The authors have presented a novel and very promising approach for adaptive feature abstraction that can significantly improve video captioning tasks. The experiments conducted on multiple benchmark datasets provide strong evidence of the effectiveness of the proposed approach. The paper is well-written, and the research work is well-executed.","model":"chatGPT","source":"peerread","label":1,"id":2298}
{"text":"The paper titled \"Adaptive Feature Abstraction for Translating Video to Language\" addresses the problem of generating rich and context-dependent video captions. The authors point out that previous models for video captioning rely on a specific layer of a Convolutional Neural Network (CNN) for video representations, which limits their ability to capture the varying spatiotemporal semantics in videos. \n\nTo address this problem, the authors propose an attention-based approach to creating adaptive spatiotemporal representations of videos for the task of video captioning. The approach employs novel attention mechanisms with spatiotemporal alignment, which adaptively and sequentially focus on different layers of CNN features (levels of feature abstraction) and local spatiotemporal regions of the feature maps at each layer. The authors evaluate their approach on three benchmark datasets - YouTube2Text, M-VAD, and MSR-VTT.\n\nOne of the strengths of the paper is its meticulous description of the proposed approach. The authors provide a detailed overview of the attention mechanisms used in the proposed model, which makes it easy to understand their approach. The paper includes a comprehensive experimental evaluation of the proposed approach on three different benchmark datasets. The authors visualize the results and show how the model works, which is a significant strength of the paper.\n\nHowever, one weakness of the paper is that it does not provide a comparison of the proposed approach with the state-of-the-art methods in video captioning. Although the authors compare their approach with some existing methods, a comparison with the state-of-the-art would provide a clearer understanding of the contributions of the proposed approach.\n\nIn summary, the paper proposes an attention-based approach that leverages spatiotemporal alignment to generate adaptive spatiotemporal representations of videos for video captioning tasks. The experiments demonstrate that the proposed approach is effective in generating rich and context-dependent video captions. The paper's strength lies in its detailed description of the proposed approach and a comprehensive experimental evaluation that demonstrates its effectiveness. However, the paper could be strengthened by comparing the proposed approach with the state-of-the-art in video captioning.","model":"chatGPT","source":"peerread","label":1,"id":2299}
{"text":"Paper Review for \"On Robust Concepts and Small Neural Nets\":\n\nThe paper \"On Robust Concepts and Small Neural Nets\" proposes a novel approach for achieving robustness in neural networks using small-sized networks with concept-based representations. The authors argue that while large neural networks have been successful in a variety of applications, they suffer from several shortcomings like poor interpretability, overfitting, and susceptibility to adversarial attacks.\n\nThe authors propose an alternative approach of using small-sized neural networks that learn concept-based representations. Concepts represent abstract ideas or categories that allow neural networks to generalize to new examples, interpret results and provide insights into how the neural network processes and learns information. The authors demonstrate that using concepts-based representations not only improves the robustness of the neural network but also provides explainability and interpretability.\n\nThe paper presents a thorough experimental evaluation of the proposed approach on several image classification datasets. The results show that the proposed approach outperforms state-of-the-art methods in terms of both accuracy and robustness. The experiments also show that the small-sized neural networks with concept-based representations can achieve better results than the large, complex neural networks, while remaining computationally efficient.\n\nThe paper is well-written and clearly presents the proposed approach with sufficient theoretical underpinnings. The authors also provide an extensive literature review that highlights the shortcomings of the current state-of-the-art methods, which further strengthens the novelty of their approach.\n\nOverall, this paper presents an important contribution to the field of neural networks by proposing a novel approach to improve the robustness of neural networks while maintaining interpretability and efficiency. The results are promising, and the authors provide a clear roadmap for future directions to further improve the proposed approach. Therefore, I highly recommend accepting this paper for publication.","model":"chatGPT","source":"peerread","label":1,"id":2300}
{"text":"The paper \"On Robust Concepts and Small Neural Nets\" aims to address the problem of developing a compact neural network architecture that retains robustness properties. Specifically, the paper proposes a novel approach of training small neural networks to learn robust concepts that generalize beyond the training data. The authors aim to demonstrate the efficacy of the proposed approach on challenging image classification tasks.\n\nOne of the main strengths of the paper is the novelty of the proposed approach. The idea of training small neural networks to learn robust concepts is a promising avenue for reducing the complexity of neural network architectures while maintaining their effectiveness. The authors provide a thorough theoretical framework for their approach and present compelling experimental results on four benchmark image classification datasets.\n\nAnother strength of the paper is the authors' clear writing style, which makes their technical ideas accessible to a wide range of readers. The organization of the paper is logical and easy to follow, with clear explanations of key concepts and algorithms.\n\nOne potential weakness of the paper is the limited scope of the experimental evaluation. While the authors demonstrate the effectiveness of their approach on four benchmark image classification datasets, it would have been beneficial to evaluate its performance on a wider variety of tasks to further demonstrate the generality of their proposed method.\n\nAnother potential weakness is the lack of comparison to other state-of-the-art approaches that aim to achieve similar goals, such as adversarial training or robust optimization. Providing a direct comparison with these methods would have allowed for a more thorough evaluation of the proposed approach.\n\nOverall, the paper \"On Robust Concepts and Small Neural Nets\" presents a novel approach to training compact neural networks for image classification tasks. The paper is well-written and the authors provide convincing experimental results. However, further evaluation of the proposed approach on a wider variety of tasks and a comparison with other state-of-the-art approaches would enhance the robustness of the paper's claims.","model":"chatGPT","source":"peerread","label":1,"id":2301}
{"text":"Review of \"On Robust Concepts and Small Neural Nets\"\n\nThe paper presents an efficient analog of the universal approximation theorem for noise-stable boolean functions on the boolean hypercube. The authors prove that any noise-stable boolean function on n boolean-valued input variables can be well-approximated by a two-layer linear threshold circuit with a small number of hidden-layer nodes and small weights, which depend only on the noise-stability and approximation parameters and are independent of n. They also provide a polynomial time learning algorithm that outputs a small two-layer linear threshold circuit that approximates such a given function. Furthermore, the paper shows weaker generalizations of this to noise-stable polynomial threshold functions and noise-stable boolean functions in general.\n\nOverall, the paper is well-written and the results are interesting and significant. The paper is technically sound and the proofs are rigorous. The methodology used in the paper is appropriate and the experiments demonstrate the effectiveness of the proposed algorithm.\n\nThe authors could improve the paper by providing a more detailed explanation of the intuition behind the results. The motivation for the paper is not entirely clear, and the authors could explain why small neural networks are important for robust concepts. Additionally, some of the technical terms used in the paper may not be familiar to all readers, and the authors could provide more explanations for these terms.\n\nIn conclusion, the paper presents valuable contributions to the field of neural networks and approximation theory. The proposed algorithm provides an efficient alternative to the universal approximation theorem for noise-stable boolean functions. The paper could be improved by providing more context and explanations for technical terms.","model":"chatGPT","source":"peerread","label":1,"id":2302}
{"text":"The paper, \"On Robust Concepts and Small Neural Nets\" addresses the problem of finding efficient ways to approximate noise-stable boolean functions on the n-dimensional boolean hypercube using small neural networks. The authors propose an analog of the universal approximation theorem that guarantees well-approximation of any noise-stable boolean function by a two-layer linear threshold circuit with small weights and few hidden-layer nodes, independent of n.\n\nStrengths:\nThe paper presents a clear and concise problem statement, followed by an efficient solution. The proof of the main theorem is rigorous and well-structured. The proposed learning algorithm for finding a small two-layer linear threshold circuit that approximates the given function is polynomial-time and effective. The paper also provides weaker generalizations of the main theorem for noise-stable polynomial threshold functions and general noise-stable boolean functions.\n\nWeaknesses:\nOne potential weakness of the paper is that the results are restricted to boolean hypercube inputs, which may limit the applicability of the proposed technique. The paper could benefit from a more in-depth discussion on the implications and potential applications of their approach beyond the boolean hypercube. Additionally, while the authors provide a polynomial-time algorithm for finding a small linear threshold circuit, they do not provide any experimental evaluation of its performance or efficiency.\n\nOverall, the paper presents a promising solution to the problem of efficiently approximating noise-stable boolean functions using small neural networks, and its theoretical contributions are significant. The authors could potentially extend their work to more general cases and provide further empirical evaluations of their approach in future work.","model":"chatGPT","source":"peerread","label":1,"id":2303}
{"text":"Title of the Paper: Learning Continuous Semantic Representations of Symbolic Expressions\n\nAbstract:\nThe paper proposes a method for learning continuous semantic representations of symbolic expressions. The approach combines the use of neural networks and symbolic mathematics, enabling it to recursively decompose expressions into their functional components and learn their continuous-valued embeddings. The authors evaluated the proposed method on a dataset of mathematical expressions and demonstrated significant improvements over several baseline models.\n\nPeer Review:\nThe paper presents an interesting approach to learn continuous semantic representations of symbolic expressions by combining symbolic mathematics with neural networks. The authors have done a good job in explaining the motivation behind the approach, the methodology used and the experimental setup. The paper is well structured and is easy to follow.\n\nOne of the strengths of the paper is the thorough evaluation of the proposed method. The authors have considered several baseline models and demonstrated improvements in performance over them. The proposed method achieved state-of-the-art performance on a dataset of mathematical expressions. The authors have also provided ablation studies which showcase the importance of the individual components of the proposed method.\n\nAnother strength of the paper is the use of mathematical expressions as a test-bed for evaluating the proposed method. Mathematics is a domain where symbolic representations are very important, and the proposed method shows promising results in learning continuous semantic representations of the same.\n\nHowever, there are a few areas where the paper can be improved. While the proposed method is focused on learning continuous semantic representations of symbolic expressions, the paper misses on explaining how these representations can be utilized in downstream tasks such as equation solving or equation recognition. Additionally, the authors could have provided more insights into how the learned embeddings relate to the underlying structure of symbolic expressions.\n\nDespite these minor limitations, the paper is well-written and presents an innovative approach to learning continuous semantic representations of symbolic expressions. I recommend the paper for acceptance after minor revisions.","model":"chatGPT","source":"peerread","label":1,"id":2304}
{"text":"Review of \"Learning Continuous Semantic Representations of Symbolic Expressions\"\n\nProblem\/Question Addressed:\n\nThe paper proposes a method for learning continuous semantic representations of symbolic expressions, which is a significant problem in natural language processing (NLP). Symbolic expressions, such as mathematical equations or programming code, are challenging for traditional machine learning algorithms to process because they require a combination of logical and semantic understanding. As such, the authors aim to bridge the gap between symbolic and continuous representations to enable more effective processing of these expressions.\n\nStrengths:\n\nOne of the strengths of the paper is the extensive experiments and evaluations conducted to test the proposed method. The authors used various benchmarks, such as algebraic equation solving and mathematical formula completion, to show that their approach outperformed several baselines. Additionally, they conducted thorough ablation studies to demonstrate the effectiveness of different components of their model.\n\nAnother strength of the paper is the presentation of the method itself. The authors explain the details of the architecture and the learning algorithm in a clear and concise manner. They also provide several visualizations to help the reader understand the learned representations and their relationship to symbolic expressions.\n\nWeaknesses:\n\nOne potential weakness of the paper is the lack of a comparison against state-of-the-art methods in the field of NLP. While the authors compare their approach to several baselines, it would be useful to know how their method stacks up against more recent and sophisticated approaches.\n\nAnother weakness is the limited scope of the evaluation metrics. While the proposed method performs well on the tasks evaluated, it would be more convincing if the authors could demonstrate the effectiveness of their approach on a broader range of tasks relevant to symbolic expressions in NLP.\n\nConclusion:\n\nOverall, \"Learning Continuous Semantic Representations of Symbolic Expressions\" is a well-written and thoroughly researched paper that proposes a promising method for dealing with challenging symbolic expressions in NLP. The authors provide valuable insights into the architecture and learning algorithm, and the paper is supported by comprehensive experiments and evaluations. However, further evaluation and comparison against state-of-the-art methods would strengthen the findings.","model":"chatGPT","source":"peerread","label":1,"id":2305}
{"text":"Title: Peer Review for \"Learning Continuous Semantic Representations of Symbolic Expressions\"\n\nThe paper presents a new architecture, called neural equivalence networks, to address the problem of learning continuous semantic representations for symbolic expressions. The proposed architecture is aimed at addressing the challenge of representing symbolic expressions in a continuous neural reasoning framework. The paper claims that semantic representations need to be computed in a syntax-directed manner, as semantics is compositional, but small changes in syntax can lead to significant changes in semantics. The authors claim that their proposed architecture outperforms the existing architectures, as presented in the exhaustive evaluation conducted on a class of symbolic algebraic and boolean expression types.\n\nThe paper seems well-motivated and well-written, and the proposed neural equivalence networks architecture is an interesting and novel technique for addressing the problem of learning continuous semantic representations for symbolic expressions. The paper adequately presents the rationale and background of the problem, and the authors' reasoning is clear and easy to follow. \n\nThe paper's strengths include:\n\n1. Clear Presentation: The authors provide a very clear and straightforward introduction to the problem of learning continuous semantic representations for symbolic expressions. They also provide a clear motivation for their proposed approach, and the methodology is explained in detail.\n\n2. Novelty: The authors' approach proposes a neural architecture that combines symbolic and continuous reasoning, and the proposed method for computing semantic equivalents between syntactically diverse expressions is new to the field.\n\n3. Exhaustive Evaluation: The authors conduct an exhaustive evaluation of their model across multiple expression types, demonstrating that it significantly outperforms existing architectures.\n\nDespite the paper's strengths, there are some minor limitations:\n\n1. The scope and limitations of the proposed architecture's approach are not discussed in detail. It is unclear how the proposed method is expected to generalize when presented with novel expression types.\n\n2. The paper lacks a theoretical explanation of the proposed method's inner workings. The paper would be strengthened by an explanation of how structure and semantic features of the symbolic expressions are extracted during the encoding phase and analyzed during the semantic equivalence check.\n\n3. The paper does not discuss the potential applications of this work.\n\nOverall, the paper presents a well-motivated and interesting approach that is worth further investigation, although some minor limitations need to be addressed.","model":"chatGPT","source":"peerread","label":1,"id":2306}
{"text":"This paper \"Learning Continuous Semantic Representations of Symbolic Expressions\" addresses the fundamental problem in machine learning and artificial intelligence of how procedural knowledge is represented and inferred. Unlike recent work on program induction that operates on abstract computational machines or on execution traces, this paper proposes a new architecture called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. The main goal of this paper is to present a method to combine symbolic reasoning with continuous neural reasoning.\n\nThe strengths of this paper include its innovative approach to combining abstract, symbolic reasoning with continuous neural reasoning. The authors have proposed an exhaustive evaluation technique showing that their model outperforms the existing architectures. The authors have used theoretical reasoning and mathematical formulation to establish the proposed approach.\n\nThere are a few weaknesses that need to be addressed in this paper. The paper lacks a thorough explanation of the evaluation method used to check for the equivalence of the symbolic algebraic and boolean expression types. The authors have not clearly defined what makes their proposed approach unique when compared to existing architectures, and also lacking empirical evidence to support the claim that the model significantly outperforms existing architectures. Additionally, the paper does not provide detailed explanations of some of the technical terms used in the paper.\n\nOverall, this paper presents an innovative approach that might lead to significant advances in the field of machine learning and artificial intelligence. However, the authors should address the noted weaknesses to make their proposed approach easier to understand and more robust for future research.","model":"chatGPT","source":"peerread","label":1,"id":2307}
{"text":"Title: Modularized Morphing of Neural Networks\n\nAuthors: [Names of the authors]\n\nOriginality: The paper proposes a novel approach of modularizing neural networks to allow for more efficient and effective morphing. The authors have clearly demonstrated the originality of their work by highlighting the limitations of the current methods used for morphing neural networks and proposing a new approach that allows for improved modularity, scalability and flexibility.\n\nTechnical Merit: The methodology used to achieve the proposed approach is clearly presented and has been thoroughly evaluated. The authors have provided explanations, illustrations and evidence-based examples to demonstrate the technical merit of their approach. The presentation of results clearly indicates the potential of the proposed method for increasing the modularity of neural networks without adversely affecting performance.\n\nWriting Style: The paper is well-written and structured, providing a clear introduction, methodology, results and conclusion. However, there are some minor issues with clarity in some parts of the paper which could be improved with some additional editing. In particular, some sections could benefit from more explanation and clarification, particularly for readers that are not familiar with the topic.\n\nRecommendation: The paper presents a novel and promising approach for the morphing of neural networks. The results demonstrate clear benefits over existing methods, particularly for applications requiring dynamic change in model architecture or scalable models. The paper could be significantly improved with additional editing to improve clarity in some parts. Based on this review, I recommend this paper for acceptance with minor revisions.","model":"chatGPT","source":"peerread","label":1,"id":2308}
{"text":"The paper \"Modularized Morphing of Neural Networks\" addresses the problem of improving the ability of artificial neural networks to dynamically adapt to changing conditions while minimizing computational costs. The authors propose a modularized morphing approach for neural networks, where the network is divided into several modules, and the weights associated with these modules can be dynamically and selectively scaled based on the input data. The authors demonstrate the efficacy of their approach through experiments on several standard benchmark datasets.\n\nOne of the major strengths of this paper is that it addresses a very important and timely issue in the field of artificial intelligence, which is the need for efficient and adaptive learning algorithms. The proposed approach is based on a solid theoretical foundation and should be useful across a wide range of applications. Additionally, the authors provide thorough experimental evaluations and comparisons with baseline methods to demonstrate the effectiveness of their approach.\n\nHowever, there are a few weaknesses in the paper that need to be addressed. First of all, the authors could have provided more detailed explanations regarding certain technical aspects of their methodology, such as the exact criteria used for selecting modules or the exact algorithm used for scaling the weights. Additionally, the authors could have provided more nuanced discussions regarding the limitations of their approach and potential avenues for further research.\n\nOverall, the paper \"Modularized Morphing of Neural Networks\" represents a promising contribution to the field of machine learning and artificial intelligence. While there are certain areas where the paper could be improved, the proposed approach offers a potential solution to the important challenge of efficient and adaptive neural network training.","model":"chatGPT","source":"peerread","label":1,"id":2309}
{"text":"Title: Peer Review for \"Modularized Morphing of Neural Networks\"\n\nAuthors: Anonymous \n\nIntroduction:\nThis paper presents a novel approach for network morphism to morph a well-trained neural network to a new one with the network function completely preserved. The authors introduce two atomic morphing operations to compose the graphs, based on which modules are classified into two families, i.e., simple morphable modules and complex modules. The authors present practical morphing solutions for both of these two families and provide extensive experimental results to demonstrate the effectiveness of the proposed solution. \n\nStrengths: \nThe paper addresses a challenging problem of network morphism in neural networks and proposes a novel approach to solve it. The authors introduce a graph-based representation that is suitable for the morphing process and classify modules into two families. The authors also provide practical solutions for both families and demonstrate the effectiveness of their approach on benchmark datasets. \n\nWeaknesses: \nThe paper lacks sufficient background and related work in the introduction. It is unclear how this work relates to existing work on network morphism. The authors do not provide any comparison with existing approaches or state-of-the-art methods. Additionally, the authors should provide a more detailed discussion of why their proposed approach is better than existing ones.\n\nConclusion: \nOverall, this paper presents a novel approach to the challenging problem of network morphism in neural networks. The authors present a well-formulated graph-based representation of modules and provide practical solutions for both simple and complex morphable modules. The experimental results demonstrate the effectiveness of the proposed approach. However, the paper could be strengthened by providing more details on related work and comparisons with existing approaches.","model":"chatGPT","source":"peerread","label":1,"id":2310}
{"text":"The paper \"Modularized Morphing of Neural Networks\" addresses the problem of network morphism, which involves transforming a well-trained neural network into a new one while preserving its network function. Unlike prior work that focuses on basic morphing types at the layer level, this paper addresses the central problem of network morphism at a higher level by considering how a convolutional layer can be morphed into an arbitrary module of a neural network.\n\nThe strengths of this paper include its clear problem definition, its approach of abstracting a module as a graph with blobs as vertices and convolutional layers as edges, and its introduction of practical morphing solutions for both simple morphable modules and complex modules. The paper is well-organized and easy to follow.\n\nHowever, there are also some weaknesses that should be addressed. One concern is that the paper fails to provide a detailed explanation of the atomic morphing operations that are introduced to compose the graphs. Additionally, the authors have not provided a clear discussion on the limitations or possible drawbacks of their approach. Future research should explore these areas and offer possible solutions. \n\nOverall, the work presented in this paper represents a significant contribution to the field of neural networks and provides valuable insights into the problem of network morphism.","model":"chatGPT","source":"peerread","label":1,"id":2311}
{"text":"Overall, I find this paper on modular multitask reinforcement learning with policy sketches to be well-written and informative. The authors present a novel approach to multitask reinforcement learning that leverages policy sketches to facilitate the transfer of knowledge between tasks. The paper is structured in a logical and clear manner, with a thorough review of related work, a detailed description of the proposed method, and comprehensive experimental results.\n\nOne of the strengths of this paper is the clarity with which the method is presented. The authors provide a clear and concise explanation of policy sketches and how they can be used to simplify the training of modular policy networks. They also describe how their approach can be extended to include multiple tasks and demonstrate the effectiveness of their method on a variety of benchmark tasks.\n\nAnother strength of this paper is the experimental results. The authors conduct a thorough analysis of the proposed method, demonstrating that it consistently outperforms baseline methods on a range of benchmarks, including both continuous and discrete control tasks. They also provide a detailed analysis of the effect of the hyper-parameters and the number of policy sketches on the performance of the method.\n\nOne area where the paper could be improved is in the description of the limitations and future work. While the authors briefly touch on the limitations of their approach, they could benefit from expanding on this discussion and providing more insight into the potential challenges when working with more complex tasks. Additionally, the authors could also consider discussing potential extensions of their method that may further improve the performance on multitask reinforcement learning.\n\nOverall, I highly recommend this paper to researchers in the field of reinforcement learning and multitask learning. The presented method is innovative and effective, and the experimental results are convincing. With some minor revisions, this paper has the potential to make a valuable contribution to the field.","model":"chatGPT","source":"peerread","label":1,"id":2312}
{"text":"The paper \"Modular Multitask Reinforcement Learning with Policy Sketches\" addresses the problem of designing an efficient framework for automated multitask reinforcement learning. The authors of this paper propose a modular and structured approach to tackle the challenges of multitask reinforcement learning by utilizing policy sketches to transfer knowledge across tasks. The policy sketch is a high-level specification of the policy that captures the general structure of the task rather than the specific details.\n\nOne of the significant strengths of this paper is the proposed framework's modularity, which allows for flexible and easy integration with existing reinforcement learning algorithms. The framework's performance is evaluated through experiments on a range of multitask learning environments, and the results demonstrate the effectiveness of the policy sketches approach compared to conventional multitask learning algorithms.\n\nThe paper's clear writing style and extensive experimental results are some of its significant strengths. The authors thoroughly explain the proposed framework's architecture and provide detailed experimental findings to support their claims.\n\nHowever, one notable weakness of the paper is the lack of investigation into the robustness of the framework's performance in the presence of noise and perturbations in the environment. It would have been interesting to see how the proposed approach performs in non-stationary settings and its adaptability to dynamic environments. Additionally, the paper does not explore the interpretability of policy sketches and their impact on providing meaningful insights into the learned policies' decision-making process.\n\nOverall, the paper presents a methodical and comprehensive framework for tackling multitask reinforcement learning challenges. The proposed approach's modularity and the empirical evaluation's thoroughness make it a valuable contribution to the field of AI and reinforcement learning.","model":"chatGPT","source":"peerread","label":1,"id":2313}
{"text":"The paper presents a framework for multitask deep reinforcement learning guided by policy sketches, which annotate each task with a sequence of named subtasks. This annotation provides high-level structural relationships among tasks that enable the learning of common behaviors from dissimilar reward functions. The approach associates each subtask with its own modular subpolicy and jointly optimizes full task-specific policies by tying parameters across shared subpolicies.\n\nThe paper's contributions are significant, and the approach is novel, as it is different from previous work that relied on learning policy abstractions for RL. The experimental evaluation is performed on two games that feature extremely sparse rewards, where the proposed method is shown to outperform standard baselines that learn task-specific or shared monolithic policies. Additionally, the method introduces a library of primitive behaviors that can be recombined to rapidly acquire policies for new tasks.\n\nHowever, the experimental evaluation could have been more comprehensive. For example, it would have been useful to see how the proposed approach performs against state-of-the-art multitask reinforcement learning methods. Moreover, while the proposed approach has shown improvements, it is unclear how generalizable the results are for tasks beyond maze navigation game and 2-D Minecraft inspired crafting game.\n\nDespite these limitations, the paper's contribution is valuable to the field of multitask reinforcement learning. The presented approach is novel and shows promising results in dealing with arbitrary task hierarchies while maintaining a degree of modularity. Thus, I recommend this paper for acceptance after the authors address the aforementioned limitations in the manuscript.","model":"chatGPT","source":"peerread","label":1,"id":2314}
{"text":"The paper \"Modular Multitask Reinforcement Learning with Policy Sketches\" addresses the problem of optimizing multitask deep reinforcement learning for tasks with sparse rewards. The authors propose a framework guided by policy sketches, which provide high-level structural relationships among tasks, but not detailed guidance like intermediate rewards or subtask completion signals.\n\nOne of the strengths of the paper is its clear description of the proposed approach, which uses modular subpolicies for each subtask and jointly optimizes across shared subpolicies. The proposed approach also outperforms standard baselines and generates a library of primitive behaviors that can be applied to new tasks.\n\nHowever, one potential weakness of the paper is the limited scope of experiments, which only include two games. It would be beneficial to evaluate the proposed approach on a wider range of tasks to demonstrate its generalizability.\n\nOverall, the paper provides an interesting approach to optimizing multitask deep reinforcement learning and presents promising results for tasks with sparse rewards.","model":"chatGPT","source":"peerread","label":1,"id":2315}
{"text":"Title: Development of JavaScript-based deep learning platform and application to distributed training\n\nAuthor(s): [names of the authors]\n\nAbstract:\nThe paper presents a novel JavaScript-based deep learning platform that supports distributed training. The authors have developed the platform as an extension of TensorFlow.js, which allows users to train complex deep learning models in a distributed manner. The proposed platform is evaluated through a case study which enhances the performance and scalability of the model during the training process. \n\nStrengths:\n1. The paper presents a novel application of deep learning in JavaScript that has not been explored before. \n2. The proposed platform leverages the extensibility of TensorFlow.js for enabling distributed training of machine learning models. \n3. The authors have offered a comprehensive evaluation of their deep learning platform through a case study, which clearly demonstrates the benefits of using their proposed platform for training large scale machine learning models.\n\nWeaknesses:\n1. The authors do not provide any comparison with existing platforms or benchmarks to demonstrate the superiority and performance of their proposed platform. \n2. The paper lacks details on the experimental set-up and evaluation metrics used in the case study. \n3. The paper lacks sufficient discussion on the implications of their work and how it can be extended for future research in the area of distributed deep learning training.\n\nSuggestions for improvement:\n1. The authors are recommended to provide a comprehensive comparison of their proposed platform with existing platforms in terms of performance, scalability and the accuracy of the models trained. \n2. The authors should provide more detailed information about the experimental set-up, evaluation metrics and the dataset used in the case study. \n3. The authors should further extend their discussion on the implications of their work and how it can be further applied in different machine learning domains for distributed training of deep learning models.\n\nOverall, this paper presents a novel deep learning platform extension for training machine learning models in a distributed manner that leverages the capabilities of TensorFlow.js. The paper presents a comprehensive case study to demonstrate the performance and scalability of the platform in training complex machine learning models. However, there are areas for improvement as outlined above, which would further strengthen the research contribution of the paper.","model":"chatGPT","source":"peerread","label":1,"id":2316}
{"text":"The paper \"Development of JavaScript-based deep learning platform and application to distributed training\" presents a new JavaScript-based deep learning platform, along with its application to distributed training. The authors aim to address the challenge of developing a user-friendly and efficient deep learning platform, primarily suited for web-based applications, which can deal with distributed training across various devices.\n\nThe paper has several strengths. First, the authors have presented a novel approach to developing a deep learning platform by using JavaScript. This is a unique approach, considering that most deep learning platforms are built using low-level programming languages such as Python and C++. Developing a JavaScript-based platform could attract many web developers and programmers who are well-versed in this language, but not necessarily in other low-level programming languages. Second, the authors have demonstrated how their platform can handle distributed training using web workers, which increases the efficiency of the training process. This is a crucial advantage over other deep learning platforms that lack this feature. Third, the authors have provided a detailed evaluation of their platform's performance, comparing it with other popular deep learning frameworks, which gives substantial proof of the platform's effectiveness.\n\nThere are also a few weaknesses in the paper that need to be addressed. Firstly, the authors could have provided more details on the architecture of the platform to enable readers to understand it better. Secondly, the authors should have discussed the limitations of their platform, including its potential security vulnerabilities. Additionally, it would have been beneficial if the authors had provided more information on how their platform could be extended to incorporate other deep learning models and time-series analyses.\n\nIn conclusion, the paper \"Development of JavaScript-based deep learning platform and application to distributed training\" presents an innovative and significant contribution to the field of deep learning platforms. The authors have done an excellent job of developing a platform that is user-friendly, efficient, and can handle distributed training across various devices. The paper's strengths significantly outweigh its weaknesses, and I think this work should be published with minor revisions.","model":"chatGPT","source":"peerread","label":1,"id":2317}
{"text":"General comments:\n\nThis paper presents a novel approach to deep learning using JavaScript-based framework capable of running on ordinary personal computers and smartphones. The authors have developed a matrix library and deep learning framework using JavaScript, which can be used in a diverse set of environments without the need for additional software installation. The paper also demonstrates the practicality of their approach by training VGGNet in a distributed manner using web browsers as the client. Overall, the paper is well-structured and presents relevant and interesting results. \n\nMajor comments:\n\n1. Though the authors have mentioned developing a matrix library and deep learning framework, the paper could benefit from providing more details about these components. An overview of these libraries and frameworks would help the readers understand the approach better. The authors could elaborate on how these libraries and frameworks make their approach unique and different from existing frameworks.\n\n2. The paper could benefit from providing more details about the experimental setup. The authors have mentioned VGGNet and ResNet architectures, but no information about the dataset used for training, hyperparameters, and the GPU utilization has been provided. The readers would appreciate such details for better understanding and replicating the experiments.\n\n3. The paper could benefit from providing more detailed comparisons with existing frameworks. For example, it would be interesting to know how much faster\/slower the proposed approach is in comparison to existing approaches for training deep learning models. Also, the authors could elaborate on the limitations of their approach compared to existing frameworks.\n\nMinor comments:\n\n1. Figure 2 could be improved by providing more details about the different layers and their sizes.\n\n2. The use of different font sizes within the paper could be made more consistent.\n\n3. The authors could provide more details about the operating systems and browsers used in their experiments.\n\n4. The experimental results could be presented in a more concise manner without losing the important details.\n\n5. It would be great if the authors could provide access to their code for better reproducibility of their results.\n\nOverall, this paper presents a novel approach to deep learning using JavaScript-based libraries and frameworks that can run on personal computers and smartphones. The results demonstrate the practicality of their approach, and the paper is well-structured, though it could benefit from providing more details about their libraries and frameworks, experimental setup, comparisons with existing frameworks, and presenting the results in a more concise manner.","model":"chatGPT","source":"peerread","label":1,"id":2318}
{"text":"The paper \"Development of JavaScript-based deep learning platform and application to distributed training\" addresses the challenge of deploying and maintaining specialized computer systems for deep learning, which can be too costly for many organizations. The authors propose a solution that uses a matrix library and deep learning framework implemented in JavaScript. They demonstrate that their framework can run on web browsers, including those on personal computers and smartphones, making deep learning accessible in a wide range of environments without requiring software installation.\n\nThis paper presents several strengths. Firstly, the authors provide a detailed description of their framework's architecture, implementation, and performance assessment. They demonstrate the feasibility of using WebCL to harness the power of GPGPU for deep learning computation in web browsers. Secondly, the authors demonstrate the practicality of their approach by showing that their framework can be used to train large-scale convolutional neural networks, such as VGGNet and ResNet, in a distributed manner using web browsers as clients. This capability could offer significant cost savings for organizations with limited computing resources for deep learning.\n\nHowever, the paper also has some weaknesses. Firstly, the paper does not adequately address the issue of security in a distributed training environment. Training data distributed among web browsers can pose a significant risk of data breaches. Secondly, the authors do not provide enough detail regarding the framework's limitations, stability, and scalability on different browsers and devices, which is essential for assessing its feasibility in real-world scenarios. Lastly, the paper would benefit from a more comprehensive experimental evaluation with comparison to state-of-the-art deep learning frameworks.\n\nOverall, this paper provides a promising approach for democratizing deep learning by reducing the cost and hardware requirements for deployment. However, more work needs to be done to address the security risks, scalability, and robustness of the framework, which could significantly impact its practicality and impact in real-world applications.","model":"chatGPT","source":"peerread","label":1,"id":2319}
{"text":"Review of \"A Differentiable Physics Engine for Deep Learning in Robotics\"\n\nThe paper presents a novel differentiable physics engine for use in deep learning for robotics. The authors built their engine on top of an existing physics engine, MuJoCo, and implemented a differentiable version of it using automatic differentiation. The differentiability property allows for easy integration with deep learning techniques and enables end-to-end learning.\n\nThe paper is well-written and clearly presents the main ideas and contributions of the work. The authors provide a detailed description of the differentiable physics engine and its implementation, including the use of automatic differentiation software, such as PyTorch and TensorFlow. The authors also provide experimental evidence that shows the superiority of their approach over other non-differentiable physics engines currently available. \n\nThe experimental section of the paper clearly demonstrates the usefulness of the differentiable physics engine in multiple robotic tasks. The experiments are well-designed, and the results are presented in a clear and concise manner. The differentiable physics engine demonstrated remarkable performance in several robotics applications, including simulation-based robot control and robotic manipulation.\n\nOne potential limitation of the proposed approach is that it depends heavily on the performance of the automatic differentiation software used. Furthermore, the authors do not provide an in-depth analysis of the scalability of their approach. However, these areas could be potential avenues for future research.\n\nOverall, the paper presents a valuable contribution to the field of robotics and deep learning, and the proposed differentiable physics engine has a lot of potential in enabling more efficient and effective end-to-end learning in robotics. I recommend this paper for publication.","model":"chatGPT","source":"peerread","label":1,"id":2320}
{"text":"The paper, \"A Differentiable Physics Engine for Deep Learning in Robotics,\" addresses the challenge of integrating physics-based models with deep learning algorithms to improve the performance of robots. The authors propose a differentiable physics engine that enables robots to learn by interacting with their environment, while also providing a framework for updating the model based on the robot's experience.\n\nOne of the strengths of this paper is its comprehensive approach to addressing the challenges of developing a differentiable physics engine. The authors provide a detailed description of the model architecture, as well as the training and optimization procedures used to improve the performance of the model. The paper also includes several experiments that demonstrate the effectiveness of the differentiable physics engine in improving the performance of robots.\n\nHowever, there are also some weaknesses in the paper that need to be addressed. One of the main weaknesses is that the experiments are conducted in a simulated environment, and it is unclear how well the proposed method will perform in real-world situations. Additionally, the authors do not provide a detailed analysis of the limitations of the differentiable physics engine, and it is uncertain whether the proposed approach is scalable to more complex robotic systems.\n\nOverall, the paper presents an innovative approach for integrating physics-based models with deep learning algorithms to improve the performance of robots. However, more research is needed to fully understand the limitations of the differentiable physics engine and its potential applications in real-world settings.","model":"chatGPT","source":"peerread","label":1,"id":2321}
{"text":"Overall, the paper \"A Differentiable Physics Engine for Deep Learning in Robotics\" addresses an important issue in the field of robotics - the optimization of controllers. The authors propose an implementation of a modern physics engine that can differentiate control parameters, which allows for more efficient optimization using gradient-based methods, even for large-scale problems such as those encountered in deep learning.\n\nThe paper is well-written and structured, with clear and concise explanations of the proposed implementation and its advantages. The authors provide a detailed evaluation of the implementation's performance on both CPU and GPU, demonstrating its effectiveness for both small and large-scale problems.\n\nOne potential limitation of the proposed implementation is that it only supports a limited set of physics models, as noted by the authors. Future work could include expanding the range of supported models to increase the versatility of the implementation.\n\nOverall, this paper makes a significant contribution to the field of robotics by providing a new approach to optimize controllers using deep learning. The proposed implementation offers a promising avenue for further research in the field and has the potential to significantly improve the optimization process for robotics. I recommend that this paper be accepted for publication.","model":"chatGPT","source":"peerread","label":1,"id":2322}
{"text":"The paper \u201cA Differentiable Physics Engine for Deep Learning in Robotics\u201d proposes a novel implementation of a physics engine for deep learning in robotics which allows for the differentiation of control parameters, facilitating the optimization process. The paper argues that this approach is an alternative to deep Q-learning and contributes to advancing deep learning in robotics. \n\nOverall, the paper\u2019s topic is highly relevant to the field of robotics and optimization, as the current methods used for optimization are often black box approaches that can be expensive, and the implementation of a differentiable physics engine had not yet been explored. This paper offers a clear and well-written explanation of the implementation of a modern physics engine for the differentiation of control parameters, which makes it a strong contribution to the robotics literature.\n\nThe strengths of the paper include a clear explanation of the problem addressed and an in-depth analysis of the proposed solution. The paper provides a comprehensive evaluation of both CPU and GPU implementations, and the results are presented in a well-organized manner, making them easy to follow. The proposed implementation has also been compared to state-of-the-art methods, demonstrating the significant gains in speed and efficiency offered by this approach.\n\nHowever, the paper does have some weaknesses. One major weakness is a lack of discussion regarding the limitations of the proposed implementation, which could have been addressed in greater depth. Additionally, although the paper argues that this approach is an alternative to deep Q-learning, the connection is not explained in a clear and concise manner.\n\nIn summary, \u201cA Differentiable Physics Engine for Deep Learning in Robotics\u201d offers an insightful proposal for a differentiable physics engine and its implementation for use in optimization. Although there are some limitations and areas for improvement, this paper provides a strong contribution to the field of robotics and optimization, demonstrating the potential benefits of implementing a differentiable physics engine.","model":"chatGPT","source":"peerread","label":1,"id":2323}
{"text":"Title: Compositional Kernel Machines \n\nAuthors: John Smith and Jane Doe \n\nIn this paper, Smith and Doe propose an approach to natural language processing using Compositional Kernel Machines (CKMs) as an alternative to traditional approaches. The authors argue that CKMs can effectively capture the compositional nature of language by combining individual word embeddings and syntax information.\n\nThe paper presents a thorough explanation of the CKM model and its implementation. The authors provide a detailed analysis of the model's effectiveness on several benchmark datasets, comparing it to state-of-the-art methods such as recurrent neural networks.\n\nThe experiments demonstrate that the proposed approach outperforms existing methods in terms of accuracy and computational efficiency. Furthermore, the authors discuss the interpretability of the model, highlighting that the use of a kernel function makes it possible to analyze the contribution of each component to the final prediction.\n\nOne of the strengths of the paper is the clear and concise explanation of the proposed approach, as well as the sound experimental methodology used to evaluate it. However, there are a few areas where the paper could be improved.\n\nFirstly, the paper could benefit from a more detailed discussion of the limitations of the model. While the authors acknowledge the limitations of the method in the conclusion, providing more in-depth analysis would help readers better understand where the model may fall short.\n\nSecondly, the authors could provide more context and justification for the choice of kernel function used in the model. This would strengthen the argument for the effectiveness of the proposed method.\n\nLastly, the authors could consider including a comparison with other compositional models in addition to the traditional approaches, such as recursive neural networks.\n\nOverall, the paper provides a valuable contribution to the field of natural language processing by proposing a new method for capturing the compositional nature of language. With a few revisions, this paper has the potential to become an important reference for researchers in the field.","model":"chatGPT","source":"peerread","label":1,"id":2324}
{"text":"The paper \"Compositional Kernel Machines\" by K. Swersky and M. Kummerfeld addresses the problem of enhancing the performance of kernel machines by incorporating compositional features. Specifically, the authors propose a novel composite kernel function that allows for the combination of diverse types of information, including semantic, syntactic, and structural features. The paper presents experimental results demonstrating the efficacy of the approach on several benchmark datasets.\n\nThe paper is notable for its clarity of presentation and careful attention to detail. The authors provide a clear and intuitive explanation of the compositional kernel function and the rationale behind the approach. The experiments are well-designed and offer compelling evidence for the improved performance of the proposed method compared to traditional kernel machines.\n\nOne of the key strengths of this paper is its innovation. The proposed compositional kernel machine is a novel approach that offers a promising avenue for improving the effectiveness of machine learning models. The paper is also well-structured, with clear sections and a cohesive argument that is easy to follow.\n\nHowever, there are also some weaknesses that should be addressed. For example, the paper does not provide a comprehensive evaluation of the proposed method's limitations or robustness to noise and other confounding factors. Additionally, the authors could have provided additional discussion of the implications and potential applications of their work.\n\nOverall, this paper offers a well-motivated and innovative approach to improving kernel machine performance. Although there are some limitations to the approach that should be addressed, the authors provide compelling evidence of improved performance and clear explanations of the approach, making this a valuable contribution to the field of NLP.","model":"chatGPT","source":"peerread","label":1,"id":2325}
{"text":"The paper \"Compositional Kernel Machines\" presents an alternative approach to train convolutional neural networks by incorporating the compositionality and symmetry of convnets into kernel methods. The authors propose the use of compositional kernel machines (CKMs) to learn nonlinear concepts from fewer samples without data augmentation. The paper is well-written and provides a clear explanation of the proposed method, its advantages, and its potential applications.\n\nThe idea of CKMs to create an exponential number of virtual training instances by composing transformed sub-regions of the original ones is innovative and promising. The authors provide a good justification of this approach, showing how it can combat the curse of dimensionality and increase the invariance to translations and other symmetries. Furthermore, the proposal to compute CKM discriminant functions efficiently using ideas from sum-product networks is well-motivated and adds to the practicality of the approach.\n\nThe experiments presented in the paper are sound and demonstrate the effectiveness of the proposed method. The results show that CKMs can outperform SVMs and be competitive with convnets in a number of dimensions. However, some aspects of the experiments could be improved. For instance, it would be helpful to provide more details about the experimental setup, such as the hyperparameters used for each method and the number of trials conducted. Moreover, additional experiments on other datasets and tasks would strengthen the generalizability of CKMs.\n\nOverall, the paper presents a promising approach with potential impact on object recognition and other vision problems. The authors have done an excellent job of presenting the idea and demonstrating its effectiveness, and as such, the paper is a valuable contribution to the field of computer vision and machine learning.","model":"chatGPT","source":"peerread","label":1,"id":2326}
{"text":"The paper \"Compositional Kernel Machines\" proposes a novel approach for object recognition and other computer vision problems. The authors propose a kernel method that incorporates the compositionality and symmetry of convolutional neural networks (convnets) called Compositional Kernel Machines (CKMs). The paper demonstrates that CKMs can learn similar nonlinear concepts as convnets with easier training and architecture selection.\n\nThe strengths of this paper lie in its innovation, the clear presentation of ideas and the experiments conducted. The idea of creating an exponential number of virtual training instances by composing transformed sub-regions of the original ones is a novel solution that provides invariance to translations and other symmetries while combating the curse of dimensionality. The authors also provide a clear presentation of the idea behind CKMs and the computation of their discriminant functions using ideas from sum-product networks. The experiments conducted on NORB datasets show that CKMs' superior performance to SVMs and its comparability with convnets in a number of dimensions.\n\nHowever, the weaknesses of the paper lie in its limitations. The results of the experiments suggest that CKMs perform well with fewer samples and without data augmentation, but it is not clear how CKMs would scale with more complex data and problems. The paper does not discuss possible challenges or limitations for the proposed method, which could be significant given that CKMs are still novel and warrants further investigation. Additionally, the comparison of CKMs to convnets is limited to NORB datasets, and it remains unclear how CKMs would perform in other computer vision benchmarks.\n\nOverall, the paper's innovation and potential contributions to machine learning are significant, and the authors provided clear presentations of their ideas and experiments. However, further investigation is necessary to establish the full scope and limitations of CKMs.","model":"chatGPT","source":"peerread","label":1,"id":2327}
{"text":"Title: Extrapolation and Learning Equations\n\nAbstract:\nThe paper proposes a novel framework for performing regression analysis that involves extrapolating beyond the range of the input variables, in order to make predictions for out-of-sample data. To do this, a theoretical foundation is presented that involves learning equations from data, which can be used to make predictions outside of the observed input space. The proposed method is evaluated on a number of benchmark datasets and compared to other state-of-the-art regression techniques.\n\nSummary:\nThe paper makes a valuable contribution to the field of extrapolation in regression analysis. The proposed approach of learning equations from data and using them to make predictions beyond the observed input space is innovative and has the potential to yield significant improvements in prediction accuracy. The paper presents a clear and concise theoretical framework for the proposed method, which should make it easy for other researchers to replicate the experiments presented in the paper. The results on the benchmark datasets are impressive, showing that the proposed approach consistently outperforms other state-of-the-art regression techniques.\n\nStrengths:\n- The proposed approach is novel and has great potential for improving prediction accuracy in regression analysis\n- The theoretical foundation of the proposed method is presented in a clear and concise manner\n- The benchmark results demonstrate the effectiveness of the proposed approach and provide a good basis for comparison with other regression techniques\n\nWeaknesses:\n- The paper would benefit from a more detailed description of the datasets used in the experiments, including the size and characteristics of the datasets\n- The paper does not provide a discussion of the limitations of the proposed approach or potential areas for further research\n- The paper could benefit from a more detailed explanation of the implementation of the proposed method\n\nOverall, the paper presents a valuable contribution to the field of extrapolation in regression analysis and should be of interest to researchers working in this area.","model":"chatGPT","source":"peerread","label":1,"id":2328}
{"text":"The paper \"Extrapolation and Learning Equations\" addresses the problem of extrapolating predictions beyond the range of data that has been used to train a model. The authors argue that existing machine learning techniques often fail to accurately predict values outside of the training data range due to overfitting or underfitting.\n\nA major strength of this paper is its clear explanation of the problem of extrapolation and comprehensive review of current approaches in machine learning. The authors also propose a novel method called \"Learning Equations\" that aims to address the issue of extrapolation.\n\nHowever, a weakness of the paper is the limited evaluation of the proposed method, which only includes synthetic data experiments. It would have been beneficial to evaluate the method on real-world datasets to demonstrate its practical usefulness. Additionally, the paper does not provide a thorough discussion of the limitations of the method and its applicability to different types of data or models.\n\nOverall, the paper presents a well-written and insightful analysis of the problem of extrapolation in machine learning and introduces a promising new approach. With additional evaluations and considerations of potential limitations, the method proposed in this paper may bring significant advancements to the field of machine learning.","model":"chatGPT","source":"peerread","label":1,"id":2329}
{"text":"The paper entitled \"Extrapolation and learning equations\" proposes a novel type of function learning network called equation learner (EQL), which allows for learning analytical expressions that can be used to gain insight into the mechanism connecting inputs and outputs. The authors argue that while regression is commonly used in machine learning, it is often treated as a black box process that does not attempt to gain insight into the underlying mechanisms. However, in the natural sciences, finding an interpretable function for a phenomenon is essential to understanding and generalizing results.\n\nThe paper presents an end-to-end differentiable feed-forward network that allows for efficient gradient-based training, which can learn analytical expressions and extrapolate to unseen domains. By using sparsity regularization, concise and interpretable expressions can be obtained, sometimes even identifying the true underlying source expression.\n\nOverall, the paper is well-written and clearly presents the proposed approach, which is novel and has great potential for many practical applications, particularly in science and engineering. The experiments demonstrate that the proposed method outperforms other existing methods in terms of accuracy, readability, and interpretability. The authors also provide extensive discussions on the properties and limitations of the method and offer insightful suggestions for future research.\n\nHowever, there are some areas that could be improved. Firstly, while the experiments demonstrate the effectiveness of the proposed approach, a more in-depth analysis of the results would have further strengthened the paper, particularly in comparing the method with other state-of-the-art methods in a wider range of applications. Moreover, the paper could provide more details on the implementation of the proposed EQL. It would also be helpful if the authors could clarify how the proposed approach could be extended to deal with more complex functions.\n\nOverall, this is a well-written and promising paper that proposes a novel approach to learning analytical expressions that is both accurate and interpretable. With further exploration and development, this approach has the potential to greatly enhance the flexibility and accuracy of machine learning systems. Therefore, I believe that this paper holds strong potential for publication in a relevant conference or journal.","model":"chatGPT","source":"peerread","label":1,"id":2330}
{"text":"This paper addresses the problem of acquiring an interpretable and generalizable function learning network, rather than just identifying a suitable function from a hypothesis set in machine learning. Specifically, the goal is to learn analytical expressions and extrapolate to unseen domains using a novel type of function learning network called Equation Learner (EQL). The paper introduces this network as an end-to-end differentiable feed-forward network that allows for efficient gradient based training and sparsity regularization to obtain concise and interpretable expressions.\n\nOne strength of this paper is that it proposes a new approach to machine learning problems that seeks interpretable and generalizable solutions. This is important in fields where understanding the mechanism connecting inputs and outputs is a critical aspect. Moreover, the EQL network is shown to be effective in identifying concise and interpretable expressions that are more likely to reflect the true underlying source expression.\n\nHowever, there are also some weaknesses in this paper that need to be addressed. One of the limitations is that the EQL network is not compared to other types of function learning networks, making it difficult to understand its advantages over existing approaches. Another issue is that the paper does not provide details on how the network behaves under different experimental settings, such as varying the size of the input data or tuning hyperparameters.\n\nTo conclude, the paper makes a valuable contribution to the field of machine learning by proposing a novel approach to function learning that prioritizes interpretability and generalization. Nonetheless, the paper could be improved by including more detailed comparisons with other approaches and providing more information on how the EQL network behaves across different settings.","model":"chatGPT","source":"peerread","label":1,"id":2331}
{"text":"Title: Charged Point Normalization: An Efficient Solution to the Saddle Point Problem\n\nSummary:\n\nThe paper presents a new method called Charged Point Normalization (CPN) that aims to tackle the Saddle Point Problem in optimization algorithms. The authors argue that the Saddle Point Problem, which is a common issue in many optimization problems, can cause the optimization process to become inefficient or even impossible. In response to this, the authors propose CPN, which is based on the concept of \"charging\" the optimization variables to prevent them from getting stuck in saddle points.\n\nThe authors provide a detailed explanation of CPN and its advantages over other normalization methods, such as Batch Normalization (BN). They also demonstrate the efficiency of CPN in various benchmark datasets and show how it can improve the accuracy and speed of optimization algorithms.\n\nStrengths:\n\nThe paper presents a well-written and well-organized research study that tackles an important issue in optimization algorithms. The introduction and background sections provide a clear and concise overview of the Saddle Point Problem, making it easy for readers without an extensive background in optimization to understand the issue. The authors have also made a strong case for CPN by highlighting its advantages over other normalization methods, and the experimental results they present demonstrate its effectiveness in improving the accuracy and speed of optimization algorithms.\n\nWeaknesses:\n\nOne weakness of this paper is the lack of comparison to other state-of-art methods for tackling the Saddle Point Problem. The authors only compared CPN to BN, which is a commonly used normalization method in deep learning. Additionally, it would have been helpful if the authors had included a more detailed explanation of how CPN works and the mathematical proofs behind it, as this would make it easier for readers to understand the underlying principles of the method.\n\nConclusion:\n\nOverall, Charged Point Normalization proposes a promising solution to the Saddle Point Problem in optimization algorithms. The paper is well-written, and the experimental results demonstrate the effectiveness of CPN. However, further research is required to fully evaluate the performance of CPN against other state-of-the-art methods and understand the underlying principles of the technique.","model":"chatGPT","source":"peerread","label":1,"id":2332}
{"text":"The paper titled \"Charged Point Normalization: An Efficient Solution to the Saddle Point Problem\" proposes a new normalization method to address the issue of saddle points in the training of deep neural networks. \n\nThe problem of saddle points is well-recognized as a significant challenge in training deep learning models. The proposed solution in this paper, called charged point normalization, involves dividing the weights of the model by the sum of their absolute values, which according to the authors, helps in scaling the weights in a manner that avoids the saddle points problem.\n\nThe strengths of this paper include its clear explanation of the problem, the presentation of empirical results demonstrating the effectiveness of the proposed normalization method, and a comparison with state-of-the-art techniques. The analysis of the results shows that the approach is effective in reducing the impact of saddle points while improving the convergence speed and achieving good performance on a range of popular datasets.\n\nOne of the weaknesses of this paper is a lack of analysis on the potential limitations of the proposed method. Though the empirical results are promising, a more in-depth analysis of the theoretical properties and limitations of the approach would have been an added strength.\n\nAnother minor weakness is the lack of comparison with other normalization techniques, such as batch normalization, layer normalization or weight normalization. Given that normalization methods are an active area of research, this would have provided more insight into the effectiveness of the proposed technique.\n\nOverall, Charged Point Normalization is a well-written and well-thought-out paper that presents an effective solution to the saddle point problem in deep learning. The paper provides strong empirical evidence supporting the effectiveness of the proposed technique in improving the training of deep neural networks, and thus, it is a valuable contribution to the field.","model":"chatGPT","source":"peerread","label":1,"id":2333}
{"text":"Review of \"Charged Point Normalization: An Efficient Solution to the Saddle Point Problem\"\n\nThe paper presents a novel approach called Charged Point Normalization (CPN) to address the issue of saddle points in non-convex optimization. The proposed method is different from other existing methods as it does not use second-order information and can be learned by a gradient descent algorithm. The authors evaluated the proposed approach on a range of deep neural networks on various data-sets and demonstrated that it significantly improves learning in comparison to non-CPN neural networks.\n\nOverall, the paper is well-written and the proposed approach provides an interesting alternative to existing methods for addressing the saddle point problem. However, there are some points that should be addressed to improve the clarity and comprehensibility of the paper.\n\nFirst, it would be better if the authors had provided more details and explanations regarding their proposed CPN approach. The term \"Charged Point Normalization\" is not clearly defined, and it is not clear exactly how the proposed approach works. More details on the formulation and the underlying mathematical concepts would be beneficial for readers to follow and understand the method.\n\nSecond, the paper would benefit from additional clarity in terms of the experimental setup and results. It is unclear how the hyperparameters were tuned and whether the proposed method is sensitive to hyperparameter settings. More details on the experimental setup would help readers better understand the results and the effectiveness of the proposed approach.\n\nIn conclusion, the proposed approach provides an interesting alternative to existing methods to address the saddle point problem. However, the authors should provide more details and explanations, and improved clarity in terms of experimental setup and results to make the paper more comprehensible and accessible to readers.","model":"chatGPT","source":"peerread","label":1,"id":2334}
{"text":"The paper \"Charged Point Normalization: An Efficient Solution to the Saddle Point Problem\" addresses the problem of local minima in very high dimensional non-convex optimization and introduces a dynamic normalization approach that helps escape saddle points in the optimization process. This is an important problem to address as local minima and saddle points have been a challenge in deep learning and can lead to poor model performance and longer training times. \n\nThe paper presents the Charged Point Normalization (CPN) approach, which is a novel method that does not require second-order information and can be used with any gradient descent learner. The authors demonstrate that the CPN approach significantly improves learning in a range of deep neural networks on various datasets in comparison to non-CPN neural networks.\n\nOne of the strengths of this paper is the experimental evaluation. The authors conducted extensive experiments on several datasets, including MNIST, CIFAR-10, and CIFAR-100, to show the effectiveness of the CPN approach. They also compared the performance of the approach with state-of-the-art methods, which provides a useful benchmark for future research. Another strength of the paper is its accessibility and clarity, which makes it easy to understand and replicate the experiments.\n\nHowever, there are also some weaknesses in this paper that need to be addressed. One weakness is the lack of a comprehensive theoretical analysis of the method. Although the authors provide some theoretical insights into the CPN approach, a more formal analysis would strengthen the paper. Additionally, the paper could benefit from a more in-depth comparison with other saddle point escaping algorithms, to showcase the superiority of CPN in comparison to other approaches.\n\nOverall, the paper \"Charged Point Normalization: An Efficient Solution to the Saddle Point Problem\" presents a novel approach to address the saddle point problem in deep learning. The results of the experimental evaluation are promising, making this paper a valuable contribution to the field of deep learning.","model":"chatGPT","source":"peerread","label":1,"id":2335}
{"text":"Title: \"Energy-Based Spherical Sparse Coding\"\n\nAuthors: [Names of authors]\n\nIn this paper, the authors proposed a novel approach called \"Energy-Based Spherical Sparse Coding\" for unsupervised feature learning on spherical data. The proposed approach is based on the notion of sparse coding, where a set of basis functions is learned that best represent the input data. The authors utilized energy-based models as a way to learn the basis functions that can better capture complex data distribution on the sphere.\n\nThe authors provide an extensive experimental evaluation of their proposed approach on various benchmark datasets, including MNIST, CIFAR-10, and SHREC, where they compared their method to other state-of-the-art approaches. The experimental results demonstrate that the proposed approach performs significantly better than other methods in terms of representation and classification accuracy.\n\nThe paper is well-structured with clear and concise writing style. The authors provide comprehensive mathematical formulation of their proposed approach, which is helpful for understanding the proposed method. The experimental methodology and results are clearly presented with appropriate visualization to aid in understanding the results. The authors also provide detailed analysis and discussion of their results, which is an essential component of a quality research paper.\n\nHowever, there are a few issues that need to be addressed:\n\n1. The authors could provide more information on the computational complexity of their proposed approach, especially since it aims to tackle high-dimensional data with complex structure.\n\n2. The authors could also provide more information on how the proposed approach handles noisy data and outliers, which is an important consideration in practical applications.\n\nOverall, \"Energy-Based Spherical Sparse Coding\" is a well-written and valuable contribution to the field of unsupervised feature learning on spherical data. The proposed approach shows promising results and has the potential to advance the state-of-the-art in this area.","model":"chatGPT","source":"peerread","label":1,"id":2336}
{"text":"The paper \"Energy-Based Spherical Sparse Coding\" addresses the problem of signal representation on the unit sphere, specifically for compression and denoising tasks in signal processing applications, such as image and sound processing. The authors propose a novel approach for spherical signal representation, called energy-based spherical sparse coding (E-SSC), which combines an energy function with sparse coding principles to learn a dictionary of basis functions for signal representation.\n\nThe paper presents several notable strengths, including the clarity and organization of the presentation, the use of mathematical formulations to explain the proposed approach, and the thorough experimental evaluation of the proposed approach. The authors provide a detailed comparison of E-SSC with existing spherical signal representation methods, showing superior performance in terms of signal reconstruction and denoising capabilities. Furthermore, the paper provides extensive sensitivity analysis of the key parameters in the proposed method, which is a valuable contribution to the literature.\n\nHowever, there are a few weaknesses of the paper that need addressing. Firstly, the authors could have provided more detailed information on the computation and complexity of the proposed method, particularly with respect to the dictionary learning process. Additionally, the authors could have provided more discussion on the practical applications of the proposed approach and potential limitations of the method.\n\nOverall, the paper presents a valuable contribution to the field of spherical signal representation, particularly for compression and denoising tasks. The strengths of the paper outweigh the weaknesses, making it a useful resource for researchers in the field.","model":"chatGPT","source":"peerread","label":1,"id":2337}
{"text":"The paper titled \"Energy-Based Spherical Sparse Coding\" proposes a novel approach for image classification using efficient variants of convolutional sparse coding. The authors introduce a model called Energy-Based Spherical Sparse Coding (EB-SSC) that incorporates a learned linear bias into the coding step to enable discriminative classification. \n\nThe paper presents a well-structured and easy-to-follow presentation of the proposed method. The authors provide a thorough explanation of the theoretical foundations and assumptions underpinning the approach. The experimental evaluation of the method is thorough, and the results demonstrate the effectiveness of the proposed approach. The authors provide visualizations that illustrate the high-quality features learned by the model, which is vital for understanding the strengths and weaknesses of the approach.\n\nThe use of unit norm code vectors and the inner product cosine distance for the reconstructions is an interesting variation to the conventional sparse coding approach. The EB-SSC model takes an energy-based approach to classification and benefits from additional training data; this is due to the proposed model's ability to incorporate additional features into the training of the model.\n\nThe authors did an excellent job of relating their proposed method to the state-of-the-art approaches in image classification, and the results show that the proposed method outperforms many other methods. One potential limitation of the approach is the computational complexity, especially when the method is applied to very large datasets. However, the authors address this issue by suggesting the use of a stacked encoder model.\n\nOverall, this paper presents an innovative approach to image classification, incorporating a novel energy-based approach and a \"deep\" stacked encoder model. Its experimental evaluation is thorough, and the results presented demonstrate the efficacy of the proposed approach. The paper is well-written, and the authors have provided compelling theoretical and empirical evidence to support their claims. I recommend this paper for publication in peer-reviewed journals.","model":"chatGPT","source":"peerread","label":1,"id":2338}
{"text":"The paper \"Energy-Based Spherical Sparse Coding\" by Yue Wu and Yann LeCun, addresses the problem of efficient variant convolutional sparse coding with unit norm code vectors and reconstructions evaluated using cosine distance. The authors describe a model, Energy-Based Spherical Sparse Coding (EB-SSC), which introduces a learned linear bias into the coding step to enable discriminative classification.\n\nStrengths:\nThe paper presents a novel approach for improving the performance of convolutional sparse coding in image classification. The authors convincingly argue the reasons for introducing unit norm code vectors and the use of cosine similarity to evaluate reconstructions. The proposed model, EB-SSC, is well explained and has been evaluated with support from empirical results. The visualization of the learned features and their class separability is an essential aspect of the paper and adds significant weight to the proposed approach. The stacking of this encoder to create deep-layered models for image classification is an exciting direction that has been explored in this paper.\n\nWeaknesses:\nWhile the empirical results presented in the paper are well supported, the authors have not compared their proposed approach with the state-of-the-art methods in image classification. Additionally, the paper could provide more theoretical underpinnings of the proposed approach and its relationship with existing approaches.\n\nOverall, the paper \"Energy-Based Spherical Sparse Coding\" presents a compelling approach that makes a valuable contribution to the field of image classification. The authors have done a commendable job of presenting their findings, and the work appears rigorous and well-executed with a great potential for follow-up research.","model":"chatGPT","source":"peerread","label":1,"id":2339}
{"text":"Introduction:\nThe paper entitled \"Revisiting Denoising Auto-Encoders\" delves into a topic of great relevance in the field of Natural Language Processing (NLP) and machine learning in general. The authors aim to revisit the current state of the art on denoising auto-encoders (DAEs), providing theoretical insights and experimental results that show how different types of noise impact the performance of these models.\n\nStrengths:\nThe paper is well-structured and clearly written, with an introduction that motivates the need for the study and provides a detailed background on denoising auto-encoders. The authors do a good job of contextualizing their work within the broader literature on DAEs and NLP. \n\nThe experimental section is thorough, providing a detailed description of the datasets and experimental setup, including the choice of hyperparameters and the evaluation metrics used. The authors provide a solid argument for their choice of datasets and argue that their methodology aligns with previous work in the field.\n\nThe results of the experiments are presented in a clear and concise manner, with tables and graphs to visualize the performance of different models across different noise types. The findings of the study are well-supported and provide valuable contributions to the field of NLP and machine learning.\n\nWeaknesses:\nThe paper could benefit from a more in-depth analysis of the theoretical underpinnings of denoising auto-encoders. While the authors provide some insights into how different types of noise affect the performance of these models, more discussion and analysis of why certain types of noise are more effective than others would strengthen the paper's contribution to the field.\n\nFurthermore, the authors could provide more insight into the future directions of research related to denoising auto-encoders for NLP tasks. While the results of the study are promising, more discussion of the limitations of the models and potential avenues for further study would be beneficial.\n\nOverall, the paper is a valuable contribution to the field of NLP and machine learning, providing a comprehensive analysis of denoising auto-encoders and their performance on various datasets and noise types. The authors provide a thorough methodology, and their results are well-supported and provide valuable insights into the strengths and limitations of these models.","model":"chatGPT","source":"peerread","label":1,"id":2340}
{"text":"Paper Title: Revisiting Denoising Auto-Encoders\n\nProblem Addressed:\nThe paper investigates the performance of denoising auto-encoders (DAEs), a deep learning technique used for the processing of noisy data. DAEs have been used for several applications such as image and speech recognition, and removing noise from signals. The paper examines the architecture of DAEs and evaluates their performance on various datasets.\n\nStrengths:\nThe paper provides a comprehensive review of the applications and use of DAEs in a variety of research areas. It discusses the advantage of DAEs and how they can be used to pre-process noisy data. The paper highlights the architecture of different types of DAEs and provides optimal parameter settings for maximum performance. The authors also analyze the performance of different types of DAEs on several datasets and compare it to other state-of-the-art deep learning techniques. This comparison demonstrates the effectiveness of DAEs in noise reduction tasks.\n\nWeaknesses:\nOne of the weaknesses of the paper is that it lacks a detailed comparison of the performance of the different types of DAEs evaluated in the study. The authors only provide a brief comparison of the different architectures without providing a detailed explanation of how each architecture performs relative to others. The authors could benefit from providing more insights into the factors that influence the choice of the best DAE architecture and optimal parameter settings. \n\nConclusion:\nOverall, the paper is well-written and provides a comprehensive analysis of DAEs in noise reduction tasks. The authors provide valuable insights into the architecture of DAEs and their ability to pre-process noisy data effectively. However, it would be helpful to provide more details on the performance of different DAE architectures evaluated in the study.","model":"chatGPT","source":"peerread","label":1,"id":2341}
{"text":"Title: Peer Review for Revisiting Denoising Auto-Encoders\n\nThe paper of \"Revisiting Denoising Auto-Encoders\" proposes a modification to the original objective function of denoising auto-encoders (DAE)s, which is intended to force the encoder to carry out most of the denoising effort rather than allowing denoising to occur at any stage of the process. The paper argues that this modification will result in a better representation and improved performance.\n\nOverall, the paper is well-written and clearly presents the motivation, methodology, and results of the study. The paper expands upon prior research within the field of unsupervised learning and provides a valuable contribution to the development of denoising auto-encoders.\n\nThe experimental results are thoroughly presented and demonstrate the effectiveness of the proposed modification by comparing the original objective function of DAEs to the new objective function. The paper provides convincing evidence that the encoder is being forced to carry out most of the denoising process by showing that the DAE with the modified objective function achieves higher reconstruction accuracy for all levels of noise corruption.\n\nIn addition, the paper provides a clear explanation of the potential real-world implications of the findings, stating that the proposed modification has the potential to improve the performance of DAEs in applications such as image denoising, speech recognition, and natural language processing.\n\nHowever, there are some areas where the paper could be improved. Firstly, the paper could provide a more detailed comparison of the proposed modification with existing methods for achieving similar results, such as denoising convolutional auto-encoders. Secondly, the paper could present more in-depth analysis of the speed and computation time required for the proposed modification compared to the original objective function.\n\nIn conclusion, the paper presents a valuable contribution to the field of denoising auto-encoders and unsupervised learning as a whole by proposing a simple modification to the objective function that results in an improved representation. The paper is well-written and presents convincing experimental results, but could benefit from a more detailed comparison with existing methods and more in-depth analysis of computational requirements.","model":"chatGPT","source":"peerread","label":1,"id":2342}
{"text":"The paper \"Revisiting Denoising Auto-Encoders\" addresses the problem of improving the performance of denoising auto-encoders (DAEs) by modifying their objective function to better restrict the denoising to the encoding stages. The authors argue that this modification leads to more effective representations in unsupervised learning compared to the original DAE objective function.\n\nOne of the strengths of the paper is its clear writing style and organization. The authors present a well-defined problem and clearly articulate their proposed solution. The paper also provides a rational and evidence-based justification for the proposed modification, thus enhancing its validity.\n\nAnother notable strength is the authors' use of experimental evaluation to support their argument. They compare the performance of the proposed modification to the original DAE on multiple datasets, demonstrating consistent accuracy improvements. This strengthens the credibility of the authors' proposal and the practical value of their work.\n\nHowever, there are some weaknesses in the paper that deserve attention. Firstly, there could be more explanation of the limitations of the proposed modification. Secondly, the authors could expand further on the real world applications of their modification, as to why it might be better suited to some specific tasks. Lastly, the authors could provide more contexts of other related research and empirical evaluation on the same problem.\n\nOverall, the proposed modification to the DAE objective function for better noise reduction is interesting and promising. Although there are some limitations and areas for improvement, it has the potential to enhance the performance of unsupervised learning in the field of machine learning.","model":"chatGPT","source":"peerread","label":1,"id":2343}
{"text":"The paper extends the NTM by a trainable memory addressing scheme.\nThe paper also investigates both continuous\/differentiable as well as discrete\/non-differentiable addressing mechanisms.\n\nPros:\n* Extension to NTM with trainable addressing.\n* Experiments with discrete addressing.\n* Experiments on bAbI QA tasks.\n\nCons:\n* Big gap to MemN2N and DMN+ in performance.\n* Code not available.\n* There could be more experiments on other real-world tasks.","model":"human","source":"peerread","label":0,"id":4701}
{"text":"This paper proposes some novel architectural elements, and the results are not far from published DNC results. However, the main issues of this paper are the complexity of the model, lack of justification for certain architectural choices, gaps with reported DNC numbers on BABI, and also a somewhat toy-ish task.","model":"human","source":"peerread","label":0,"id":4702}
{"text":"The authors proposed a dynamic neural Turing machine (D-NTM) model that overcomes the rigid location-based memory access used in the original NTM model. The paper has two main contributions: 1) introducing a learnable addressing to NTM. 2) curriculum learning using hybrid discrete and continuous attention. The proposed model was empirically evaluated on Facebook bAbI task and has shown improvement over the original NTM.\n\nPros:\n+ Comprehensive comparisons of feed-forward controllers v.s. recurrent controllers\n+ Encouraging results on the curriculum learning on hybrid discrete and continuous attentions\n\nCons:\n- Very weak NTM baseline (due to some hyper-parameter engineering?) in Table 1, 31% err. comparing to the NTM 20% err. reported in Table 1 in(Graves et al, 2016, Hybrid computing using a neural network with dynamic external memory). In fact, the NTM baseline in (Graves et al 2016) is better than the proposed D-NTM with GRU controller. Maybe it is worthwhile to reproduce their results using the hyper-parameter setting in their Table2 which could potentially lead to better D-NTM performance?\n- Section 3 of the paper is hard to follow. The overall clarity of the paper needs improvement.","model":"human","source":"peerread","label":0,"id":4703}
{"text":"Dear Reviewers and Readers,\n\nFor the codes of the models and the tasks which we have explored\/experimented in our paper, please see our repo:\n","model":"human","source":"peerread","label":0,"id":4704}
{"text":"This paper introduces a variant of the neural Turing machine (NTM, Graves et al. 2014) where key and values are stored. They try both continuous and discrete mechanisms to control the memory.\n\nThe model is quite complicated and seem to require a lot of tricks to work. Overall it seems that more than 10 different terms appear in the cost function and many different hacks are required to learn the model. It is hard to understand the justification for all of these tricks and sophisticated choices. There is no code available nor plan to release it (afaik).\n\nThe model is evaluated on a set of toy problems (the \u201cbabi task\u201d) and achieves performance that are only slightly above those of a vanilla LSTM but are much worse than the different memory augmented models proposed in the last few years.  \n\nIn terms of writing, the description of the model is quite hard to follow, describing different blocks independently, optimization tricks and regularization. The equations are hard to read, using non standard notation (e.g., \u201csoftplus\u201d), overloading notations (w_t, b\u2026), or write similar equations in different ways (for example, eq (8-9) compared to (10-11). Why are two equations in scalar and the other in vectors? Why is there an arrow instead of an equal?\u2026).\n\nOverall it is very hard to put together all the pieces of this model(s), there is no code available and I\u2019m afraid there is not enough details to be able to reproduce their numbers. Finally, the performance on the bAbI tasks are quite poor compared to other memory augmented models.\n","model":"human","source":"peerread","label":0,"id":4705}
{"text":"The paper extends the NTM by a trainable memory addressing scheme.\nThe paper also investigates both continuous\/differentiable as well as discrete\/non-differentiable addressing mechanisms.\n\nPros:\n* Extension to NTM with trainable addressing.\n* Experiments with discrete addressing.\n* Experiments on bAbI QA tasks.\n\nCons:\n* Big gap to MemN2N and DMN+ in performance.\n* Code not available.\n* There could be more experiments on other real-world tasks.\n","model":"human","source":"peerread","label":0,"id":4706}
{"text":"The model described in his paper is quite complicated and reproducing the method from its description may be challenging: Are you planing to release the code and what is your estimate release date?\n\nDifferent cost functions and regularization are introduce in the paper. Would it be possible to summarize the overall cost function minimized by this model? \n\nSome variables seems to have different definition. In particular w_t and b, would it be possible to clarify this?\n\n\u201cgamma_t is a shallow MLP\u201d: What does it mean? Is gamma_t a function or a variable? It seems from eq. (10) that it is a vector (or a scalar?).\n\n\"curriculum learning for the discrete attention\": Can you compare this to simpler schemes? Like rounding the continuous attention?\n\nIn your introduction, you state that \"it is possible to use the discrete non-differentiable attention mechanism\", referencing Zaremba & Sutskever, 2016 that use REINFORCE and a simple controller. However, in your sec. 4, you are stating that  \"Training discrete attention with feed-forward controller and REINFORCE is challenging\". It seems that these two statements contradict each other, could you comment on it? \n\nIn the introduction, you state that \"Memory network (Weston et al. 2015b) [..] uses an attention-based mechanism to index them\". To the best of my knowledge, it is actually Sukhbaatar et al., 2015 that has introduce the attention mechanism to memory networks. \n\nIn the introduction, the authors state that \u201cmemory networks [\u2026] [are] used in real tasks (Bordes et al. 2015, Dodge et al. 2015)\u201d. However in both papers, they use memory network systems different from Weston et al. The current statement is quite misleading, would it be possible to clarify it?\n","model":"human","source":"peerread","label":0,"id":4707}
{"text":"I think the footnote is missing.","model":"human","source":"peerread","label":0,"id":4708}
{"text":"I reviewed the manuscript as of December 6th.\n\nSummary:\nThe authors build upon generative adversarial networks for the purpose of steganalysis -- i.e. detecting hidden messages in a payload. The authors describe a new model architecture in which a new element, a 'steganalyser' is added a training objective to the GAN model.\n\nMajor Comments:\nThe authors introduce an interesting new direction for applying generative networks. That said, I think the premise of the paper could stand some additional exposition. How exactly would a SGAN method be employed? This is not clear from the paper. Why does the model require a generative model? Steganalysis by itself seems like a classification problem (i.e. a binary decision if there a hidden message?) Would you envision that a user has a message to send and does not care about the image (container) that it is being sent with? Or does the user have an image and the network generates a synthetic version of the image as a container and then hide the message in the container? Or is the SGAN somehow trained as a method for detecting hidden codes performed by any algorithm in an image? Explicitly describing the use-case would help with interpreting the results in the paper.\n\nAdditionally, the experiments and analysis in this paper is quite light as the authors only report a few steganalysis performance numbers in the tables (Table 1,2,3). A more extensive analysis seems warranted to explore the parameter space and provide a quantitative comparison with other methods discussed (e.g. HUGO, WOW, LSB, etc.) When is it appropriate to use this method over the others? Why does the seed effect the quality of results? Does a fixed seed correspond realistic scenario for employing this method?\n\nMinor comments:\n- Is Figure 1 necessary?\n- Why does the seed value effect the quality of the predictive performance of the model?","model":"human","source":"peerread","label":0,"id":4709}
{"text":"This paper examines an application of that deviates from the usual applications presented at ICLR. The idea seems very interesting to the reviewers, but a number of reviewers had trouble really understanding why the proposed SGAN would be attractive for this problem, and this problem setup with the SGAN in general. Clearer concrete 'use case scenarios' and experimentation that helps clarify the precise application setting and the advantages of the SGAN formulation would help make this work more impactful on the community. Given the quality of other paper submitted to ICLR this year the reviewer scores are just short of the threshold for acceptance","model":"human","source":"peerread","label":0,"id":4710}
{"text":"This paper proposes an interesting application of the GAN framework in steganography domain. In addition to the normal GAN discriminator, there is a steganalyser discriminator that receives the negative examples from the generator and positive examples from the generator images that contain a hidden payload. As a result, the generator, not only learn to generate realistic images by fooling the discriminator of the GAN, but also learn to be a secure container by fooling steganalyser discriminator. The method is tested by training an independent steganalyser S* on real images and generated images.\n\nGiven that in the ICLR community, not many people are familiar with the literature of steganography, I think this paper should have provided more context about how exactly this method can be used in practice, what are the related works on setganalysis-secure message embedding and probably a more thorough sets of experiments on more than one dataset.\n\nThe proposed SGAN framework (Figure 2) does make sense to me, and I think it is very general and can have more applications other than the steganography domain. But it is not clear to me why fooling the steganalyser discriminator S, necessarily mean that we can fool an independent discriminator S*?\n\nAlso I find it surprising that a different seed value, can make such a huge difference in the accuracy.\n\nIn short, the ideas of this paper are interesting and potentially useful, but I think the presentation of this paper should be improved so that it becomes more suitable for the ICLR and machine learning community.","model":"human","source":"peerread","label":0,"id":4711}
{"text":"I found this paper very original and thought-provoking, but also a bit difficult to understand. It is very exciting to see a practical use case for image-generating GANs, with potentially meaningful benchmarks aside from subjective realism.\n\nI found eq. 4 interesting because it introduces a potentially non-differentiable black-box function Stego(...) into the training of (S, G). Do you in fact backprop through the Stego function?\n\n- For the train\/test split, why is the SGAN trained on all 200k images? Would it not be cleaner to use the same splits for training SGAN as for \"steganalysis purposes\"? Could this account for the sensitivity to random seed shown in table 2?\n- Sec. 5.3: \"Steganographic Generative Adversarial Networks can potentially be used as a universal tool for generating Steganography containers tuned to deceive any specific steganalysis algorithm.\". This experiment showed that SGAN can fool HUGO, but I do not see how it was \"tuned\" to deceive HUGO, or how it could be tuned in general for a particular steganalyzer.\n\nAlthough S* seems to be fooled by the proposed method, in general for image generation the discriminator D is almost never fooled. I.e. contemporary GANs never converge to actually fooling the discriminator, even if they produce samples that sometimes fool humans. What if I created an additional steganalyzer S**(x) = S*(x) * D(x)? This I think would be extremely difficult to fool reliably because it requires realistic image generation.\n\nAfter reading the paper several times, it is still a bit unclear to me how or why precisely one would use a trained SGAN. I think the paper could be greatly improved by detailing, step by step, the workflow of how a hypothetical user would use a trained SGAN. This description should be aimed at a reader who knows nothing or very little about steganography (e.g. most of ICLR attendees).\n","model":"human","source":"peerread","label":0,"id":4712}
{"text":"Summary: The paper proposes a large-scale dataset for reading comprehension, with the final goal of releasing 1 million questions and answers. The authors have currently released 100,000 queries and their answers. The dataset differs from existing reading comprehension datasets mainly w.r.t queries being sampled from user queries rather than being generated by crowd-workers and answers being generated by crowd-workers rather than being spans of text from the provided passage. The paper presents some analysis of the dataset such as distribution of answer types. The paper also presents the results of some generative and some cloze-style models on the MS MARCO dataset.\n\nStrengths:\n\n1. The paper provides useful insights about the limitations of the existing reading comprehension datasets \u2013 questions asked by crowd-workers have different distribution compared to that of questions asked by actual users of intelligent agents, answers being restricted to span from the reading text rather than requiring reasoning across multiple pieces of text\/passages.\n\n2. MS MARCO dataset has novel useful characteristics compared to existing reading comprehension datasets \u2013 questions are sampled from user queries, answers are generated by humans.\n\n3. The experimental evaluation of the existing baseline models on the MS MARCO dataset is satisfactory.\n\nWeaknesses\/Suggestions:\n\n1. The paper does not report human performance on the dataset. Human performance should be reported to estimate the difficulty of the dataset. The degree of inter-human agreement will also reflect how well the metric (being used to compute inter-human agreement and accuracies of the baseline models) can deal with variance in the sentence structure with similar semantics.\n\n2. I would like to see the comparison between the answer type distribution in the MS MARCO dataset and that in existing reading comprehension datasets such as SQuAD. This would ground the claim made in the paper the distributions of questions asked by crowd-workers is different from that of user queries.\n\n3. The paper uses automatic metrics such as ROUGE, BLEU for evaluating natural language answers. However, it is known that such metrics poorly correlate with human judgement for tasks such as image caption evaluation (Chen et al., Microsoft COCO Captions: Data Collection and Evaluation Server, CoRR abs\/1504.00325 (2015)). So, I wonder how authors justify using such metrics for evaluating open-ended natural language answers.\n\n4. The paper mentions that a classifier was used to filter answer seeking queries from all Bing queries. It would be good to mention the accuracy of this classifier. This will provide insights into what percentage of the MS MARCO questions are answer seeking queries. Similarly, what is the accuracy of the information retrieval based system used to retrieve passages for filtered queries?\n\n5. Please include the description of the best passage baseline in the paper.\n  \n6. Fix opening quotes, i.e. \u201d -> \u201c (for instance, on page 5, \u201dwhat\u201d -> \u201cwhat\u201d).\n\nReview Summary: The paper is well motivated, the use of user queries and human generated answers makes the dataset different from existing datasets. However, I would like to see the human performance on the dataset and quantitative comparison between the distribution of questions obtained from user queries and that of crowd-sourced questions. I would also like the authors to comment on the use of automatic metrics (such as ROUGE, BLEU) in the light of the fact that such metrics do not correlate well with human judgements for tasks such as image caption evaluation.","model":"human","source":"peerread","label":0,"id":4713}
{"text":"Though the dataset is likely to have a large impact on the community, there is a general consensus among the reviewers that the authors could have done a better job characterizing the dataset (lack discussion on answer types, careful comparison with previous datasets, human performance on the dataset). So sadly, though the dataset is potentially very important, the paper does not quite cut it.\n \n Positive:\n -- an important dataset\n -- a good job with establishing baselines\n \n Negative\n -- analysis and discussions are very limited","model":"human","source":"peerread","label":0,"id":4714}
{"text":"While this dataset would undoubtedly benefit the community, the paper itself lacks sufficient details to to establish reproducible baselines, which is a key part of a dataset submission in my opinion.\n\nFirstly, the model descriptions are lacking:\n\n1. What exactly is the \"best passage\" model?\n2. What is \"a DSSM-alike passage ranking model\"?\n3. For each model, how does the model handle each case where there are multiple selected passages, a single selected passage, and zero selected passage?\n\nSecondly, the evaluation set on which the metrics are computed are not clearly defined.\n\n1. The authors refer to \"a subset of MS MARCO\" in all evaluation tables (5, 6, and 7). What examples do these subsets contain? Without this information non of the evaluations are reproducible.\n2. How does the author evaluate the case where there are no answers or there are several answers? It is ambiguous how one computes the BLEU or ROUGE scores in these cases.\n\nLastly, there are no human baselines on this dataset, making the upper bounds unclear (unlike some of the other datasets released).","model":"human","source":"peerread","label":0,"id":4715}
{"text":"Paper Summary: \nThis paper presents a new large scale machine reading comprehension dataset called MS MARCO. It is different from existing datasets in that the questions are real user queries, the context passages are real web documents, and free form answers are generated by humans instead of spans in the context. The paper also includes some analysis of the dataset and performance of QA models on the dataset.\n\nPaper Strengths: \n-- The questions in the dataset are real queries from users instead of humans writing questions given some context.\n-- Context passages are extracted from real web documents which are used by search engines to find answers to the given query.\n-- Answers are generated by humans instead of being spans in context.\n-- It is large scale dataset, with an aim of 1 million queries. Current release includes 100,000 queries.\n\nPaper Weaknesses: \n-- The authors say, \"We have found that the distribution of actual questions users ask intelligent agents can be very different from those conceived from crowdsourcing them from the text.\", but the statement is not backed up with any study.\n-- The paper doesn't clearly present what additional information can today's QA models learn from MS MARCO which they can't from existing datasets. \n-- The paper should talk about what challenges are involved in obtaining a good performance on this dataset.\n-- What are the human performances as compared to the models presented in the paper?\n-- In section 4.1, what are the train\/test splits? The results are for the subset of MS MARCO where every query has multiple answers. How big is that subset?\n-- What is DSSM mentioned in row 2, Table 5?\n-- The authors should include in the paper how experiments in section 4.2 prove that MS MARCO is a better dataset.\n-- In Table 6, the performance of Memory Networks is already close to Best Passage. Does that mean there is not enough room for improvement there?\n-- The paper seems to be written in hurry, with partial analysis, evaluation and various mistakes in the text.\n\nPreliminary Evaluation: \nThe proposed dataset MS MARCO is unique from existing datasets as it is a good representative of the QA task encountered by search engines. I think it can be a very useful dataset for the community to benefit from. Given the huge potential in the dataset, this paper lacks the analysis and evaluation needed to present the dataset's worth. I think it can benefit a lot with a more comprehensive analysis of the dataset.","model":"human","source":"peerread","label":0,"id":4716}
{"text":"This is a dataset paper that brings unique values over existing reading comprehension challenges. Unlike others, MS MARCO is derived from query logs, thus represents real questions that people ask, rather than solicited questions that might be rather artificial in practical settings.\n\nThere are potential downsides of using query logs however. It may be that people adapt their language and questions for search engines such that users ask questions that they know current search engines can reasonably answer. Thus, it may be that people limit the complexity of questions or language or both. I think authors could have addressed this concern by being more selective about the query logs, by down-sampling on simple questions that can be easily answered by keyword matching without any sophisticated reading comprehension, and up-sampling more complex questions that require at least paraphrasing and ideally synthesis of information taken from more than one sentences.\n\nIt\u2019s great that there are several new efforts to construct large-scale reading comprehension challenges, but my main concern is whether the majority of the questions can be answered through relatively easy text matching without intelligent reading or reasoning.\n\nAlso, the paper reads like the authors were running out of time before the deadline. I would appreciate more analytic and quantitative comparisons against other existing datasets, and more insights on the degree of challenges required to handle QAs in MS MARCO. For example, the authors could collect statistics on QAs: (1) exact match exists in the text snippet, (2) paraphrasing is required but otherwise the relevant answer is directly available in the text snippet, (3) requires synthesizing information taken from more than one sentences, (4) requires external knowledge. The author response mentions that (4) is unlikely, but a more formal and complete analysis would be helpful.\n\n","model":"human","source":"peerread","label":0,"id":4717}
{"text":"The authors of this work propose a learnable approach to reducing the dimensionality of learned filters in deep neural networks. This is an interesting approach, but the presented work looks a bit raw.\n\n1. There are many typos in this manuscript. \n2. The experimental results are rather weak and don't show much improvement in accuracy. Instead the authors could position this work as a compression mechanism and would have to compare to low rank approximation of filters for DNNs. Yet this is not done. \n3. Aside from compression, OMG can be viewed as a form of regularization to reduce the unnecessary capacity of the network to improve generalization. Again, this is not addressed in enough detail.\n4. If the authors care to compare their approach to other 1-shot learning methods, then they would have to evaluate their approach with siamese and triplet learning networks. This isn't done.","model":"human","source":"peerread","label":0,"id":4718}
{"text":"All three reviewers point to significant deficiencies. No response or engagement from the authors (for the reviews). I see no basis for supporting this paper.","model":"human","source":"peerread","label":0,"id":4719}
{"text":"This paper proposes a k-shot learning framework that can be used on existing pre-trained networks by grouping filters that produce similar activations. The grouped filters are learned together to address overfitting when only few training samples are available. \n\nThe idea of the paper is interesting there are some encouraging results, but the current version doesn't seem ready for publication:\n\nPerformance:\nThe method should be compared with other state-of-the-art k-shot learning methods (e.g., Matching Networks by Vinyals et al., 2016). It's not clear how this method compares against them.\n\nMissing explanation:\nExperimental setting for k-shot learning should be more detailed.\n\nMeasure:\nAccuracy difference does not look like a good idea for comparing the baseline method and the proposed one. Just raw accuracies would be fine. \n\nMany grammatical errors and inappropriate formatting of citations, such as:\nM. et al. (2011)\nImageNet (Alex et al. (2012))\nJudy et al. (2013): this reference appears three times in the reference section.\n","model":"human","source":"peerread","label":0,"id":4720}
{"text":"This paper proposes a regularization technique for k-shot learning based on orthogonal grouping of units in a neural network. The units within a group are forced to be maximally similar, at the same time the units from different groups are encouraged to be orthogonal. While I like the motivation of the approach, the empirical analysis provided in the paper doesn\u2019t look particularly convincing.\n\nMy main concerns are the following:\n\n1. The method is sensitive to the values of alpha and beta and a poor choice of those hyperparameters can lead to a quite drastic drop in performance comparing the minor gains one gets when alpha and beta are set properly.\n\n2. It seems strange that the best performance is obtained when the group's size ratio is 0.5. From the figures in the paper, it follows that usually, one has more \u201corthogonal\u201d groups in a filter bank. I have an impression that the empirical evidence doesn\u2019t align well with the motivation of the proposed approach.\n\n3. The paper contains a significant amount of typos and incorrectly formatted references. There are also several places in the manuscript that I found hard to understand due to unusual phrasing.\n\nI would like to thank the authors for answering\/addressing my pre-review questions. I would be grateful if the authors could provide more clarifications of the following:\n\n1. Question 2: I\u2019m not sure if modifying \\theta_{map} alone would result in any learning at all. Do I understand correctly that \\theta_{map} is only used to define groups? If so, then I don\u2019t see how the proposed method can be used in the purely unsupervised regime.\n\n2. Question 3: I was not referring to the fixed clustering based on the filter of the pre-trained network. One can perform that clustering at every step of the k-shot learning process. I\u2019m not sure I understand why the authors visualize grouping of _filters_ while in the actual algorithm they group _activations_. \n\nOverall, the paper is quite interesting but needs a stronger empirical justification of the approach as well as a better presentation of the material.","model":"human","source":"peerread","label":0,"id":4721}
{"text":"SUMMARY.\nThis paper presents a method for enriching medical concepts with their parent nodes in an ontology.\nThe method employs an attention mechanism over the parent nodes of a medical concept to create a richer representation of the concept itself.\nThe rationale of this is that for  infrequent medical concepts the attention mechanism will rely more on general concepts, higher in the ontology hierarchy, while for frequent ones will focus on the specific concept.\nThe attention mechanism is trained together with a recurrent neural network and the model accuracy is tested on two tasks.\nThe first task aims at prediction the diagnosis categories at each time step, while the second task aims at predicting whether or not a heart failure is likely to happen after the T-th step.\n\nResults shows that the proposed model works well in condition of data insufficiency.\n\n----------\n\nOVERALL JUDGMENT\nThe proposed model is simple but interesting.\nThe ideas presented are worth to expand but there are also some points where the authors could have done better.\nThe learning of the representation of concepts in the ontology is a bit naive, for example the authors could have used some kind of knowledge base factorization approach to learn the concepts, or some graph convolutional approach.\nI do not see why the the very general factorization methods for knowledge bases do not apply in the case of ontology learning.\nI also found strange that the representation of leaves are fine tuned while the inner nodes are not, it is a specific reason to do so?\n\nRegarding the presentation, the paper is clear and the qualitative evaluation is insightful.\n\n\n----------\n\nDETAILED COMMENTS\n\nFigure 2. Please use the same image format with the same resolution.","model":"human","source":"peerread","label":0,"id":4722}
{"text":"The reviewers all agreed that the paper was well written, that the proposed approach is very sensible and intuitive and that the experiments are convincing. However, they are concerned that the proposed work is of limited interest to the ICLR community. The technical contribution is not significant enough for any of the reviewers to strongly recommend an acceptance.","model":"human","source":"peerread","label":0,"id":4723}
{"text":"Major changes in the revision\n-- Figures 2 are replaced with tables for better readability\n-- Related works have been expanded\n-- Incorporated the reviewers\u2019 comments in the experiment section.\n-- Experiments for GRAM+ and RNN+ in heart failure prediction were re-run\n\nMore details\nWe discovered that, for GRAM+ and RNN+ in HF prediction with varying amounts of training data, we initialized the basic embeddings e_i\u2019s and the embedding matrix W_emb with GloVe vectors that were trained always on the 100% training data instead of the downsampled training data. This could have exaggerated the AUC of both models. Therefore we conducted a correct experiment where the initialization was done with GloVe vectors trained on the downsampled training data. We found that GRAM+\u2019s performance did not show noticeable difference, but RNN+\u2019s performance dropped approximately 0.5-1% AUC.\n","model":"human","source":"peerread","label":0,"id":4724}
{"text":"This paper addresses the problem of data sparsity in the healthcare domain by leveraging hierarchies of medical concepts organized in ontologies. The paper focuses on sequential prediction given a patient\u2019s medical record (a sequence of medical codes, some of which might occur very rarely). Instead of simply assigning each medical code an independent embedding before feeding it to an RNN, the proposed approach assigns each node in the medical ontology a \u201cbasic\u201d embedding, and composes a \u201cfinal\u201d embedding for each medical code by taking a learned weighted average (via an attention mechanism) of the medical code\u2019s ancestors in the ontology. Notably, the paper is well written and the approach is quite intuitive.\nI have the following comments: \n- Why is the patient\u2019s visit taken as just the sum of medical codes found in the visit, and not say the average or a learned weighted average? Wouldn\u2019t this bias for\/against the number of codes in the visit?\n- I don\u2019t see why basic embeddings are not fine tuned as well. Did you find that to hurt performance? Do you have an explanation for that?\n- Looking at Figure 2, the results seem very close and the figures are not very clear (figure (b) top is missing). Also, I am wondering how significant the differences are so it would be nice to comment on that.\nFinally, I think this is an interesting application paper applying well-established deep learning techniques. The paper deals with an important issue that arises when applying deep learning models in domains with scarce data resources. However, I would like the authors to comment on what there paper offers as new insights to the ICLR community and why they think ICLR is a good avenue for their work.\n","model":"human","source":"peerread","label":0,"id":4725}
{"text":"Nice paper, I have one comment however. \n\nI read your KDD 2016 paper about \"Multi-layer Representation Learning for Medical Concepts\" where you presented a Med2vec model to map patients' visits and codes into a different space. You showed that the new representation is effective and interpretable. \n\nYour current submission to ICLR is very similar to your KDD paper except that you have regularized the model using DAG of concepts as a prior knowledge. \n1) Am I right on that? \n2) why you did not compare your current model to Med2vec? You can use Med2vec as input to any other model such as RNN or CNN for classification. This case, we can have a clear picture about the contribution of the prior knowledge to the model accuracy. If there is no significant difference in the results then why we would care about the prior knowledge. ","model":"human","source":"peerread","label":0,"id":4726}
{"text":"I read the authors' response and maintain my rating.\n\n---\n\nThis paper introduces an approach for integrating a direct acyclic graph structure of the data into word \/ code embeddings, in order to leverage domain knowledge and thus help train an RNN with scarce data. It is applied to codes of medical visits. Each code is part of an ontology, which can be represented by a DAG, where codes correspond to leaf nodes, and where different codes may share common ancestors (non-leaf nodes) in the DAG. Instead of embedding merely the leaf nodes, one can also embed the non-leaf nodes, and the embeddings of the code and its ancestors can be combined using a convex sum. That convex sum can be seen as an attention mechanism over the representation. The attention weights depend on the embeddings and the weights of an MLP, meaning that the model can separate learning the code embeddings and the interaction between the codes. Embedding codes are pretrained using GloVe, then fine-tuned.\n\nThe model is properly evaluated on two medical datasets, with several variations to isolate the contribution of the DAG (GRAM or GRAM+ vs. RNN or RandomDAG) and of pretraining the embeddings (RNN+ vs RNN, GRAM+ vs GRAM). Both are shown to help achieve the best performance and the evaluation methodology seems thorough.\n\nThe paper is also well written, and the case for MLP attention instead of a plain dot product of embeddings was made by the authors.\n\nMy only two comments would be:\n1) Why is there a softmax in equation 4, given that the loss is multivariate cross-entropy (in the predicted visit, several codes could be equal to 1), not a a single-class cross-entropy?\n2) What is the embedding dimension m?","model":"human","source":"peerread","label":0,"id":4727}
{"text":"SUMMARY.\nThis paper presents a method for enriching medical concepts with their parent nodes in an ontology.\nThe method employs an attention mechanism over the parent nodes of a medical concept to create a richer representation of the concept itself.\nThe rationale of this is that for  infrequent medical concepts the attention mechanism will rely more on general concepts, higher in the ontology hierarchy, while for frequent ones will focus on the specific concept.\nThe attention mechanism is trained together with a recurrent neural network and the model accuracy is tested on two tasks.\nThe first task aims at prediction the diagnosis categories at each time step, while the second task aims at predicting whether or not a heart failure is likely to happen after the T-th step.\n\nResults shows that the proposed model works well in condition of data insufficiency.\n\n----------\n\nOVERALL JUDGMENT\nThe proposed model is simple but interesting.\nThe ideas presented are worth to expand but there are also some points where the authors could have done better.\nThe learning of the representation of concepts in the ontology is a bit naive, for example the authors could have used some kind of knowledge base factorization approach to learn the concepts, or some graph convolutional approach.\nI do not see why the the very general factorization methods for knowledge bases do not apply in the case of ontology learning.\nI also found strange that the representation of leaves are fine tuned while the inner nodes are not, it is a specific reason to do so?\n\nRegarding the presentation, the paper is clear and the qualitative evaluation is insightful.\n\n\n----------\n\nDETAILED COMMENTS\n\nFigure 2. Please use the same image format with the same resolution.\n\n","model":"human","source":"peerread","label":0,"id":4728}
{"text":"The authors propose a simple idea. They penalize confident predictions by using the entropy of the predictive distribution as a regularizer. The authors consider two variations on this idea. In one, they penalize the divergence from the uniform distribution. In the other variation, they penalize distance from the base rates. They term this variation \"unigram\" but I find the name odd as I've never seen multi-class labels described as unigrams before. What would a bigram be? \n\nThe idea is simple,  and while it's been used in the context of reinforcement learning, it hasn't been popularized as a regularizer for improving generalization in supervised learning. \n\nThe justifications for the idea still lacks analysis. And the author responses comparing it to L2 regularization have some holes. A simple number line example with polynomial regression makes clear how L2 regularization could prevent a model from badly overfitting to accommodate every data point. In contrast, it seems trivial to fit every data point and satisfy arbitrarily high entropy. Of course, the un-regularized optimization is to maximize log likelihood, not simply to maximize accuracy.  And perhaps something interesting may be happening at the interplay between the log likelihood objective and the regularization objective. But the paper doesn't indicate precisely what.\n\nI could imagine the following scenario: when the network outputs probabilities near 0, it can get high loss (if the label is 1). The entropy regularization could be stabilizing the gradient, preventing sharp loss on outlier examples. The regularization then might owe mainly to faster convergence. Could the authors analyze the effect empirically, on the distribution of the gradient norms? \n\nThe strength of this paper is its empirical rigor. The authors take their idea and put it through its paces on a host of popular and classic benchmarks spanning CNNs and RNNs. It appears that on some datasets, especially language modeling, the confidence penalty outperforms label smoothing. \n\nAt present, I rate this paper as a borderline contribution but I'm open to revising my review pending further modifications. \n\nTypo:\nIn related work: \"Penalizing entropy\" - you mean penalizing low entropy","model":"human","source":"peerread","label":0,"id":4729}
{"text":"The reviewers agreed that the idea proposed in this paper is sensible and possibly very useful, and that the experiments are thorough with good results. However, they share strong doubts regarding the novelty of the proposed approach. Hopefully the discussion will help the authors refine this work. With a more thorough discussion of related ideas, such as entropy maximization, within the machine learning literature and a careful placement of this idea within that context, this could be a strong submission to a future conference.","model":"human","source":"peerread","label":0,"id":4730}
{"text":"The paper experimentally investigates a slightly modified version of label smoothing technique for neural network training, and reports results on various tasks. Such smoothing idea is not new, but was not investigated previously in wide range of machine learning tasks.\n\nComments:\nThe paper should report the state-of-the-art results for speech recognition tasks (TIMIT, WSJ), even if models are not directly comparable.\nThe error back-propagation of label smoothing through softmax is straightforward and efficient. Is there an efficient solution for BP of the entropy smoothing through softmax?\nAlthough the classification accuracy could remain the same, the model will not estimate the true posterior distribution with this kind of smoothing.\nThis might be an issue in complex machine learning problems where the decision is made on higher level and based on the posterior estimations, e.g. language models in speech recognition.\nMore motivation is necessary for the proposed smoothing.\n","model":"human","source":"peerread","label":0,"id":4731}
{"text":"Specifically, this paper suggests regularizing the estimator of a probability distribution to prefer high-entropy distributions.  This avoids overfitting.\n\nI generally like this idea.  Regularizing the behavior of the model often makes more sense than regularizing its parameters.  After all, the behavior is interpretable, whereas the parameters are uninterpretable and work together in mysterious ways to produce the behavior.  So one might be able to choose a more sensible prior over the behavior.  In other words, prefer parameters not because they are individually close to 0 but because they jointly lead to a distribution that is plausible or low-risk a priori.\n\nPro: I believe that the idea is natural and sound (that is, I do not share the doubts of AnonReviewer5).\n\nPro: It's possible that this hasn't been well-explored yet in neural networks (not sure).\n\nPro: The experimental results look good.  So maybe everyone should use this kind of regularizer. \n\nCon: It is a kind of pollution of the scientific literature to introduce this idea to the community as if it were unconnected to (almost) anything else in machine learning.  There are many, many papers that include a scaled entropy term in the optimization objective!  It's not just for reinforcement learning.  Please see the long list of connections in my pre-review questions \/ comments.  \n\nCon: Experimental results should always be accompanied by significance tests and error analysis.  Is your trained model actually doing better on the distribution of test data, or was your test set too small to tell?  Are the improvements robust across many different training sets?  What errors does your model fix, and what errors does it introduce?  \n\nSummary recommendation: Revise and resubmit.  ICLR has lots of submissions.  I would prefer to reward authors who not only tried something, but who properly contextualized it and carefully evaluated it.  Otherwise, there's a race to the bottom where everyone wants to be the first to try something, so that readers are confronted with a confusing sea of slapdash papers with unclear relationships.","model":"human","source":"peerread","label":0,"id":4732}
{"text":"The authors propose a simple idea. They penalize confident predictions by using the entropy of the predictive distribution as a regularizer. The authors consider two variations on this idea. In one, they penalize the divergence from the uniform distribution. In the other variation, they penalize distance from the base rates. They term this variation \"unigram\" but I find the name odd as I've never seen multi-class labels described as unigrams before. What would a bigram be? \n\nThe idea is simple,  and while it's been used in the context of reinforcement learning, it hasn't been popularized as a regularizer for improving generalization in supervised learning. \n\nThe justifications for the idea still lacks analysis. And the author responses comparing it to L2 regularization have some holes. A simple number line example with polynomial regression makes clear how L2 regularization could prevent a model from badly overfitting to accommodate every data point. In contrast, it seems trivial to fit every data point and satisfy arbitrarily high entropy. Of course, the un-regularized optimization is to maximize log likelihood, not simply to maximize accuracy.  And perhaps something interesting may be happening at the interplay between the log likelihood objective and the regularization objective. But the paper doesn't indicate precisely what.\n\nI could imagine the following scenario: when the network outputs probabilities near 0, it can get high loss (if the label is 1). The entropy regularization could be stabilizing the gradient, preventing sharp loss on outlier examples. The regularization then might owe mainly to faster convergence. Could the authors analyze the effect empirically, on the distribution of the gradient norms? \n\nThe strength of this paper is its empirical rigor. The authors take their idea and put it through its paces on a host of popular and classic benchmarks spanning CNNs and RNNs. It appears that on some datasets, especially language modeling, the confidence penalty outperforms label smoothing. \n\nAt present, I rate this paper as a borderline contribution but I'm open to revising my review pending further modifications. \n\nTypo:\nIn related work: \"Penalizing entropy\" - you mean penalizing low entropy\n","model":"human","source":"peerread","label":0,"id":4733}
{"text":"Overall, this is a nice paper. Developing a unifying framework for these newer\nneural models is a worthwhile endeavor.\n\nHowever, it's unclear if the DRAGNN framework (in its current form) is a\nsignificant standalone contribution. The main idea is straightforward: use a\ntransition system to unroll a computation graph. When you implement models in\nthis way you can reuse code because modules can be mixed and matched. This is\nnice, but (in my opinion) is just good software engineering, not machine \nlearning research.\n\nMoreover, there appears to be little incentive to use DRAGNN, as there are no\n'free things' (benefits) that you get by using the framework. For example:\n\n- If you write your neuralnet in an automatic differentiation library (e.g.,\n  tensorflow or dynet) you get gradients for 'free'.\n\n- In the VW framework, there are efficiency tricks that 'the credit assignment\n  compiler' provides for you, which would be tedious to implement on your\n  own. There is also a variety of algorithms for training the model in a\n  principled way (i.e., without exposure bias).\n\nI don't feel that my question about the limitations of the framework has been\nsatisfactorily addressed. Let me ask it in a different way: Can you give me\nexamples of a few models that I can't (nicely) express in the DRAGNN framework?\nWhat if I wanted to implement","model":"human","source":"peerread","label":0,"id":4734}
{"text":"This work proved to be a controversial submission. This paper has two main components: 1) a neural framework TBRU\/DRAGNN, 2) experimental results on some NLP tasks. Generally there was lack of consensus about the originality of (1) and a general feeling that even if there are aspect of novelties, that the paper was lacking clarity about is contributions One reviewer was an outlier, highlighting the benefit of the ability \"incorporate dynamic recurrent connections through the definition of the transition system\" which is claimed to be really novel. Others claim this is specific to the framework used. Negative reviewers felt that (1) is probably not novel within itself and represents a slight departure from stack-lstm. The controversy here is whether DRAGNN is simple \"software engineering with no inherent \"free things\" that would lead to impact within the community. This question of impact is also inherent to (2), in particular whether the authors really got new benefit of using DragNN or whether these are \"reimplementations of things in the literature\". I felt the reviewers did seem to put in due diligence here, so the recommendation would be to put further effort into clarification and further back up of novelty claims in future versions.","model":"human","source":"peerread","label":0,"id":4735}
{"text":"The paper proposes a new neural architecture, called DRAGNN, for the transition-based framework. A DRAGNN uses TBRUs which are neural units to compute hidden activations for the current state of a transition-based system. The paper proves that DRAGNNs can cover a wide range of transition-based methods in the literature. In addition, one can easily implement multitask learning systems with DRAGNNs. The experimental results shows that using DRAGNNs the authors built (near) state-of-the-art systems for 2 tasks: parsing and summarization. \n\nThe paper contains two major parts: DRAGNN and demonstrations of its usages. \n\nRegarding to the first part, the proposed DRAGNN is a neat tool for building any transition-based systems. However, it is difficult to say whether the DRAGNN is novel. Transition-based framework is already well defined and there's a huge trend in NLP using neural networks to implement transition-based systems. In my opinion, the difference between the Stack-LSTM (Dyer et al., 2015) and DRAGNN is slight. Of course, the DRAGNN is a powerful architecture but the contribution here should be considered mainly in terms of software engineering.\n\nIn the second part, the authors used DRAGNN to implement new transition-based systems for different (multi-)tasks. The implementations are neat, confirming that DRAGNN is a powerful architecture, especially for multitask learning. However, we should bear in mind that the solutions employed are already there in the literature, thus making difficult to judge the novelty of this part w.r.t. the theme of the conference.  ","model":"human","source":"peerread","label":0,"id":4736}
{"text":"The authors present a general framework for defining a wide variety of recurrent neural network architectures, including seq2seq models, tree-structured models, attention, and a new family of dynamically connected architectures. The framework defines a new, general-purpose recurrent unit called the TBRU, which takes a transition system, defining and constraining its inputs and outputs, and input function which defines the mapping between raw inputs and fixed-width vector representations, and recurrence function that defines the inputs to each recurrent step as a function of the current state, and an RNN cell that computes the output from the input (fixed and recurrent). Many example instantiations of this framework are provided, including sequential tagging RNNs, Google\u2019s Parsey McParseface parser, encoder\/decoder networks, tree LSTMs and less familiar examples that demonstrate the power this framework. \n\nThe most interesting contribution of this work is the ease by which it can be used to incorporate dynamic recurrent connections through the definition of the transition system. In particular, this paper explores the application of these dynamic connections to syntactic dependency parsing, both as a standalone task, and by multitasking parsing with extractive summarization, using the same compositional phrase representations as features for the parser and summarization (previous work used discrete parse features), which is particularly simple\/elegant in this framework. In experimental results, the authors demonstrate that such multitasking leads to more accurate summarization models, and using the framework to incorporate more structure into existing parsing models also leads to increased accuracy with no big-oh efficiency loss (compared with e.g. attention). \n\nThe \u201craison d\u2019etre,\u201d in particular the example, perhaps described even more thoroughly\/explicitly, should be made as clear as possible as soon as possible. This is the most important contribution, but it gets lost in the description and presentation as a framework \u2014 emphasizing that attention, seq2seq, etc can be represented in the framework is distracting and makes it seem less novel than it is. AnonReviewer6 clearly missed this point, as did I in my first pass over the paper. To get this idea across and to emphasize the benefits of this representation, I\u2019d love to see more detailed analysis of these representations and their importance to achieving your experimental results. I think it would also be helpful to emphasize the difference between a stack LSTM and Example 6. \n\nOverall I think this paper presents a valuable contribution, though the exposition could be improved and analysis of experimental results expanded. ","model":"human","source":"peerread","label":0,"id":4737}
{"text":"Overall, this is a nice paper. Developing a unifying framework for these newer\nneural models is a worthwhile endeavor.\n\nHowever, it's unclear if the DRAGNN framework (in its current form) is a\nsignificant standalone contribution. The main idea is straightforward: use a\ntransition system to unroll a computation graph. When you implement models in\nthis way you can reuse code because modules can be mixed and matched. This is\nnice, but (in my opinion) is just good software engineering, not machine \nlearning research.\n\nMoreover, there appears to be little incentive to use DRAGNN, as there are no\n'free things' (benefits) that you get by using the framework. For example:\n\n- If you write your neuralnet in an automatic differentiation library (e.g.,\n  tensorflow or dynet) you get gradients for 'free'.\n\n- In the VW framework, there are efficiency tricks that 'the credit assignment\n  compiler' provides for you, which would be tedious to implement on your\n  own. There is also a variety of algorithms for training the model in a\n  principled way (i.e., without exposure bias).\n\nI don't feel that my question about the limitations of the framework has been\nsatisfactorily addressed. Let me ask it in a different way: Can you give me\nexamples of a few models that I can't (nicely) express in the DRAGNN framework?\nWhat if I wanted to implement ","model":"human","source":"peerread","label":0,"id":4738}
{"text":"This paper proposes an alternative to Conditional Variational Auto-Encoders and Conditional MultiModal Auto-Encoders to perform inference of missing modalities in dataset with multiple modalities. The proposed approach is a Variational Auto-Encoder jointly on all the modalities  with additional KL divergence penalties between the approximate posterior given all the modalities and the approximate posterior given a subset of the modalities. The approach is named Joint Multimodal Variational Auto-Encoder.\nThe authors make a connection between this approach and the Variation of Information. It is unclear why the authors chose the JMVAE approach instead of a more elegant Variation of Information approach.\nAnother unaddressed issue is the scalability of the method. As far as I can tell (given that no code is provided and the specification of the encoder is missing), this approach requires a new encoder per subset of missing modalities. Right now this approach seems to scale since there are only two modalities.\nThe fact that the estimating the log-likelihood log(p(x)) using multiple modalities provide a lower value than with just one in Table 1 is a bit odd. Could you explain that ?\nThe comparison with between the representation learned by JMVAE and CVAE might be unfair given that the representation of CVAE is learned conditionally, on the label in the case of MNIST, and should therefore not consider the label in this representation. Intuitively, this representation could represent \"style\" as shown in (Kingma et al., 2014) in their conditional generation figure.\nFor CelebA, comparing log-likelihood on models that use GANs is probably not significant since GAN does not optimizes log-likelihood. \nOverall this is an interesting problem and there are also interesting ideas worth exploring further, but the execution of the paper requires more work.","model":"human","source":"peerread","label":0,"id":4739}
{"text":"This paper addresses the importance task of learning generative models of multiple modalities. There are two concerns about the paper: limited novelty, which will not have sufficient impact; ineffectiveness of evaluation. The paper extends VAEs in an interesting way, but this extension on its own does provide sufficient new insight understanding. And the log-likelihood evaluations and data sets are not enough to be convincing. As a result, the paper is not yet ready for acceptance at the conference.","model":"human","source":"peerread","label":0,"id":4740}
{"text":"The paper introduces the joint multimodal variational autoencoder, a directed graphical model for modeling multimodal data with latent variable. the model is rather straightforward extension of standard VAE where two data modalities are generated from a shared latent representation independently. In order to deal with missing input modalities or bi-directional inference between two modalities the paper introduces modality-specific encoder that is trained to minimize the KL divergence of latent variable distributions between joint and modality-specific recognition networks. The paper demonstrates its effectiveness on MNIST and CelebA datasets, both in terms of test log-likelihoods and the conditional image generation and editing.\n\nThe proposed method is rather straightforward extension of VAE and therefore the model should inherent the probabilistic inference methods of VAE. For example, for missing data modalities, the model should be able to infer joint representation as well as filling in the missing modalities via iterative sampling as introduced by Rezende et al. (2014). Given marginal improvement, I am not convinced by the contribution of modality-specific encoders in Section 3.3. In addition, the inference methods introduced for generating Figure 5 looks somewhat unprincipled; I am wondering the conditional image generation results by following more principled approach (e.g., iterative sampling). Experimental results on joint image-attribute generation is also missing.","model":"human","source":"peerread","label":0,"id":4741}
{"text":"The proposed method of modeling multimodal datasets is a VAE with an inference network for every combination of missing and present modalities. The method is evaluated on modeling MNIST and CelebA datasets.\n\nMNIST is hardly a multimodal dataset. The authors propose to use the labels as a separate modality that gets modeled with a variational autoencoder. The reviewer finds this choice perplexing.\nEven then the modalities are never actually missing, so the applicability of the suggested method is questionable.\nIn addition the differences in log-likelihoods between different models are tiny, and likely to be due to noise.\n\nThe other experiment reports log-likelihood of models that were not trained to maximize log-likelihood. It is not clear what conclusions can be drawn from such comparison.","model":"human","source":"peerread","label":0,"id":4742}
{"text":"Have you considered the following related work : \"Multi-modal Auto-Encoders as Joint Estimators for\nRobotics Scene Understanding\" Cadena et al. ? Could you explain how your method differs from theirs ?\n\nThanks,\n\nDiane","model":"human","source":"peerread","label":0,"id":4743}
{"text":"Summary of the paper\n\nThe paper studies the invertiblity of convolutional neural network in the random model. A reconstruction algorithm similar to IHT is proposed for layer-wise inversion of the network.\n \n\nClarity:\n\n- The paper is confusing wrt to standard notations in deep learning.\n\nComments:\n\nThe paper makes two simplifications in the analysis of a CNN, that makes it map to a model based compressive sensing framework:\n\n1-  The non linearity (RELU) is dropped. This is a big simplification, for random gaussian weights for instance we know by JL that we can preserve L_2 distance, when RELU is applied the metric changes (see for instance the kernel for n=1 in","model":"human","source":"peerread","label":0,"id":4744}
{"text":"While the reviewers found some interest in this work, I'm afraid I have to agree with the critique that the model studied is too simple that its relevance for deep learning is questionable.","model":"human","source":"peerread","label":0,"id":4745}
{"text":"The paper proposes to provide a theoretical explanation for why deep convolutional neural networks are invertible (at-least, when going back from certain intermediate layers to the image itself). It does so by considering the invertibility of a single layer, assuming the convolutional filters essentially correspond to incoherent measurements satisfying RIP.\n\nIn my opinion, while this is an interesting direction of research, the paper is not ready for publication. I feel the treatment does not go sufficiently towards explaining the phenomenon in deep neural networks. Even after reading the response from the authors, I feel the results are only a minor variation of the standard results from compressive sensing for sparse reconstruction with incoherent measurements.\n\nA deep neural network is fundamentally different from a single layer---it is the \"deep\" part that makes the forward task work. As the authors note, there is significant deterioration when IHT is applied recursively----therefore, at best the theory explains the partial invertibility of a single layer. That a single layer is approximately invertible isn't surprising, that a cascade of layers *is*.\n\nFor any theoretical analysis of this phenomenon to be useful, I believe it must go beyond analyzing a single compressive measurement-type layer, and try to explain how much of the same theory holds for a cascade. I say this because it's entirely possible that the sparse recovery theory breaks down beyond a single layer, and invertibility ends up being a property caused by correlations between the weights of different layers. In other words, there is no way to tell from the current results for individual layers whether they are in fact a step towards explaining the invertibility of whole networks.\n","model":"human","source":"peerread","label":0,"id":4746}
{"text":"\nThe authors propose a theoretical framework to analyze the recoverability of sparse activations in intermediate layers of deep networks, using theoretical tools from compressed sensing. They relate the computations that are performed by a CNN and a particular recovery algorithm (Iterative Hard Thresholding, IHT). They present proofs of necessary conditions for recoverability to hold, and also show detailed empirical evidence of how they hold in practice.\n\nThis is a well-written paper that presents a new angle on why the current CNN architectures work so well. The authors give a brief but sufficient review of the fundamentals of compressed sensing, present their main result relating feed-forward networks and IHT (a surprising result), and progress naturally to a detailed experimental section. The introductory analysis at the beginning of Section 3, in particular, delivers the gist of why the method should work with very approachable and simple math, which is not common in theoretical papers. The increasing complexity of the experiments, done in small steps, shows a nice progression from artificial distributions to a realistic experiment.\n\nA few aspects should be improved. First of all, although the treatment of ReLU non-linearities is sufficient, it is assumed with little discussion that max-pooling non-linearities shouldn't present a problem as well. A discussion of how this is inverted (e.g., with pooling switches) is needed.\n\nThe relationship between feed-forward nets and Algorithm 1 assumes tied weights. It might be worthwhile to mention that the result is stronger for the case of RNNs, where this is the case by design.\n\nAlthough it might be obvious, it might help some readers to briefly note that the reconstruction algorithm is meant to be applied to each layer sequentially, basing the activations of each layer on the one above it (in back-propagation order).\n\nFinally, the filter coherence measure must be defined either mathematically or with a proper reference.\n\n","model":"human","source":"peerread","label":0,"id":4747}
{"text":"\n\n\nSummary of the paper\n\nThe paper studies the invertiblity of convolutional neural network in the random model. A reconstruction algorithm similar to IHT is proposed for layer-wise inversion of the network.\n \n\nClarity:\n\n- The paper is confusing wrt to standard notations in deep learning.\n\nComments:\n\nThe paper makes two simplifications in the analysis of a CNN, that makes it map to a model based compressive sensing framework:\n\n1-  The non linearity (RELU) is dropped. This is a big simplification, for random gaussian weights for instance we know by JL that we can preserve L_2 distance, when RELU is applied the metric changes (see for instance the kernel for n=1 in  ","model":"human","source":"peerread","label":0,"id":4748}
{"text":"Section 4 Continue:\nThe experiments in 4.5 are misleading and together with 4.4 they are contradictory to their theoretical results. \nForemost, the readers need to be aware that the decoders with switch units are very powerful (if you ever trained one you would know it too). Therefore what figure 4 presents is extremely misleading because it *only* takes the very last conv layer and use their proposed linear reconstruction algorithm to recover that layer\u2019s input only.  The rest was propagated with the very powerful decoder plus switch units information (see [6] for how powerful the decoder is).  Visually, we can also see how much info switch units carry: for their conv 5 reconstruction with random activation, there is no meaningful info carried by the activation values but all through switch units and we can still obtain the silhouette of the objects in the original images; in fact, pool-1\u2019s switch units alone carries at least 64x224x224 bit information. Their results back in the appendix (Figure5) also shows their reconstruction algorithm\u2019s weakness. The lower layer has extremely bad reconstruction quality from the proposed algorithms. The higher the layer, the better the reconstruction from their proposed algorithms because decoder together switch units has more chance to correct the reconstruction.What the authors should do, is to use their IHT to reconstruct all the way, following what has been done in [1]\u2019s linear reconstruction algorithm, which the authors here failed to compare against, despite its high relevance. In fact, the author of [1] used to be in charge of this submitted project and proposed this experimental approach, with a slightly different algorithm that is presented in [1]; but current authors of this submission failed in achieving similar reconstruction results with their algorithm and hence can only perform their reconstruction algorithm over a single layer. The biggest difference between the reconstruction algorithm from [1] and this submission is that the former used pseudo-inverse of the weight matrix and the latter used the transpose because in their theory, the weight matrix is approximate orthogonal hence its transpose is approximately its inverse.\n There is another serious issue with using high-up conv layer to verify their theoretical claim: recall that the excuse this submission used to discard ReLU non-linearity is that [1] introduced the pairing phenomenon, however, [1] stressed that it only appeared in the first few layers. Hence the authors should have at least chosen the first few layers for their experiments; yet, they knowingly only presented their analysis of the highest layer. \nTable 3 again presents very misleading results. First their \u201crandom activation\u201d has a relative error 1.414 for activation space. But if we simply pick zero vectors instead of \u201crandom\u201d, it gives 1 as a relative error, which is much smaller than 1.414. Therefore, this \u201crelative error\u201d in the activation space is a very inappropriate measurement for the reconstruction quality from their algorithm. But measuring the relative error in the input space is also a bad metric because of the powerful effects from the decoders. In fact, the term \u201crelative error\u201d is a paraphrase of the evaluation method used in [1], where it is called \u201creconstruction ratio\u201d. However, it is a much more appropriate measurement coupling the reconstruction algorithm used in [1] because the reconstruction from [1] will always be a subset of the original input by using the pseudo-inverse.\n Recall that in 4.4, the authors approximate the conv weight matrix for conv(5,2) has approximately 0.05-0.1 distortion constant, which means that the \u201crelative error\u201d based on their Theorem 3.3 is 0.6-0.2, but in reality, the relative error is 1.051, which is not only worse than their theoretical guarantee but even worse than simply choosing zero vector as reconstruction. Keep in mind that these results are built on top of the assumption that ReLU does not exist, if we add it on top, these results in Table3 and Figure5 can get further downgraded. One question why there is such a huge gap. In fact, such bad reconstruction results based on their algorithm is not surprising because after all, unlike random weights, the learned conv weights are not approximately orthogonal so Model-RIP does not fit into realistic CNN models. In fact, the coherence is a very direct measurement of how \u201corthogonal\u201d the learned weights are, which both this submission and [5] (again this submission took over [5]\u2019s idea) measure this quantity: the higher the coherence the less orthogonal.  \n","model":"human","source":"peerread","label":0,"id":4749}
{"text":"Section 4, experiments: This section contains so much misleading information to misguide the audience.\n The issues with 4.1 has been discussed in Section2(1).\nThe experiments in 4.4 are very deceiving.The minor misleading part is the title of Figure 3(b), ( c)  saying \u201cbefore\/after ReLU\u201d, note the ReLU they refer to is at the input level not the *activation* level. In other words, they did not consider ReLU after the linear responses,  and only compare whether the input to this conv layer goes through ReLU or not, which does not add any difficulty to their theoretical analysis, because they assumed the input comes from the span of the conv weights, which is also [1] and [5]\u2019s assumption. However, the most serious issue is how they collect the results for Figure 3. I tried to replicate their result for conv(1,2) because the authors claim that \u201cwe choose conv(5, 2), while other layers show similar results\u201d. Precisely, I pass randomly sampled ImageNet Validation images through VGG-16, take the activation of conv(1,2) after max-pooling to obtain c, and then reconstruct conv(1,1) activation by multiplying c with the transpose of the conv weights W^T after subtracting the bias to obtain the reconstruction (W^T*c), and calculate the term, ||W^T c||\/||c||.  However, the norm ratio I obtained is 3.16 +- 0.10, very far from their 1 +- 0.1.  My experiments give a distortion constant \\delta strictly bigger than 2, which is out of the range of 0 and 1 hence a meaningless value. This gap between our result and their result can potentially originate from the following causes: the authors using fancier sparse approximation method for c instead of following the procedures of feedforward CNNs, the authors discarding ReLU on the linear response therefore their c has negative components, and the authors have bugs in their code.  \n","model":"human","source":"peerread","label":0,"id":4750}
{"text":"Section 3 Continue:\nThere are again several issues with their proof to the reconstruction bounds. Lemma B.3 and B.4 carry out a very crude estimation on \\|c-h\\|, since the author discard any potential contribution of filters activated from max-pooling but belong to the set of original filters that form the input signal x. That is, \\|c_{\\Omega^c}\\| can potentially be covered by \\|h_{\\Pi - \\Omega}\\|. Intuitively, if max-pooling chooses to keep a shifted filter positive and the rest zero and if the shifts of filters are highly correlated, then even if the activated filter is not one of the original filters that constitute x, this activated filter still potentially cover x along the direction of the correct filter and therefore too much information can be lost without taking into such contribution into consideration. Moreover, the logic (i.e. proof technique) behind the reconstruction bounds (Theorem 3.3) is the same as that in Theorem A.5 from [1]. The idea is to decompose the l_2 norm of the reconstruction (or reconstruction signal) into \u201coriginal\/ground truth\u201d +  \u201cdifference\u201d  and then bound the reconstruction ratio [1] (this term is renamed as \u201crelative error\u201d in this work) with the aid of the singular values of the weight matrix W. But again, the authors failed to address the similarity but rather attempt to present the proof as different as possible to create artificial novelty.  Finally, their final bound is very suboptimal and only consider 2 max-pooling regions. \n The entire section 3 is conducted under the assumption of linear responses. Because the compressed sensing model, Model-RIP, is fundamentally a linear model. The formulation (see equation1) only bounds the norm but not the metric. In other words, (1-\\delta)|x|<|Wx|<(1+\\delta)|x| is what the authors attempt to prove (but with flaws), i.e. a norm preserving property between the input and its linear responses\u2014this has been discussed in [5] but the authors did not cite or relate to this work\u2014despite [5] informs the authors to explain the similarity during their previous submission attempt. However, preserving linear norms do not give any reconstruction guarantee because of ReLU can theoretically ruin the upper bound, by, for example, zeroing all activations if they are all negative.\nthe authors cannot draw any conclusion from their theoretical results to any implication in training or network design, all they concluded is the last paragraph on page 14, which is rather meaningless.  \n","model":"human","source":"peerread","label":0,"id":4751}
{"text":"Section 3, Analysis: This section contains many contents that are either a copy from [1]&[5] or invalid.\nProposition 1 is a trivial special case of Proposition A.1 from [1], because the pseudo-inverse of an orthogonal matrix is simply its transpose, which is the set up in Prop 1. Also since Prop 1 assumes that \\Phi spans the entire input space, then range(W) easily becomes R^(M*D), however this assumption is generally invalid, which was empirically verified by Table S.1 from [1]\u2019s appendix for the case of deep CNNs for CIFAR-10\/100. The authors clearly copied [1]\u2019s idea and decide to present Prop 1 since they know [1] very well, but they did not address this copying action. They should have cited [1]\u2019s Prop A.1, then present Porp 1 as a corollary and explain their set-up is a simplified version with no novel technicality. \n Following Proposition 1, the author cited [1] (the only time they mentioned it throughout this submission) and claims that based on [1]\u2019s result they can assume linear responses (\u201cassuming that all of the entries in the vectors are real numbers, rather than only non-negative.\u201d) However, [1] never claims that linear responses are acceptable (the authors of [1] in fact even did experiments with no non-linearity and the results significantly dropped) and [1] finds that only the first few convolution convolutional layers appear such pairing phenomenon. But in this submission, the experimental section focuses on the last convolutional layer of VGGNet, which by no means has such negative and positive pairing property. Therefore, their arguments of leaving out ReLU and assuming linear responses are groundless.\nThe authors fail to address convolution in their proof of Theorem 3.1 which resulting the proof to the convolution layer is no different from fully connected layer. More specifically, when proving theorem 3.1, the authors need Lemma A.1, however there is a severe issue with the proof of Lemma A.1\u2014it does not properly take convolution into consideration. Note that they assume the \u201cshift\u201d of the same convolution filter has expected dot product 0 (E((w_{I,m1}^j_1)T(w_{I,m1}^{j2})) = 0 if j_1 \\neq j_2, this assumption only holds if the convolution filters are of high frequency but it is a common knowledge that real convolution filters are more often smooth than highly oscillatory, and the dot products between shifts by a small number of pixels can be large, opposite from being 0 as the authors claim here. Their assumption contradicts the important fact that convoluted smooth filters stay highly correlated. Also, there is a constant C that is not tracked in Theorem 3.1 \u2014only a \u201cthere exists\u201d in a mathematical sense. Not to mention the whole proof is based on random filters which, we stress again, is far away from being realistic. \n","model":"human","source":"peerread","label":0,"id":4752}
{"text":"Section 2, Preliminaries:\nThere are serious issues with the random filter assumption (Section 2.1 and Section 4.1 are addressed together). First of all, we all know very well that random filters have extremely limited ability, especially with very deep networks. No one has in any way successfully applied random filters to deep networks. Second of all, their one-layer results in Table 1 is very off. For instance, [4] used one-layer unsupervised learning to train 1600 conv filters (this submission uses 2048 and 1024 conv filters) that achieved 77.9% accuracy on CIFAR-10, versus their 66% (random)\/68% (learned), despite their architecture is far more complicated than [4], which ironically is far beyond the scope of their theoretical setup. But the most inappropriate component is the number of convolutional filters they used. None of the commonly used deep convolutional networks carry these many filters--for their first layer, there are about 30 times more filters than the dimension of its convolutional kernel! Recall that the most valuable part of convolutional layer is that it can use a small number of conv filters to extract abstract features by stacking many of them together. Unsurprisingly, as a consequence of using excessive amount of filters in order to span as much input space as possible, their models carry a huge number of parameters, an architecture design that is totally against the principles of CNNs. Finally, Table 4 shows, random filters can form a weight matrix that is a lot more \u201corthogonal\u201d than learned filters. And such low coherence, i.e. \u201corthogonal\u201d property of the weight matrix W, is the backbone of their Model-RIP. Thus some simple, unconvincing shallow networks do not manage to fill in the gap between random filters and learned filters, especially when this gap breaks their primary theoretical promise. \n Notation in 2.2: The notations introduced in 2.2 are identical to the notations and problem set up used in [1]&[5] except that in [1]&[5], M = 1, which does not change the theoretical fundamentals. In addition, the terminologies such as \u201cshift\u201d, \u201cblock\u201d are also shared between [1]&[5] and this submission.\nThere are serious issues with the Model-RIP setup: the model-RIP mathematical model is in fact nothing fancy. The only difference between model-RIP and a generic compressed sensing RIP model is that model-RIP requires model-sparsity. To be more straightforward, in this case, the model-sparsity corresponds with max-pooling\u2014meaning sparsity is structured so that each pooling region only has at most 1 non-zero element and correspondingly, the assumption on the data space is that it is a linear combination of filters such that at most one filter from each pooling region is used. Note that this is the same assumption as appeared in [1] (see Appendix, equation S1), in fact, this assumption was originally proposed by the first author of [1]. However, the fundamental flaw of this model-RIP setup are first the lack of non-linearity, notice that if ReLU is added after the linear response then the activation can be very close to zero which makes the reconstruction has norm arbitrarily small\/close to zero; second real weight matrix W cannot meet the conditions that random matrix can meet, in other words, real W is not even close to being \u201corthogonal\u201d, even the authors give support along this direction (see Table 4). Hence the actual values of the reconstruction bounds in reality are far from being significant. \n(4)  IHT: This algorithm, again, does not take into any non-linearity on the activation into account.\n ","model":"human","source":"peerread","label":0,"id":4753}
{"text":"Section 1, Introduction:\nA neural network that can be shown theoretically of high invertibility does not necessarily perform well. For example, ResNets over 34 layers, which are impossible to analyze using the proposed theoretical framework, perform significantly better than VGGNets with much fewer parameters but a lot more depth; by comparison, a standard variational autoencoder would have a lot worse generative power if we set the penalty for the prior KL-divergence term to be very small and encourage good reconstruction.\n \u201cDespite these interesting results, there is no clear theoretical explanations as to why CNNs are invertible yet\u201d. There are multiple issues with this statement. First of all, this invertibility is a rather trivial common knowledge among ML researchers; people don\u2019t like to analytically explore because its theoretical bounds are too bad\u2014as proved once again in this submission. Second of all, their theoretical proof does not consider non-linear layer and it does not have the correct convolutional setup (details to come at Section 3 (3)) and their theoretical bounds have no meaningful implications for training or network design. Third of all, there have been works exploring such signal recovery property with random filters on fully connected layers ([2] and [3]) as well as exploring such property with learned filters on convolutional layers ([1] and [5]). They all have very similar underlying mathematical models and highly resembling reconstruction algorithms ([1] and [2]) with this submission. The authors ignore addressing these important related works. Lastly, their analysis only tackles a single layer, which does not improve upon existing works, especially considering that [2] takes into account of multiple layers as well as dropout stability.\n","model":"human","source":"peerread","label":0,"id":4754}
{"text":"The invertibility of CNNs has been explored fairly extensively in recent years both empirically [6,7,8,9] and theoretically [1,2,3]. The most critical problem of this submission is \u201cthe amount of overlap\u201d to [1,5] in terms of theoretical results as well as approaches for experimental validation. Before providing a list of overlapping contents, we point out the most fundamental differences between these two works:\nThis submission assumes random filters for their theoretical results whereas [1]&[5] impose assumptions on the learned filters.\n[1,5] base their theoretical results (as well as experimental validation) on concatenated ReLU (CReLU) whereas this submission assumes linear responses for theoretical analysis yet their experimental setup uses ReLU networks.\n\n\nHere is a list of overlaps between these two works:\nThe notation defined in Section 2.2 is identical as in [1] and [5].\nProposition 1 has identical proof outline as Proposition A.1 in appendix of [1]. In fact, Prop.1 is a special case of Prop.A.1 [1] with more stringent constraints on the weight matrix.\n[5] introduced the equivalent theoretical results and called it \u201cnorm preserving\u201d (see Theorem 2.2 in [5]) instead of the RIP used in this submission.\nThe idea behind the proof of reconstruction bounds and the reconstruction algorithm is very similar to that of [1] with a minor difference that they use weight matrix transpose instead of inverse. The \u201crelative error\u201d used for evaluation in this submission is termed as \u201creconstruction ratio\u201d in [1].\nThe coherence of the convolution filters (Table 4) is measured to estimate the level of orthogonality of the weight matrix as is done in [5] (see Table 1 and 2 in [5]) and came to the same conclusion that weight matrix is in fact incoherent.\n\n\nIn conclusion this submission adds minor tweaks under rather unrealistic assumptions and misleading empirical analysis -- which the author of [1,5] is morally against -- without notifying her or explaining that the ideas and the proofs are originated from her. ","model":"human","source":"peerread","label":0,"id":4755}
{"text":"Summary:\nThe paper proposes to use Model-RIP, a simple concept from compressed sensing community, to theoretically describe a invertibility of a single convolution + max-pool operation (i.e., without non-linear activation function) assuming that the convolution filters are drawn from random Gaussian distribution. In the high level, they attempted to show that convolution weights consisting of random filters form a roughly orthogonal matrix, hence its transpose times the activation roughly reconstruct the input. Empirically, this submission tries to verify the model-RIP property of learned CNN filters by computing the norm ratio between the activation and the weight transpose multiplying the activation, and verify the reconstruction bounds by computing the relative errors between the input and the reconstructed input through the proposed reconstruction algorithm. Qualitatively, they also try to invert the activation to the previous layer output through the same algorithm then followed up by a learned decoder to reconstruct the raw input.\n\n\nHowever, there are many flaws in their assumptions but little useful consequence from enforcing CNNs to their mathematical model. The most evident flaws in their theorems are (1) the weight matrix of learned CNNs does not meet their model-RIP assumption (i.e., low coherence) and (2) even without the first flaw, their reconstruction bound doesn\u2019t hold analytically with ReLU non-linearity: RIP can only bound the norm of the transposed weights times the activation (see Eq. 1) instead of providing enough conditions for a metric-preserving property between input space and activation space [3] since ReLU zeroes out the negative activations, i.e., the activation can be arbitrarily close to zero. Furthermore, the presentations of their experimental results are very misleading and self-contradicting: a pre-trained decoder with switch units is responsible for most of the reconstruction and they only applied their proposed recon algorithm on one layer; the \u201crelative error\u201d does not give reasonable measurement of the reconstruction property, especially in the pixel space; the norm ratio ||W^Tc||\/||c|| is also an inappropriate measurement for model-RIP condition, not to mention the discrepancy between the presented norm ratio and the learned filter\u2019s coherence level (if ||W^Tc||\/||c|| is close to 1, coherence should be close to 0, but it is actually much larger than 0).  \n\n\n[1] W. Shang, K. Sohn, D. Almeida, and H. Lee. Understanding and improving convolutional neural networks via concatenated rectified linear units. In ICML, 2016.\u2028\n[2] S. Arora, Y. Liang, and T. Ma. Why are deep nets reversible: A simple theory, with implications for training.\n[3] R. Giryes, G. Sapiro, and A. M. Bronstein. Deep neural networks with random gaussian weights: A universal classification strategy? IEEE Transactions on Signal Processing, 2016.\u2028\n[4] A. Coates, and A. Ng. The importance of encoding versus training with sparse coding and vector quantization. In ICML, 2015. \n[5] W. Shang. A Preliminary Study of the Norm Preservation Properties of CNN\n. In WiML Workshop, 2015. (","model":"human","source":"peerread","label":0,"id":4756}
{"text":"This paper presents a thorough analysis of different methods to do curriculum learning. The major issue I have with it is that the dataset used seems very specific and does not necessarily justified, as mentioned by AnonReviewer3. It would have been great to see experiments on more standard tasks. Also, I really can't understand how the performance of FFNN models can be so good, please elaborate on this (see last comment).\nHowever, the paper is well written, the comparisons of the described methods are interesting and would probably apply to some other datasets as well.\n\nThe paper is way too long (18 pages!). Please reduce it or move some of the results to an appendix section.\n\nThe method described is extremely similar to the one described in Reinforcement learning neural turing machines (Zaremba et al., 2016,","model":"human","source":"peerread","label":0,"id":4757}
{"text":"This is an empirical paper which compares three different instantiations of a kind of incremental\/curriculum learning for sequences.\n \n The reviews from R1 and R3 (which gave confidence scores of 4) were negative. The main concerns addressed by the reviewers:\n * Paper is too long -- 17 pages -- and length is due to experiments (e.g. transfer learning) which are tangential to the main message of the paper (R3, R1) \n * Lack of novelty (R3)\n * Tests only on single, synthetic, small dataset and questioning the claim that this new synthetic dataset is helpful to the community (R3, R1)\n \n However, R3 and R1 both pointed out that they found the ablation studies interesting. R4 (who gave a confidence score of 3) was gave a more positive score but also expressed similar concerns with R1 & R3 (page length, similarity to existing work, dataset too specific and not necessarily justified).\n \n The author argued for the novelty of the paper, agreed to reduce the paper length and also argued that the data was indeed helpful (giving a specific case of another researcher who was extending the data). The author also provided a \"twitter trail\" countering the argument that the dataset was created for the sole purpose of showing that the method works.\n \n After engaging the reviewers in discussion, R4 admitted they were originally too generous with their score and downgraded to 5. The AC has decided that, while the paper has merits as acknowledged by the reviewers, it's not strong enough for acceptance in its present form. The AC encourages the author to work on an improved version (perhaps with experiments on an additional real dataset) and organize it with the audience in mind.","model":"human","source":"peerread","label":0,"id":4758}
{"text":"Two reviewers had questions about which problems the approach is expected to work for. I did not find the time to do further experiments, but I did in the mean time think about when the approach can be expected to confer benefit. When there are long range dependencies where it is necessary to learn relations with a substantial part of the history, it is to be expected that gradually growing the length of the history over which relations are learned can bring benefit. \nAs noted, I am willing to reduce the paper to the suggested 8 pages, and use an appendix for remaining material.\n\n","model":"human","source":"peerread","label":0,"id":4759}
{"text":"First up, I want to point out that this paper is really long. Like 17 pages long -- without any supplementary material. While ICLR does not have an official page limit, it would be nice if authors put themselves in the reviewer's shoes and did not take undue advantage of this rule. Having 1 or 2 pages in addition to the conventional 8 page limit is ok, but more than doubling the pages is quite unfair. \n\nNow for the review: The paper proposes a new artificial dataset for sequence learning. I call it artificial because it was artificially generated from the original MNIST dataset which is a smallish dataset of real images of handwritten digits. In addition to the dataset, the authors propose to train recurrent networks using a schedule over the length of the sequence, which they call \"incremental learning\". The experiments show that their proposed schedule is better than not having any schedule on this data set. Furthermore, they also show that their proposed schedule is better than a few other intuitive schedules. The authors verify this by doing some ablation studies over the model on the proposed dataset. \n\nI have following issues with this paper: \n\n-- I did not find anything novel in this paper. The proposed incremental learning schedule is nothing new and is a natural thing to try when learning sequences. Similar idea have already been tried by a number of authors, including Bengio 2015, and Ranzato 2015. The only new piece of work is the ablation studies which the authors conduct to tease out and verify that indeed the improvement in performance is due to the curriculum used. \n\n-- Furthermore, the authors only test their hypothesis on a single dataset which they propose and is artificially generated. Why not use it on a real sequential dataset, such as, language modeling. Does the technique not work in that scenario? In fact I am quite positive that for language modeling where the vocabulary size is huge, the performance gains will be no where close to the 74% reported in the paper.\n\n-- I'm not convinced about the value of having this artificial dataset. Already there are so many real world sequential dataset available, including in text, speech, finance and other areas. What exactly does this dataset bring to the table is not super clear to me. While having another dataset may not be a bad thing in itself, I almost felt that this dataset was created for the sole purpose of making the proposed ideas work. It would have been so much better had the authors shown experiments on other datasets. \n\n-- As I said, the paper is way too long. A significant part of the length of the paper is due to a collection of experiments which are completely un-related to the main message of the paper. For instance, the experiment in Section 6.2 is completely unrelated to the story of the paper. Same is true with the transfer learning experiments of Section 6.4.\n","model":"human","source":"peerread","label":0,"id":4760}
{"text":"The submitted paper proposes a new way of learning sequence predictors. In the lines of incremental learning and curriculum learning, easier samples are presented first and the complexity is increased during training. The particularity here is that the complexity is defined as the length of the sequences given for training, the premise being is that longer sequences are harder to learn, since they need a more complex internal representation.\n\nThe targeted application is sequence prediction from primed prefixes, tested on a single dataset, which the authors extract themselves from MNIST.\n\nThe idea in the paper is interesting and worth reading. There are also many interesting aspects of evaluation part, as the authors perform several ablation studies to rule out side-effects of the tests. The proposed learning strategy is compared to other strategies.\n\nHowever, my biggest concern is still with evaluation. The authors tested the method on a single dataset, which is non standard and derived from MNIST. Given the general nature of the claim, in order to confirm the interest of the proposed algorithm, it need to be tested on other datasets, public datasets, and on a different application.\n\nThe paper is too long and should be trimmed significantly.\n\nThe transfer learning part (from prediction to classification) is a different story and I do not see a clear connection to the main contribution of the paper.\n\nThe presentation and organization of the paper could be improved. It is quite sequentially written and sometimes reads like a student's report.\n\nThe loss given in the long unnumbered equation on page 6 should be better explained: provide explanations for each term, and make clearer what the different symbols mean. Learning is supervised, so which variables are predictions, and which are observations from the data (ground truth).\n\nNames in table 2 do not correspond to the descriptions in section 4.\n","model":"human","source":"peerread","label":0,"id":4761}
{"text":"This paper presents a thorough analysis of different methods to do curriculum learning. The major issue I have with it is that the dataset used seems very specific and does not necessarily justified, as mentioned by AnonReviewer3. It would have been great to see experiments on more standard tasks. Also, I really can't understand how the performance of FFNN models can be so good, please elaborate on this (see last comment).\nHowever, the paper is well written, the comparisons of the described methods are interesting and would probably apply to some other datasets as well.\n\nThe paper is way too long (18 pages!). Please reduce it or move some of the results to an appendix section.\n\nThe method described is extremely similar to the one described in Reinforcement learning neural turing machines (Zaremba et al., 2016, ","model":"human","source":"peerread","label":0,"id":4762}
{"text":"You mention the \"Best value for the average over 10 runs\".\nCan you explain what you calculated here?","model":"human","source":"peerread","label":0,"id":4763}
{"text":"\n- Clarified the contribution (see abstract, intro, and conclusion)\n- Added figures to illustrate the architecture of the network and the difference between training and generation\n- Adapted the selection of experiments in Section 6.4 (more logical selection of experimental settings)\n- Some textual edits\n\nArticle length: the current version of the article aims to provide a description of the work that is as clear and complete as possible. As a result the number of pages is higher than requested for final versions (this is in line with the ICLR submission policy, which permits submitting longer articles). If accepted, a condensed version will be produced that satisfies the ICLR recommended guidelines (\"8 pages, plus 1 page for the references and as many pages as needed in an appendix section\").","model":"human","source":"peerread","label":0,"id":4764}
{"text":"Updated version:\n- added a section with generative results, including movies showing what the network has learned over the course of training\n- used the ICLR style file","model":"human","source":"peerread","label":0,"id":4765}
{"text":"Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format for your submission to be considered. Thank you!","model":"human","source":"peerread","label":0,"id":4766}
{"text":"The paper presents a method to learn graph embeddings in a unsupervised way using random walks. It is well written and the execution appears quite accurate. The area of learning whole graph representations does not seem to be very well explored in general, and the proposed approach enjoys having very few competitors.\n\nIn a nutshell, the idea is to linearize the graph using random walks and to compute the embedding of the central segment of each walk using the skip-thought criterion. Being not an expert in biology, I can not comment whether or not this makes sense, but the gains reported in Table 2 are quite significant. \n\nAn anonymous public comment compared this work to a number of others in which the problem of learning representations of nodes is considered. While this is arguably a different goal, one natural baseline would be to pool these representations using mean- or max- pooling. It would very interesting to do such a comparison, especially given that the considered approach heavily relies on pooling (see Figure 3(c))\n\nTo sum up, I think it is a nice paper, and with more baselines I would be ready to further increase the numerical score.","model":"human","source":"peerread","label":0,"id":4767}
{"text":"The idea of applying skip-graphs to this graph domain to learn embeddings is good. The results demonstrate that this approach is competitive, but do not show a clear advantage. This is difficult, as the variety of approaches in this area is rapidly increasing. But comparisons to other methods could be improved, notably deep graph kernels.","model":"human","source":"peerread","label":0,"id":4768}
{"text":"Dear reviewers, thank you very much for all the insightful comments and suggestions. Please find below a summary of our response to each of the points that were raised. Please note that we have paraphrased some of the comments and combined similar ones.\n\nAnonymous Reviewer 2:\n\nComment 1. Use the same classifier for all the compared methods.\n\nWe have updated the results in our paper to reflect the changes from using a common classifier for all the compared methods. The changes can be found in section 3.2.\n\nComment 2. Use mean- or max-pooling to aggregate the embeddings learned by methods such as DeepWalk or node2vec and include this as a baseline.\n\nThe paper has been updated to include results from Deepwalk which was added as an additional baseline. Changes were made to sections 3.2 and 3.3.\n\nAnonymous Reviewer 3:\n\nComment 3. Provide more information about the exact values of the parameters that were used in the grid search.\n\nThe information have been added to the paper. This can be found in section 3.2.\n\nAnonymous Reviewer 1:\n\nComment 4. Need more experiment to demonstrate the power of their feature extraction methods. (Clustering, Search, Prediction etc.). Only comparing the classification accuracy by using the proposed method is not enough.\n\nWe plan to extend the experiments by using the embedding for graph clustering and graph search tasks in future works\/journal extension. Due to the suggested page limit of ICLR, it is not easy to add these additional experiments without deleting some existing results in the paper. In the existing results, we have demonstrated the power of the embedding through prediction tasks and visualization of the embedding.\n\nComment 5. Presentation of the paper is weak. There are lots of typos and unclear statements. \n\nThe latest version of the paper has been proofread by several individuals and to the best of our knowledge we have removed the major typos and\/or unclear statements. \n\nComment 6. What is the difference from graph kernel methods? The comparison with graph kernel is missing. \n\nWe agree that the problem studied in the deep graph kernel paper seems to be very similar, or even the same, to the one we are studying. However, the underlying approach is slightly different and we argue that that in itself is a good motivation for the work as there has not been too many work published in the area. We use an encoder-decoder model which has not been used, and for one we can identify functionally similar subgraphs. \n\nRegrettably, due to time constraints we are unable to add a comparison with Deep Graph Kernels. However, we have tested against methods that have been shown to achieve good results on the type of graphs we used (ECFP and NeuralFPS) and we have also tested against DeepWalk. The introduction section has been updated to include some discussion on this.\n\nAnonymous Reviewer 4:\n\nComment 7. Comparisons with recent work like LINE and node2vec are missing. You can compare them easily by applying the same aggregation strategy to their node embeddings.\n\nWe have already addressed the comment of a previous reviewer and have included DeepWalk as a baseline. We did not add comparisons against node2vec and LINE as these, in general, belong to the same family of methods.\n\nComment 8. As the current datasets are small (e.g., the average number of nodes per graph is around 30), it would be great to explore larger graph datasets to further investigate the method. \n\nTo compensate in a way, we tested the method on four different molecular graph datasets. Regrettably, we are unable to add experiments on more datasets at the moment.\n\nComment 9. The description about how to split the random walk sequence into 3 sub-sequences is missing. Also, the line \u201cl_min >= (n_k - 1), \u2026 >= l_max\u201d in section 2.2.2 is a mistake.\n\nSection 2.2.2 has been updated to correct this. Thank you very much for pointing this out.\n\n","model":"human","source":"peerread","label":0,"id":4769}
{"text":"This paper proposes an unsupervised graph embedding learning method based on random walk and skip-thought model. They show promising results compared to several competitors on four chemical compound datasets.\n\nStrength:\n1, The idea of learning the graph embedding by applying skip-thought model to random walk sequences is interesting. \n2, The paper is well organized.\n\nWeakness:\n1, As the current datasets are small (e.g., the average number of nodes per graph is around 30), it would be great to explore larger graph datasets to further investigate the method. \n2, Comparisons with recent work like LINE and node2vec are missing. You can compare them easily by applying the same aggregation strategy to their node embeddings.\n\nDetailed Questions:\n1, The description about how to split the random walk sequence into 3 sub-sequences is missing. Also, the line \u201cl_min >= (n_k - 1), \u2026 >= l_max\u201d in section 2.2.2 is a mistake.\n2, Can you provide the standard deviations of the 5-fold cross validation in Table 2? I\u2019m curious about how stable the algorithm is.","model":"human","source":"peerread","label":0,"id":4770}
{"text":"This paper studies the graph embedding problem by using the encoder-decoder method. The experimental study on real network data sets show the features extracted by the proposed model is good for classification.\n\nStrong points of this paper:\n  1. The idea of using the methods from natural language processing to graph mining is quite interesting.\n  2. The organization of the paper is clear\n\nWeak points of this paper:\n  1. Comparisons with state-of-art methods (Graph Kernels) is missing. \n  2. The problem is not well motivated, are there any application of this. What is the different from the graph kernel methods? The comparison with graph kernel is missing. \n  3. Need more experiment to demonstrate the power of their feature extraction methods. (Clustering, Search, Prediction etc.)\n  4. Presentation of the paper is weak. There are lots of typos and unclear statements. \n  5. The author mentioned about the graph kernel things, but in the experiment they didn't compare them. Also, only compare the classification accuracy by using the proposed method is not enough.","model":"human","source":"peerread","label":0,"id":4771}
{"text":"Authors take the skip-graph architecture (Kiros 2015) and apply it to classifying labeled graphs (molecular graphs). They do it by creating many sentences by walking the graph randomly, and asking the model to predict previous part and next part from the middle part. Activations of the decoder part of this model on a walk generated from a new graph are used as features for a binary classifier use to predict whether the molecule has anti-cancer properties.\n\nPaper is well written, except that evaluation section is missing details of how the embedding is used for actual classification (ie, what classifier is used)\n\nUnfortunately I'm not familiar with the dataset and how hard it is to achieve the results they demonstrate, that would be the important factor to weight on the papers acceptance.","model":"human","source":"peerread","label":0,"id":4772}
{"text":"The paper presents a method to learn graph embeddings in a unsupervised way using random walks. It is well written and the execution appears quite accurate. The area of learning whole graph representations does not seem to be very well explored in general, and the proposed approach enjoys having very few competitors.\n\nIn a nutshell, the idea is to linearize the graph using random walks and to compute the embedding of the central segment of each walk using the skip-thought criterion. Being not an expert in biology, I can not comment whether or not this makes sense, but the gains reported in Table 2 are quite significant. \n\nAn anonymous public comment compared this work to a number of others in which the problem of learning representations of nodes is considered. While this is arguably a different goal, one natural baseline would be to pool these representations using mean- or max- pooling. It would very interesting to do such a comparison, especially given that the considered approach heavily relies on pooling (see Figure 3(c))\n\nTo sum up, I think it is a nice paper, and with more baselines I would be ready to further increase the numerical score.  \n","model":"human","source":"peerread","label":0,"id":4773}
{"text":"Hi,\n\nIn the abstract you have mentioned, \"Many of the existing work in the area are task-specific and based on supervised techniques\". I don't think this is a valid statement, since lot of recent work in graph embeddings is based on unsupervised methods. Like LINE and DeepWalk. \n\nAnd is there any particular reason you didn't compare your method with DeepWalk and LINE, considering the fact that they are state-of-the art methods in representation learning for graphs?\n\nRegards,","model":"human","source":"peerread","label":0,"id":4774}
{"text":"The paper presents one of the first neural translation systems that operates purely at the character-level, another one being","model":"human","source":"peerread","label":0,"id":4775}
{"text":"This paper is concurrently one of the first successful attempts to do machine translation using a character-character MT modeling, and generally the authors liked the approach. However, the reviewers raised several issues with the novelty and experimental setup of the work. \n \n Pros:\n - The analysis of the work was strong. This illustrated the underlying property, and all reviewers praised these figures.\n \n Mixed: \n - Some found the paper clear, praising it as a \"well-written paper\", however other found that important details were lacking and the notation was improperly overloaded. As the reviewers were generally experts in the area, this should be improved\n - Reviewers were also split on results. Some found the results quite \"compelling\" and comprehensive, but others thought there should be more comparison to BPE and other morphogically based work\n - Modeling novelty was also questionable. Reviewers like the partial novelty of the character based approach, but felt like the ML contributions were too shallow for ICLR\n \n Cons:\n - Reviewers generally found the model itself to be overly complicated and the paper to focus too much on engineering. \n - There were questions about experimental setup. In particular, a call for more speed numbers and broader comparison.","model":"human","source":"peerread","label":0,"id":4776}
{"text":"Dear reviewers,\n\nWe uploaded an updated version with an appendix.  In the appendix, we describe our model in detail and add more translation samples. \nWe have added the size of models in Table 1. Table 1 becomes more comprehensive, thanks for your insightful suggestions.\n\nThanks.\n","model":"human","source":"peerread","label":0,"id":4777}
{"text":"\n* Summary: This paper proposes a neural machine translation model that translates the source and the target texts in an end to end manner from characters to characters. The model can learn morphology in the encoder and in the decoder the authors use a hierarchical decoder. Authors provide very compelling results on various bilingual corpora for different language pairs. The paper is well-written, the results are competitive compared to other baselines in the literature.\n\n\n* Review:\n     - I think the paper is very well written, I like the analysis presented in this paper. It is clean and precise. \n     - The idea of using hierarchical decoders have been explored before, e.g. [1]. Can you cite those papers?\n     - This paper is mainly an application paper and it is mainly the application of several existing components on the character-level NMT tasks. In this sense, it is good that authors made their codes available online. However, the contributions from the general ML point of view is still limited.\n                   \n* Some Requests:\n -Can you add the size of the models to the Table 1? \n- Can you add some of the failure cases of your model, where the model failed to translate correctly?\n\n* An Overview of the Review:\n\nPros:\n    - The paper is well written\n    - Extensive analysis of the model on various language pairs\n    - Convincing experimental results.    \n    \nCons:\n    - The model is complicated.\n    - Mainly an architecture engineering\/application paper(bringing together various well-known techniques), not much novelty.\n    - The proposed model is potentially slower than the regular models since it needs to operate over the characters instead of the words and uses several RNNs.\n\n[1] Serban IV, Sordoni A, Bengio Y, Courville A, Pineau J. Hierarchical neural network generative models for movie dialogues. arXiv preprint arXiv:1507.04808. 2015 Jul 17.\n","model":"human","source":"peerread","label":0,"id":4778}
{"text":"Dear reviewers,\n\nWe uploaded a slightly modified version. \nWe have further explained the novelty of the hierarchical decoder in Section 3.2.\nWe have added the comparison with Luong & Manning, 2016 [1] and the trivial baseline (CNMT) to Table 1.\nThe trivial baseline is the old version of this submission which takes the last hidden state of RNN as the representation of the source word. You could find it on arxiv (","model":"human","source":"peerread","label":0,"id":4779}
{"text":"Dear reviewers,\n\nWe uploaded a slightly modified version. We have added the training time for each model to Table 1 and clarified some notations.\n\nThanks!","model":"human","source":"peerread","label":0,"id":4780}
{"text":"Update after reading the authors' responses & the paper revision dated Dec 21:\nI have removed the comment \"insufficient comparison to past work\" in the title & update the score from 3 -> 5.\nThe main reason for the score is on novelty. The proposal of HGRU & the use of the R matrix are basically just to achieve the effect of \"whether to continue from character-level states or using word-level states\". It seems that these solutions are specific to symbolic frameworks like Theano (which the authors used) and TensorFlow. This, however, is not a problem for languages like Matlab (which Luong & Manning used) or Torch.\n\n-----\n\nThis is a well-written paper with good analysis in which I especially like Figure 5. However I think there is little novelty in this work. The title is about learning morphology but there is nothing specifically enforced in the model to learn morphemes or subword units. For example, maybe some constraints can be put on the weights in w_i in Figure 1 to detect morpheme boundaries or some additional objective like MDL can be used (though it's not clear how these constraints can be incorporated cleanly). \n\nMoreover, I'm very surprised that litte comparison (only a brief mention) was given to the work of (Luong & Manning, 2016) [1], which trains deep 8-layer word-character models and achieves much better results on English-Czech, e.g., 19.6 BLEU compared to 17.0 BLEU achieved in the paper. I think the HGRU thing is over-complicated in terms of presentation. If I read correctly, what HGRU does is basically either continue the character decoder or reset using word-level states at boundaries, which is what was done in [1]. Luong & Manning (2016) even make it more efficient by not having to decode all target words at the morpheme level & it would be good to know the speed of the model proposed in this ICLR submission. What end up new in this paper are perhaps different analyses on what a character-based model learns & adding an additional RNN layer in the encoder.\n\nOne minor comment: annotate h_t in Figure 1.\n\n[1] Minh-Thang Luong and Christopher D. Manning. 2016. Achieving Open Vocabulary Neural Machine Translation\nwith Hybrid Word-Character Models. ACL. ","model":"human","source":"peerread","label":0,"id":4781}
{"text":"The paper presents one of the first neural translation systems that operates purely at the character-level, another one being ","model":"human","source":"peerread","label":0,"id":4782}
{"text":"Overview: This work seems very promising, but I believe it should be compared with more baselines, and more precisely described and explained, from a signal processing point of view.\n\nPros:\nNew descriptor\nFast implementation\n\nCons:\na) Lack of rigor\nb) Too long accordingly to the content\nc) The computational gain of the algorithm is not clear\nd) The work is not compared with its most obvious baseline: a scattering transform\n\nI will detail each cons.\n\na) Section 1:\nThe author\u00a0 motivates the use of scattering transform because it defines a contraction of the space that relies on geometric features.\n\" The nonlinearity used in the scattering network is the complex modulus which is piecewise linear.\"\nA real modulus is piecewise linear. A complex modulus has a shape of bell when interpreting C as R^2. Could you clarify?\n\\Omega is not introduced.\n\nCould you give a precise reference (page+paper) of this claim: \u201cHigher order nonlinearity refers to |x|^2 instead of |x| as it is usually done in the scattering network.\u201d ?\n\nSection 2:\nThe motivation of the non-linearity is not clear. First, this non-linearity might potentially increase a lot the variance of your architecture since it depends on higher moments(up to 4). I think a fair analysis would be to compute numerically the normalized variance (e.g. divided by the averaged l^2 norm), as a sanity check. Besides, one should prove that the energy is decreasing. It is not possible to argue that this architecture is similar to a scattering transform which has precise mathematical foundations and those results are required, since the setting is different.\n\nPermutation is not a relevant variability.\n\nThe notion of sparsity during the whole paper sometimes refers to the number of 0 value, either the l^1 norm. Mathematically, a small value, even 10^-1000 is still a non 0 value.\n\nDid you compute the graph of the figure 4 on the bird dataset? You might use a ratio instead for clarity. \n\nThe wavelet that is defined is not a morlet wavelet (","model":"human","source":"peerread","label":0,"id":4783}
{"text":"This paper proposes a to use squared modulus nonlinearities within convolutional architectures. Because point-wise squaring can be written as a convolution in the Fourier domain, when doing all the operations in the Fourier this architecture becomes 'dual': convolutions become pointwise operations, and pointwise square-nonlinearities become convolutions. \n The authors study this architecture in the context of scattering transforms and produce a complexity analysis that exploits the previous property, along with preliminary numerical experiments. \n \n All reviewers agreed that, while this is an interesting paper with potentially useful outcomes, its exposition and current experimental section are insufficient. The AC agrees with this assessment, and therefore recommends rejection. \n I agree that the main unanswered question and a 'show-stopper' is the lack of comparisons with its most immediate baseline, scattering using complex modulus, both in terms of accuracy and computational complexity.","model":"human","source":"peerread","label":0,"id":4784}
{"text":"I find the general direction of the work is promising but, in my opinion, the paper has three main drawback. While the motivation and overall idea seem very reasonable, the derivation is not convincing mathematically. The experiments are limited and the presentation needs significant improvement.  The writing and wording are in general poorly structured to the point that it is sometimes difficult to follow the proposed ideas. The overall organization needs improvement and the connection between sections is not properly established. The paper could be significantly improved by simply re-writing it.\n\nI'm not fully convinced by the motivation for the proposed non-linearity (|c|^2), as described on page 5. The authors argue that  (Waldspurger, 2016) suggests that higher order nonlinearities might be  beneficial for sparsity. But unless I'm missing something, that work seems  to suggest that in the general case higher order nonlinearities can be neglected. Could you please comment on this?\n\nOn the other hand, adding a second order term to the descriptor seems\nan interesting direction, as long as stability to small variations is preserved (which should be shown experimentally)\n\nThe experimental section is rather limited. The paper would be stronger with a thorough numerical evaluation. The presented results, in my opinion, do not show convincingly a clear advantage of the proposed method over a standard implementation of the scattering transform. In order to show the merits of the proposed approach, it would be really helpful to directly compare running times and compression rates.\n\nQuestions: \n- Can you show empirically that the proposed higher order nonlinearity\nproduces sparser representations than the complex modulus?\n\nOther minor issues:\n- The proof of Section 2.1, should be preceded by a clear statement in the form of a proposition\n- \"Hadamart\" -> Hadamard\n- \"Valid set\" -> Validation set\n- \"nonzeros coefficients\" -> nonzero coefficients\n- Figure 3 is difficult to understand. Please provide more details.\n- Figure 5 is supposed to show a comparison to a standard implementation of the Scattering network, but it doesn't seem to be such comparison in that figure. Please explain.\n- Please verify the references. The first reference states \"MALLAT\".\n","model":"human","source":"peerread","label":0,"id":4785}
{"text":"This work proposes 3 improvements to scattering networks: (1) a non-linearity that allows Fourier-domain computation, (2) compact-supported (in the Fourier domain) representations, and (3) computing additional variance features.\n\nThe technical contributions seem worthwhile, since #1 and #2 may result in better speed, while #3 may improve accuracy. Unfortunately, they are poorly described and evaluated. If the writing was clear and the evaluation more broad, I would have recommended acceptance since the ideas have merit.\n\nOne of the biggest faults of the presentation is that many sentences are overly long and full of unnecessary obfuscating language, e.g. the last paragraph of Section 1 (though unfortunately this permeates the whole paper).\n\nLikewise, most equations are made unnecessarily complicated. For example, Eq. 5 does not need 4 lines and so many indexes, but just 2:\nX_0 = x\nX_l = |X_{l-1} * Psi_l|\nwith the |.| operator being element-wise. Most of the hyperparameter dependencies and indexes are not necessary, as well as the repetition of iterations. The same reasoning can be applied to most Equations 5 to 13.\n\nThe argument of cardinality (Eq. 14) does not really help prove that variance is more informative. In fact, we could just as easily write that the cardinality of S concatenated with any (!) other quantity is >= the cardinality of S. Another argument from machine learning theory would be better.\n\nThe authors should strive to make the arguments in the paper less hyperbolic and better substantiated. The claims about finding invariants of any input (Abstract) and fundamental structures (last paragraph of Section 1.2.1) are not really backed up by any math. How can we have any guarantees about singling out, for example, semantically relevant representations? The learning procedures in machine learning give at least some guarantees, while here the feature building seems a bit more heuristic. This does not take away from the main idea, but this part needs to be better researched.\n","model":"human","source":"peerread","label":0,"id":4786}
{"text":"Overview: This work seems very promising, but I believe it should be compared with more baselines, and more precisely described and explained, from a signal processing point of view.\n\nPros:\nNew descriptor\nFast implementation\n\nCons:\na) Lack of rigor\nb) Too long accordingly to the content\nc) The computational gain of the algorithm is not clear\nd) The work is not compared with its most obvious baseline: a scattering transform\n\nI will detail each cons.\n\na) Section 1:\nThe author\u00a0 motivates the use of scattering transform because it defines a contraction of the space that relies on geometric features.\n\" The nonlinearity used in the scattering network is the complex modulus which is piecewise linear.\"\nA real modulus is piecewise linear. A complex modulus has a shape of bell when interpreting C as R^2. Could you clarify?\n\\Omega is not introduced.\n\nCould you give a precise reference (page+paper) of this claim: \u201cHigher order nonlinearity refers to |x|^2 instead of |x| as it is usually done in the scattering network.\u201d ?\n\nSection 2:\nThe motivation of the non-linearity is not clear. First, this non-linearity might potentially increase a lot the variance of your architecture since it depends on higher moments(up to 4). I think a fair analysis would be to compute numerically the normalized variance (e.g. divided by the averaged l^2 norm), as a sanity check. Besides, one should prove that the energy is decreasing. It is not possible to argue that this architecture is similar to a scattering transform which has precise mathematical foundations and those results are required, since the setting is different.\n\nPermutation is not a relevant variability.\n\nThe notion of sparsity during the whole paper sometimes refers to the number of 0 value, either the l^1 norm. Mathematically, a small value, even 10^-1000 is still a non 0 value.\n\nDid you compute the graph of the figure 4 on the bird dataset? You might use a ratio instead for clarity. \n\nThe wavelet that is defined is not a morlet wavelet ( ","model":"human","source":"peerread","label":0,"id":4787}
{"text":"The authors mention that they are not aiming to have SOTA results.\nHowever, that an ensemble of resnets has lower performance than some of single network results, indicates that further experimentation preferably on larger datasets is necessary.\nThe literature review could at least mention some existing works such as wide resnets","model":"human","source":"peerread","label":0,"id":4788}
{"text":"This paper proposes a boosting based ensemble procedure for residual networks by adopting the Deep Incremental Boosting method that was used for CNN's(Mosca & Magoulas, 2016a). At each step t, a new block of layers are added to the network at a position p_t and the weights of all layers are copied to the current network to speed up training.\n\nThe method is not sufficiently novel since the steps of Deep Incremental Boosting are slightly adopted. Instead of adding a layer to the end of the network, this version adds a block of layers to a position p_t (starts at a selected position p_0) and merges layer accordingly hence slightly adopts DIB. \n\nThe empirical analysis does not use any data-augmentation. It is not clear whether the improvements (if there is) of the ensemble disappear after data-augmentation.  Also, one of the main baselines, DIB has no-skip connections therefore this can negatively affect the fair comparison. The authors argue that they did not involve state of art Res Nets since their analysis focuses on the ensemble approach, however any potential improvement of the ensemble can be compensated with an inherent feature of Res Net variant. The boosting procedure can be computationally restrictive in case of ImageNet training and Res Net variants may perform much better in that case too. Therefore the baselines should include the state of art Res Nets and Dense Convolutional networks hence current results are preliminary.\n\nIn addition, it is not clear how sensitive the boosting to the selection of injection point.\n\nThis paper adopts DIB to Res Nets and provides some empirical analysis however the contribution is not sufficiently novel and the empirical results are not satisfactory for demonstrating that the method is significant.\n\nPros\n-provides some preliminary results for boosting of Res Nets\nCons\n-not sufficiently novel: an incremental approach \n-empirical analysis is not satisfactory","model":"human","source":"peerread","label":0,"id":4789}
{"text":"The authors mention that they are not aiming to have SOTA results.\nHowever, that an ensemble of resnets has lower performance than some of single network results, indicates that further experimentation preferably on larger datasets is necessary.\nThe literature review could at least mention some existing works such as wide resnets ","model":"human","source":"peerread","label":0,"id":4790}
{"text":"The paper under consideration proposes a set of procedures for incrementally expanding a residual network by adding layers via a boosting criterion.\n\nThe main barrier to publication is the weak empirical validation. The tasks considered are quite small scale in 2016 (and MNIST with a convolutional net is basically an uninteresting test by this point). The paper doesn't compare to the literature, and CIFAR-10 results fail to improve upon rather simple, single-network published baselines (Springenberg et al, 2015 for example, obtains 92% without data augmentation) and I'm pretty sure there's a simple ResNet result somewhere that outshines these too. The CIFAR100 results are a little bit interesting as they are better than I'm used to seeing (I haven't done a recent literature crawl), and this is unsurprising -- you'd expect ensembles to do well when there's a dearth of labeled training data, and here there are only a few hundred per label. But then it's typical on both CIFAR10 and CIFAR100 to use simple data augmentation schemes which aren't employed here, and these and other forms of regularization are a simpler alternative to a complicated iterative augmentation scheme like this.\n\nIt'd be easier to sell this method either as an option for scarce labeled datasets where data augmentation is non-trivial (but then for most image-related applications, random crops and reflections are easy and valid), but that would necessitate different benchmarks, and comparison against simpler methods like said data augmentation, dropout (especially, due to the ensemble interpretation), and so on.","model":"human","source":"peerread","label":0,"id":4791}
{"text":"- Can you give the details of the experiment setup e.g. parameters to be tuned, algorithm to train at each step of boosting etc? Also can you give the details of networks architecture and references? \n- Can you elaborate on comparison to state of resNet variants, dense convolutional network?\n- Can you give also comparison on training time?\n- Do you have any result on Imagenet?","model":"human","source":"peerread","label":0,"id":4792}
{"text":"In this submission, an interesting approach to character-based language modeling is pursued that retains word-level representations both in the context, and optionally also in the output. However, the approach is not new, cf. (Kim et al. 2015) as cited in the submission, as well as (Jozefowicz et al. 2016). Both Kim and Jozefowicz already go beyond this submission by applying the approach using RNNs\/LSTMs. Also, Jozefowicz et al. provide a comparative discussion of different approaches to character-level modeling, which I am missing here, at least by discussing this existing work. THe remaining novelty of the approach then would be its application to machine translation, although it remains somewhat unclear, inhowfar reranking of N-best lists can handle the OOV problem - the translation-related part of the OVV problem should be elaborated here. That said, some of the claims of this submission seems somewhat exaggerated, like the statement in Sec. 2.3: \"making the notion of vocabulary obsolete\", whereas the authors e.g. express doubts concerning the interpretation of perplexity w\/o an explicit output vocabulary. For example modeling of especially frequent word forms still can be expected to contribute, as shown in e.g. arXiv:1609.08144\n\nSec. 2.3: You claim that the objective requires a finite vocabulary. This statement only is correct if the units considered are limited to full word forms. However, using subwords and even individual characters, implicitly larger and even infinite vocabularies can be covered with the log-likelihood criterion. Even though this require a model different from the one proposed here, the corresponding statement should qualified in this respect.\n\nThe way character embeddings are used for the output should be clarified. The description in Sec. 2.4 is not explicit enough in my view.\n\nConcerning the configuration of NCE, it would be desirable to get a better idea of how you arrived at your specific configuration and parameterization described in Sec. 3.4.\n\nSec. 4.1: you might want to mention that (Kim et al. 2015) came to similar conclusions w.r.t. the performance of using character embeddings at the output, and discuss the suggestions for possible improvements given therein.\n\nSec. 4.2: there are ways to calculate and interpret perplexity for unknown words, cf. (Shaik et al. IWSLT 2013).\n\nSec. 4.4 and Table 4: the size of the full training vocabulary should be provided here.\n\nMinor comments:\np. 2, bottom: three different input layer -> three different input layers (plural)\nFig. 1: fonts within the figure are way too small\np. 3, first item below Fig. 1: that we will note WE -> that we will denote WE\nSec. 2.3: the parameters estimation -> the parameter estimation (or: the parameters' estimation)\np. 5, first paragraph: in factored way -> in a factored way\np. 5, second paragraph: a n-best list, a nk-best list -> an n-best list, an nk-best list\nSec. 4.2, last sentence: Despite adaptive gradient, -> verb and article missing","model":"human","source":"peerread","label":0,"id":4793}
{"text":"The reviewers raise several important questions about modeling and methodology that should be answered in later versions of the paper. The paper also overstates its findings.","model":"human","source":"peerread","label":0,"id":4794}
{"text":"This paper proposes an extension of neural network language (NLM) models to better handle large vocabularies. The main idea is to obtain word embeddings by combining character-level embeddings with a convolutional network.\n\nThe authors compare word embeddings (WE),character embeddings (CE) as well a combined character and word embeddings (CWE). It's quite obvious how CE or CWE embeddings can be used at the input of an NLM, but this is more tricky at the output layer. The authors propose to use NCE to handle this problem.  NCE allows to speed-up training, but has no impact on inference during testing: the full softmax output layer must be calculated and normalized (which can be very costly).\n\nIt was not clear to me how the network is used during TESTING with an open-vocabulary. Since the NLM is only used during reranking, the unnormalized probability of the requested word could be obtained at the output. However, when reranking n-best lists with the NLM feature, different sentences are compared and I wonder whether this does work well without proper normalization.\n\nIn addition, the authors provide perplexities in Table 2 and Figures 2 and 3.  This needs normalization, but it is not clear to me how this was performed.  The authors mention a 250k output vocabulary. I doubt that the softmax was calculated over 250k values. Please explain.\n\nThe model is evaluated by reranking n-best lists of an SMT systems for the IWSLT 2016 EN\/CZ task.  In the abstract, the authors mention a gain of 0.7 BLEU. I do not agree with this claim. A vanilla word-based NLM, i.e. a well-known model, achieves already a gain of 0.6 BLEU. Therefore, the new model proposed in this paper brings only an additional improvement of 0.1 BLEU. This is not statistically significant. I conjecture that a similar variation could be obtained by just training several models with different initializations, etc.\n\nUnfortunately, the NLM models which use a character representation at the output do not work well. There are already several works which use some form of character-level representations at the input.\n\nCould you please discuss the computational complexity during training and inference.\n\nMinor comments\n - Figure 2 and 3 have the caption \"Figure 4\". This is misleading.\n - the format of the citations is unusual, eg.\n   \"While the use of subword units Botha & Blunsom (2014)\"\n   -> \"While the use of subword units (Botha & Blunsom, 2014)\"","model":"human","source":"peerread","label":0,"id":4795}
{"text":"this paper proposes a model for representing unseen words in a neural language model. the proposed model achieves poor results in LM and a slight improvement over a baseline model. \n\nthis work needs a more comprehensive analysis:\n- there's no comparison with related work trying to address the same problem\n- an intrinsic evaluation and investigation of why\/how their work should be better are missing.\n- to make a bolder claim, more investigation should be done with other morphologically rich languages. Especially for MT, in addition to going from En-> Language_X, MRL_X -> En or MRL_X -> MRL_Y should be done.\n","model":"human","source":"peerread","label":0,"id":4796}
{"text":"In this submission, an interesting approach to character-based language modeling is pursued that retains word-level representations both in the context, and optionally also in the output. However, the approach is not new, cf. (Kim et al. 2015) as cited in the submission, as well as (Jozefowicz et al. 2016). Both Kim and Jozefowicz already go beyond this submission by applying the approach using RNNs\/LSTMs. Also, Jozefowicz et al. provide a comparative discussion of different approaches to character-level modeling, which I am missing here, at least by discussing this existing work. THe remaining novelty of the approach then would be its application to machine translation, although it remains somewhat unclear, inhowfar reranking of N-best lists can handle the OOV problem - the translation-related part of the OVV problem should be elaborated here. That said, some of the claims of this submission seems somewhat exaggerated, like the statement in Sec. 2.3: \"making the notion of vocabulary obsolete\", whereas the authors e.g. express doubts concerning the interpretation of perplexity w\/o an explicit output vocabulary. For example modeling of especially frequent word forms still can be expected to contribute, as shown in e.g. arXiv:1609.08144\n\nSec. 2.3: You claim that the objective requires a finite vocabulary. This statement only is correct if the units considered are limited to full word forms. However, using subwords and even individual characters, implicitly larger and even infinite vocabularies can be covered with the log-likelihood criterion. Even though this require a model different from the one proposed here, the corresponding statement should qualified in this respect.\n\nThe way character embeddings are used for the output should be clarified. The description in Sec. 2.4 is not explicit enough in my view.\n\nConcerning the configuration of NCE, it would be desirable to get a better idea of how you arrived at your specific configuration and parameterization described in Sec. 3.4.\n\nSec. 4.1: you might want to mention that (Kim et al. 2015) came to similar conclusions w.r.t. the performance of using character embeddings at the output, and discuss the suggestions for possible improvements given therein.\n\nSec. 4.2: there are ways to calculate and interpret perplexity for unknown words, cf. (Shaik et al. IWSLT 2013).\n\nSec. 4.4 and Table 4: the size of the full training vocabulary should be provided here.\n\nMinor comments:\np. 2, bottom: three different input layer -> three different input layers (plural)\nFig. 1: fonts within the figure are way too small\np. 3, first item below Fig. 1: that we will note WE -> that we will denote WE\nSec. 2.3: the parameters estimation -> the parameter estimation (or: the parameters' estimation)\np. 5, first paragraph: in factored way -> in a factored way\np. 5, second paragraph: a n-best list, a nk-best list -> an n-best list, an nk-best list\nSec. 4.2, last sentence: Despite adaptive gradient, -> verb and article missing\n","model":"human","source":"peerread","label":0,"id":4797}
{"text":"In Sec. 4.2 you mention that perplexity is hard to interpret for models not using an explicit output vocabulary. When analysing open vocabulary approaches, perplexity can also be renormalized to character level, cf. e.g. Shaik et al. IWLST 2013. Did you consider this?","model":"human","source":"peerread","label":0,"id":4798}
{"text":"Also Kim et al. AAAI 2015 got the similar conclusions w.r.t. the performance of character-level embeddings and also provided a discussion with suggestions for improvements. Did you consider these?","model":"human","source":"peerread","label":0,"id":4799}
{"text":"Can you provide more details on the configuration of the NCE training?","model":"human","source":"peerread","label":0,"id":4800}
{"text":"From your notation I get that you used a feed-forward NN, can you confirm?","model":"human","source":"peerread","label":0,"id":4801}
{"text":"Can you confirm that the character-level word embedding used here is the same as in the google paper by Kim et al. AAAI 2015? It is not cited in Sec. 2.1.\n\n","model":"human","source":"peerread","label":0,"id":4802}
{"text":"Pls. define the use of the colon in the first equation of Sec. 2.1\n\nP^H((w:H)\\in D) is not defined before the second equation in Sec. 2.3. Also, in the sentence introducing this equation to refer to \"this probability\" - please provide an explicit reference to what probability is meant here.\n\nPls. define e^{out} and e^{char-out} in Sec. 2.4 - are they the same as e^{out}_w in Sec. 2.2 and e^{char} in Sec. 2.1?","model":"human","source":"peerread","label":0,"id":4803}
{"text":"The paper describes an extension of the HasheNets work, with several novel twists. Instead of using a single hash function, the proposed HFH approach uses multiple hash function to associate each \"virtual\" (to-be-synthesized) weight location to several components of an underlying parameter vector (shared across all layers). These components are then passed through a small MLP to synthesize the final weight.\n\nThis is an interesting and novel idea, and the experiments demonstrate that it improves substantially over HashedNets. However, HashedNets is not a particularly compelling technique for neural network model compression, especially when compared with more recent work on pruning- and quantization-based approaches. The experiments in this paper demonstrate that the proposed approach yields worse accuracy at the same compression ratios as pruning-based approaches, while providing no runtime speedup benefits. While the authors mention the technique is only 20% slower (which I am pleasantly surprised by), I don't understand why this technique should ever be used over competing approaches for the kinds of networks the authors present experimental results on. The authors suggest that the technique could be combined with pruning based approaches... this may be true, but no experiments to this effect are provided. The paper also suggests that ease of setting the compression ratio is a benefit of HFH, but I don't think that's a sufficient win to justify the numerous other downsides (in accuracy and speed). \n\nIn response to a question, the authors point out that the technique works very well for compressing embeddings, and for this setting the technique does appear like a genuinely useful contribution, given the marginal overhead and substantial train-time benefits. If the paper focused on this setting and showed experimental results on e.g. language modeling tasks or other scenarios with high-dimensional sparse\/one-hot inputs require large embedding layers, I could enthusiastically recommend acceptance. However for the CNN and MLP networks which are the main focus of the experiments, I don't think the technique is suitable, as much as I like the basic idea.","model":"human","source":"peerread","label":0,"id":4804}
{"text":"This paper presents some interesting and potentially useful ideas, but multiple reviewers point out that the main appeal of the paper's contributions would be in potential follow-up work and that the paper as-is does not present a compelling use case for the novel ideas. For that reason, the recommendation is to reject the paper. I would encourage the authors to reframe and improve the paper and extend it to cover the more interesting possible empirical investigations brought up in the discussion. Unfortunately, the paper is not sufficiently ground breaking to be a good fit for the workshop track.","model":"human","source":"peerread","label":0,"id":4805}
{"text":"The paper proposed a very complex compression and reconstruction method (with additional parameters) for reducing the memory footprint of deep networks.\n\nThe authors show that this complex proposal is better than simple hashed net proposal. One question: Are you also counting the extra parameters for reconstruction network for the memory comparison? Otherwise, the experiments are unfair.  \n\nSince hashing and reconstruction cost will dominate the feed-forward and back-propagation updates, it is imperative to compare the two methods on running time. For hashed net, this is quite simple, yet it created an additional bottleneck.  Please also show the impact on running time. Small improvements for a big loss in computational cost may not be acceptable. I am not convinced that this method will be lightweight. If we are allowed complicated compression and reconstruction then we can use any off-shelf methods, but the cost will be huge\n\n","model":"human","source":"peerread","label":0,"id":4806}
{"text":"The paper presents a method to reduce the memory footprint of a neural network at some increase in the computation cost. This paper is a generalization of HashedNets by Chen et al. (ICML'15) where parameters of a neural network are mapped into smaller memory arrays using some hash functions with possible collisions. Instead of training the original parameters, given a hash function, the elements of the compressed memory arrays are trained using back-propagation. In this paper, some new tricks are proposed including: (1) the compression space is shared among the layers of the neural network (2) multiple hash functions are used to reduce the effects of collisions (3) a small network is used to combine the elements retrieved from multiple hash tables into a single parameter. Fig 1 of the paper describes the gist of the approach vs. HashedNets.\n\nOn the positive side,\n+ The proposed ideas are novel and seem useful.\n+ Some theoretical justification is presented to describe why using multiple hash functions is a good idea.\n+ All of the experiments suggest that the proposed MFH approach outperforms HashedNets.\nOn the negative side,\n- The computation cost seems worse than HashedNets and is not discussed.\n- Immediate practical implication of the paper is not clear given that alternative pruning strategies perform better and should be faster at inference.\n\nThat said, I believe this paper benefits the deep learning community as it sheds light into ways to share parameters across layers of a neural network potentially leading to more interesting follow-ups. I recommend accept, while asking the authors to address the comments below.\n\nMore comments:\n- Please discuss the computation cost for both HashedNets and MFH for both fully connected and convolutional layers.\n- Are the experiments only run once for each configuration? Please run multiple times and report average \/ standard error.\n- For completeness, please add U1 results to Table 1.\n- In Table 1, U4-G3 is listed twice with two different numbers.\n- Some sentences are not grammatically correct. Please improve the writing.\n","model":"human","source":"peerread","label":0,"id":4807}
{"text":"This paper presents iterative PoWER, an off-policy variation on PoWER, a policy gradient algorithm in the reward-weighted family.\n\nI'm not familiar enough with this type lower bound scheme to comment on it. It looks like the end result is less conservative step sizes in policy parameter space. All expectation-based algorithms (and their KL-regularized cousins a-la TRPO) take smallish steps, and this might be a sensible way to accelerate them.\n\nThe description of the experiments in Section VI is insufficient for reproducibility. Is \"The cart moved right\" supposed to be \"a positive force is applied to the cart\"? How is negative force applied? What is the representation of the state? What is the distribution of initial states? A linear policy is insufficient for swing up and balance of a cart-pole. Are you only doing balancing? What is the noise magnitude of the policy? How was it chosen? How long were the episodes?\n\nThe footnote at the bottom of page 8 threw me off. If you're using Newton's method, where is the discussion of gradients and Hessians? I thought the argmax_theta operator was a stand-in for an EM-style step, which I how I read Eq (8) in the Kober paper.","model":"human","source":"peerread","label":0,"id":4808}
{"text":"The reviewers generally agreed that exploring policy search methods of this type is interesting, but the results presented in the paper are not at the standard required for publication. There are no comparisons of any sort, and the only task that is tested is trivially simple, so it's impossible to conclude anything about the effectiveness of the method. Despite the author promising to add additional experiments, nothing was added in the current draft. Besides this, reviewers raised concerns about the relevance of this approach to ICLR. The crucial point here is that it's unclear if the method will scale -- while there is nothing wrong in principle in proposing a general policy search method, there isn't really a compelling argument that can be made that it is suitable for learning representations if there is no plausible story for how it will scale to sufficiently complex policy classes that can actually learn representations.","model":"human","source":"peerread","label":0,"id":4809}
{"text":"This paper presents iterative PoWER, an off-policy variation on PoWER, a policy gradient algorithm in the reward-weighted family.\n\nI'm not familiar enough with this type lower bound scheme to comment on it. It looks like the end result is less conservative step sizes in policy parameter space. All expectation-based algorithms (and their KL-regularized cousins a-la TRPO) take smallish steps, and this might be a sensible way to accelerate them.\n\nThe description of the experiments in Section VI is insufficient for reproducibility. Is \"The cart moved right\" supposed to be \"a positive force is applied to the cart\"? How is negative force applied? What is the representation of the state? What is the distribution of initial states? A linear policy is insufficient for swing up and balance of a cart-pole. Are you only doing balancing? What is the noise magnitude of the policy? How was it chosen? How long were the episodes?\n\nThe footnote at the bottom of page 8 threw me off. If you're using Newton's method, where is the discussion of gradients and Hessians? I thought the argmax_theta operator was a stand-in for an EM-style step, which I how I read Eq (8) in the Kober paper. \n","model":"human","source":"peerread","label":0,"id":4810}
{"text":"The paper considers the problem of reinforcement learning where the number of policy updates is required to be low. The problem is well motivated and the author provides an interesting modification to the PoWER algorithm, along with variational bounds on the value function (lemmas 3,4) which are interesting in themselves. They also provide numerical results on the cartpole problem and a problem in online advertising with real data. Overall this is a strong, well-written paper. My main reservation is whether it is completely appropriate for ICLR, since the log-concavity assumption the model relies on appear to restrict to simpler models where representations will be not in fact be learned.\n\nOther comments:\n- There is a general lack of baselines in the numerical experiment section. I acknowledge this is somewhat of an unusual setting, but even a simple, well-justified baseline would have been welcome. Since cartpole is a relatively simple problem and the advertising dataset is presumably private, perhaps a way to generate a synthetic advertising dataset would have been interesting.\n\n- I was confused by the control variates as constant scalars - are they meant to be constant baselines? And if so, they appear to be treated as hyperparameters -  why are they not learned or estimated?\n\n- There is an interesting section on constrained optimization, but as it is, feels a bit disconnected from the rest of the paper. It appears applicable to the problem of online advertising, but is not mentioned in the corresponding experimental section. Also might be worth adding a citation to the literature of constrained MDPs which develops similar lagrangian ideas.","model":"human","source":"peerread","label":0,"id":4811}
{"text":"The paper presents an interesting modification to PoWER algorithm that is well motivated. The main limitation of this paper is the lack of comparison with other methods and on richer problems. The experiments haven't given confidence to show its claimed benefits, generality and scalability over prior methods. Giving this confidence doesn\u2019t necessarily require running your method on all large-scale domains or doing exhaustic hyper-parameter search, but for example it could go beyond current domains. Cartpole only optimizes 5 parameters. Ad targeting task lacks comparison with alternative methods. Since this method is built on PoWER and closely connected with RWR, it is likely there are limits to performance which may become apparent when the method is tried on other domains and with other benchmark methods, e.g. even standard ones like importance sampling-based off-policy learning is known to suffer in high-dimensional or continuous action space; limits of RWR\/PoWER-like methods based on their connection with entropy-regularized RL. ","model":"human","source":"peerread","label":0,"id":4812}
{"text":"A few issues with this paper:\n1- I find finding #2 trivial and unworthy of mention, but the author don't seem to agree with me that it is. See discussions.\n2- Finding #1 relies on Fig #4, which appears very noisy and doesn't provide any error analysis. It makes me question how robust this finding is. One would have naively expected the power usage trend to mirror Fig #3, but given the level of noise, I can't convince myself whether the null hypothesis of there being no dependency between batch size and power consumption is more likely than the alternative.\n3- Paper is unfriendly to colorblind readers (or those with B\/W printers)\n\nOverall, this paper is a reasonable review of where we are in terms of SOTA vision architectures, but doesn't provide much new insight. I found most interesting the clear illustration that VGG models stand out in terms of being a bad tradeoff in resource-constrained environments (too many researchers are tempted to benchmark their model compression algorithm on VGG-class models because that's always where one can show 10x improvements without doing much.)","model":"human","source":"peerread","label":0,"id":4813}
{"text":"The paper presents an evaluation of off-the-shelf image classification architectures. The findings are not too surprising and don't provide much new insight.","model":"human","source":"peerread","label":0,"id":4814}
{"text":"The paper evaluates recent development in competitive ILSVRC CNN architectures from the perspective of resource utilization. It is clear that a lot of work has been put into the evaluations. The findings are well presented and the topic itself is important.\n\nHowever, most of the results are not surprising to people working with CNNs on a regular basis. And even if they are, I am not convinced about their practical value. It is hard to tell what we actually learn from these findings when approaching new problems with computational constraints or when in production settings. In my opinion, this is mainly because the paper does not discuss realistic circumstances.\n\nMain concerns:\n1) The evaluation does not tell me much for realistic scenarios, that mostly involve fine-tuning networks, as ILSVRC is just a starting point in most cases. VGG for instance really shines for fine-tuning, but it is cumbersome to train from scratch. And VGG works well for compression, too. So possibly it is a very good choice if these by now standard steps are taken into account. Such questions are of high practical relevance!\n\n2) Compressed networks have a much higher acc\/parameter density, so comparison how well models can be compressed is important, or at least comparing to some of the most well-known and publicly available compressed networks.\n\n3) There is no analysis on the actual topology of the networks and where the bottlenecks lie. This would be very useful to have as well.\n\nMinor concern:\n1) Why did the authors choose to use batch normalization in NiN and AlexNet?","model":"human","source":"peerread","label":0,"id":4815}
{"text":"The authors did solid work in collecting all the reported data. However, most findings don't seem to be too surprising to me:\n\n- Finding #1 mainly shows that all architectures and batch sizes manage to utilize the GPU fully (or to the same percentage).\n\n- Regarding Finding #2, I agree that from a linear relationship in Figure 9 you could conclude said hyperbolic relationship.\nHowever, for this finding to be relevant, it has to hold especially for the latest generations of models. These cluster in the upper left corner of Figure 9 and on their own do not seem to show too much of a linear behaviour. Therefore I think there is not enough evidence to conclude asymptotic hyperbolic behaviour: For this the linear behaviour would have to be the stronger, the more models approach the upper left corner.\n\n- Finding #3 seems to be a simple conclusion from finding #1: As long as slower models are better and faster models do draw the same power, finding #3 holds.\n\n- Finding #4 is again similar to finding #1: If all architectures manage to fully utilize the GPU, inference time should be proportional to the number of operations.\n\nMaybe the most interesting finding would be that all tested models seem to use the same percentage of computational resources available on the GPU, while one might expect that more complex models don't manage to utilize as much computational resources due to inter-dependencies. However actual GPU utilization was not evaluated and as the authors choose to use an older GPU, one would expect that all models manage to make use of all available computational power.\n\nAdditionally, I think these findings would have to be put in relation with compressing techniques or tested on actual production networks to be of more interest.\n","model":"human","source":"peerread","label":0,"id":4816}
{"text":"This paper presents a hierarchical attention-based method for document classification. \nThe main idea is to first run a bidirectional LSTM to get global context vector, and then run another attention-based bidirectional LSTM that uses the final hidden state from the first pass to weight local context vectors (TS-ATT). \nA simpler architecture that removes the first LSTM and uses the output of the second LSTM as the global context vector is also proposed (SS-ATT). \nExperiments on three datasets are presented, however the results are mostly not state-of-the-art.\n\nI think the idea is nice, but the experiment results are not convincing enough to justify this new model architecture. \nWhy is your Yelp 2013 dataset smaller than the original Tang et al, 2015 paper that has ~300k documents? \nI noticed your other datasets are also quite small. Is it because your model is difficult to scale to large datasets?\nYou should also include results from Tang et al., 2015 in Table 2 that achieves 65.1% accuracy on Yelp 2013 (why is your number so much lower?)\nI also suggest removing phrases such as \"Learning to Understand\" when presenting their model.\nOverall, I think that this submission is a better fit for the workshop.\n\nMinor comments:\n- gloal -> global\n- Not needing a pretrained embeddings, while of course nice, is not that big of a deal. Various models will work just fine without pretrained embeddings.","model":"human","source":"peerread","label":0,"id":4817}
{"text":"The consensus amongst reviewers' was that this paper, incorporating global context into classification, is not ready for publication. It provides no novelty over similar methods. The evaluation did not convince most of the reviewers. The paper seems peppered with unjustified and (as rather bluntly, but accurately, put by one reviewer) unscientific claims. Disappointingly, the authors did not respond to pre-review questions. Perhaps more understandably, they did not respond to the uniformly negative reviews of their paper to defend it. I see no reason to diverge from the reviewers' recommendation, and advocate rejection of this paper.","model":"human","source":"peerread","label":0,"id":4818}
{"text":"The authors did not bother responding or fixing any of the pre-review comments. Hence I repeat here:\n\nPlease do not make incredibly unscientific statements like this one:\n\"The working procedure of this model is just like how we human beings read a text and then answer a related question. \"\nReally, \"humans beings\" have an LSTM like model to read a text? Can you cite an actual neuroscience paper for such a claim? The answer is no, so please delete such statements from future drafts.\n\nGenerally, your experiments are about simple classification and the methods you're competing against are simple models like NB-SVM. So I would change the title, abstract ad introduction accordingly and not attempt hyperbole like \"Learning to Understand\" in the title.\n\nLastly, your attention level approach seems similar to dynamic memory networks by Kumar et al. they also have experiments for sentiment and it would be interesting to understand the differences to your model and compare to them.\n\nOther reviewers included further missing related work and fitting this paper into the context of current literature.\nGiven that no efforts were made to fix the pre-review questions and feedback, I doubt this will become ready in time for publication.","model":"human","source":"peerread","label":0,"id":4819}
{"text":"The paper proposes to enhance the attention mechanism for sentiment classification by using global context computed by a Bi-LSTM. The proposed models outperform many existing models in the literature on 3 sentiment analysis datasets. \n\nThe key idea of using Bi-LSTM to compute global context for attention is actually not novel, as proposed several times in the literature, e.g., Luong et al (2015) and Shen & Lee (2016). Especially, Luong et al (2015) already proposed to combine global context with local context for attention.\n\nRegarding to the experiments, of course it would be nice if the model can work well without the need of tricks like dropout or pre-trained word embeddings. However, it would be even better if the model can work well using those tricks. The authors should show results of the models using those tricks and compare them to the results in the literature.  \n\n\nRef:\nLuong et al. Effective Approaches to Attention-based Neural Machine Translation. EMNLP 2015","model":"human","source":"peerread","label":0,"id":4820}
{"text":"The paper explores a VAE architecture and training procedure that allows to generate new samples of a concept based on several exemplars that are shown to the model. The proposed architecture processes the set of exemplars with a recurrent neural network and aggregation procedure similar to the one used in Matching Networks. The resulting \"summary\" is used to condition a generative model (a VAE) that produces new samples of the same kind as the exemplars shown. The proposed aggregation and conditioning procedure are better suited to sets of exemplars that come from several classes than simple averaging.\nPerhaps surprisingly the model generalizes from generation conditioned on samples from 2 classes to generation conditioned on samples from 4 classes.\nThe experiments are conducted on the OMNIGLOT dataset and are quite convincing. An explicit comparison to previous works is lacking, but this is explained in the appendices, and a comparison to architectures similar to previous work is presented.","model":"human","source":"peerread","label":0,"id":4821}
{"text":"This work extends variational autoencoders to adapt to a new dataset containing a small number of examples. While this work is promising, two of the reviewers had serious concerns about clarity. A new version of the paper has been submitted, however I still find it too hard to follow and would find it hard to accurately describe what was done having read the main body of the paper.","model":"human","source":"peerread","label":0,"id":4822}
{"text":"We would like to thank the reviewers for their efforts!\n\nWe received many useful comments that helped us to improve the text.\nUsing the provided feedback, we have edited the paper and uploaded a new version which, hopefully, addresses the issues raised in reviews.\n\nWe have also sent our response to all three reviews and would be glad to continue the discussion.","model":"human","source":"peerread","label":0,"id":4823}
{"text":"This paper presents a meta-learning algorithm which learns to learn generative models from a small set of examples. It\u2019s similar in structure to the matching networks of Vinyals et al. (2016), and is trained in a meta-learning framework where the inputs correspond to datasets. Results are shown on Omniglot in terms of log-likelihoods and in terms of generated samples. \n\nThe proposed idea seems reasonable, but I\u2019m struggling to understand various aspects of the paper. The exposition is hard to follow, partly because existing methods are described using terminology fairly different from that of the original authors. Most importantly, I can\u2019t tell which aspects are meant to be novel, since there are only a few sentences devoted to matching networks, even though this work builds closely upon them. (I brought this up in my Reviewer Question, and the paper has not been revised to make this clearer.)\n\nI\u2019m also confused about the meta-learning setup. One natural formulation for meta-learning of generative models would be that the inputs consist of small datasets X, and the task is to predict the distribution from which X was sampled. But this would imply a uniform weighting of data points, which is different from the proposed method. Based on 3.1, it seems like one additionally has some sort of query q, but it\u2019s not clear what this represents. \n\nIn terms of experimental validation, there aren\u2019t any comparisons against prior work. This seems necessary, since several other methods have already been proposed which are similar in spirit. \n","model":"human","source":"peerread","label":0,"id":4824}
{"text":"This paper proposes an interesting idea for rapidly adapting generative models in the low data regime. The idea is to use similar techniques that are used in one-shot learning, specifically ideas from matching networks. To that end, the authors propose the generative matching networks model, which is effectively a variational auto-encoder that can be conditioned on an input dataset. Given a query point, the model matches the query point to points in the conditioning set using an attention model in an embedding space (this is similar to matching networks). The results on the Omniglot dataset show that this method is successfully able to rapidly adapt to new input distributions given few examples.\n\nI think that the method is very interesting, however the major issue for me with this paper is a lack of clarity. I outline more details below, but overall I found the paper somewhat difficult to follow. There are a lot of details that I feel are scattered throughout, and I did not get a sense after reading this paper that I would be able to implement the method and replicate the results. My suggestion is to consolidate the major implementation details into a single section, and be explicit about the functional form of the different embedding functions and their variants.\n\nI was a bit disappointed to see that weak supervision in the form of labels had to be used. How does the method perform in a completely unsupervised setting? This could be an interesting baseline.\n\nThere is a lack of definition of the different functions. Some basic insight into the functional forms of f, g, \\phi, sim and R would be nice. Otherwise it is very unclear to me what\u2019s going on.\n\nSection 3.2: \u201conly state of the recurrent controller was used for matching\u201d, my reading of this section (after several passes) is that the pseudo-input is used in the place of a regular input. Is this correct? Otherwise, this sentence\/section needs more clarification. I noticed upon further reading in section 4.2 that there are two versions of the model: one in which a pseudo input is used, and one in which a pseudo input is not used (the conditional version). What is the difference in functional form between these? That is, how do the formulas for the embeddings f and g change between these settings?\n\n\u201csince the result was fully contrastive we did not apply any further binarization\u201d what does it mean for a result to be fully contrastive?\n\nFor clarity, the figures and table refer to the number of shots, but this is never defined. I assume this is T here. This should be made consistent.\n\nFigure 2: why is the value of T only 9 in this case? What does it mean for it to be 0? It is stated earlier that T should go up to 20 (I assume #shot corresponds to T). It also looks like the results continue to improve with an increased number of steps, I would like to see the results for 5 and maybe 6 steps as well. Presumably there will come a point where you get diminishing returns.\n\nTable 1: is the VAE a fair baseline? You mention that Ctest affects Pd() in the evaluation. The fact that the VAE does not have an associated Ctest implies that the two models are being evaluated with a different metric. Can the authors clarify this? It\u2019s important that the comparison is apples-to-apples.\n\nMNIST is much more common than Omniglot for evaluating generative models. Would it be possible to perform similar experiments on this dataset? That way it can be compared with many more models.\n\nFurther, why are the negative log-likelihood values monotonically decreasing in the number of shots? That is, is there ever a case where increasing the number of shots can hurt things? What happens at T=30? 40?\n\nAs a minor grammatical issue, the paper is missing determiners in several sentences. At one point, the model is referred to as \u201cshe\u201d instead of \u201cit\u201d. \u201cOn figure 3\u201d should be changed to \u201cin figure 3\u201d in the experiments section.\n","model":"human","source":"peerread","label":0,"id":4825}
{"text":"UPDATE:  I have read the authors' responses.  I did not read the social media comments about this paper prior to reviewing it.  \n\nI appreciate the authors' updates in response to the reviewer comments.  Overall, however, my review stands.  The authors have taken a task that had not yet been addressed with a straightforward modern deep learning approach, and addressed it with such an approach.  I assume that if we pick up any task that hasn't been worked on for a while, and give it a solid deep learning treatment, we will do well.  I do not see such papers as a contribution to ICLR, unless they also provide new insights, analysis, or surprising results (which, to my mind, this paper does not).  This is a general point and the program chairs may disagree with it, of course.\n\nI have removed my recommendation that this be accepted as a workshop paper, as I have since noticed that the workshop track this year has a different focus.  \n\n************************\n\nORIGINAL REVIEW:\n\nThe authors show that an appropriately engineered LSTM+CNN+CTC network does an excellent job of lipreading on the GRID corpus.  This is a nice result to know about--yet another example of a really nice result that one can get the first time one applies such methods to an old task--and all of the work that went into getting it looks solid (and likely involved some significant engineering effort).  However, this in itself is not sufficiently novel for publication at ICLR.  The paper also needs to be revised to better represent prior work, and ideally remove some of the vague motivational language.  Some specifics on what I think needs to be revised:\n\n- First, the claim of being the first to do sentence-level lipreading.  As mentioned in a pre-review comment, this is not true.  The paper should be revised to discuss the prior work on this task (even though much of it used data that is not public).  Ideally the title should also be changed in light of this.\n\n- The comparison with human lipreaders needs to be qualified a bit.  This task is presumably very unnatural for humans because of the unusual grammar, so perhaps what you are showing is that a machine can better take into account the strong contraints.  This is great, but not a general statement about LipNet vs. humans.\n\n- The paper contains some unnecessary motivational platitudes.  We do not need to invoke Easton and Basala 1982 to motivate modeling context in a linguistic sequence prediction task, and prior work using older sequence models (e.g. HMMs) for lipreading has modeled context as well.  The McGurk effect does not show that lipreading plays a crucial role in human communication.\n\n- It is worth noting that even without the spatial convolution, your Baseline-2D already does extremely well.  So I am not sure about the \"importance of spatiotemporal feature extraction\" as stated in the conclusion.\n\nSome more minor comments, typos, etc.:\n\n- citations for LSTMs, CTC, etc. should be provided the first time they are mentioned.\n- I did not quite follow the justification for upsampling.\n- what is meant by \"lip-rounding vowels\"?  They seem to include almost all English vowels.\n- Did you consider keeping the vowel visemes V1-V4 separate rather than collapsing them into one?  Since you list Neti et al.'s full viseme set, it is worth mentioning why you modified it.\n- \"Given that the speakers are British, the confusion between \/aa\/ and \/ay\/...\" -- I am not sure what this has to do with British speakers, as the relationship between these vowels exists in other English dialects as well (e.g. American).\n- The discussion about confusions within bilabial stops and within alveolar stops is a bit mismatched with the actual confusion data in Fig. 3(b,c).  For example, there does not seem to be any confusion between \/m\/ and \/b\/ or between \/m\/ and \/p\/.\n- \"lipreading actuations\":  I am not sure what \"actuations\" means in this context\n- \"palato-alvealoar\" --> \"palato-alveolar\"\n- \"Articulatorily alveolar\" --> \"Alveolar\"?","model":"human","source":"peerread","label":0,"id":4826}
{"text":"Let me start by saying that your area chair does not read Twitter, Reddit\/ML, etc. The metareview below is, therefore, based purely on the manuscript and the reviews and rebuttal on OpenReview.\n \n The goal of the ICLR review process is to establish a constructive discussion between the authors of a paper on one side and reviewers and the broader machine-learning community on the other side. The goal of this discussion is to help the authors leverage the community for improving their manuscript.\n \n Whilst one may argue that some of the initial reviews could have provided a more detailed motivation for their rating, there is no evidence that the reviewers were influenced (or even aware of) discussions about this paper on social or other media --- in fact, none of the reviews refers to claims made in those media. Suggestions by the authors that the reviewers are biased by (social) media are, therefore, unfounded: there can be many valid reasons for the differences in opinion between reviewers and authors on the novelty, originality, or importance of this work. The authors are free to debate the opinion of the reviewers, but referring to the reviews as \"absolute nonsense\", \"unreasonable\", \"condescending\", and \"disrespectful\" is not helping the constructive scientific discussion that ICLR envisions and, frankly, is very offensive to reviewers who voluntarily spend their time in order to improve the quality of scientific research in our field.\n \n Two area chairs have read the paper. They independently reached the conclusion that (1) the reviewers raise valid concerns with respect to the novelty and importance of this work and (2) that the paper is, indeed, borderline for ICLR. The paper is an application paper, in which the authors propose the first\u00caend-to-end sentence level lip reading using deep learning. Positive aspects of the paper include:\n \n - A comprehensive and organized review about previous work.\n - Clear description of the model and experimental methods.\n - Careful reporting of the results, with attention to detail.\n - Proposed method appears to perform better than the prior state-of-the-art, and generalizes across speakers.\n \n However, the paper has several prominent negative aspects as well:\n \n - The GRID corpus that is used for experimentation has very substantial (known) limitations. In particular, it is constructed in a way that leads to a very limited (non-natural) set of sentences.\u00ca(For every word, there is an average of just 8.5 possible options the model has to choose from.)\n - The paper overstates some of its claims. In particular, the claim that the model is \"outperforming experienced human lipreaders\" is questionable: it is not unlikely that model achieves its performance by exploiting unrealistic statistical biases in the corpus that humans cannot \/ do not exploit. Similarly, the claims about the \"sentence-level\" nature of the model are not substantiated: it remains unclear what aspects of the model make this a sentence-level model, nor is there much empirical evidence that the sentence-level treatment of video data is helping much (the NoLM baseline is almost as good as LipNet, despite the strong biases in the GRID corpus).\n - The paper makes several other statements that are not well-founded. As one of the reviewers correctly remarks, the McGurk effect does not show that lipreading plays a crucial role in human communication (it merely shows that vision can influence speech recognition). Similarly, the claim that \"Bi-GRUs are crucial for efficient further aggregation\" is not supported by empirical evidence.\n \n A high-level downside of this paper is that, while studying a relevant application of deep learning, it presents no technical contributions or novel insights that have impact beyond the application studied in the paper.","model":"human","source":"peerread","label":0,"id":4827}
{"text":"We strongly request that our paper be reviewed for its contents, contributions, significance, originality, and impact. We also would like to highlight the following three points to reviewers and readers. \n\n(i) Claim of \"sentence-level\" lipreading: This was the main concern of reviewers 1 and 2, and likely why they gave us the surprisingly low scores of 4 (7 being borderline for acceptance). Our claim is that we proposed the first \"end-to-end sentence-level\" lipreading approach, and we have edited the paper to make this precise. The phrase \u201cend-to-end\u201d is very important. The previous few attempts at sentence-level lipreading used heuristic pipelines and obtained poor results. LipNet, on the other hand, is fully end-to-end and achieves state-of-the-art results by a significant margin. on the largest available public dataset  (with over 24 hours of video, making it far from trivial). Given the removal of the phrase sentence-level, the reviews of reviewers 1 and 2 regarding prior work are not longer applicable.\n\n(ii) Is the dataset too simple? GRID despite all its shortcomings, is the largest available public dataset with over 24 hours of video (more than 2,000,000 frames and 64,000 possible sentences), making it more comparable to ImageNet than to say MNIST or CIFAR-10. May deep learning papers have used simplified versions of this dataset (eg restrictions to speaker-dependent recognition, or classification only) and even then obtained worse results. Moreover, we have embarked on more commercial datasets with success as reported in this keynote by NVIDIA's CEO at CES (177,393 attendees). See this video (","model":"human","source":"peerread","label":0,"id":4828}
{"text":"The authors present a well thought out and constructed system for performing lipreading. The primary novelty is the end-to-end nature of the system for lipreading, with the sentence-level prediction also differentiating this with prior work. The described neural network architecture contains convolutional and recurrent layers with a CTC sequence loss at the end, and beam search decoding with an LM is done to obtain best results. Performance is evaluated on the GRID dataset, with some saliency map and confusion matrix analysis provided as well.\n\nOverall, the work seems of high quality and clearly written with detailed explanations. The final results and analysis appear good as well. One gripe is that that the novelty lies in the choice of application domain as opposed to the methods. Lack of word-level comparisons also makes it difficult to determine the importance of using sentence-level information vs. choices in model architecture\/decoding, and finally, the GRID dataset itself appears limited with the grammar and use of a n-gram dictionary. Clearly the system is well engineered and final results impress, though it's unclear how much broader insight the results yield.","model":"human","source":"peerread","label":0,"id":4829}
{"text":"- Proven again that end to end training with deep networks gives large\ngains over traditional hybrid systems with hand crafted features. The results \nare very nice for the small vocabulary grammar task defined by the GRID corpus. The engineering here is clearly very good, will be interesting to see the performance on large vocabulary LM tasks. Comparison to human lip reading performance for conversational speech will be very interesting here.\n\n- Traditional AV-ASR systems which apply weighted audio\/visual posterior fusion reduce to pure lip reading when all the weight is on the visual, there are many curves showing performance of this channel in low audio SNR conditions for both grammar and LM tasks.\n\n- Traditional hybrid approaches to AV-ASR are also sentence level sequence trained with fMPE\/MPE\/MMI etc. objectives (see old references), so we cannot say here that this is the first sentence-level objective for lipreading model (analogous to saying there was no sequence training in hybrid LVCSR ASR systems before CTC). \n\n","model":"human","source":"peerread","label":0,"id":4830}
{"text":"\n\nThis is an interesting paper. Here are some comments:\n\n\n1) Testing on 4 subjects only is rather limited given that there are 33 subjects. For example Ngiam et al. used 18 subjects for training and 18 for testing on CUAVE which contains a similar number of subjects. So a similar setup would be 17 subjects for training and 16 for testing. This would make the results much more convincing.\n\n2) The authors state that the performance of human lipreaders is around 20% and cite Hilder et al. In that paper, the human performance is around 70%, please correct this claim. In addition, in that paper machine lip-reading outperforms humans as well so please make this point clear. \n\n3) The state of the art performance on GRID is not 79.6% as mentioned in the text. It is the one mentioned in the other comments. Actually, as of last week there is a new state-of-the-art ","model":"human","source":"peerread","label":0,"id":4831}
{"text":"Dear all,\n\nmany thanks for sharing the paper, which I did find hugely interesting - while the GRiD corpus is small, I do agree that it is currently a good task for taking first steps towards video-based and audio-visual speech recognition, and I believe the work you did here will ultimately be very useful for lipreading (and for lipreading-based speech enhancement) in more general scenarios.\n\nI do have one comment regarding the prior-state-of-the-art system that you cite: As far as I know, our group's Interspeech 2016 holds the record for best lipreading performance on the GRiD corpus, with 86.4% word accuracy (this does include a grammar, but it might still be interesting).\nPlease cite: S. Gergen, S. Zeiler, A. Hussen Abdelaziz, R. Nickel and D. Kolossa: \" Dynamic Stream Weighting for Turbo-Decoding-Based Audiovisual ASR,\" in Proc. Interspeech 2016, San Francisco, Sept. 2016.\n","model":"human","source":"peerread","label":0,"id":4832}
{"text":"- It is not fair to use a character 5-gram language model and compare to [1] which doesn't use any language modeling, the authors should also report their results without using any language modeling (IMHO using a language model with a limited vocabulary corpus like GRID which has only 51 words and 64000 possible sentence makes the results misleading, which, I think, is the reason why no language modeling was used in [1])\n\n- It is not fair to augment the training data to 15x, then compare to [1] which doesn't use any data augmentation\n\n- It should be mentioned and cited that a CNN+RNN+CTC architecture is not novel and has been widely used in literature for sequence recognition tasks (e.g. [2],[3],[4])\n\n- I encourage the authors to also try a ConvLSTM [5], which have recently shown very promising performance in a number of video-related tasks\n\n---------------------------------------------------\n[1] M. Wand, J. Koutnik, and J. Schmidhuber. Lipreading with long short-term memory.\n[2] B.  Shi,  X.  Bai,  and  C.  Yao. An  end-to-end  trainable  neural  network for  image-based  sequence  recognition  and  its  application  to  scene  text recognition.\n[3]  Z.  Xie,  Z.  Sun,  L.  Jin,  Z.  Feng,  and  S.  Zhang. Fully  convolutional recurrent network for handwritten chinese text recognition.\n[4] Li, H., Shen, C.: Reading car license plates using deep convolutional neural networks and lstms\n[5] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional LSTM network: A machine learning approach for precipitation nowcasting.","model":"human","source":"peerread","label":0,"id":4833}
{"text":"This corpus is a small data set created 10 years ago by colleagues and friends (Martin Cooke, Jon Barker, Stuart Cunningham and Xu Shao) at the Department of Computer Science. I recall that Martin gave me a bottle of Spanish wine for my trouble.\nAs far as I remember the corpus, it was designed to remove higher order language structure. That structure that (I believe) is used by humans to cue on when reading lips.\n\nThe corpus has a limited vocabulary and a single syntax grammar. So while it's promising to perform well on this data, it's not really ground breaking, particularly if you are interested in sentence models: the corpus sentence structure is super simple.\nSo while the model may be able to read my lips better than a human, it can only do so when I say a meaningless list of words from a highly constrained vocabulary in a specific order. That may be an advance, but it's not one worthy of disturbing me on a Sunday (serves me right for reading Twitter on a Sunday).\n\nI'm not making a comment about whether the paper should be accepted or not, but merely reacting to the large number of claims for the paper we are seeing on social media. The particular result for this data set may well be state of the art.\n","model":"human","source":"peerread","label":0,"id":4834}
{"text":"all reviewers agree that the paper is not convincing enough at this stage but needs more work to be ready for ICLR (e.g. missing comparisons to other existing methods).","model":"human","source":"peerread","label":0,"id":4835}
{"text":"This paper misses discussion of 'Diversity Networks' published in ICLR 2016 (","model":"human","source":"peerread","label":0,"id":4836}
{"text":"Summary:\nIn this paper, the authors introduce NoiseOut, a way to reduce parameters by pruning neurons from a network. \nThey do this by identifying pairs of neurons produce the most correlated outputs, and replacing the pair by one neuron, and then appropriately adjusting weights.\nThis technique relies on neurons having high correlations however, so they introduce an additional output neuron -- a noise output, which results in the network trying to predict the mean of the noise distribution.\nAs this is a constant, it increases correlation between neurons.\nExperiments test this out on MNIST and SVHN\n\nComments:\nThis is an interesting suggestion on how to prune neurons, but more experiments (on larger datasets) are probably need to be convincing that this is an approach that is guaranteed to work well. \n\nEquation (5) seems to be very straightforwards?\n\nIt seems like that for larger datasets, more noise outputs might have to be added to ensure higher correlations? Is there a downside to this in terms of the overall accuracy?\n\nThe paper is presented clearly, and was definitely interesting to read, so I encourage the authors to continue this line of work.\n","model":"human","source":"peerread","label":0,"id":4837}
{"text":"This paper proposes and tests two ideas. (1) a method of pruning networks by identifying highly correlated neuron pairs, pruning one of the pair, and then modifying downstream weights to compensate for the removal (which works well if the removed neurons were highly correlated). (2) a method, dubbed NoiseOut, for increasing neuron correlation by adding auxiliary noise target outputs to the network during training.\n\n\nThe first idea (1) is fairly straightforward, and it is not clear if it has been tried before. It does seem to work.\n\n\nThe second idea (2) is of unclear value and seems to this reviewer that it may merely add a regularizing effect. Comments in this direction:\n - In Fig 4 (right), the constant and Gaussian treatments seem to produce the same effect in both networks, right? And the Binomial effect seems the same as No_Noise. If this is true, can we conclude that the NoiseOut targets are simply serving to regularize the network, that is, to reduce its capacity slightly?\n - To show whether this effect is true, one would need to compare to other methods of reducing the network capacity, for example: by reducing the number of neurons, by applying L2 regularization of various values, or by applying Dropout of various strengths. Fig 7 makes an attempt at this direction, but critically misses several comparison treatments: \u201cPruned without any regularization\u201d, \u201cPruned with only L2\u201d, and \u201cPruned with only DropOut\u201d. Have these experiments been run? Can their results be included and used to produce plots like Fig 5 and Fig 7?\n\nWithout these comparisons, it seems impossible to conclude that NoiseOut does anything but provide similar regularization to DropOut or L2.\n\n\nThe combined ideas (1) + (2) DO produce a considerable reduction in parameters, but sadly the experiments and exposition are somewhat too lacking to really understand what is going on. With a little more work the paper could be quite interesting, but as is it should probably not be accepted.\n\n\nAdditional comments:\n - Section 4 states: \u201cIn all of these experiments, the only stop criteria is the accuracy decay of the model. We set the threshold for this criteria to match the original accuracy; therefore all the compressed network have the same accuracy as the original network.\u201d Is this accuracy the train accuracy or test accuracy? If train, then test accuracy needs to be shown (how much test performance is lost when pruning?). If test, then this would typically be referred to as \u201ccheating\u201d and so the choice needs to be very clearly stated and then defended.\n - Lowercase rho is used to indicate correlation but this is never actually specified, which is confusing for. Just state once that it indicates correlation.\n - How do these results compare to other pruning methods? No numerical comparison is attempted.","model":"human","source":"peerread","label":0,"id":4838}
{"text":"The paper proposes to prune a neural network by removing neurons whose operation is highly correlated with other neurons. The idea is nice and somewhat novel - most pruning methods concentrate on removal of individual weights, however I haven't done a through research on this topic. However, the experimental and theoretical justification of this method need to be improved before publication:\n\n1. Experiments. The authors do not report accuracy degradation while pruning in the tables, laconically stating that the networks did not degrade. This is not convincing. The only details are given in Figure 5, however this Figure disagrees with Table 2: in the Table, the number of parameters ranges from 40k-600k, while the Figure pictures the range 12k-24k. Unless more details are provided, simply claiming that a network can remove 50% neurons with no number on the degradation of accuracy is not convincing.\n\n2. Theory. The proofs do not match the experimental conditions and make unreasonable assumptions. The proofs show that in the absence of biases a network with a constant output will have two correlated neurons that generate the output offset. However, this is exactly why networks have biases and doesn't explain why noise injection helps (the proof suggests that all should be fine with deterministic auxiliary neuron). My interpretation is that the noisy output injects gradient noise (see e.g. the concurrent ICLR submission ","model":"human","source":"peerread","label":0,"id":4839}
{"text":"The paper proposed to analyze several recently developed machine readers and found that some machine readers could potentially take advantages of the entity marker (given that the same marker points out to the same entity). I usually like analysis papers, but I found the argument proposed in this paper not very clear.\n\nI like the experiments on the Stanford reader, which shows that the entity marker in fact helps the Stanford reader on WDW. I found that results rather interesting.\n\nHowever, I found the organization and the overall message of this paper quite confusing. First of all, it feels that the authors want to explain the above behavior with some definition of the \u201cstructures\u201d. However, I am not sure that how successful the attempt is. For me, it is still not clear what the structures are. This makes reading section 4 a bit frustrating. \n\nI am also not sure what is the take home message of this paper. Does it mean that the entity marking should be used in the MR models? Should we design models that can also model the entity reference at the same time? What are the roles of the linguistic features here? Should we use linguistic structure to overcome the reference issue?\n\nOverall, I feel that the analysis is interesting, but I feel that the paper can benefit from having a more focused argument.","model":"human","source":"peerread","label":0,"id":4840}
{"text":"The reviewers mostly agree that this paper offers valuable insights about a family of problems in automatic \"reading\" of text, as well as current solutions. The paper seems to fail to generate excitement because it doesn't really point the way forward. I disagree with some reviewers about the work's suitability for ICLR (as opposed to an ACL venue) since ML researchers are also thinking about these tasks now. The consensus is that the paper will have greater impact (wherever it is published) with a clearer message.","model":"human","source":"peerread","label":0,"id":4841}
{"text":"The paper aims to consolidate some recent literature in simple types of \"reading comprehension\" tasks involving matching questions to answers to be found in a passage, and then to explore the types of structure learned by these models and propose modifications. These reading comprehension datasets such as CNN\/Daily Mail are on the simpler side because they do not generally involve chains of reasoning over multiple pieces of supporting evidence as can be found in datasets like MCTest. Many models have been proposed for this task, and the paper breaks down these models into \"aggregation readers\" and \"explicit reference readers.\" The authors show that the aggregation readers organize their hidden states into a predicate structure which allows them to mimic the explicit reference readers. The authors then experiment with adding linguistic features, including reference features, to the existing models to improve performance.\n\nI appreciate the re-naming and re-writing of the paper to make it more clear that the aggregation readers are specifically learning a predicate structure, as well as the inclusion of results about dimensionality of the symbol space. Further, I think the effort to organize and categorize several different reading comprehension models into broader classes is useful, as the field has been producing many such models and the landscape is unclear. \n\nThe concerns with this paper are that the predicate structure demonstrated is fairly simple, and it is not clear that it provides insight towards the development of better models in the future, since the \"explicit reference readers\" need not learn it, and the CNN\/Daily Mail dataset has very little headroom left as demonstrated by Chen et al. 2016. The desire for \"dramatic improvements in performance\" mentioned in the discussion section probably cannot be achieved on these datasets. More complex datasets would probably involve multi-hop inference which this paper does not discuss. Further, the message of the paper is a bit scattered and hard to parse, and could benefit from a bit more focus.\n\nI think that with the explosion of various competing neural network models for NLP tasks, contributions like this one which attempt to organize and analyze the landscape are valuable, but that this paper might be better suited for an NLP conference or journal such as TACL.\n","model":"human","source":"peerread","label":0,"id":4842}
{"text":"This paper aims to provide an insightful and analytic survey over the recent literature on reading comprehension with the distinct goal of investigating whether logical structure (or predication, as the authors rephrased in their response) arises in many of the recent models. I really like the spirit of the paper and appreciate the efforts to organize rather chaotic recent literature into two unified themes: \"aggregation readers\" and \"explicit reference models\u201d. Overall the quality of writing is great and section 3 was especially nice to read. I\u2019m also happy with the proposed rewording from \"logical structure\" to \u201cpredication\", and the clarification by the authors was detailed and helpful.\n\nI think I still have slight mixed feelings about the contribution of the work. First, I wonder whether the choice of the dataset was ideal in the first place to accomplish the desired goal of the paper. There have been concerns about CNN\/DailyMail dataset (Chen et al. ACL\u201916) and it is not clear to me whether the dataset supports investigation on logical structure of interesting kinds. Maybe it is bound to be rather about lack of logical structure.\n\nSecond, I wish the discussion on predication sheds more practical insights into dataset design or model design to better tackle reading comprehension challenges. In that sense, it may have been more helpful if the authors could make more precise analysis on different types of reading comprehension challenges, what types of logical structure are lacking in various existing models and datasets, and point to specific directions where the community needs to focus more.\n\n","model":"human","source":"peerread","label":0,"id":4843}
{"text":"Hi, Dear reviewers,\n     \n   Thanks for the valuable suggestions on our paper and sorry to make you confused about the notations, we have updated the paper to make it more clear. Please see the latest version.","model":"human","source":"peerread","label":0,"id":4844}
{"text":"The paper proposed to analyze several recently developed machine readers and found that some machine readers could potentially take advantages of the entity marker (given that the same marker points out to the same entity). I usually like analysis papers, but I found the argument proposed in this paper not very clear.\n\nI like the experiments on the Stanford reader, which shows that the entity marker in fact helps the Stanford reader on WDW. I found that results rather interesting.\n\nHowever, I found the organization and the overall message of this paper quite confusing. First of all, it feels that the authors want to explain the above behavior with some definition of the \u201cstructures\u201d. However, I am not sure that how successful the attempt is. For me, it is still not clear what the structures are. This makes reading section 4 a bit frustrating. \n\nI am also not sure what is the take home message of this paper. Does it mean that the entity marking should be used in the MR models? Should we design models that can also model the entity reference at the same time? What are the roles of the linguistic features here? Should we use linguistic structure to overcome the reference issue?\n\nOverall, I feel that the analysis is interesting, but I feel that the paper can benefit from having a more focused argument.\n","model":"human","source":"peerread","label":0,"id":4845}
{"text":"This paper proposes to initialize the weights of a deep neural network layer-wise with a marginal Fisher analysis model, making use of potentially the similarity metric.\n \nPros: \nThere are a lot of experiments, albeit small datasets, that the authors tested their proposed method on.\n\nCons:\nlacking baseline such as discriminatively trained convolutional network on standard dataset such as CIFAR-10.\nIt is also unclear how costly in computation to compute the association matrix A in equation 4.\n\nThis is an OK paper, where a new idea is proposed, and combined with other existing ideas such as greedy-layerwise stacking, dropout, and denoising auto-encoders.\nHowever, there have been many papers with similar ideas perhaps 3-5 years ago, e.g. SPCANet. \n\nTherefore, the main novelty is the use of marginal Fisher Analysis as a new layer. This would be ok, but the baselines to demonstrate that this approach works better is missing. In particular, I'd like to see a conv net or fully connected net trained from scratch with good initialization would do at these problems.\n\nTo improve the paper, the authors should try to demonstrate without doubt that initializing layers with MFA is better than just random weight matrices.","model":"human","source":"peerread","label":0,"id":4846}
{"text":"The proposed approach consists in a greedy layer wise initialization strategy for a deep MLP model, which is followed by global gradient-descent with dropout for fine-tuning. The initialization strategy uses a first randomly initialized sigmoid layer for dimensionality expansion followed by 2 sigmoid layers whose weights are initialized by Marginal Fisher Analysis (MFA) which learns a linear dimensionality reduction based on a neighborhood graph constructed using class label information (i.e. supervised dimensionality reduction). Output layer is a standard softmax layer.\n\nThe approach is thus to be added to a growing list of heuristic layer-wise initialization schemes.\nThe particular choice of initialization strategy, while reasonable, is not sufficiently well motivated in the paper relative to alternatives, and thus feels rather arbitrary.\nThe paper lacks clarity in the description of the approach:  MFA is poorly explained with undefined notations (in Eq. 4, what is A? It has not been properly defined); the precise use of alluded denoising in the model is also unclear (is there really training of an additional denoting objective, or just input corruption?).\n\nThe question of the (arguably mild) inconsistency of applying a linear dimensionality reduction algorithm, that is trained without any sigmoid, and then passing its learned representation through a sigmoid is not even raised. This, in addition to the fact that sigmoid hidden layers are no longer commonly used (why did you not also consider using RELUs?).\n\nMore importantly I suspect methodological problems with the experimental comparisons: the paper mentions using *default* values for learning-rate and momentum, and having (arbitrarily?) fixed epoch to 400 (no early stopping?) and L2 regularization to 1e-4 for some models. \n*All* hyper parameters should always be properly hyper-optimized using a validation set (or cross-validation) including early-stopping, and this separately for each model under comparison (ideally also including layer sizes). This is all the more important since you are considering smallish datasets, so that the various initialization strategies act mainly as different indirect regularization schemes: they thus need to be carefully tuned. This casts serious doubts as to the amount of hyper-parameter tuning (close to none?) that went into training the alternative models used for comparison. \n\nThe Marginal Fisher Analysis dimensionality reduction initialization strategy may well offer advantages, but as it currently stands this paper doesn\u2019t yet make a sufficiently convincing case for it, nor provide useful insights into the nature of the expected advantages.\n\nI would also suggest, for image inputs such as CIFAR10, to use the qualitative tool of showing the filters (back projected to input space) learned by the different initialization schemes under consideration, as this could help visually gain insight as to what sets methods apart. \n\n","model":"human","source":"peerread","label":0,"id":4847}
{"text":"The authors pointed out some limitations of existing deep architectures, in particular hard to optimize on small or mid size datasets, and proposed to stack marginal fisher analysis (MFA) to build deep models. The proposed method is tested on several small to mid size datasets and compared with several feature learning methods. The authors also applied some existing techniques in deep learning, such as backprop, denoising and dropout to improve performance. \n\nThe new contribution of the paper is limited. MFA has long been proposed. The authors fail to theoretically or empirically justify the stacking of MFAs. The authors did not include any deep architectures that requires backprop over multiple layers in the comparison, which the authors set out to address, instead all the methods compared were learned layer by layer. Will a randomly initialized deep model such as DBN or CNN perform poorly on these datasets? It is also not clear how the authors came up with each particular model architecture and hyper-parameters used in the different datasets. The writing of the paper needs to be significantly improved. A lot of details were omitted, for example, how is dropout applied in the MFA. ","model":"human","source":"peerread","label":0,"id":4848}
{"text":"This paper proposes to initialize the weights of a deep neural network layer-wise with a marginal Fisher analysis model, making use of potentially the similarity metric.\n \nPros: \nThere are a lot of experiments, albeit small datasets, that the authors tested their proposed method on.\n\nCons:\nlacking baseline such as discriminatively trained convolutional network on standard dataset such as CIFAR-10.\nIt is also unclear how costly in computation to compute the association matrix A in equation 4.\n\nThis is an OK paper, where a new idea is proposed, and combined with other existing ideas such as greedy-layerwise stacking, dropout, and denoising auto-encoders.\nHowever, there have been many papers with similar ideas perhaps 3-5 years ago, e.g. SPCANet. \n\nTherefore, the main novelty is the use of marginal Fisher Analysis as a new layer. This would be ok, but the baselines to demonstrate that this approach works better is missing. In particular, I'd like to see a conv net or fully connected net trained from scratch with good initialization would do at these problems.\n\nTo improve the paper, the authors should try to demonstrate without doubt that initializing layers with MFA is better than just random weight matrices.\n","model":"human","source":"peerread","label":0,"id":4849}
{"text":"This paper presents a new technique for adapting a neural network to a new task for which there is not a lot of training data. The most widely used current technique is that of fine-tuning. The idea in this paper is to instead learn a network that learns features that are complementary to the fixed network. Additionally, the authors consider the setting where the new network\/features are \u201cstitched\u201d to the old one at various levels in the hieararchy, rather that it just being a parallel \u201ctower\u201d. \n\nThis work is similar in spirit (if not in some details) to the Progressive Nets paper by Rusu et al, as already discussed. The motivations and experiments are certainly different so this submission has merit on its own.\n\nThe idea of learning a \u201cresidual\u201d with the stitched connnections is very similar in spirit to the ResNet work. It would be nice to compare and contrast those approaches.\n\nI\u2019ve never seen a batch being used 5 times in a row during training, does this work better than just regular SGD?\n\nIn Figure 5 it\u2019d be nice to label the y-axis. That Figure would also benefit from not being a bar chart, but simply emulating Figure 4, which is much more readable!\n\nFigure 5 again: what is an untrained model? It\u2019s not immediately obvious why this is a good idea at all. Is TFT-1 simply fine-tuning one more layer than \u201cRetrain Softmax\u201d?\n\nI think that the results at the end of section 3 are a bit weak because of usage of a big network. I would definitely like to see how the results change if using a smaller net.\n\nThe authors claim throughout the paper that the purpose of the added connections and layers is to learn *complementary* features and they show this with some figures. The latter are a convinving evidence, but not proof or guarantee that this is what is actually happening. I suggest the authors consider adding an explicit constraint in their loss that encourages that, e.g. by having a soft orthogonality constraing (assuming one can project intermediate features to some common feature dimensionality). The usage of very small L2 regularization maybe achieves the same thing, but there\u2019s no evidence for that in the paper (in that we don\u2019t have any visualizations of what happens if there\u2019s no L2 reg.).\n\nOne of the big questions for me while reading the paper was how would an ensemble of 2 pre-trained nets would do on the tasks that the authors consider. This is especially relevant in the cars classification example, where I suspect that a strong baseline is that of fine-tuning VGG on this task, fine-tuning resnet on this task, and possibly training a linear combination of the two outputs or just averaging them naively.\n\nDisappointing that there are no results in figure 4, 5 and 8 except the ones from this paper. It\u2019s really hard to situate this paper if we don\u2019t actually know how it compares to previously published results.\n\n\nIn general, this was an interesting and potentially useful piece of work. The problem of efficiently reusing the previously trained classifier for retraining on a small set is certainly interesting to the community. While I think that this paper takes a good step in the right direction, it falls a bit short in some dimensions (comparisons with more serious baselines, more understanding etc).","model":"human","source":"peerread","label":0,"id":4850}
{"text":"The method was developed to provide an alternative for fine-tuning by augmenting a pre-trained network with new capacity. The differential from other related methods is low, and the evaluated baselines were not well-chosen, so this is not a strong submission.","model":"human","source":"peerread","label":0,"id":4851}
{"text":"This paper proposes a method of augmenting pre-trained networks for one task with an additional inference path specific to an additional task, as a replacement for the standard \u201cfine-tuning\u201d approach.\n\nPros:\n-The method is simple and clearly explained.\n-Standard fine-tuning is used widely, so improvements to and analysis of it should be of general interest.\n-Experiments are performed in multiple domains -- vision and NLP.\n\nCons:\n-The additional modules incur a rather large cost, resulting in 2x the parameters and roughly 3x the computation of the original network (for the \u201cstiched\u201d network).  These costs are not addressed in the paper text, and make the method significantly less practical for real-world use where performance is very often important.\n\n-Given these large additional costs, the core of the idea is not sufficiently validated, to me.  In order to verify that the improved performance is actually coming from some unique aspects of the proposed technique, rather than simply the fact that a higher-capacity network is being used, some additional baselines are needed:\n(1) Allowing the original network weights to be learned for the target task, as well as the additional module.  Outperforming this baseline on the validation set would verify that freezing the original weights provides an interesting form of regularization for the network.\n(2) Training the full module\/stitched network from scratch on the *source* task, then fine-tuning it for the target task.  Outperforming this baseline would verify that having a set of weights which never \u201csees\u201d the source dataset is useful.\n\n-The method is not evaluated on ImageNet, which is far and away the most common domain in which pre-trained networks are used and fine-tuned for other tasks.  I\u2019ve never seen networks pre-trained on CIFAR deployed anywhere, and it\u2019s hard to know whether the method will be practically useful for computer vision applications based on CIFAR results -- often improved performance on CIFAR does not translate to ImageNet.  (In other contexts, such as more theoretical contributions, having results only on small datasets is acceptable to me, but network fine-tuning is far enough on the \u201cpractical\u201d end of the spectrum that claiming an improvement to it should necessitate an ImageNet evaluation.)\n\nOverall I think the proposed idea is interesting and potentially promising, but in its current form is not sufficiently evaluated to convince me that the performance boosts don\u2019t simply come from the use of a larger network, and the lack of ImageNet evaluation calls into question its real-world application.\n\n===============\n\nEdit (1\/23\/17): I had indeed missed the fact that the Stanford Cars does do transfer learning from ImageNet -- thanks for the correction.  However, the experiment in this case is only showing late fusion ensembling, which is a conventional approach compared with the \"stitched network\" idea which is the real novelty of the paper.  Furthermore the results in this case are particularly weak, showing only that an ensemble of ResNet+VGG outperforms VGG alone, which is completely expected given that ResNet alone is a stronger base network than VGG (\"ResNet+VGG > ResNet\" would be a stronger result, but still not surprising). Demonstrating the stitched network idea on ImageNet, comparing with the corresponding VGG-only or ResNet-only finetuning, could be enough to push this paper over the bar for me, but the current version of the experiments here don't sufficiently validate the stitched network idea, in my opinion.","model":"human","source":"peerread","label":0,"id":4852}
{"text":"This paper proposed to perform finetuning in an augmentation fashion by freezing the original network and adding a new model aside it. The idea itself is interesting and complements existing training and finetuning approaches, although I think there are a few baseline approaches that can be compared against, such as:\n\n(1) Ensemble: in principle, the idea is similar to an ensembling approach where multiple networks are ensembled together to get a final prediction. The approach in Figure 1 should be compared with such ensemble baselines - taking multiple source domain predictors, possibly with the same modular setting as the proposed method, and compare the performance.\n\n(2) comparison with late fusion: if we combine the pretrained network and a network finetuned from the pretrained one, and do a late fusion?\n\nBasically, I think it is a valuable argument in section 3.2 (and Figure 4) that finetuning with a small amount of data may hurt the performance in general. This builds the ground for freezing a pretrained network and only augmenting it, not changing it. I agree with the authors on this argument, although currently other than Figure 4 there seem to be little empirical study that justifies it.\n\nIt is worth noting that Figure 3 seems to suggest that some of the module filters are either not converging or are learning unuseful features - like the first two filters in 3(a).\n\nOverall I think it is an interesting idea and I would love to see it better developed, thus I am giving a weak accept recommendation, but with a low confidence as the experiments section is not very convincing.","model":"human","source":"peerread","label":0,"id":4853}
{"text":"This paper presents a new technique for adapting a neural network to a new task for which there is not a lot of training data. The most widely used current technique is that of fine-tuning. The idea in this paper is to instead learn a network that learns features that are complementary to the fixed network. Additionally, the authors consider the setting where the new network\/features are \u201cstitched\u201d to the old one at various levels in the hieararchy, rather that it just being a parallel \u201ctower\u201d. \n\nThis work is similar in spirit (if not in some details) to the Progressive Nets paper by Rusu et al, as already discussed. The motivations and experiments are certainly different so this submission has merit on its own.\n\nThe idea of learning a \u201cresidual\u201d with the stitched connnections is very similar in spirit to the ResNet work. It would be nice to compare and contrast those approaches.\n\nI\u2019ve never seen a batch being used 5 times in a row during training, does this work better than just regular SGD?\n\nIn Figure 5 it\u2019d be nice to label the y-axis. That Figure would also benefit from not being a bar chart, but simply emulating Figure 4, which is much more readable!\n\nFigure 5 again: what is an untrained model? It\u2019s not immediately obvious why this is a good idea at all. Is TFT-1 simply fine-tuning one more layer than \u201cRetrain Softmax\u201d?\n\nI think that the results at the end of section 3 are a bit weak because of usage of a big network. I would definitely like to see how the results change if using a smaller net.\n\nThe authors claim throughout the paper that the purpose of the added connections and layers is to learn *complementary* features and they show this with some figures. The latter are a convinving evidence, but not proof or guarantee that this is what is actually happening. I suggest the authors consider adding an explicit constraint in their loss that encourages that, e.g. by having a soft orthogonality constraing (assuming one can project intermediate features to some common feature dimensionality). The usage of very small L2 regularization maybe achieves the same thing, but there\u2019s no evidence for that in the paper (in that we don\u2019t have any visualizations of what happens if there\u2019s no L2 reg.).\n\nOne of the big questions for me while reading the paper was how would an ensemble of 2 pre-trained nets would do on the tasks that the authors consider. This is especially relevant in the cars classification example, where I suspect that a strong baseline is that of fine-tuning VGG on this task, fine-tuning resnet on this task, and possibly training a linear combination of the two outputs or just averaging them naively.\n\nDisappointing that there are no results in figure 4, 5 and 8 except the ones from this paper. It\u2019s really hard to situate this paper if we don\u2019t actually know how it compares to previously published results.\n\n\nIn general, this was an interesting and potentially useful piece of work. The problem of efficiently reusing the previously trained classifier for retraining on a small set is certainly interesting to the community. While I think that this paper takes a good step in the right direction, it falls a bit short in some dimensions (comparisons with more serious baselines, more understanding etc).\n\n\n","model":"human","source":"peerread","label":0,"id":4854}
{"text":"This paper provides two RNN-based architectures for extractive document summarization. The first, \"Classify\", reads in the whole document and traverses the sentences a second time to decide whether to include them or not (0\/1 decisions). The second, \"Select\",  reads in the whole document and picks the most relevant sentence one at the time. The models assume that oracle extractive summaries exist, and a pseudo ground-truth generation procedure is used, which mimics Svore et al. (2007) among others. \n\nOverall, this paper seems a small increment over Cheng & Lapata (2016) and performance is similar or worse to that paper. The problem of single document extractive summarization is not particularly exciting since in DUC 2002 (14 years ago) existing models could not beat the lead baseline (which selects the first sentences of the document). It's a pity that this paper doesn't address the most interesting problems of abstractive summarization or apply the proposed approach to multi-document summarization. It's also a little disappointing that the maximum sentence length had to be capped to 50, which suggests the model has some trouble to scale.","model":"human","source":"peerread","label":0,"id":4855}
{"text":"Reviewers found this paper clear to read, but leaned negative on in terms of impact and originality of the work. Main complaint is that the paper is neither significantly novel in terms of modeling (pointing to Cheng & Lapata), nor significantly more performative on this task (\"only slightly better\"). One reviewer also has a side complaint that the task itself is also somewhat simplistic and simplified, and suggests other tasks. This comment is perhaps harsh, but reflects a mandate for revisiting \"old\" problems to provide significant improvements in accuracy or novel modeling.","model":"human","source":"peerread","label":0,"id":4856}
{"text":"This paper presents two models for extractive document summarization: the classifier architecture and the selector architecture. These two models basically use either classification or ranking in a sequential order to pick the candidate sentences for summarization. Experiments in this paper show the results are either better or close to the SOTA.\n\nTechnical comments:\n\n- In equation (1), there is a position-relevant component call \"positional importance\". I am wondering how important this component is? Is it possible to show the performance without this component? Especially, for the discussion on impact of document structure, when the model is trained on the shuffled order but tested on the original order.\n- A similar question about equation (1), is the content-richness component really necessary? Since the score function already has salience part, which could measure how important of $h_j$ with respect to the whole document.\n- For the dynamic summary representation in equation (3), why not use the same updating equation for both training and test procedures? During test time, the model actually knows the decisions that have been made so far by the decoder. In this way, the model will be more consistent during training and test. \n- I think section 5 is the most interesting part of this paper, and it is also convincing on the difference between the two architectures.\n- It is a little disappointing that the decoding algorithm used in this paper is too simple. In a minimal case, both of them could use beam search and the results could be better.","model":"human","source":"peerread","label":0,"id":4857}
{"text":"This paper presents two RNN architectures for extractive document summarization. The first one, Classifier, takes into account the order in which sentences appear in the original document, whereas the second one, Selector, chooses sentences in an arbitrary order. For each architecture, the concatenated RNN hidden state from a sentence forward and backward pass  is used as features to compute a score that captures content richness, salience, positional importance, and redundancy. Both models are trained in a supervised manner, so the authors used \"pseudo-ground truth generation\" to create training data from abstractive summaries. Experiments show that the Classifier model performs better, and it achieves near state-of-the-art performance for some evaluation metrics.\n\nThe proposed model is in general an extension of Cheng and Lapata, 2016. Unfortunately, the performance is only slightly better or sometimes even worse. The authors mentioned that one key difference how they transform abstractive summaries to become gold labels for the supervised method. However, in the experiment results, the authors described that one potential reason their models do not consistently outperform the extractive model of Cheng & Lapata, 2016 is that the unsupervised greedy approximation may generate noisier ground truth labels than Cheng & Lapata. Is there a reason to construct the training data similar to Cheng & Lapata, if that turns out to be a better method?\nIn order for the proposed models to be convincing, they need to outperform this baseline that's very similar to the proposed methods more consistently, since the main contribution is improved neural architectures for extractive document summarization.\n","model":"human","source":"peerread","label":0,"id":4858}
{"text":"Very interesting paper.\nI have one clarifying question.\nHow are the embeddings for the forward and backward position indices of the sentence in the document computed? Basically, I want to understand how the positional embedding for the sentences (Pj) calculated. \nThank you.\n","model":"human","source":"peerread","label":0,"id":4859}
{"text":"This paper proposed a hardware accelerator for DNN. It utilized the fact that DNN are very tolerant to low precision inference and outperforms a state-of-the-art bit-parallel accelerator by 1.90x without any loss in accuracy while it is 1.17x more energy efficient. TRT requires no network retraining. It achieved super linear scales of performance with area.\n\nThe first concern is that this paper doesn't seem very well-suited to ICLR. The circuit diagrams makes it more interesting for the hardware or circuit design community. \n\nThe second concern is the \"take-away for machine learning community\", seeing from the response, the take-away is using low-precision to make inference cheaper. This is not novel enough. In last year's ICLR, there were at least 4 papers discussing using low precision to make DNN more efficient. These ideas have also been explored in the authors' previous papers.","model":"human","source":"peerread","label":0,"id":4860}
{"text":"This metareview is written based on the reviews of the two expert reviewers (scores of 5(5), 5(5)),\n given the wide disparity of reviewer expertise, and the relevance of hardware expertise to this particular paper.\n \n Quality, Clarity: The paper is clearly written. It does require hardware expertise to read and appreciate the ideas, methodology, and the results. There was a request to post a clear \"take home\" message for ML research, and the authors did post a clear statement in this regard.\n \n Originality: The size of the contribution is the major point of contention for this paper. The expert reviewers believe it to be an incremental\/small idea, particularly in relation to the prior work of the authors. The authors provide rebut that while the implementation builds on their previous work, it can be seen as a general concept that could be applied to other architectures, and that this work targets the important case of fully-connected architectures, vs the case of convolutional architectures that were the case where the prior work did well.\n \n Overall: I have asked the expert reviewers for further input. In the absence of further information, updated scores, or some reasonably strong indication of excitement about the ideas from the expert reviewers, there is little choice but to currently lean towards \"reject\" in terms of this being an inspirational paper for ICLR.","model":"human","source":"peerread","label":0,"id":4861}
{"text":"Here are our post-layout results with actual data-driven activity factors. The 2-bit configuration, detailed in the updated paper, is a design that processes two bits at once using half as many SIPs, increasing efficiency and reducing area overhead. It requires that precision is an even number.\n\nWe report results for the typical design case. Pre-layout results showed that the worst case design corner results in a larger advantage for TRT. The results show increased energy efficiency compared to the pre-layout results. This is expected as the pre-layout results used 50% activity factors whereas here we use actual activity factors measured on a typical layer. This is consistent with the behavior observed during the STRIPES work.\n\nFully-connected layers, whole chip (TSMC 65nm typical case):\n\nBaseline (DaDN) area - 80.41 mm2\n\nTRT area - 120.04 mm2\nEfficiency (TRT vs baseline):\nAlexNet  1.062\nVGG_S    1.059\nVGG_M    1.063\nVGG_19   1.059\ngeomean  1.061\n\nTRT 2-bit area - 100.43 mm2\nEfficiency (TRT 2-bit vs baseline):\nAlexNet  1.228\nVGG_S    1.235\nVGG_M    1.268\nVGG_19   1.237\ngeomean  1.242\n\nNumbers greater than 1 mean less energy used overall by TRT.\n","model":"human","source":"peerread","label":0,"id":4862}
{"text":"We feel uncomfortable completing the reviewer ratings as provided except for one case where a review misrepresents the facts and which we have addressed. This is because the reviews primarily state that the paper does not fit into ICLR. This is a question for the organizers and an interpretation of the CFP which clearly states \"hardware\".\n\nIn summary, the take-away for the ML community is this:\n\n1. Adjusting the precision used per layer or even at a finer granularity of groups of 256 or activations and weights can lead to faster processing and higher energy efficiency. This adjustment can be done at runtime and at a single-bit granularity. Performance can be had without sacrificing accuracy, but if one is willing to sacrifice accuracy more performance and more energy efficiency can be had. We envision follow up work on runtime adjustment of precisions (e.g,, incremental adjustment) to achieve better response times or higher throughput.\n\nIn more detail:\n\n2. There is an energy efficient high-performance hardware design that can offer performance inversely proportional to the precision being used per layer or even at a finer granularity (we do not present any results on this but the design obviously support it as-is). \n\n2. This design does not hardwire the precisions at manufacturing time but instead allows programmatic control at runtime.\n\n3. This capability opens up an additional design know that network designers can use to tradeoff execution time and accuracy.\n\n4. Earlier we had proposed a method to choose per layer precisions  for convolutional layers here we extend this method to fully-connected layers. This method was published on arxiv and has never been accepted to any peer reviewed publication. The only related publication to the work here is STRIPES (MICRO) (12 pages) and a pre-print at iEEE Computer Architecture Letters (4 pages) that explained the basic idea behind Stripes. This work extends STRIPES for Fully-connected layers --- Stripes did not improve performance for FC layers and its energy efficiency was worse than DaDianNao for those layers.\n\nConcern summary:\n\n1. Some concerns were raised about the energy and area measurements. We have posted an update and we will deliver the final results post-layout in a day or so. We will also deliver results on an optimized configuration that drastically reduces area overhead and improves energy efficiency as well. The delay in response was due to having received the reviews during the Xmas break when the author that can perform these measurements was unreachable due to travel. \n\n2. Incremental over DaDianNao: Tartan is a general concept which can be integrated to many different architectures. We chose DaDN as the baseline architecture as it widely known and often compared against and offers the additional challenge of being a very wide vector-like architecture (doing bit-serial computation for a single lane -- product -- independently is easy but doing it for 4K terms in parallel without extremely wide memories is hard). So, we disagree that this is an incremental improvement of DaDianNao. To draw an analogy, from hardware the seminal work on pattern based branch prediction was not an incremental improvement over out-of-order execution even though previous techniques included branch predictors. Not that we feel that TARTAN is at the same level as pattern based branch prediction but we use this example to illustrate what such an argument can lead to. Also, STRIPES is receiving a honorable mention in the upcoming IEEE MICRO Topic PIcs in Computer Architecture researcher, which is akin to a best paper award in the field of computer architecture (each accepted paper in the last year in a top-tier conference receives 10 more peer reviews that state whether they feel the paper has the potential for high impact). So, at least some people that are credible enough to be invited to that panel in the comp arch community think it's not incremental.\n\n3. FC layers are not important: this is not true. They are still in use and more so in different applications. Moreover, last years best paper award in ICLR rightfully went to an excellent work that addressed both pruning the model and proposing an optimized hardware architecture solely for FC layers.\n\n\n4. TARTAN takes too much area: The units are larger but this is not where most of the area cost is. The area cost is in the surrounding memory so in the big picture the overall area cost is much lower. also, in modern technology area is not the concern.\n\n5. Energy efficiency is not that much better than DaDN and within the error margin of the tools. We used industry standard tools and the results are positive. moreover, TARTAN is faster and has we can use frequency and voltage scaling to improve energy efficiency (which depends on voltage square) while still performing better. We show results assuming same voltage and frequency.\n\n6. You did not use power gating for DaDN. Did we not use it for TARTAN either. It is not straightforward to do for DaDN as it is a bit parlallel engine and there are not that many zero values to necessarily justify the logic needed to enable power gating. Which is to say that this ia a non-trivial task. Moreover, the same technique can be applied to TATRAN. We have results reported in another submission that show a heavy bias toward the zero bit value which suggestes that power gating will most likely be a lot more effective for TARTAN than DaDN.\n\nIn summary, combined this work, with STRIPES and the earlier arxiv report present a comprehensive hardware\/software approach to exploiting precision to improve performance and energy efficiency for CNNs. Stripes performed very well for convolutional layers but did poorly for fully-connected layers. TARTAN fixes this.\n","model":"human","source":"peerread","label":0,"id":4863}
{"text":"Our apologies for the long delay. Our co-author that has the expertise to do this work was overseas for the Christmas break and she returned this Tuesday.\n\nShe has synthesized the designs for three cases: bc, tc, and wc for best, typical and worst case respectively. The previous results were for the bc. The detailed results are below. In summary, efficiency improves for tc and wc. We have layout being synthesized and we expect to have the results in a day or so. The results below are pre-layout and use 50% activity factors. The layout results that we will post as soon as possible will be a testbench for a typical layer. As with STRIPES we expect that energy efficiency will be better with real inputs as the inputs exhibit many more zero bits. Please keep in mind that the results in the STRIPES publication in MICRO are post-layout.\n\nMore importantly, we are also synthesizing a different configuration cuts down area costs and improves energy efficiency even further. We will post the post-layout results shortly.\n\nWe hope that we will be given the opportunity to post the updated results before a decision is made.\n\nThank you.\n\nHere are the detailed pre-layout results for all three cases and for fully-connected layers only. Best case is the configuration used in the original submission. We will update the writing with post-layout and actual activity-based results shortly. Effieciency numbers above 1.0 mean that TRT is better than DaDN.\n\nArea & Efficiency, Fully-connected layers, whole chip TRT vs DaDN:\n\nBest case\n\nBaseline area - 77.91 mm2\nTRT area - 108.61 mm2 (+39.4%)\nEfficiency:\nAlexNet  0.935\nVGG_S    0.932\nVGG_M    0.935\nVGG_19   0.932\ngeomean  0.933\n\nTypical case\n\nBaseline area - 79.28 mm2\nTRT area - 111.29 mm2 (+40.4%)\nEfficiency:\nAlexNet  1.014\nVGG_S    1.011\nVGG_M    1.014\nVGG_19   1.010\ngeomean  1.012\n\nWorst case\n\nBaseline area - 83.5 mm2\nTRT area - 121.39 mm2 (+45.3%)\nEfficiency:\nAlexNet  1.048\nVGG_S    1.046\nVGG_M    1.049\nVGG_19   1.045\ngeomean  1.047\n\n\n\n\n\n","model":"human","source":"peerread","label":0,"id":4864}
{"text":"The authors present TARTAN, a derivative of the previously published DNN accelerator architecture: \u201cDaDianNao\u201d. The key difference is that TARTAN\u2019s compute units are bit-serial and unroll MAC operation over several cycles. This enables the units to better exploit any reduction in precision of the input activations for improvement in performance and energy efficiency.\n\nComments:\n\n1. I second the earlier review requesting the authors to be present more details on the methodology used for estimating energy numbers for TARTAN. It is claimed that TARTAN gives only a 17% improvement in energy efficiency. However, I suspect that this small improvement is clearly within the margin of error ij energy estimation.  \n\n2. TARTAN is a derivative of DaDianNao, and it heavily relies the overall architecture of DaDianNao. The only novel aspect of this contribution is the introduction of the bit-serial compute unit, which (unfortunately) turns out to incur a severe area overhead (of nearly 3x over DaDianNao's compute units).\n\n3. Nonetheless, the idea of bit-serial computation is certainly quite interesting. I am of the opinion that it would be better appreciated (and perhaps be even more relevant) in a circuit design \/ architecture focused venue.","model":"human","source":"peerread","label":0,"id":4865}
{"text":"Summary:\n\nThe paper describes how the DaDianNao (DaDN) DNN accelerator can be improved by employing bit serial arithmetic.  They replace the bit-parallel multipliers in DaDN with multipliers that accept the weights in parallel but the activations serially (serial x parallel multipliers).  They increase the number of units keeping the total number of adders constant.  This enables them to tailor the time and energy consumed to the number of bits used to represent activations.  They show how their configuration can be used to process both fully-connected and convolutional layers of DNNs.\n\nStrengths:\n\nUsing variable precision for each layer of the network is useful - but was previously reported in Judd (2015)\n\nGood evaluation including synthesis - but not place and route - of the units.  Also this evaluation is identical to that in Judd (2016b)\n\nWeaknesses:\n\nThe idea of combining bit-serial arithmetic with the DaDN architecture is a small one.\n\nThe authors have already published almost everything that is in this paper at Micro 2016 in Judd (2016b).  The increment here is the analysis of the architecture on fully-connected layers.  Everything else is in the previous publication.\n\nThe energy gains are small - because the additional flip-flop energy of shifting the activations in almost offsets the energy saved on reducing the precision of the arithmetic.\n\nThe authors don\u2019t compare to more conventional approaches to variable precision - using bit-parallel arithmetic units but data gating the LSBs so that only the relevant portion of the arithmetic units toggle.  This would not provide any speedup, but would likely provide better energy gains than the bit-serial x bit-parallel approach.\n\nOverall:\n\nThe Tartan and Stripes architectures are interesting but the incremental contribution of this paper (adding support for fully-connected layers) over the three previous publications on this topic, and in particular Judd (2016b) is very small.  This idea is worth one good paper, not four.","model":"human","source":"peerread","label":0,"id":4866}
{"text":"This seems like a reasonable study, though it's not my area of expertise. I found no fault with the work or presentation, but did not follow the details or know the comparable literature. \n\nThere seem to be real gains to be had through this technique, though they are only in terms of efficiency in hardware, not changing accuracy on a task. The tasks chosen (Alexnet \/ VGG) seem reasonable. The results are in simulation rather than in actual hardware.\n\nThe topic seems a little specialized for ICLR, since it does not describe any new advances in learning or representations, albeit that the CFP includes \"hardware\".  I think the appeal among attendees will be rather limited. \n\nPlease learn to use parenthetical references correctly. As is your references make reading harder. ","model":"human","source":"peerread","label":0,"id":4867}
{"text":"Hardware is listed on the call-for-papers as relevant topic for ICLR 2017, and so the paper is on-topic.\n\nWe worked hard to improve on the initial reviewer assignment for this paper in ensure that we would get hardware-knowledgeable reviewers on board for this paper, although we only partly succeeded (due to conflicts, tight review deadlines, and more).\n\nWe are also still missing questions and comments from one reviewer.\n\nICLR papers should succeed in communicating with the ICLR audience, i.e., introducing concepts for this audience, and using a language that is accessible for this community.  But it need not connect with all of the ICLR community, which is dominated by algorithmic concerns; many may not have much background or interest in hardware and that is fine.  This situation is not unique, e.g., hardware-related papers in computer graphics and computer vision conferences are in the same situation.  It is also true, I think, that the audience and reviewers of hardware-related conferences may not be well-placed to fully understand and comment on all relevant algorithmic considerations that a hardware implementation targets. \n\nThere remains disagreement as to the utility of achieving improvements related to fully connected layers, which would be good to resolve. And there also remains disagreement on the improvements (conceptual and performance-related) with respect to the state of the art, as recently rebutted by the authors. \n\n\n","model":"human","source":"peerread","label":0,"id":4868}
{"text":"I do not feel very qualified to review this paper. I studied digital logic back in university, that was it. I think the work deserves a reviewer with far more sophisticated background in this area. It certainly seems useful. My advice is also to submit it another venue.","model":"human","source":"peerread","label":0,"id":4869}
{"text":"This paper proposed a hardware accelerator for DNN. It utilized the fact that DNN are very tolerant to low precision inference and outperforms a state-of-the-art bit-parallel accelerator by 1.90x without any loss in accuracy while it is 1.17x more energy efficient. TRT requires no network retraining. It achieved super linear scales of performance with area.\n\nThe first concern is that this paper doesn't seem very well-suited to ICLR. The circuit diagrams makes it more interesting for the hardware or circuit design community. \n\nThe second concern is the \"take-away for machine learning community\", seeing from the response, the take-away is using low-precision to make inference cheaper. This is not novel enough. In last year's ICLR, there were at least 4 papers discussing using low precision to make DNN more efficient. These ideas have also been explored in the authors' previous papers. \n","model":"human","source":"peerread","label":0,"id":4870}
{"text":"CONTRIBUTIONS \nThis paper introduces a method for learning semantic \"word-like\" units jointly from audio and visual data. The authors use a multimodal neural network architecture which accepts both image and audio (as spectrograms) inputs. Joint training allows one to embed both image and spoken language captions into a shared representation space. Audio-visual groundings are generated by measuring affinity between image patches and audio clips. This allows the model to relate specific visual regions to specific audio segments. Experiments cover image search (audio to image) and annotation (image to audio) tasks and acoustic word discovery.\n\n\nNOVELTY+SIGNIFICANCE\nAs correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval. With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).\n\nHowever, bidirectional image\/audio retrieval has already been explored by the authors in prior work (Harwath et al, NIPS 2016). Apart from minor differences in data and CNN architecture, the training procedure in this submission is identical to this prior work. The novelty in this submission is therefore the procedure for using the trained model for associating image regions with audio subsequences.\n\nThe methods employed for this association are relatively straightforward combination of standard techniques with limited novelty. The trained model is used to compute alignment scores between densely sampled image regions and audio subsequences; from these alignment scores a number of heuristics are applied to associate clusters of image regions with clusters of audio subsequences.\n\n\nMISSING CITATION\nThere is a lot of work in this area spanning computer vision, natural language, and speech recognition. One key missing reference:\n\nNgiam, et al. \"Multimodal deep learning.\" ICML 2011\n\n\nPOSITIVE POINTS\n- Using more data and an improved CNN architecture, this paper improves on prior work for bidirectional image\/audio retrieval\n- The presented method performs efficient acoustic pattern discovery\n- The audio-visual grounding combined with the image and acoustic cluster analysis is successful at discovering audio-visual cluster pairs\n\nNEGATIVE POINTS\n- Limited novelty, especially compared with Harwath et al, NIPS 2016\n- Although it gives good results, the clustering method has limited novelty and feels heuristic\n- The proposed method includes many hyperparameters (patch size, acoustic duration, VAD threshold, IoU threshold, number of k-means clusters, etc) and there is no discussion of how these were set or the sensitivity of the method to these choices","model":"human","source":"peerread","label":0,"id":4871}
{"text":"This paper received borderline reviews. All reviewers as well as AC agree that the authors pursue a very interesting and less explored direction. The paper essentially addresses the problem of double grounding; visual information helping to group acoustic signal into words, and words helping to localize object-like regions in images. While somewhat hidden under the rug, this is what makes the paper different from the authors' previous work. The reviewers mentioned this to be a minor contribution. The AC agrees with the authors that this is an interesting and novel problem worth studying. However, the AC also agrees with the reviewers that this major novelty over the previous work is missing technical depth. The AC strongly encourages the authors to improve on this aspect of the paper. A simple intuition would be to look at ","model":"human","source":"peerread","label":0,"id":4872}
{"text":"We'd like to first thank the reviewers for taking the time to read our submission and offer their thoughtful opinions and encouragement. Since the three reviewers raise some similar questions and concerns, I'll try to address them in one post instead of multiple replies.\n\nI'll first address the issue of novelty, which is the main criticism raised by all three reviewers. When thinking about research, I think it's important to consider the problem to be solved separately from the tool(s) used to solve it. While I agree that the deep neural network architecture used in this submission is not a significant departure from the one used in our NIPS 2016 paper, the way we use it is very different. I believe that the problem we address in this submission - that is, joint localization\/isolation, clustering, and association of word-like speech patterns and object-like visual patterns - is significantly novel and wasn't studied in our NIPS paper. I don't believe that this submission qualifies as just an analysis paper, because it actually attempts to solve a specific and novel problem, and does so surprisingly well.\n\nThere exists an active sub-field of the speech\/linguistics\/cogsci communities which studies the problem of acquiring language from untranscribed speech audio alone (see relevant citations in the submission), and most of the techniques used in that problem space rely on segmentation and clustering of the speech signal into linguistically meaningful units (often called unsupervised term discovery or UTD when the desired granularity of the units is at the word or phrase level). The most widely-used and successful techniques for UTD are based on segmental dynamic time warping, which is inherently O(N^2) complexity. I believe that one of the most significant contributions of this submission is the demonstration that the addition of contextually relevant visual information is sufficient to reduce the computational complexity of UTD to O(N), allowing it to scale to much larger datasets.\n\nI'd also like to respond to a few specific points:\n\nFrom reviewer 2:\n\"As correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval. With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).\"\n\nI respectfully disagree that the move from text to speech audio constitutes an incremental step. The field of automatic speech recognition research has been grappling with the problem of mapping speech audio to symbolic strings for over 65 years, so it's not a trivial problem. By marrying language and vision at the raw signal level as we are here, we're not just creating models that learn the associations between words and images, but actually forcing the models to simultaneously learn how to perform speech recognition in their own, non-symbolic way.\n\nFrom reviewer 1:\n\"Clustering and grouping in section 4, is hacky. Instead of gridding the image, the authors could actually use an object detector (SSD, Yolo, FasterRCNN, etc.) to estimate accurate object proposals; rather than using k-means, a spectral clustering approach would alleviate the gaussian assumption of the distributions. In assigning visual hypotheses with acoustic segments, some form of bi-partite matching should be used.\"\n\nI think that these optimizations, while good suggestions if one's goal is to squeeze every last bit of performance out of the system, are not crucial to the central theme of the paper. We chose not to use an off-the-shelf object detection system for the same reason that we chose not to give the system oracle word boundaries derived from a speech recognizer. We wanted the model to learn to localize, identify, and associate spoken words and visual objects without being explicitly trained to do so. Spectral clustering is an O(N^3) complexity algorithm, and our problem deals with clustering on the order of a million points; a full similarity matrix at floating point precision would require on the order of 4 terabytes of memory, and that's just an O(N^2) subroutine in the spectral clustering algorithm. I realize that various approximate algorithms could possibly be made to work, but in our case I think that the fact that a classic algorithm like k-means works so well is actually a testament to the separability of the embeddings that are being learned by the multimodal CNN.","model":"human","source":"peerread","label":0,"id":4873}
{"text":"This paper is a follow-up on the NIPS 2016 paper \"Unsupervised learning of spoken language with visual context\", and does exactly what that paper proposes in its future work section: \"to perform acoustic segmentation and clustering, effectively learning a lexicon of word-like units\" using the embeddings that their system learns. The analysis is very interesting and I really like where the authors are going with this.\n\nMy main concern is novelty. It feels like this work is a rather trivial follow-up on an existing model, which is fine, but then the analysis should be more satisfying: currently, it feels like the authors are just illustrating some of the things that the NIPS model (with some minor improvements) learns. For a more interesting analysis, I would have liked things like a comparison of different segmentation approaches (both in audio and in images), i.e., suppose we have access to the perfect segmentation in both modalities, what happens? It would also be interesting to look at what is learned with the grounded representation, and evaluate e.g. on multi-modal semantics tasks.\n\nApart from that, the paper is well written and I really like this research direction. It is very important to analyze what models learn, and this is a good example of the types of questions one should ask. I am afraid, however, that the model is not novel enough, nor the questions deep enough, to make this paper better than borderline for ICLR.","model":"human","source":"peerread","label":0,"id":4874}
{"text":"This work proposes a joint classification of images and audio captions for the task of word like discovery of acoustic units that correlate to semantically visual objects. The general this is a very interesting direction of research as it allows for a richer representation of data: regularizing visual signal with audio and visa versa. This allows for training of visual models from video, etc. \n\nA major concern is the amount of novelty between this work and the author's previous publication at NIPs 2016. The authors claim a more sophisticated architecture and indeed show an improvement in recall. However, the improvements are marginal, and the added complexity to the architecture is a bit ad hoc. Clustering and grouping in section 4, is hacky. Instead of gridding the image, the authors could actually use an object detector (SSD, Yolo, FasterRCNN, etc.) to estimate accurate object proposals; rather than using k-means, a spectral clustering approach would alleviate the gaussian assumption of the distributions. In assigning visual hypotheses with acoustic segments, some form of bi-partite matching should be used.\n\nOverall, I really like this direction of research, and encourage the authors to continue developing algorithms that can train from such multimodal datasets. However, the work isn't quite novel enough from NIPs 2016.","model":"human","source":"peerread","label":0,"id":4875}
{"text":"CONTRIBUTIONS \nThis paper introduces a method for learning semantic \"word-like\" units jointly from audio and visual data. The authors use a multimodal neural network architecture which accepts both image and audio (as spectrograms) inputs. Joint training allows one to embed both image and spoken language captions into a shared representation space. Audio-visual groundings are generated by measuring affinity between image patches and audio clips. This allows the model to relate specific visual regions to specific audio segments. Experiments cover image search (audio to image) and annotation (image to audio) tasks and acoustic word discovery.\n\n\nNOVELTY+SIGNIFICANCE\nAs correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval. With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).\n\nHowever, bidirectional image\/audio retrieval has already been explored by the authors in prior work (Harwath et al, NIPS 2016). Apart from minor differences in data and CNN architecture, the training procedure in this submission is identical to this prior work. The novelty in this submission is therefore the procedure for using the trained model for associating image regions with audio subsequences.\n\nThe methods employed for this association are relatively straightforward combination of standard techniques with limited novelty. The trained model is used to compute alignment scores between densely sampled image regions and audio subsequences; from these alignment scores a number of heuristics are applied to associate clusters of image regions with clusters of audio subsequences.\n\n\nMISSING CITATION\nThere is a lot of work in this area spanning computer vision, natural language, and speech recognition. One key missing reference:\n\nNgiam, et al. \"Multimodal deep learning.\" ICML 2011\n\n\nPOSITIVE POINTS\n- Using more data and an improved CNN architecture, this paper improves on prior work for bidirectional image\/audio retrieval\n- The presented method performs efficient acoustic pattern discovery\n- The audio-visual grounding combined with the image and acoustic cluster analysis is successful at discovering audio-visual cluster pairs\n\nNEGATIVE POINTS\n- Limited novelty, especially compared with Harwath et al, NIPS 2016\n- Although it gives good results, the clustering method has limited novelty and feels heuristic\n- The proposed method includes many hyperparameters (patch size, acoustic duration, VAD threshold, IoU threshold, number of k-means clusters, etc) and there is no discussion of how these were set or the sensitivity of the method to these choices\n","model":"human","source":"peerread","label":0,"id":4876}
{"text":"UPDATE: I have read the authors' rebuttal and also the other comments in this paper's thread. My thoughts have not changed.\n\nThe authors propose using a mixture prior rather than a uni-modal\nprior for variational auto-encoders. They argue that the simple\nuni-modal prior \"hinders the overall expressivity of the learned model\nas it cannot possibly capture more complex aspects of the data\ndistribution.\"\n\nI find the motivation of the paper suspicious because while the prior\nmay be uni-modal, the posterior distribution is certainly not.\nFurthermore, a uni-modal distribution on the latent variable space can\ncertainly still lead to the capturing of complex, multi-modal data\ndistributions. (As the most trivial case, take the latent variable\nspace to be a uniform distribution; take the likelihood to be a\npoint mass given by applying the true data distribution's inverse CDF\nto the uniform. Such a model can capture any distribution.)\n\nIn addition, multi-modality is arguably an overfocused concept in the\nliterature, where the (latent variable) space is hardly anymore worth\ncapturing from a mixture of simple distributions when it is often a\ncomplex nonlinear space. It is unclear from the experiments how much\nthe influence of the prior's multimodality influences the posterior to\ncapture more complex phenomena, and whether this is any better than\nconsidering a more complex (but still reparameterizable) distribution\non the latent space.\n\nI recommend that this paper be rejected, and encourage the authors to\nmore extensively study the effect of different priors.\n\nI'd also like to make two additional comments:\n\nWhile there is no length restriction at ICLR, the 14 page document can\nbe significantly condensed without loss of describing their innovation\nor clarity. I recommend the authors do so.\n\nFinally, I think it's important to note the controversy in this paper.\nIt was submitted with many significant incomplete details (e.g., no experiments,\nmany missing citations, a figure placed inside that was pencilled in\nby hand, and several missing paragraphs). These details were not\ncompleted until roughly a week(?) later. I recommend the chairs discuss\nthis in light of what should be allowed next year.","model":"human","source":"peerread","label":0,"id":4877}
{"text":"This paper explores a variational autoencoder variant.\n \n ICLR gives authors some respect that other conferences don't. It is flexible about the length of the paper, and allows revisions to be submitted. The understanding should be that authors should in turn treat reviewers with respect. The paper should still be finished. Reviewers can't be expected to read a churn of large revisions. The final paper should be roughly the right length, unless with very good reason.\n \n This paper was clearly not finished, and now is too long, with issues remaining. I hope that it will be submitted again, but not until it is actually ready.","model":"human","source":"peerread","label":0,"id":4878}
{"text":"The authors introduce some new prior and approximate posterior families for variational autoencoders, which are compatible with the reparameterization trick, as well as being capable of expressing multiple modes. They also introduce a gating mechanism between prior and posterior. They show improvements on bag of words document modeling, and dialogue response generation. The original abstract is overly strong in its assertion that a unimodal latent prior p(z) cannot fit a multimodal marginal int_z p(x|z)p(x)dz with a DNN response model p(x|z) (\"it cannot possibly capture more complex aspects of the data distribution\", \"critical restriction\", etc).\n\nWhile the assertion that a unimodal latent prior is necessary to model multimodal observations is false, there are sensible motivations for the piecewise constant prior and posterior. For example, if we think of a VAE as a sort of regularized autoencoder where codes are constrained to \"fill up\" parts of the prior latent space, then there is a sphere-packing argument to be made that filling a Gaussian prior with Gaussian posteriors is a bad use of code space. Although the authors don't explore this much, a hypercube-based tiling of latent code space is a sensible idea.\n\nAs stated, I found the message of the paper to be quite sloppy with respect to the concept of \"multi-modality.\" There are 3 types of multimodality at play here: multimodality in the observed marginal distribution p(x), which can be captured by any deep latent Gaussian model, multimodality in the prior p(z), which makes sense in some situations (e.g. a model of MNIST digits could have 10 prior modes corresponding to latent codes for each digit class), and multimodality in the posterior z for a given observation x_i, q(z_i|x_i). The final type of multimodality is harder to argue for, except in so far as it allows the expression of flexibly shaped distributions without highly separated modes. I believe flexible posterior approximations are important to enable fine-grained and efficient tiling of latent space, but I don't think these need to have multiple strong modes. I would be interested to see experiments demonstrating otherwise for real world data.\n\nI think this paper should be more clear about the different types of multi-modality and which parts of their analysis demonstrate which ones. I also found it unsatisfactory that the piecewise variable analysis did not show different components of the multi-modal prior corresponding to different words, but rather just a separation between the Gaussian and the piecewise variables.\n\nAs I mention in my earlier questions, I found it surprising that the learned variance and mean for the Gaussian prior helps so dramatically with G-NVDM likelihood when the powerful networks transforming to and from latent space should make it scale-invariant. Explicitly separating out the contributions of a reimplemented base model, prior-posterior interpolation and the learned prior parameters would strengthen these experiments. Overall, the very strong improvements on the text modeling task over NVDM seem hard to understand, and I would like to see an ablation analysis of all the differences between that model and the proposed one.\n\nThe fact that adding more constant components helps for document modeling is interesting, and it would be nice to see more qualitative analysis of what the prior modes represent. I also would be surprised if posterior modes were highly separated, and if they were it would be interesting to explore if they corresponded to e.g. ambiguous word-senses.\n\nThe experiments on dialog modeling are mostly negative results, quantitatively. The observation that the the piecewise constant variables encode time-related words and the Gaussian variables encode sentiment is interesting, especially since it occurs in both sets of experiments. This is actually quite interesting, and I would be interested in seeing analysis of why this is the case. As above, I would like to see an analysis of the sorts of words that are encoded in the different prior modes and whether they correspond to e.g. groups of similar holidays or days.\n\nIn conclusion, I think the piecewise constant variational family is a good idea, although it is not well-motivated by the paper. The experimental results are very good for document modeling, but without ablation analysis against the baseline it is hard to see why they should be with such a small modification in G-NVDM. The fact that H-NVDM performs better is interesting, though. This paper should better motivate the need for different types of multi-modality, and demonstrate that those sorts of things are actually being captured by the model. As it is, the paper introduces an interesting variational family and shows that it performs better for some tasks, but the motivation and analysis is not clearly focused. To demonstrate that this is a broadly applicable family, it would also be good to do experiments on a more standard datasets like MNIST. Even without an absolute log-likelihood improvement, if the method yielded interpretable multiple modes this would be a valuable contribution.","model":"human","source":"peerread","label":0,"id":4879}
{"text":"This paper proposes a piecewise constant parameterisation for neural variational models so that it could explore the multi-modality of the latent variables and develop more powerful neural models. \nThe experiments of neural variational document models and variational hierarchical recurrent encoder-decoder models show that the introduction of the piecewise constant distribution helps achieve better perplexity on modelling documents and seemly better performance on modelling dialogues.\nThe idea of having a piecewise constant prior for latent variables is interesting, but the paper is not well-written (even 14 pages long) and the design of the experiments fails to demonstrate the most of the claims.  \n\nThe detailed comments are as follows:\n\n--The author explains the limitations of the VAEs with standard Gaussian prior in the last paragraph of 3.1 and the last paragraph of 5.1. Hence, a multimodal prior would help the VAEs overcome the issues of optimisation. However, there is a lack of evidence showing the multimodality of the prior helps break the bottleneck. \n\n--In the last paragraph of 6.1, the author claimed the decoder parameter matrix is directly affected by the latent variables. But what the connects the decoder is a combination of a piecewise constant and Gaussian latent variables. No matter what is discovered in the experiments, it only shows z=","model":"human","source":"peerread","label":0,"id":4880}
{"text":"The reparameterization does not seem to be differentiable? I.e. eq. (8) consists of indicator functions of the parameters you are optimizing with respect to.","model":"human","source":"peerread","label":0,"id":4881}
{"text":"This paper is incomplete. Most of results are blank.  What is the meaning of \"table XXXX\"? \nSuch strategy seems unfair....\n\nHowever, I agree that methods are good.  This paper should be submitted to ICML or workshop in ICLR.\n\nIf this type of method is allowed, I would wonder the credibility of papers in this conference. \n\n","model":"human","source":"peerread","label":0,"id":4882}
{"text":"This paper introduces pointer-network neural networks, which are applied to referring expressions in three small-scale language modeling tasks: dialogue modeling, recipe modeling and news article modeling. When conditioned on the co-reference chain, the proposed models outperform standard sequence-to-sequence models with attention.\n\nThe proposed models are essentially variants of pointer networks with copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Ling et al., 2016), which have been modified to take into account reference chains. As such, the main architectural novelty lies in 1) restricting the pointer mechanism to focus on co-referenced entities, 2) applying pointer mechanism to 2D arrays (tables), and 3) training with supervised alignments. Although useful in practice, these are minor contributions from an architectural perspective.\n\nThe empirical contributions are centred around measuring perplexity on the three language modeling tasks. Measuring perplexity is typical for standard language modeling tasks, but is really an unreliable proxy for dialogue modeling and recipe generation performance. In addition to this, both the dialogue and recipe tasks are tiny compared to standard language modeling tasks. This makes it difficult to evaluate the impact of the dialogue and recipe modeling results. For example, if one was to bootstrap from a larger corpus, it seems likely that a standard sequence-to-sequence model with attention would yield performance comparable to the proposed models (with enough data, the attention mechanism could learn to align referring entities by itself). The language modeling task on news article (Gigaword) seems to yield the most conclusive results. However, the dataset for this task is non-standard and results are provided for only a single baseline. Overall, this limits the conclusions we can draw from the empirical experiments.\n\n\nFinally, the paper itself contains many errors, including mathematical errors, grammatical errors and typos:\n- Eq. (1) is missing a sum over $z_i$.\n- \"into the a decoder LSTM\" -> \"into the decoder LSTM\"\n- \"denoted as his\" -> \"denoted as\"\n- \"Surprising,\" -> \"Surprisingly,\"\n- \"torkens\" -> \"tokens\"\n- \"if follows that the next token\" -> \"the next token\"\n- In the \"COREFERENCE BASED LANGUAGE MODEL\" sub-section, what does $M$ denote?\n- In the sentence: \"The attribute of each column is denoted as $s_c, where $c$ is the c-th attribute\". For these definitions to be make sense, $s_c$ has to be a one-hot vector. If yes, please clarify this in the text.\n- \"the weighted sum is performed\" -> \"the weighted sum is computed\"\n- \"a attribute\" -> \"an attribute\"\n- In the paragraph on Pointer Switch, change $p(z_{i,v} |s_{i,v}) = 1$ -> $p(z_{i,v} |s_{i,v}) = 0$.\n- In the \"Table Pointer\" paragraph, I assume you mean outer-product instead of cross-product? Otherwise, I don't see how the equations add up.\n\n\nOther comments:\n- For the \"Attention based decoder\", is the attention computed using the word embeddings themselves or the hidden states of the sentence encoder? Also, it applied only to the previous turn of the dialogue or to the entire dialogue history? Please clarify this.\n- What's the advantage of using an \"Entity state update\" rule, compared to a pointer network or copy network, which you used in the dialogue and recipe tasks? Please elaborate on this.\n- In the Related Work section, the following sentence is not quite accurate: \"For the task oriented dialogues, most of them embed the seq2seq model in traditional dialogue systems while our model queries the database directly.\". There are task-oriented dialogue models which do query databases during natural language generation. See, for example, \"A Network-based End-to-End Trainable Task-oriented Dialogue System\" by Wen et al.","model":"human","source":"peerread","label":0,"id":4883}
{"text":"All of the reviewers point out clarity problems; while these may have been resolved in an updated version, the reviewers have not expressed that the matter is resolved. There are several questions raised about the use of perplexity, both whether the comparison is fair, and whether it is a valid proxy for more standard measures in NLP. The former seems to be more of an issue for this area chair, and the discussion did not convince me that it was adequately resolved.","model":"human","source":"peerread","label":0,"id":4884}
{"text":"This paper presents a new type of language model that treats entity references as latent variables. The paper is structured as three specialized models for three applications: dialog generation with references to database entries, recipe generation with references to ingredients, and text generation with coreference mentions.\n\nDespite some opaqueness in details that I will discuss later, the paper does a great job making the main idea coming through, which I think is quite interesting and definitely worth pursuing further. But it seems the paper was rushed into the deadline, as there are a few major weaknesses.\n\nThe first major weakness is that the claimed latent variables are hardly latent in the actual empirical evaluation. As clarified by the authors via pre-review QAs, all mentions were assumed to be given to all model variants, and so, it would seem like an over-claim to call these variables as latent when they are in fact treated as observed variables. Is it because the models with latent variables were too difficult to train right?\n\nA related problem is the use of perplexity as an evaluation measure when comparing reference-aware language models to vanilla language models. Essentially the authors are comparing two language models defined over different event space, which is not a fair comparison. Because mentions were assumed to be given for the reference-aware language models, and because of the fact that mention generators are designed similar to a pointer network, the probability scores over mentions will naturally be higher, compared to the regular language model that needs to consider a much bigger vocabulary set. The effect is analogous to comparing language models with aggressive UNK (and a small vocabulary set) to a language models with no UNK (and a much larger vocabulary set).\n\nTo mitigate this problem, the authors need to perform one of the following additional evaluations: either assuming no mention boundaries and marginalizing over all possibilities (treating latent variables as truly latent), or showing other types of evaluation beyond perplexity, for example, BLEU, METEOR, human evaluation etc on the corresponding generation task.\n\nThe other major weakness is writing in terms of technical accuracy and completeness. I found many details opaque and confusing even after QAs. I wonder if the main challenge that hinders the quality of writing has something to do with having three very specialized models in one paper, each having a lot of details to be worked out, which may have not been extremely important for the main story of the paper, but nonetheless not negligible in order to understand what is going on with the paper.    Perhaps the authors can restructure the paper so that the most important details are clearly worked out in the main body of the paper, especially in terms of latent variable handling \u2014 how to make mention detection and conference resolution truly latent, and if and when entity update helps, which in the current version is not elaborated at all, as it is mentioned only very briefly for the third application (coreference resolution) without any empirical comparisons to motivate the update operation.\n\n","model":"human","source":"peerread","label":0,"id":4885}
{"text":"This paper explores 3 language modeling applications with an explicit modeling of reference expressions: dialog, receipt generation and coreferences. While these are important tasks for NLP and the authors have done a number of experiments, the paper is limited for a few reasons:\n\n1. This paper is not clearly written and is pretty hard to follow some details. In particular,  there are many obvious math errors, such as missing the marginalization sum in Eq (1), and P(z_{i,v}...) = 1 (should be 0 here) on page 5, pointer switch section.\n\n2. The major novelty seems to be the 2-dimensional attention from the table and the pointer to the 2-D table. These are more of a customization of existing work to a particular task with 2-D tables as a part of the input to seq2seq model with both attentions and pointer networks.\n\n3. The empirical results are not very conclusive yet, limited by either the relatively small data size, or the lack of well-established baseline for some new applications (e.g., the recipe generation task).\n\nOverall, this paper, as it is for now, is more suitable for a workshop rather than for the main conference.","model":"human","source":"peerread","label":0,"id":4886}
{"text":"Hi, can you tell me the reference-chain is obtained by manual annotation or by some automatical tools such stanford corenlp ? ","model":"human","source":"peerread","label":0,"id":4887}
{"text":"The authors propose a method to generate adversarial examples w\/o relying on knowledge of the network architecture or network gradients.\n\nThe idea has some merit, however, as mentioned by one of the reviewers, the field has been studied widely, including black box setups.\n\nMy main concern is that the first set of experiments allows images that are not in image space. The authors acknowledge this fact on page 7 in the first paragraph. In my opinion, this renders these experiments completely meaningless. At the very least, the outcome is not surprising to me at all.\n\nThe greedy search procedure remedies this issue. The description of the proposed method is somewhat convoluted. AFAICT, first a candidate set of pixels is generated by using PERT. Then the pixels are perturbed using CYCLIC.\nIt is not clear why this approach results in good\/minimal perturbations as the candidate pixels are found using a large \"p\" that can result in images outside the image space. The choice of this method does not seem to be motivated by the authors.\n\nIn conclusion, while the authors to an interesting investigation and propose a method to generate adversarial images from a black-box network, the overall approach and conclusions seem relatively straight forward. The paper is verbosely written and I feel like the findings could be summarized much more succinctly.","model":"human","source":"peerread","label":0,"id":4888}
{"text":"While this is an interesting topic, both the method description and experimental setup could be improved.","model":"human","source":"peerread","label":0,"id":4889}
{"text":"\n\nPaper summary:\nThis work proposes a new algorithm to generate k-adversarial images by modifying a small fraction of the image pixels and without requiring access to the classification network weight.\n\n\nReview summary:\nThe topic of adversarial images generation is of both practical and theoretical interest. This work proposes a new approach to the problem, however the paper suffers from multiple issues. It is too verbose (spending long time on experiments of limited interest); disorganized (detailed description of the main algorithm in sections 4 and 5, yet a key piece is added in the experimental section 6); and more importantly the resulting experiments are of limited interest to the reader, and the main conclusions are left unclear.\nThis looks like an interesting line of work that has yet to materialize in a good document, it would need significant re-writing to be in good shape for ICLR.\n\n\nPros:\n* Interesting topic\n* Black-box setup is most relevant\n* Multiple experiments\n* Shows that with flipping only 1~5% of pixels, adversarial images can be created\n\n\nCons:\n* Too long, yet key details are not well addressed\n* Some of the experiments are of little interest\n* Main experiments lack key measures or additional baselines\n* Limited technical novelty\n\n\n\n\nQuality: the method description and experimental setup leave to be desired. \n\n\nClarity: the text is verbose, somewhat formal, and mostly clear; but could be improved by being more concise.\n\n\nOriginality: I am not aware of another work doing this exact same type of experiments. However the approach and results are not very surprising.\n\n\nSignificance: the work is incremental, the issues in the experiments limit potential impact of this paper.\n\n\nSpecific comments:\n* I would suggest to start by making the paper 30%~40% shorter. Reducing the text length, will force to make the argumentation and descriptions more direct, and select only the important experiments.\n* Section 4 seems flawed. If the modified single pixel can have values far outside of the [LB, UB] range; then this test sample is clearly outside of the training distribution; and thus it is not surprising that the classifier misbehaves (this would be true for most classifiers, e.g. decision forests or non-linear SVMs). These results would be interesting only if the modified pixel is clamped to the range [LB, UB].\n* [LB, UB] is never specified, is it ? How does p = 100, compares to [LB, UB] ? To be of any use, p should be reported in proportion to [LB, UB]\n* The modification is done after normalization, is this realistic ? \n* Alg 2, why not clamping to [LB, UB] ?\n* Section 6, \u201cimplementing algorithm LocSearchAdv\u201d, the text is unclear on how p is adjusted; new variables are added. This is confusion.\n* Section 6, what happens if p is _not_ adjusted ? What happens if a simple greedy random search is used (e.g. try 100 times a set of 5 random pixels with value 255) ?\n* Section 6, PTB is computed over all pixels ? including the ones not modified ? why is that ? Thus LocSearchAdv PTB value is not directly comparable to FGSM, since it intermingles with #PTBPixels (e.g. \u201cin many cases far less average perturbation\u201d claim).\n* Section 6, there is no discussion on the average number of model evaluations. This would be equivalent to the number of requests made to a system that one would try to fool. This number is important to claim the \u201ceffectiveness\u201d of such black box attacks. Right now the text only mentions the upper bound of 750 network evaluations. \n* How does the number of network evaluations changes when adjusting or not adjusting p during the optimization ?\n* Top-k is claimed as a main point of the paper, yet only one experiment is provided. Please develop more, or tune-down the claims.\n* Why is FGSM not effective for batch normalized networks ? Has this been reported before ? Are there other already published techniques that are effective for this scenario ? Comparing to more methods would be interesting.\n* If there is little to note from section 4 results, what should be concluded from section 6 ? That is possible to obtain good results by modifying only few pixels ? What about selecting the \u201ctop N\u201d largest modified pixels from FGSM ? Would these be enough ? Please develop more the baselines, and the specific conclusions of interest.\n\n\nMinor comments:\n* The is an abuse of footnotes, most of them should be inserted in the main text.\n* I would suggest to repeat twice or thrice the meaning of the main variables used (e.g. p, r, LB, UB)\n* Table 1,2,3 should be figures\n* Last line of first paragraph of section 6 is uninformative.\n* Very tiny -> small","model":"human","source":"peerread","label":0,"id":4890}
{"text":"The paper presents a method for generating adversarial input images for a convolutional neural network given only black box access (ability to obtain outputs for chosen inputs, but no access to the network parameters).  However, the notion of adversarial example is somewhat weakened in this setting: it is k-misclassification (ensuring the true label is not a top-k output), instead of misclassification to any desired target label.\n\nA similar black-box setting is examined in Papernot et al. (2016c).  There, black-box access is used to train a substitute for the network, which is then attacked.  Here, black-box access in instead exploited via local search.  The input is perturbed, the resulting change in output scores is examined, and perturbations that push the scores towards k-misclassification are kept.\n\nA major concern with regard to novelty is that this greedy local search procedure is analogous to gradient descent; a numeric approximation (observe change in output for corresponding change in input) is used instead of backpropagation, since one does not have access to the network parameters.  As such, the greedy local search algorithm itself, to which the paper devotes a large amount of discussion, is not surprising and the paper is fairly incremental in terms of technical novelty.\n","model":"human","source":"peerread","label":0,"id":4891}
{"text":"Authors propose the use of layer-wise language model-like pretraining for encoder-decoder models. This allows to leverage separate source and target corpora (in unsupervised manner) without necessity of large amounts of parallel training corpora. The idea is in principle fairly simple, and rely on initial optimising both encoder and decoder with LSTM tasked to perform language modelling. \n\nThe ideas are not new, and the paper is more like a successful compilation of several approaches that have been around for some time. The experimental validation, though, offers some interesting insights into importance of initialization, and the effectiveness of different initialisations approaches in enc-dec setting.\n\nThe regulariser you propose to use on page 3, looks like typical multi-task objective function, especially it is used in an alternating manner would be interesting to see whether similar performance might have been obtained starting with this objective, from random initialisation.\n\nYou should probably give credit for encoder-decoder like-RNN models published in 1990s.\n\nMinors:\nPg. 2, Sec 2.1 2nd paragraph: can be different sizes -> can be of different sizes","model":"human","source":"peerread","label":0,"id":4892}
{"text":"This paper effectively demonstrates that the use of pretraining can improve the performance of seq2seq models for MT and summarization tasks. However, despite these empirical gains, the reviewers were not convinced enough by the novelty of the work itself and did not feel like there were technical contributions to make this a fit for ICLR.\n \n Pros:\n - All reviewers agree that the empirical gains in this paper are convincing and lead to BLEU improvements on a large scale translation and translation like tasks. Reviewer 4 also praises the detailed analysis that demonstrates that these gains come from the pretraining process itself. \n - From an impact perspective, the reviewers found the approach clear and implementable. \n \n Cons:\n - Novelty criticisms are that the method is a \"compilation\" of past approaches (although at a larger scale) and therefore primarily experimental, and that the objectives given are \"highly empirical\" and not yet motivated by theory. The authors did respond, but the reviewer did not change their score.\n - There are suggestions that this type of work would perhaps be more widely impactful in an NLP venue, where a BLEU improvement of this regard is a strong supporting piece on its own.","model":"human","source":"peerread","label":0,"id":4893}
{"text":"In this paper, the authors propose to pretrain the encoder\/decoder of seq2seq models on a large amount of unlabeled data using a LM objective. They obtain improvements using this technique on machine translation and abstractive summarization.\n\nWhile the effectiveness of pretraining seq2seq models has been known among researchers and explored in a few papers (e.g. Zoph et al. 2016,  Dai and Le 2015), I believe this is the first paper to pretrain using a LM for both the encoder\/decoder. The technique is simple, but the gains are large (e.g. +2.7 BLEU on NMT). In addition, the authors perform extensive ablation studies to analyze where the performance is coming from. Hence, I think this paper should be accepted.\n","model":"human","source":"peerread","label":0,"id":4894}
{"text":"strengths:\n\nA method is proposed in this paper to initialize the encoder and decoder of the seq2seq model using the trained weights of language models with no parallel data. After such pretraining, all weights are jointly fine-tuned with parallel labeled data with an additional language modeling loss.\n\nIt is shown that pretraining accelerates training and improves generalization of seq2seq models.\n\nThe main value of the proposed method is to leverage separate source and target corpora, contrasting the common methods of using large amounts of parallel training corpora.\n\n\nweaknesses:\n\nThe objective function shown in the middle of pg 3 is highly empirical, not directly linked to how non-parallel data helps to improve the final prediction results. The paper should compare with and discuss the objective function based on expectation of cross entropy which is directly linked to improving prediction results as proposed in arXiv:1606.04646, Chen et al.:  Unsupervised Learning of Predictors from Unpaired Input-Output Samples, 2016.\n\nThe pre-training procedure proposed in this paper is also closely connected with the DNN pretraining method presented in Dahl et al. 2011, 2012. Comparisons should be made in the paper, highlighting why the proposed one is conceptually superior if the authors believe so. \n","model":"human","source":"peerread","label":0,"id":4895}
{"text":"what's your own baseline for NMT without pre-training? Jean et al. (2015) uses a more shallow architecture than this paper, so I presume your baseline would be higher (this is also corrobated in figure 3).\n\nyou should make it clear how much of your improvement over related work comes from pretraining, and how much from having a deeper architecture.","model":"human","source":"peerread","label":0,"id":4896}
{"text":"The paper is a novel application for the sticky HDP-HMM, focused on correctly identifying the number of components in bird and whale song across a variety of datasets. It's nice to see the model applied to an interesting dataset. My main issues with the paper have to do with structure and the choice of representation used in the model. Namely:\n\nThe organization of the paper could be significantly improved. There is a lot of repetitive introduction that adds little to the paper. The first and last two sentences of the abstract could be cut. Many other parts of the abstract basically repeat the introduction. The second paragraph of section 2.3 also repeats your introduction - by now we know what you're doing. I think most people reading this will have no idea what Kershenbaum (2014) is. The description of the data should go in the experiments section. \"Different hypotheses for the songs were emitted\" in the introduction is odd phrasing. Figure 4 should be the first figure and go in the introduction. Figure 5 should be in the methods section. A summary of Table 1 should be in the experiments section. Generally the writing could be tightened quite a bit, which would make space for these figures. The description of the HDP-HMM, which mostly follows the existing literature, is well done.\n\nSome general questions about the methods used:\n\nIf you're interested in scalable inference, why use Gibbs sampling? Why not the beam sampler (van Gael 2008), which at least recently was the state of the art for MCMC inference in the HDP-HMM? More generally, why use MCMC at all? For very large datasets, most of the Bayesian ML community has converged on stochastic variational inference as the most practical method (eg Wang, Paisley and Blei 2011).\n\nIf your interest is mainly in the number of clusters, how would you address the fact that DP mixture models are known not to be consistent for estimating the true number of clusters (Miller and Harrison 2013)?\n\nMFCC features are calibrated to the human auditory system, not bird or whale auditory systems. In your data, do you calibrate the MFCC scale to be closer to the auditory systems of the animals that generated the song?\n\nAnd a final suggestion for future work, which could use the results presented here as a baseline:\n\nGiven the success of LSTMs in speech recognition in recent years, it may be the case that deep learned representations are superior to linear features (like the means of each cluster in an HDP-HMM) for animal song as well. Have you considered a hybrid model, similar to recent work combining autoencoders and graphical models (Johnson, Duvenaud, Wiltschko, Datta and Adams 2016)?","model":"human","source":"peerread","label":0,"id":4897}
{"text":"The paper studies an unusual and (apparently) challenging application -- segmentation of whale and bird songs. Though the authors claim that the applications is much more challenging than previous applications of the proposed techniques (in speech processing), the evaluation is very questionable (as there is no gold standard), there is no convincing comparison with other (potentially simpler techniques). Overall, the reviewers believe the work is not mature enough to be accepted at ICLR.\n \n + interesting dataset \/ task \n \n - novelty is limited\n - evaluation is weak\n - writing is poor","model":"human","source":"peerread","label":0,"id":4898}
{"text":"This paper applies HDP-HMM to challenging bioacoustics segmentation problems including humpback whole sound and bird sound segmentation. Although the technique itself is not novel, the application of this data-driven method to bioacoustics segmentation is quite challenging, and may yield some scientific findings, and this is a valuable contribution to the bioacoustics field. My concern for this paper is that it does not have fair comparison of the other simple methods including BIC and AIC, and it is better to provide such comparisons. Especially, as the authors pointed out, the computational cost of HDP-HMM is a big issue, and the other simple methods may solve this issue.","model":"human","source":"peerread","label":0,"id":4899}
{"text":"This paper presented an unsupervised approach for the automatic segmentation of bioacoustic data. The authors applied an existing approach (Hierarchical Dirichlet Process Hidden Markov Models) to their task. The originality of their work is the investigation of this approach on a new task, which they argue is more difficult, namely bioacoustic segmentation. They provide evidence that this is a difficult task by explaining that there doesn't exist a consensus among human experts on how this should be done. However, they do not provide convincing results that their approach is successful, as it fails in many cases to replicate the correct segmentations as defined by their baseline: human experts. In addition, the clarity of the writing is extremely poor, including many grammatical errors and awkward sentences. ","model":"human","source":"peerread","label":0,"id":4900}
{"text":"The authors proposed RASOR to address the problem of finding the best answer span according to a given question. The focus of the paper is mainly on how to model the relationship between question and the answer spans. The idea proposed by this paper is reasonable, but not ground breaking. The analysis is interesting and potentially useful. I would hope the authors can go extra miles to analyze different choices of boundary prediction models and make a more convincing case for the necessity of modeling the score of the span globally.\n\nThe main idea behind RASOR is to globally normalize and rank the scores of the possible answer spans. RASOR is able to achieve this by first modeling the hidden vectors of all words with LSTMs. Then, the representation of a text span is formed by concatenating the corresponding hidden vectors of the start and the end word of the corresponding chunk. The approach is reasonable, but not earth shattering. Also, the table 6 shows that the improvement over end-prediction point is not very large.\n\nI appreciate the fact that authors conduct several analysis experiments as some of them are quite interesting. For example, it seems that question independent representation is also very import to the performance. In addition to the current analysis, I also want to get a clear idea on what makes the current model be better than the Match-LSTM. Is it hyper-parameter tuning? Or it is due to the use of the question independent representation?\n\nAnother good thing about the proposed model is that it is relatively simple, so there is a chance that the proposed techniques can be combined with other newly proposed ones.","model":"human","source":"peerread","label":0,"id":4901}
{"text":"The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, most reviewers are not leaning sufficiently towards acceptance. In particular, it is unfortunate that authors can not evaluate their model on the leaderboard due to copyright issues. The role of standard datasets and benchmarks is to allow for meaningful comparisons. Evaluation on non-standard splits defeats this purpose. Fortunately, sounds like authors are working on getting their model evaluated on the leaderboard. Resolving that and incorporating reviewers' feedback will help make the paper stronger.","model":"human","source":"peerread","label":0,"id":4902}
{"text":"We thank all three reviewers for the valuable comments and suggestions. \n\nWe agree with reviewer 1 that the lack of test results is not ideal and sadly we do not yet have a manner in which we can run on the hidden test set, as not all of our code is open-sourced. However, we hope that the significant dev set size of 10k items, along with the cross-validation results add some reassurance that our hyperparameter tuning scheme has not overfit the data.\n\nReviewer 3 points out that the results in this paper are no longer state of the art. It is true that there are other papers on the leaderboard that have now surpassed our results, largely through ensembling. However, we believe that our paper is the only work to specifically study the impact of different span representations and we agree with Reviewer 3 that our findings should be complementary to other recent work on this dataset. We have added some extra quantitative and qualitative analysis of the differences between the span classifier and the endpoints predictor to illustrate the manner in which the quality of endpoint predictions degrade for longer sentences, in particular showing the tendency of endpoint models to pick out endpoints from separate answer candidates.\n\nReviewer 2 points out that the difference in performance between our model and the Match-LSTM cannot be accounted for by the difference in label type alone, and asks for the other most salient differences between the two approaches. While there are many small differences between the two implementations, the ablations in Table 2.a. suggest that most of this gap is accounted for by the passage independent question representation that is missing in the Match-LSTM. We have added an analysis of this representation in a new Table 3 and we have updated our discussion of the Match LSTM to clarify the basis of our comparison.","model":"human","source":"peerread","label":0,"id":4903}
{"text":"This paper proposes RaSoR, a method to efficiently representing and scoring all possible spans in an extractive QA task. While the test set results on SQuAD have not been released, it looks likely that they are not going to be state-of-the-art; with that said, the idea of enumerating all possible spans proposed in this paper could potentially improve many architectures. The paper is very well-written and the analysis\/ablations in the final sections are mostly interesting (especially Figure 2, which confirms what we would intuitively believe). Based on its potential to positively impact other researchers working on SQuAD, I recommend that the paper is accepted. ","model":"human","source":"peerread","label":0,"id":4904}
{"text":"This paper presents an architecture for answer extraction task and evaluates on the SQUAD dataset. The proposed model builds fixed length representations of all spans in the answer document based on recurrent neural network. It outperforms a few baselines in exact match and F1 on SQUAD.\n\nIt is unfortunate that the blind test results are not obtained yet due to the copyright issue. There are quite a few other systems\/submissions on the SQUAD leader board that were available for comparison.\n\nGiven that there's no result on the test set reported, the grid search for hyperparameters on the dev set directly is also a concern, even though the authors did cross validation experiments.\n","model":"human","source":"peerread","label":0,"id":4905}
{"text":"The authors proposed RASOR to address the problem of finding the best answer span according to a given question. The focus of the paper is mainly on how to model the relationship between question and the answer spans. The idea proposed by this paper is reasonable, but not ground breaking. The analysis is interesting and potentially useful. I would hope the authors can go extra miles to analyze different choices of boundary prediction models and make a more convincing case for the necessity of modeling the score of the span globally.\n\nThe main idea behind RASOR is to globally normalize and rank the scores of the possible answer spans. RASOR is able to achieve this by first modeling the hidden vectors of all words with LSTMs. Then, the representation of a text span is formed by concatenating the corresponding hidden vectors of the start and the end word of the corresponding chunk. The approach is reasonable, but not earth shattering. Also, the table 6 shows that the improvement over end-prediction point is not very large.\n\nI appreciate the fact that authors conduct several analysis experiments as some of them are quite interesting. For example, it seems that question independent representation is also very import to the performance. In addition to the current analysis, I also want to get a clear idea on what makes the current model be better than the Match-LSTM. Is it hyper-parameter tuning? Or it is due to the use of the question independent representation?\n\nAnother good thing about the proposed model is that it is relatively simple, so there is a chance that the proposed techniques can be combined with other newly proposed ones. \n\n","model":"human","source":"peerread","label":0,"id":4906}
{"text":"Thank you for an interesting read.\n\nI found the application of VRNN type generative model to financial data very promising. But since I don't have enough background knowledge to judge whether the performance gap is significant or not, I wouldn't recommend acceptance at this stage. \n\nTo me, the biggest issue for this paper is that I'm not sure if the paper contains significant novelty. The RNN-VAE combination has been around for more than a year and this paper does not propose significant changes to it. Maybe this paper fits better to an application targeting conference, rather than ICLR. But I'm not exactly sure about ICLR's acceptance criteria, and maybe the committee actually prefer great performances and interesting applications?","model":"human","source":"peerread","label":0,"id":4907}
{"text":"This paper presents an interesting application of variational methods for time series, in particular VRNN-like approaches, for stochastic volatility. Applications such as these are clearly in the scope of the conference, which was a point that one of the reviewer brought up. That said, questions of context, especially with regards to relation to different approaches, especially smoothing are relevant. Also, the number of methods GRACH and stochastic volatility is immense and this makes assessing the impact of this very hard to do and is more is needed to address this point. This paper is certainly interesting, but given these concerns, the paper is not yet rady for acceptance at the conference.","model":"human","source":"peerread","label":0,"id":4908}
{"text":"The authors propose a recurrent variational neural network approach to modelling volatility in financial time series. This model consists of an application of Chung et al.\u2019s (2015) VRNN model to volatility forecasting, wherein a Variational Autoencoder (VAE) structure is repeated at each time step of the series. \n\nThe paper is well written and easy to follow (although this reviewer suggests applying a spelling checking, since the paper contains a number of harmless typos). \n\nThe paper\u2019s main technical contribution is to stack two levels of recurrence, one for the latent process and one for the observables. This appears to be a novel, if minor contribution. The larger contribution is methodological, in areas of time series modelling that are both of great practical importance and have hitherto been dominated by rigid functional forms. The demonstration of the applicability and usefulness of general-purpose non-linear models for volatility forecasting would be extremely impactful.\n\nI have a few comments and reservations with the paper:\n1) Although not  mentioned explicitly, the authors\u2019 framework are couched in terms of carrying out one-timestep-ahead forecasts of volatility. However, many applications of volatility models, for instance for derivative pricing, require longer-horizon forecasts. It would be interesting to discuss how this model could be extended to forecast at longer horizons.\n \n2) In Section 4.4, there\u2019s a mention that a GARCH(1,1) is conditionally deterministic. This is true only when forecasting 1 time-step in the future. At longer horizons, the GARCH(1,1) volatility forecast is not deterministic. \n\n3) I was initially unhappy with the limitations of the experimental validation, limited to comparison with a baseline GARCH model. However, the authors provided more comparisons in the revision, which adds to the quality of the results, although the models compared against cannot be considered state of the art. It would be well advised to look into R packages such as `stochvol\u2019 and \u2018fGarch\u2019 to get implementations of a variety of models that can serve as useful baselines, and provide convincing evidence that the modelled volatility is indeed substantially better than approaches currently entertained by the finance literature.\n\n4) In Section 5.2, more details should be given on the network, e.g. number of hidden units, as well as the embedding dimension D_E (section 5.4)\n\n5) In Section 5.3, more details should be given on the data generating process for the synthetic data experiments. \n\n6) Some results in the appendix are very puzzling: around jumps in the price series, which are places where the volatility should spike, the model reacts instead by huge drops in the volatility (Figure 4(b) and (c), respectively around time steps 1300 and 1600). This should be explained and discussed.\n\nAll in all, I think that the paper provides a nice contribution to the art of volatility modelling. In spite of some flaws, it provides a starting point for the broader impact of neural time series processing in the financial community.\n","model":"human","source":"peerread","label":0,"id":4909}
{"text":"\nThe authors propose a recurrent neural network approach for constructing a\nstochastic volatility model for financial time series. They introduce an\ninference network based on a recurrent neural network that computes the\napproximation to the posterior distribution for the latent variables given the\npast data. This variational approximation is used to maximize the marginal\nlikelihood in order to learn the parameters of the model. The proposed method\nis validated in experiments with synthetic and real-world time series, showing\nto outperform parametric GARCH models and a Gaussian process volatility model.\n\nQuality:\n\nThe method proposed seems technically correct, with the exception that in\nequation (19) the inference model is doing filtering and not smoothing, in the\nsense that the posterior for z_t' only depends on those other z_t and x_t\nvalues with t","model":"human","source":"peerread","label":0,"id":4910}
{"text":"Authors present a parameterized variant of ELU and show that the proposed function helps to deal with vanishing gradients in deep networks in a way better than existing non-linearities. They present both a theoretical analysis and practical validation for presented approach. \n\nInteresting observations on statistics of the PELU parameters are reported. Perhaps explanation for the observed evolution of parameters can help better understand the non-linearity. It is hard to evaluate the experimental validation presented given the difference in number of parameters compared to other approaches.","model":"human","source":"peerread","label":0,"id":4911}
{"text":"The paper describes a parametric version of the exponential linear unit (ELU) activation function. The novelty of the contribution is limited, and the experimental evaluation in its current form is not convincing.","model":"human","source":"peerread","label":0,"id":4912}
{"text":"The paper deals with a very important issue of vanishing gradients and the quest for a perfect activation function. Proposed is an approach of learning the activation functions during the training process. I find this research very interesting, but I am concerned that the paper is a bit premature.\n\nThere is a long experimental section, but I am not sure what the conclusion is. The authors appear to be somewhat confused themselves. The amount of \"maybe\" \"could mean\", \"perhaps\" etc. statements in the paper is exceptionally high. For this paper to be accepted it needs a bold statement about the performance, with a solid evidence. In my opinion, that is lacking as of now. This approach is either a breakthrough or a dud, and after reading the paper I am not convinced which case it is.\n\nThe theoretical section could be made a little clearer.\n\nFinally, how is the performance affected. The huge advantage if ReLU is in the fact that the formula is so simple and thus not costly to evaluate. How do PELU-s compare.","model":"human","source":"peerread","label":0,"id":4913}
{"text":"This paper presents a new non-linear function for CNN and deep neural networks. \nThe new non-linearity reports some gains on most datasets of interest, and can be used in production networks with minimal increase in computation.","model":"human","source":"peerread","label":0,"id":4914}
{"text":"This paper proposes a modification of the ELU activation function for neural networks, by parameterizing it with 2 trainable parameters per layer. This parameter is proposed to more effectively counter vanishing gradients. \n\nMy main concern regarding this paper is related to the authors' claims about the effectiveness of PELU. The analysis in Sections 2 and 3 discusses how PELU might improve training by combating gradient propagation issues. This by itself does not imply that improved generalization will result, only that models may be easier to train. However, the experiments all seek to demonstrate improved generalization performance.\nBut this could in principle be due to a better inductive bias, and have nothing to do with the optimization analysis. None of the experiments are designed to directly support the stated theoretical advantage of PELU compared to ELU in optimizing models.\n\nIn the response to the pre-review question, the authors state that the claims in Section 2 and 3.3 are meant to apply to generalization performance. I fail to see how this is true for most claims, except the flexibility claim. As the authors agree, better training may or may not lead to better out-of-sample performance. I can only agree that having flexibility can sometimes help the network adapt its inductive bias to the problem (instead of overfitting), but this is a much weaker claim compared to the mathematical justifications for improved optimization.\n\nOn selection of learning hyperparameters:\nThe authors state in the discussion on OpenReview that the learning rates selected were favorable to ReLU, and not PELU. However, this does not guarantee that they were not unfavorable to ELU. It raises the question: can a regime be constructed where ELU has better performance than PELU? If so, how can we draw the conclusion that PELU is better?\n\nOverall, I am not yet convinced by the experimental setup and the match between theory and experiments in this paper.","model":"human","source":"peerread","label":0,"id":4915}
{"text":"Authors present a parameterized variant of ELU and show that the proposed function helps to deal with vanishing gradients in deep networks in a way better than existing non-linearities. They present both a theoretical analysis and practical validation for presented approach. \n\nInteresting observations on statistics of the PELU parameters are reported. Perhaps explanation for the observed evolution of parameters can help better understand the non-linearity. It is hard to evaluate the experimental validation presented given the difference in number of parameters compared to other approaches.\n ","model":"human","source":"peerread","label":0,"id":4916}
{"text":"This paper proposed to use unsupervised learning to learn features in a reinforcement learning setting. It is unclear what \"unsupervised\" means here since the \"causality prior\" uses reward signals for training. This is reinforcement learning, not unsupervised learning.\n\nThe experiments are also very premature. The task is as simple as moving the head of the robot left or right. There is also no comparison to baselines.\n\nIn conclusions section, the authors claim the proposed method can be used for transfer learning without experiments to backup the claim.\n\nOverall this paper is confusing and premature.","model":"human","source":"peerread","label":0,"id":4917}
{"text":"The authors apply an already-published method for state representation learning in a very simple experimental scenario. They give no additional contribution or comparison, nor do they offer any empirical or analytical study.","model":"human","source":"peerread","label":0,"id":4918}
{"text":"Dear reviewers,\n\nwe agreed that comparisons with other works need to be done and that other tasks have to be tested to improve evaluation and validation.\nIt will be done in future experiments.","model":"human","source":"peerread","label":0,"id":4919}
{"text":"This paper implements the method of Jonschkowski & Brock to learn a low-dimensional state representation represented as the last layer of a neural network. The experiments apply the method for learning a one-dimensional state representation of a simulated robot\u2019s head position from synthetic images.\n\nLearning state representations is an active and useful area of research for learning representations in interactive domains such as robotics. However, there seems to be no novelty in the method, over Jonschkowki & Brock. The primary contribution is the experimental evaluation performed on one task, where the paper evaluates the correlation between the learned state representation and the ideal state representation for the task (which is the robot\u2019s head position).\n\nAs acknowledged by the authors, the experiments are very preliminary, only showing one simple task with a one-dimensional learned representation and a two-dimensional discrete action space. To make the experiments compelling, there need to be comparisons to prior methods such as Lange et al. \u201912, Watter et al. NIPS \u201915, and Finn et al. ICRA \u201916 which also learn state representations from raw images. PCA on the images would also be a useful comparison, especially for simple tasks. Without these comparisons, it is impossible to evaluate the effectiveness of the method.\n\nLastly, as mentioned in the pre-review questions, the related work should include a discussion of other state representation learning methods such as Watter et al. NIPS \u201915, Finn et al. ICRA \u201916, and van Hoof et al. IROS \u201916.\n\nIn summary, this paper lacks novelty and significance, as the paper implements an existing method and demonstrates results on only one simple task. Without comparisons, the results are impossible to interpret. More challenging tasks and experimental comparisons would significantly improve the paper. Additionally, this paper does not introduce any novel contributions to state representation learning for solving challenges in this domain. One pro is that the paper is generally written clearly.","model":"human","source":"peerread","label":0,"id":4920}
{"text":"The paper proposes to use the representation learning approach of [Jonschkowski & Brock, 2015] with a deep network as function approximator. The general task and approach are interesting, but contribution of this work is limited, and experimental evaluation is absolutely unsatisfactory, so the paper cannot be accepted for publications. \n\nThe approach is tested on a simple synthetic task with very small training and test sets and very little variation in the data. The authors admitted themselves that the results are preliminary. The proposed method is not compared with existing approaches or simple hand-crafted baselines. It is impossible to judge if the proposed method is useful and\/or performs well compared to existing approaches. This makes the paper unfit for publication. \n\nWith proper experiments, and if the method works in interesting realistic scenarios, this could become a good paper.\n","model":"human","source":"peerread","label":0,"id":4921}
{"text":"Summary: There are many different pruning techniques to reduce memory footprint of CNN models, and those techniques have different granularities (layer, maps, kernel or intra kernel), pruning ratio and sparsity of representation. The work proposes a method to choose the best pruning masks out to many trials. Tested on CIFAR-10, SVHN and MNIST.\n\nPros:\nProposes a method to choose pruning mask out of N trials. \nAnalysis on different pruning methods.\n\nCons & Questions:\n\u201cThe proposed strategy selects the best pruned network through N random pruning trials. This approach enables one to select pruning mask in one shot and is simpler than the multi-step technique.\u201d How can one get the best pruning mask in one shot if you ran N random pruning trials? (answered)\nMissing tests of the approach with bigger CNN: like AlexNet, VGG, GoogLeNet or ResNet. (extended to VGG ok)\nSince reducing model size for embedded systems is the final goal, then showing how much memory space in MB is saved with the proposed technique compared with other approaches like Han et al. (2015) would be good.\n\nMisc:\nTypo in figure 6 a) caption: \u201cFeatuer\u201d (corrected)","model":"human","source":"peerread","label":0,"id":4922}
{"text":"Unfortunately this paper is not competitive enough for ICLR. The paper is focusing on efficiency where for the results to be credible it is of utmost importance to present experiments on large scale data and state-of-the-art models.\n I am afraid the reviewers were not able to take into account the latest drafts of the paper that were submitted in January, but those submissions came very late.","model":"human","source":"peerread","label":0,"id":4923}
{"text":"Dear reviewers,\n\ncan you please take a look at the responses by the authors and add a comment indicating that you have taken them into consideration?\n\nThanks!\n","model":"human","source":"peerread","label":0,"id":4924}
{"text":"This paper proposes a simple randomized algorithm for selecting which weights in a ConvNet to prune in order to reduce theoretical FLOPs when evaluating a deep neural network. The paper provides a nice taxonomy or pruning granularity from coarse (layer-wise) to fine (intra-kernel). The pruning strategy is empirically driven and uses a validation set to select the best model from N randomly pruned models. Makes claims in the intro about this being \"one shot\" and \"near optimal\" that cannot be supported: it is \"N-shot\" in the sense that N networks are generated and tested and there is no evidence or theory that the found solution is \"near optimal.\"\n\nPros:\n- Nice taxonomy of pruning levels\n- Comparison to the recent weight-sum pruning method\n\nCons:\n- Experimental evaluation does not touch upon recent models (ResNets) and large scale datasets (ImageNet)\n- Paper is somewhat hard to follow\n- Feature map pruning can obviously accelerate computation without specialized sparse implementations of convolution, but this is not the case for finer grained sparsity; since this paper considers fine-grained sparsity it should provide some evidence that introducing that sparsity can yield performance improvements\n\nAnother experimental downside is that the paper does not evaluate the impact of filter pruning on transfer learning. For example, there is not much direct interest in the tasks of MNIST, CIFAR10, or even ImageNet. Instead, a main interest in both academia and industry is the value of the learned representation for transferring to other tasks. One might expect pruning to harm transfer learning. It's possible that the while the main task has about the same performance, transfer learning is strongly hurt. This paper has missed an opportunity to explore that direction.\n\nIn summary, the proposed method is simple, which is good, but the experimental evaluation is somewhat incomplete and does not cover recent models and larger scale datasets.","model":"human","source":"peerread","label":0,"id":4925}
{"text":"This paper proposes two pruning methods to reduce the computation of deep neural network. In particular, whole feature maps and the kernel connections can be removed with not much decrease of classification accuracy. \nHowever, this paper also has the following problems. \n1)\tThe method is somehow trivial, since the pruning masks are mainly chosen by simple random sampling. The novelty and scalability are both limited. \n2)\tExperiment results are mainly focused on the classification rate and the ideal complexity. As a paper on improving computation efficiency, it should include results on practical time consumption. It is very common that reducing numbers of operations may not lead to reduced computational time on a highly parallel platform (e.g., GPU). \n3)\tIt is more important to improve the computational efficiency on large-scale models (e.g., ImageNet classification network) than on small models (e.g., MNIST, CIFAR network). However, results on large-scale network is missing.\n4)\t(*Logical validity of the proposed method*) For feature map pruning, what if just to train reduced-size network is trained from scratch without transfer any knowledge from the pretrained large network? Is it possible to get the same accuracy? If so, it will simply indicate the hyper-parameter is not optimal for the original network. Experimental results are necessary to clarify the necessity of feature map pruning. \nNote that I agree with that a smaller network may be more generalizable than a larger network. \n\n----------------------------------------------\n\nComments to the authors's response:\n\nThanks for replying to my comments. \n\n1) I still believe that the proposed methods are trivial.\n2) It is nice to show GPU implementation. Compared to existing toolboxes (e.g., Torch, Caffe, TensorFlow), is the implementation of convolution efficient enough?\n3) Experiments on Cifar-100 are helpful (better than cifar-10), but it is not really large-scale, where speed-up is not so critical. ImageNet and Places datasets are examples of large-scale datasets.\n4) The author did not reply to the question wrt the validity of the proposed methods. This question is critical.   \n\n","model":"human","source":"peerread","label":0,"id":4926}
{"text":"Summary: There are many different pruning techniques to reduce memory footprint of CNN models, and those techniques have different granularities (layer, maps, kernel or intra kernel), pruning ratio and sparsity of representation. The work proposes a method to choose the best pruning masks out to many trials. Tested on CIFAR-10, SVHN and MNIST.\n\nPros:\nProposes a method to choose pruning mask out of N trials. \nAnalysis on different pruning methods.\n\nCons & Questions:\n\u201cThe proposed strategy selects the best pruned network through N random pruning trials. This approach enables one to select pruning mask in one shot and is simpler than the multi-step technique.\u201d How can one get the best pruning mask in one shot if you ran N random pruning trials? (answered)\nMissing tests of the approach with bigger CNN: like AlexNet, VGG, GoogLeNet or ResNet. (extended to VGG ok)\nSince reducing model size for embedded systems is the final goal, then showing how much memory space in MB is saved with the proposed technique compared with other approaches like Han et al. (2015) would be good.\n\nMisc:\nTypo in figure 6 a) caption: \u201cFeatuer\u201d (corrected)\n","model":"human","source":"peerread","label":0,"id":4927}
{"text":"The paper introduces a lightweight network for semantic segmentation that combines several acceleration ideas.\nAs indicated in my preliminary question, the authors do not make the case about why any of the techniques they propose is beyond what we know already: factorizing filters into alternating 1-D convolutions, using low-rank kernels, or any of the newer inception network architectures.\n\nI have had a hard time figuring out what is the take-home message of this paper. All of these ideas are known, and have proven their worth for detection. If a paper is going to be accepted for applying them to semantic segmentation, then in the next conference another paper should be accepted for applying them to normal estimation, another to saliency estimation and so on. \n\nAs the authors mention in their preliminary review:\n\"I agree that most improvements from classification architectures are straightforward to apply to object segmentation, and that's exactly what we've done - our network is based on current state of the art models. Instead of repeating most of the discussion on factorizing filters, etc., that has been discussed in a lot of papers already, we have decided that it's much more valuable to describe in depth the choices that are related to segmentation only - these are the most important contributions of our paper.\"\n\nI do not see however any in-depth discussion of certain choices - e.g. an analysis of how certain choices influence performance or speed. Instead all one gets are some statements \"these gave a significant accuracy boost\" \"this helped a lot\", \"that did not help\", \"this turned out to work much better than that\" . This is not informative - and is more like an informal chat rather than an in-depth discussion. \n\nIf novelty is not that important, and it is only performance or speed that matter, I am still not convinced.\nThe authors only compare to [1,2] (SegNet) in terms of both accuracy and speed. I cannot see the reason why they do so, and they do not really justify it. According to the authors' evaluation, [1] requires ~1 sec. per frame,  while Deeplab v2, without the DenseCRF, runs at 5-8fps. \n(","model":"human","source":"peerread","label":0,"id":4928}
{"text":"Three knowledgable reviewers recommend rejection and there was no rebuttal. The AC agrees with the reviewers.","model":"human","source":"peerread","label":0,"id":4929}
{"text":"\nPaper summary: this work presents ENet, a new convnet architecture for semantic labeling which obtains comparable performance to the previously existing SegNet while being ~10x faster and using ~10x less memory. \n\n\nReview summary: Albeit the results seem interesting, the paper lacks detailed experimental results, and is of limited interest for the ICLR audience.\n\n\nPros:\n* 10x faster\n* 10x smaller\n* Design rationale described in detail\n\n\nCons:\n* The quality of the reference baseline is low. For instance, cityscapes results are 58.3 IoU while state of the art is ~80 IoU. Thus the results are of limited interest.\n* The results that support the design rationale are not provided. It is important to provide the experimental evidence to support each claim.\n\n\nQuality: the work is interesting but feels incomplete. If your model is 10x faster and smaller, why not try build a model 10x longer to obtain improved results ? The paper focuses only on  nimbleness at the cost of quality (using a weak baseline). This limits the interest for the ICLR audience.\n\n\nClarity: the overall text is somewhat clear, but the model description (section 3) could be more clear. \n\n\nOriginality: the work is a compendium of \u201cpractitioners wisdom\u201d applied to a specific task. It has thus limited originality.\n\n\nSignificance: I find the work that establishes a new \u201cbest practices all in one\u201d quite interesting, but however these must shine in all aspects. Being fast at the cost of quality, will limit the impact of this work.\n\n\nMinor comments:\n* Overall the text is proper english but the sentences constructions is often unsound, specific examples below. \n* To improve the chances of acceptance, I invite the authors to also explore bigger models and show that the same \u201ccollected wisdom\u201d can be used both to reach high speed and high quality (with the proper trade-off curve being shown). Aiming for only one end of the quality versus speed curve limits too much the paper.\n* Section 1: \u201cmobile or battery powered \u2026 require rates > 10 fps\u201c. 10 fps with which energy budget ? Should not this be  > 10 fps && < X Watt.\n* \u201cRules and ideas\u201d -> rules seem too strong of a word, \u201cguidelines\u201d ?\n* \u201cIs of utmost importance\u201d -> \u201cis of importance\u201d (important is already important)\n* \u201cPresents a trainable network \u2026 therefore we compare to \u2026 the large majority of inference the same way\u201d; the sentence makes no sense to me, I do not see the logical link between before and after \u201ctherefore\u201d\n* Scen-parsing -> scene-parsing\n* It is arguable if encoder and decoder can be called \u201cseparate\u201d\n* \u201cUnlike in Noh\u201d why is that relevant ? Make explicit or remove\n* \u201cReal-time\u201d is vague, you mean X fps @ Y W ?\n* Other existing architectures -> Other architectures\n* Section 3, does not the BN layer include a bias term ? Can you get good results without any bias term ?\n* Table 1: why is the initial layer a downsampling one, since the results has half the size of the input ?\n* Section 4, non linear operations. What do you mean by \u201csettle to recurring pattern\u201d ?\n* Section 4, dimensionality changes. \u201cComputationally expensive\u201d, relative to what ?\n* Section 4, dimensionality changes. \u201cThis technique ... speeds-up ten times\u201d, but does not provide the same results. Without an experimental validation changing an apple for an orange does not make the orange better than the apple.\n* Section 4, dimensionality changes. \u201cFound one problem\u201d, problem would imply something conceptually wrong. This is more an \u201cissue\u201d or an \u201cmiss-match\u201d when using ResNet for semantic labelling.\n* Section 4, factorizing filters. I am unsure of why you call nx1 filter asymmetric. A filter could be 1xn yet be symmetric (e.g. -2 -1 0 1 2). Why not simply call them rectangular filters ?\n* Section 4, factorizing filters. Why would this change increase the variety ? I would have expected the opposite.\n* Section 4, regularization. Define \u201cmuch better\u201d.\n* Section 5.1; \u201c640x360 is adequate for practical applications\u201d; for _some_ applications.\n* Section 5.2, \u201cvery quickly\u201d is vague and depends on the reader expectations, please be quantitative.\n* Section 5.2, Haver -> have\n* Section 5.2, in this work -> In this work\n* Section 5.2, unclear what you use the class weighting for. Is this for class balancing ?\n* Section 5.2, Cityscapes was -> Cityscapes is\n* Section 5.2, weighted by the average -> is each instance weighted relative the average object size.\n* Section 5.2, fastest model in the Cityscapes -> fastest model in the public Cityscapes","model":"human","source":"peerread","label":0,"id":4930}
{"text":"This paper aims at designing a real-time semantic segmentation network. The proposed approach has an encoder-decoder architecture with many pre-existing techniques to improvement the performance and speed. \n\nMy concern is that the most of design choices are pretty ad-hoc and there is a lack of ablation study to validate each choice. \n\nMoreover, most of the components are not new to the community (indexed pooling, dilated convolution, PReLu, steerable convolution, spatial dropout). The so-called 'early down-sampling' or 'decoder size' are also just very straightforward trade-off between speed and performance through reducing the size\/depth of the layers. \n\nThe performance and inference comparison is only conducted against a rather weak baseline, SegNet, which also makes the paper less convincing. On the public benchmark the proposed model does not achieve comparable results against state-of-the-art. As some other reviewer raised, there are some stronger model that has similar efficiency compared with SegNet.\n\nThe speed-up improvement is good yet reasonable given all the components used. However, we also did see a big sacrifice in performance on some benchmarks, which makes all these tricks less promising. \n\nThe only fact I found impressive is that the model size is 0.7MB, which is of good practical use and helpful to dump on mobile devices. However, there is NO analysis over how is the trade-off between the model size and the performance, and what design would result how much reduction in model size. I did not find the memory consumption report for the inference stage, which are perhaps even more crucial for embedded systems. \n\nPerhaps this paper does have a practical value for practical segmentation network design on embedding systems. But I do not believe the paper brings insightful ideas that are worthy to be discussed in ICLR, either from the perspective of model compression or semantic segmentation. ","model":"human","source":"peerread","label":0,"id":4931}
{"text":"This paper describes a fast image semantic segmentation network.  Many different techniques are combined to create a system much faster than the baseline SegNet approach, with accuracy comparable or somewhat worse in most of three datasets evaluated.\n\nThe choices and techniques used to achieve these speed optimizations are enumerated and described along with intuitions behind them.  However, this section lacks measurements and experimental results showing the effects of these choices.  To me, that would have been a key component to the paper.  As it stands now, we only get to see final evaluation numbers, which appear to describe a speed\/accuracy tradeoff with little insight into the pieces sum to get there.\n\nIn addition, I feel there could be a more thorough comparison with different existing systems.  Only SegNet is shown in comparison tables, even though many current systems are outlined in the related work.  Additional datasets such as Pascal or COCO may be interesting here as well, perhaps with a larger version of the ENet model.\n\nThe system looks to be fast, with decent accuracy on the majority of benchmarks described.  However, as a practical implementation paper, I feel it needs to more thoroughly demonstrate the effects of each component, as well as possibly some of the sizing\/tuning, in order to provide a more robust picture.\n","model":"human","source":"peerread","label":0,"id":4932}
{"text":"I have tried the open source implementation for different tasks these several months. It's fast, reasonably accurate and useful for prototyping.\n\nHowever I see no technical quality improvements against its NIPS submission version.\n\nI am really wondering which design choice is the most dominant one. \n\nWondering what I should do if I want to design a network as efficient as this one.","model":"human","source":"peerread","label":0,"id":4933}
{"text":"Interesting work and especially relevant going forward with the plethora of mobile devices. It would be especially interesting to see the time comparisons\non a mobile device using any of the currently available mobile frameworks. For example iOS has CNN support using Metal. Here is sample code [bit.ly\/2fQQcrX]\nthat specifically runs VGG on an iPhone (not in fully convolutional format) but which you could probably modify easily to match your architecture.\nAlso, have you tried training the same architecture on the Pascal VOC segmentation challenge? SegNet has a mean IU of 59.9 according to the VOC benchmark [bit.ly\/2g9Mpa3]. What does ENet achieve? ","model":"human","source":"peerread","label":0,"id":4934}
{"text":"This paper makes three main methodological contributions:\n - definition of Neural Feature (NF) as the pixel average of the top N images that highly activation a neuron\n - ranking of neurons based on color selectivity\n - ranking of neurons based on class selectivity\n\nThe main weaknesses of the paper are that none of the methodological contributions are very significant, and no singularly significant result arises from the application of the methods.\n\nHowever, the main strengths of the paper are its assortment of moderately-sized interesting conclusions about the basic behavior of neural nets. For example, a few are:\n - \u201cIndexing on class selectivity neurons we found highly class selective neurons like digital-clock at conv2, cardoon at conv3 and ladybug at conv5, much before the fully connected layers.\u201d As far as I know, this had not been previously reported.\n - Color selective neurons are found even in higher layers. (25% color selectivity in conv5)\n - \u201cour main color axis emerge (black-white, blue-yellow, orange-cyan and cyan- magenta). Curiously, these two observations correlate with evidences in the human visual system (Shapley & Hawken (2011)).\u201d Great observation!\n\nOverall, I\u2019d recommend the paper be accepted, because although it\u2019s difficult to predict at this time, there\u2019s a fair chance that one of the \u201csmaller conclusions\u201d would turn out to be important in hindsight a few years hence.\n\n\nOther small comments:\n - The cite for \u201cLearning to generate chairs\u2026\u201d is wrong (first two authors combined resulting in a confusing cite)\n\n - What exactly is the Color Selectivity Index computing? The Opponent Color Space isn\u2019t well defined and it wasn\u2019t previously familiar to me. Intuitively it seems to be selecting for units that respond to a constant color, but the highest color selectivity NF in Fig 5 i for a unit with two colors, not one. Finally, the very last unit (lowest color selectivity) is almost the same edge pattern, but with white -> black instead of blue -> orange. Why are these considered to be so drastically different? This should probably be more clearly described.\n\n - For the sake of argument, imagine a mushroom sensitive neuron in conv5 that fires highly for mushrooms of *any* color but not for anything else. If the dataset contains only red-capped mushrooms, would the color selectivity index for this neuron be high or low? If it is high, it\u2019s somewhat misleading because the unit itself actually isn\u2019t color selective; the dataset just happens only to have red mushrooms in it. (It\u2019s a subtle point but worth considering and probably discussing in the paper)","model":"human","source":"peerread","label":0,"id":4935}
{"text":"While this is interesting work, one major concern comes from Reviewer 2 regarding the attempt of characterizing the tuning properties, which has proven useless both in neuroscience and in machine learning. Currently, no attempt so far has lived up to the promises this line of research is aiming for. In summary, this work is explorative and incremental but worthwhile. We encourage the authors to further refine their research effort and resubmit.","model":"human","source":"peerread","label":0,"id":4936}
{"text":"Hi authors,\n\nCongrats for an interesting submission! I wonder what other selectivity indices you could use beside color and class.\n\nI just noticed that your definition of Neuron Feature (weighted average image) is very close to what we use in this paper (an average image):\n\n","model":"human","source":"peerread","label":0,"id":4937}
{"text":"This paper makes three main methodological contributions:\n - definition of Neural Feature (NF) as the pixel average of the top N images that highly activation a neuron\n - ranking of neurons based on color selectivity\n - ranking of neurons based on class selectivity\n\nThe main weaknesses of the paper are that none of the methodological contributions are very significant, and no singularly significant result arises from the application of the methods.\n\nHowever, the main strengths of the paper are its assortment of moderately-sized interesting conclusions about the basic behavior of neural nets. For example, a few are:\n - \u201cIndexing on class selectivity neurons we found highly class selective neurons like digital-clock at conv2, cardoon at conv3 and ladybug at conv5, much before the fully connected layers.\u201d As far as I know, this had not been previously reported.\n - Color selective neurons are found even in higher layers. (25% color selectivity in conv5)\n - \u201cour main color axis emerge (black-white, blue-yellow, orange-cyan and cyan- magenta). Curiously, these two observations correlate with evidences in the human visual system (Shapley & Hawken (2011)).\u201d Great observation!\n\nOverall, I\u2019d recommend the paper be accepted, because although it\u2019s difficult to predict at this time, there\u2019s a fair chance that one of the \u201csmaller conclusions\u201d would turn out to be important in hindsight a few years hence.\n\n\nOther small comments:\n - The cite for \u201cLearning to generate chairs\u2026\u201d is wrong (first two authors combined resulting in a confusing cite)\n\n - What exactly is the Color Selectivity Index computing? The Opponent Color Space isn\u2019t well defined and it wasn\u2019t previously familiar to me. Intuitively it seems to be selecting for units that respond to a constant color, but the highest color selectivity NF in Fig 5 i for a unit with two colors, not one. Finally, the very last unit (lowest color selectivity) is almost the same edge pattern, but with white -> black instead of blue -> orange. Why are these considered to be so drastically different? This should probably be more clearly described.\n\n - For the sake of argument, imagine a mushroom sensitive neuron in conv5 that fires highly for mushrooms of *any* color but not for anything else. If the dataset contains only red-capped mushrooms, would the color selectivity index for this neuron be high or low? If it is high, it\u2019s somewhat misleading because the unit itself actually isn\u2019t color selective; the dataset just happens only to have red mushrooms in it. (It\u2019s a subtle point but worth considering and probably discussing in the paper)\n","model":"human","source":"peerread","label":0,"id":4938}
{"text":"The authors analyze trained neural networks by quantifying the selectivity of individual neurons in the network for a variety of specific features, including color and category.   \n\nPros:\n   * The paper is clearly written and has good figures. \n   * I think they executed their specific stated goal reasonably well technically.   E.g. the various indexes they use seem well-chosen for their purposes. \n\nCons:\n   * I must admit that I am biased against the whole enterprise of this paper.   I do not think it is well-motivated or provides any useful insight whatever.   What I view their having done is produced, and then summarized anecdotally, a catalog of piecemeal facts about a neural network without any larger reason to think these particular facts are important.  In a way, I feel like this paper suffers from the same problem that plagues a typical line of research in neurophysiology, in which a catalog of selectivity distributions of various neurons for various properties is produced -- full stop.  As if that were in and of itself important or useful information.   I do not feel that either the original neural version of that project, or this current model-based virtual electrophysiology, is that useful.   Why should we care about the distribution of color selectivities?   Why does knowing distribution as such constitute \"understanding\"?    To my mind it doesn't, at least not directly.   \n\nHere's what they could have done to make a more useful investigation:\n  \n     (a) From a neuroscience point of view, they could have compared the properties that they measure in models to the same properties as measured in neurons the real brain.   If they could show that some models are better matches on these properties to the actual neural data than others, that would be a really interesting result.   That is is to say, the two isolated catalogs of selectivities (from model neurons and real neurons)  alone seem pretty pointless.  But if the correspondence between the two catalogs was made -- both in terms of where the model neurons and the real neurons were similar, and (especially importantly) where they were different --- that would be the beginning of nontrivial understanding.   Such results would also complement a growing body of literature that attempts to link CNNs to visual brain areas.  Finding good neural data is challenging, but whatever the result, the comparison would be interesting. \n\nand\/or \n\n    (b) From an artificial intelligence point of view, they could have shown that their metrics are *prescriptive* constraints.   That is, suppose they had shown that the specific color and class selectivity indices that they compute, when imposed as a loss-function criterion on an untrained neural network, cause the network to develop useful filters and achieve significantly above-chance performance on the original task the networks were trained on.     This would be a really great result, because it would not only give us a priori reason to care about the specific property metrics they chose, but it would also help contribute to efforts to find unsupervised (or semi-supervised) learning procedures, since the metrics they compute can be estimated from comparatively small numbers of stimuli and\/or high-level semantic labels.    To put this in perspective, imagine that they had actually tested the above hypothesis and found it to be false:  that is, that their metrics, when used as loss function constraints, do not improve performance noticeably above chance performance.  What would we then make of this whole investigation?  It would then be reasonable to think that the measured properties were essentially epiphenomenal and didn't contribute at all to the power of neural networks in solving perceptual tasks.  (The same could be said about neurophysiology experiments doing the same thing.)  \n     [--> NB: I've actually tried things just like this myself over the years, and have found exactly this disappointing result.  Specifically,  I've found a number of high-level generic statistical property of DNNs that seem like they might potentially \"interesting\", e.g. because they apparently correlate with complexity or appear to illustrate difference between low, intermediate and high layers of DNNs.  Every single one of these, when imposed as optimization constraints, has basically lead nowhere on the challenging tasks (like ImageNet) that cause the DNNs to be interesting in the first place.  Basically, there is to my mind no evidence at this point that highly-summarized generic statistical distributions of selectivities, like those illustrated here, place any interesting constraints on filter weights at all.   Of course, I haven't tried the specific properties the authors highlight in these papers, so maybe there's something important there.]\n\nI know that both of these asks are pretty hard, but I just don't know what else to say -- this work otherwise seems like a step backwards for what the community ought to be spending its time on. \n ","model":"human","source":"peerread","label":0,"id":4939}
{"text":"This paper attempts to understand and visualize what deep nets are representing as one ascends from low levels to high levels of the network.  As has been shown previously, lower levels are more local image feature based, whereas higher levels correspond to abstract properties such as object identity.  In semantic space, we find higher level nodes to be more semantically selective, whereas low level nodes are more diffuse.\n\nThis seems like a good attempt to tease apart deep net representations.  Perhaps the most important finding is that color figures prominently into all levels of the network, and that performance on gray scale images is significantly diminished.  The new NF measure proposed here is sensible, but still based on the images shown to the network.  What one really wants to know is what function these nodes are computing - i.e., out of the space of *all* possible images, which most activate a unit?  Of course this is a difficult problem, but it would be nice to see us getting closer to understanding the answer.  The color analysis here I think brings us a bit closer.  The semantic analysis is nice but I'm not sure what new insight we gain from this.  \n","model":"human","source":"peerread","label":0,"id":4940}
{"text":"We have updated the pdf of the paper after the reviewer comments, in this way:\n  - Giving more details about similarities and differences between the proposed approach and previous.\n  - Adding the clarifications required by the reviewers on Figure1\n  - Improving the global understanding by joining section 1 and 2 into a single one. ","model":"human","source":"peerread","label":0,"id":4941}
{"text":"Unfortunately, the paper is not clear enough for me to understand what is being proposed. At a high-level the authors seem to propose a generalization of the standard layered neural architecture (of which MLPs are a special case), based on arbitrary nodes which communicate via messages. The paper then goes on to show that their layer-free architecture can perform the same computation as a standard MLP. This logic appears circular. The low level details of the method are also confusing: while the authors seem to be wanting to move away from layers based on matrix-vector products, Algorithm 4 nevertheless resorts to matrix-vector products for the forward and backwards pass. Although the implementation relies on asynchronously communicating nodes, the \u201clocking\u201d nature of the computation makes the two entirely equivalent.","model":"human","source":"peerread","label":0,"id":4942}
{"text":"The reviewers were consistent in their review that they thought this was a strong rejection.\n Two of the reviewers expressed strong confidence in their reviews.\n The main arguments made by the reviewers against acceptance were:\n Lack of novelty (R2, R3)\n Lack of knowledge of literature and development history; particularly with respect to biological inspiration of ANNs (R3)\n Inappropriate baseline comparison (R2)\n Not clear (R1)\n \n The authors did not provide a response to the official reviews. Therefore I have decided to follow the consensus towards rejection.","model":"human","source":"peerread","label":0,"id":4943}
{"text":"The multiagent system is proposed as a generalization of neural network. The proposed system can be used with less restrictive network structures more efficiently by computing only those necessary computations in the graph. Unfortunately, I don't find the proposed system different from the framework of artificial neural network. Although for today's neural network structures are designed to have a lot of matrix-matrix multiplications, but it is not limited to have such architecture. In other words, the proposed multiagent system can be framed in the artificial neural network with more complicated layer\/connectivity structures while considering each neuron as layer. The computation efficiency is argued among different sparsely connected denoising autoencoder in multiagent system framework only but the baseline comparison should be against the fully-connected neural network that employs matrix-matrix multiplication.","model":"human","source":"peerread","label":0,"id":4944}
{"text":"The paper reframes feed forward neural networks as a multi-agent system.\n\nIt seems to start from the wrong premise that multi-layer neural networks were created expressed as full matrix multiplications. This ignores the decades-long history of development of artificial neural networks, inspired by biological neurons, which thus started from units with arbitrarily sparse connectivity envisioned as computing in parallel. The matrix formulation is primarily a notational convenience; note also that when working with sparse matrix operations (or convolutions) zeros are neither stored not multiplied by.\n\nBesides the change in terminology, essentially renaming neurons agents, I find the paper brings nothing new and interesting to the table.\n\nPulling in useful insights from a different communitiy such as multi-agent systems would be most welcome. But for this to be compelling, it would have to be largely unheard-of elements in neural net research, with clear supporting empirical evidence that they significantly improve accuracy or efficiency. This is not achieved in the present paper.","model":"human","source":"peerread","label":0,"id":4945}
{"text":"In this paper, the author proposed an approach for feature combination of two embeddings v1 and v2. This is done by first computing the pairwise combinations of the elements of v1 and v2 (with complicated nonlinearity), and then pick the K-Max as the output vector. For triple (or higher-order) combinations, two (or more) consecutive pairwise combinations are performed to yield the final representations. It seems that the approach is not directly related to categorical data, and can be applied to any embeddings (even if they are not one-hot). So is there any motivation that brings about this particular approach? What is the connection? \n\nThere are many papers with similar ideas. CCPM (A convolutional click prediction model) that the authors have compared against, also proposes very similar network structure (conv + K-max + conv + K-max). In the paper, the author does not mention their conceptual similarity and difference versus CCPM. Compact Bilinear Pooling,","model":"human","source":"peerread","label":0,"id":4946}
{"text":"There is consensus among the three reviewers that (1) the originality of the proposed approach is limited and (2) the experimental evaluation is too limited in that it lacks strong baseline models as well as an ablation study that explores the different aspects of the proposed model.","model":"human","source":"peerread","label":0,"id":4947}
{"text":"A method for click prediction is presented. Inputs are a categorical variables and output is the click-through-rate. The categorical input data is embedded into a feature vector using a discriminative scheme that tries to predict whether a sample is fake or not. The embedding vector is passed through a series of SUM\/MULT gates and K-most important interactions are identified (K-max pooling). This process is repeated multiple times (i.e. multiple layers) and the final feature is passed into a fully connected layer to output the click prediction rate. \n\nAuthors claim:\n(1)\tUse of gates and K-max pooling allow modeling of interactions that lead to state of art results. \n(2)\tIt is not straightforward to apply ideas in papers like word2vec to obtain feature embeddings and consequently they use the idea of discriminating between fake and true samples for feature learning. \n\nTheoretically convolutions can act as \u201csum\u201d gates between pairs of input dimensions. Authors make these interactions explicit (i.e. imposed structure) by using gates. Now, the merit of the proposed method can be tested if a network using gates outperforms a network without gates. This baseline is critically missing \u2013 i.e. Embedding Vector followed by a series of convolution\/pooling layers. \n\nAnother related issue is that I am not sure if the number of parameters in the proposed model and the baseline models is similar or not. For instance \u2013 what is the total number of parameters in the CCPM model v\/s the proposed model? \n\nOverall, there is no new idea in the paper. This by itself is not grounds for rejection if the paper outperforms established baselines.  However, such comparison is weak and I encourage authors to perform these comparisons. \n\n\n","model":"human","source":"peerread","label":0,"id":4948}
{"text":"The paper proposes a way to learn continuous features for input data which consists of multiple categorical data. The idea is to embed each category in a learnable low dimensional continuous space, explicitly compute the pair-wise interaction among different categories in a given input sample (which is achieved by either taking a component-wise dot product or component-wise addition), perform k-max pooling to select a subset of the most informative interactions, and repeat the process some number of times, until you get the final feature vector of the given input. This feature vector is then used as input to a classifier\/regressor to accomplish the final task. The embeddings of the categories are learnt in the usual way. In the experiment section, the authors show on a synthetic dataset that their procedure is indeed able to select the relevant interactions in the data. On one real world dataset (iPinYou) the model seems to outperform a couple of simple baselines. \n\nMy major concern with this paper is that their's nothing new in it. The idea of embedding the categorical data having mixed categories has already been handled in the past literature, where essentially one learns a separate lookup table for each class of categories: an input is represented by concatenation of the embeddings from these lookup table, and a non-linear function (a deep network) is plugged on top to get the features of the input. The only rather marginal contribution is the explicit modeling of the interactions among categories in equations 2\/3\/4\/5. Other than that there's nothing else in the paper. \n\nNot only that, I feel that these interactions can (and should) automatically be learned by plugging in a deep convolutional network on top of the embeddings of the input. So I'm not sure how useful the contribution is. \n\nThe experimental section is rather weak. They authors test their method on a single real world data set against a couple of rather weak baselines. I would have much preferred for them to evaluate against numerous models proposed in the literature which handle similar problems, including wsabie. \n\nWhile the authors argued in their response that wsabie was not suited for their problem, i strongly disagree with that claim. While the original wsabie paper showed experiments using images as inputs, their training methodology can easily be extended to other types of data sets, including categorical data. For instance, I conjecture that the model i proposed above (embed all the categorical inputs, concatenate the embeddings, plug a deep conv-net on top and train using some margin loss) will perform as well if not better than the hand coded interaction model proposed in this paper. Of course I could be wrong, but it would be far more convincing if their model was tested against such baselines. ","model":"human","source":"peerread","label":0,"id":4949}
{"text":"In this paper, the author proposed an approach for feature combination of two embeddings v1 and v2. This is done by first computing the pairwise combinations of the elements of v1 and v2 (with complicated nonlinearity), and then pick the K-Max as the output vector. For triple (or higher-order) combinations, two (or more) consecutive pairwise combinations are performed to yield the final representations. It seems that the approach is not directly related to categorical data, and can be applied to any embeddings (even if they are not one-hot). So is there any motivation that brings about this particular approach? What is the connection? \n\nThere are many papers with similar ideas. CCPM (A convolutional click prediction model) that the authors have compared against, also proposes very similar network structure (conv + K-max + conv + K-max). In the paper, the author does not mention their conceptual similarity and difference versus CCPM. Compact Bilinear Pooling, ","model":"human","source":"peerread","label":0,"id":4950}
{"text":"A new sparse coding model is introduced that learns features jointly with their transformations. It is found that inference over per-image transformation variables is hard, so the authors suggest tying these variables across all data points, turning them into global parameters, and using multiple transformations for each feature. Furthermore, it is suggested to use a tree of transformations, where each path down the tree generates a feature by multiplying the root feature by the transformations associated with the edges. The one-layer tree model achieves similar reconstruction error as traditional sparse coding, while using fewer parameters.\n\nThis is a nice addition to the literature on sparse coding and the literature on learning transformation models. The authors identify and deal with a difficult inference problem that can occur in transformation models. That said, I am skeptical about the usefulness of the general approach. The authors take it as a given that \u201clearning sparse features and transformations jointly\u201d is an important goal in itself, but this is never really argued or demonstrated with experiments. It doesn\u2019t seem like this method enables new applications, extends our understanding of learning what\/where pathways in the brain, or improve our ability to model natural images.\n\nThe authors claim that the model extracts pose information, but although the model explicitly captures the transformation that relates different features in a tree, at test time, inference is only performed over the (sparse) coefficient associated with each (feature, transformation) combination, just like in sparse coding. It is not clear what we gain by knowing that each coefficient is associated with a transformation, especially since there are many models that do this general \u201cwhat \/ where\u201d split.\n\nIt would be good to check that the x_{v->b} actually change significantly from their initialization values. The loss surface still looks pretty bad even for tied transformations, so they may actually not move much. Does the proposed model work better according to some measure, compared to a model where x_{v->b} are fixed and chosen from some reasonable range of parameter values (either randomly or spaced evenly)?\n\nOne of the conceptually interesting aspects of the paper is the idea of a tree of transformations, but the advantage of deeper trees is never demonstrated convincingly. It looks like the authors have only just gotten this approach to work on toy data with vertical and horizontal bars.\n\nFinally, it is not clear how the method could be extended to have multiple layers. The transformation operators T can be defined in the first layer because they act on the input space, but the same cannot be done in the learned feature space. It is also not clear how the pose information should be further processed in a hierarchical manner, or how learning in a deep version should work.\n\nIn summary, I do not recommend this paper for publication, because it is not clear what problem is being solved, the method is only moderately novel and the novel aspects are not convincingly shown to be beneficial.","model":"human","source":"peerread","label":0,"id":4951}
{"text":"This paper learns affine transformations from images jointly with object features. The motivation is interesting and sound, but the experiments fail to deliver and demonstrate the validity of the claims advanced -- they are restricted to toy settings. What is presented as logical next steps for this work (extending to higher scale multilayer convolutional frameworks, beyond toy settings) seems necessary for the paper to hold its own and deliver the promised insights.","model":"human","source":"peerread","label":0,"id":4952}
{"text":"This paper trains a generative model of image patches, where dictionary elements undergo gated linear transformations before being combined. The transformations are motivated in terms of Lie group operators, though in practice they are a set of fixed linear transformations. This is motivated strongly in terms of learning a hierarchy of transformations, though only one layer is used in the experiments (except for a toy case in the appendix).\n\nI like the motivation for this algorithm. The realization seems very similar to a group or block sparse coding implementation. I was disappointed by the restriction to linear transformations. The experiments were all toy cases, demonstrating that the algorithm can learn groups of Gabor- or center surround-like features. They would have been somewhat underpowered five years ago, and seemed extremely small by today's standards.\n\nSpecific comments:\n\nBased on common practices in ML literature, I have a strong bias to think of $x$ as inputs and $w$ as network weights. Latent variables are often $z$ or $a$. Depending on your target audience, I would suggest permuting your choice of symbols so the reader can more quickly interpret your model.\n\nnit: number all equations for easier reference\n\nsec 2.2 -- It's weird that the transformation is fixed, but is still written as a function of x.\n\nsec 2.3 -- The updated text here confuses me actually. I had thought that you were using a fixed set of linear transformations, and were motivating in terms of Lie groups, but were not actually taking matrix exponentials in your algorithm. The equations in the second half of this section suggest you are working with matrix exponentials though. I'm not sure which direction I'm confused in, but probably good to clarify the text either way.\n\nBTW -- there's another possible solution to the local minima difficulty, which is the one used in Sohl-Dickstein, 2010. There, they introduce blurring operators matched to each transformation operator, and gradient descent can escape local minima by detouring through coarser (more blurred) scales.\n\nsec 3.2 -- I believe by degrees of freedom you mean the number of model parameters, not the number of latent coefficients that must be inferred? Should make this more clear. Is it more appropriate to compare reconstruction error while matching number of model parameters, or number of latent variables?\n\nI wonder if a convolutional version of this algorithm would be practical \/ would make it more suited as a generative model of whole images.\n\n====\npost rebuttal update\n\nThank you for taking the time to write the rebuttal! I have read it, but it did not significantly effect my rating.","model":"human","source":"peerread","label":0,"id":4953}
{"text":"This paper proposes an approach to unsupervised learning based on a modification to sparse coding that allows for explicit modeling of transformations (such as shift, rotation, etc.), as opposed to simple pooling as is typically done in convnets.  Results are shown for training on natural images, demonstrating that the algorithm learns about features and their transformations in the data.  A comparison to traditional sparse coding shows that it represents images with fewer degrees of freedom.\n\nThis seems like a good and interesting approach, but the work seems like its still in its early formative stages rather than a complete work with a compelling punch line.  For example one of the motivations is that you'd like to represent pose along with the identity of an object.  While this work seems well on its way to that goal, it doesn't quite get there - it leaves a lot of dots still to be connected.  \n\nAlso there are a number of things that aren't clear in the paper:\n\no The central idea of the paper it seems is the use of a transformational sparse coding tree to make tractable the inference of the Lie group parameters x_k.  But how exactly this is done is not at all clear.  For example, the sentence: \"The main idea is to gradually marginalize over an increasing range of transformations,\" is suggestive but not clear.  This needs to be much better defined.  What do you mean by marginalization in this context?  \n\n o The connection between the Lie group operator and the tree leaves and weights w_b is not at all clear.   The learning rule spells out the gradient for the Lie group operator, but how this is used to learn the leaves of the tree is not clear.  A lot is left to the imagination here.  This is especially confusing because although the Lie group operator is introduced earlier, it is then stated that its not tractable for inference because there are too many local minima, and this motivates the tree approach instead.  So its not clear why you are learning the Lie group operator.\n\n o It is stated that \"Averaging over many data points, smoothens the surface of the error function.\"  I don't understand why you would average over many data points.  It seems each would have its own transformation, no?\n\n o What data do you train on?  How is it generated?  Do you generate patches with known transformations and then show that you can recover them?  Please explain.\n\nThe results shown in Figure 4 look very interesting, but given the lack of clarity in the above, difficult to interpret and understand what this means, and its significance.\n\nI would encourage the authors to rewrite the paper more clearly and also to put more work into further developing these ideas, which seem very promising.\n\n\n\n","model":"human","source":"peerread","label":0,"id":4954}
{"text":"A new sparse coding model is introduced that learns features jointly with their transformations. It is found that inference over per-image transformation variables is hard, so the authors suggest tying these variables across all data points, turning them into global parameters, and using multiple transformations for each feature. Furthermore, it is suggested to use a tree of transformations, where each path down the tree generates a feature by multiplying the root feature by the transformations associated with the edges. The one-layer tree model achieves similar reconstruction error as traditional sparse coding, while using fewer parameters.\n\nThis is a nice addition to the literature on sparse coding and the literature on learning transformation models. The authors identify and deal with a difficult inference problem that can occur in transformation models. That said, I am skeptical about the usefulness of the general approach. The authors take it as a given that \u201clearning sparse features and transformations jointly\u201d is an important goal in itself, but this is never really argued or demonstrated with experiments. It doesn\u2019t seem like this method enables new applications, extends our understanding of learning what\/where pathways in the brain, or improve our ability to model natural images.\n\nThe authors claim that the model extracts pose information, but although the model explicitly captures the transformation that relates different features in a tree, at test time, inference is only performed over the (sparse) coefficient associated with each (feature, transformation) combination, just like in sparse coding. It is not clear what we gain by knowing that each coefficient is associated with a transformation, especially since there are many models that do this general \u201cwhat \/ where\u201d split.\n\nIt would be good to check that the x_{v->b} actually change significantly from their initialization values. The loss surface still looks pretty bad even for tied transformations, so they may actually not move much. Does the proposed model work better according to some measure, compared to a model where x_{v->b} are fixed and chosen from some reasonable range of parameter values (either randomly or spaced evenly)?\n\nOne of the conceptually interesting aspects of the paper is the idea of a tree of transformations, but the advantage of deeper trees is never demonstrated convincingly. It looks like the authors have only just gotten this approach to work on toy data with vertical and horizontal bars.\n\nFinally, it is not clear how the method could be extended to have multiple layers. The transformation operators T can be defined in the first layer because they act on the input space, but the same cannot be done in the learned feature space. It is also not clear how the pose information should be further processed in a hierarchical manner, or how learning in a deep version should work.\n\nIn summary, I do not recommend this paper for publication, because it is not clear what problem is being solved, the method is only moderately novel and the novel aspects are not convincingly shown to be beneficial. \n","model":"human","source":"peerread","label":0,"id":4955}
{"text":"Revision #1\n\nThank you for your comments and suggestions. We uploaded a new version of the paper to address them:\n\n1) We added all relevant references, corrected our claims and updated our \"Relevant Work\" section.\n\n2) We added a figure that shows the effects of each transformation.\n\n3) We added a formal definition of deeper trees and added a small example that shows learned structure.\n\n4) We added more figures of learned features.\n\n5) We changed our regularization. Instead of regularizing derivative features, we constrain root features to be of unit\nnorm and penalize transformations that change the magnitude. Inter- and intra- tree regularization is no longer required\nand we can use the feature-sign algorithm to infer the weights.\n\n6) We removed our parameter distance and magnitude figures, since these metrics depend heavily on the initialization approach of choice\nand hence are not very informative.\n\nAll results and figures are current.","model":"human","source":"peerread","label":0,"id":4956}
{"text":"This paper explores the use of Open Bigrams as a target representation of words, for application to handwriting image recognition. \n\nPros:\n- The use of OBs is novel and interesting.\n- Clearly written and explained.\n\nCons:\n- No comparison to previous state of the art, only with author-generated results. \n- More ablation studies needed -- i.e. fill in Table3 with rnn0,1 rnn0,1,2 rnn0,1' etc etc. It is not clear where the performance is coming from, as it seems that it is single character modelling (0) and word endings (') that are actually beneficial.\n- While the use of Open bigrams is novel, there are works which use bag of bigrams and ngrams as models which are not really compared to or explored. E.g.","model":"human","source":"peerread","label":0,"id":4957}
{"text":"There is consistent agreement towards the originality of this work and that the topic here is \"interesting\". Additionally there is consensus that the work is \"clearly written\", and (excepting questions of the word \"cortical\") all would be primed to accept this style of work. \n \n However there is a shared concern about the quality and potential impact of the work, in particularly in terms of the validity of empirical evaluations. Reviewers are generally not inclined to believe that the current empirical evidence validates the conclusions of the word. Suggestions are to: make greater use of a language model, compare to external baselines, or remove the handwriting aspects.","model":"human","source":"peerread","label":0,"id":4958}
{"text":"This paper explores the use of Open Bigrams as a target representation of words, for application to handwriting image recognition. \n\nPros:\n- The use of OBs is novel and interesting.\n- Clearly written and explained.\n\nCons:\n- No comparison to previous state of the art, only with author-generated results. \n- More ablation studies needed -- i.e. fill in Table3 with rnn0,1 rnn0,1,2 rnn0,1' etc etc. It is not clear where the performance is coming from, as it seems that it is single character modelling (0) and word endings (') that are actually beneficial.\n- While the use of Open bigrams is novel, there are works which use bag of bigrams and ngrams as models which are not really compared to or explored. E.g. ","model":"human","source":"peerread","label":0,"id":4959}
{"text":"This paper uses an LSTM model to predict what it calls \"open bigrams\" (bigrams of characters that may or may not have letters inbetween) from handwriting data. These open bigrams are subsequently used to predict the written word in a decoding step. The experiments indicate that the system does slightly better than a baseline model that uses Viterbi decoding. I have some major concerns about this paper:\n\n- I find the \"cortical inspired\" claim troublesome. If anything, it is psychology\/cognitive science inspired, in the sense that open bigrams appear to help for word recognition (Touzet et al. 2014). But the implied cortical characteristics, implicitly referred to e.g. by pointing to analogies between deep neural nets for object recognition and in that case the visual cortex, is unfounded. Is there any direct evidence from neuroscience that open-bigrams constitute a wholly separate layer in the cortex for a handwriting recognition task? Dehaene's work is a proposal, so you'll need to describe more \"findings in cognitive neurosciences [sic] research on reading\" (p. 8) to substantiate those claims. I am further worried by the fact that the authors seem to think that \"deep neural networks are based on a series of about five pairs of neurons [sic] layers\". Unless I misunderstand something, you are specifically referring to Krizhevsky's AlexNet here (which you should probably have cited there)? I hope you don't mean to imply that all deep neural nets need five layers. It is also not true that ten is \"quite close to the number of layers of an efficient deep NN\" -- what network? what task? etc.\n\n- The model is not clearly explained. There is a short paragraph in Appendix A.3. that roughly describes the setup, but this does not include e.g. the objective function, or answer why the network output is only considered each two consecutive time steps, rather than at each time step (or so it seems?). This is probably because the paper argues that it \"is focused on the decoder\" (p. 6), rather than on the whole problem. I find this problematic, because in that case we're effectively measuring how easy it is to reconstruct a word from its open bigrams, which has very little to do with handwriting recognition (it could have been evaluated on any text corpus). In fact, as the example on page 4 shows, handwriting is not necessary to illustrate the open bigram hypothesis. Which leads me to wonder why these particular tasks were chosen, if we are only interested in the decoding mechanism?\n\n- The comparison is not really fair. The Viterbi decoder only has access to unigrams, as far as I can tell. The only model that does better than that baseline has access to a lot more information, and does not do that much better. Did the Viterbi model have access to the word boundary information (at one point rather confusingly called \"extremities\") that pushed the open bigram model over the edge in terms of performance? Why is there no comparison to e.g. rnn_0,1' (unigram+bigram+boundary markers)? The dataset also appears to be biased in favor of the proposed approach (longer words, only ). I am not convinced that this paper really shows that open bigrams help.\n\nI very much like the idea of the paper, but I am simply not convinced by its claims.\n\nMinor points:\n- There are quite a few typos. Just a sample: \"independant\" (Fig.1), \"we evaluate an handwritten\", \", hand written words [..], an the results\", \"their approach include\", \"the letter bigrams of a word w is\", \"for the two considered database\"\n- Wouldn't it be easy to add how many times a bigram occurs, which would improve the decoding process? You can just normalize over the full counts instead of the binary occurrence counts.\n- The results in Table 5 are the same (but different precision) as the results in Table 2, except that edit distance and SER are added, this is confusing.","model":"human","source":"peerread","label":0,"id":4960}
{"text":"This submission investigates the usability of cortical-inspired distant bigram representations for handwritten word recognition. Instead of generating neural network based posterior features for character (optionally in local context), sets posterior for character bigrams of different length are used to represent words.  The aim here is to investigate the viability of this approach and to compare to the standard approach.\n\nOverall, the submission is well written, although information is missing w.r.t. to the comparison between the proposed approach and the standard approach, see below.\n\nIt would be desirable to see the model complexity of all the different models used here, i.e. the number of parameters used.\n\nLanguage models are not used here. Since the different models utilize different levels of context, language models can be expected to have a different effect on the different approaches. Therefore I suggest to include the use of language models into the evaluation.\n\nFor your comparative experiments you use only 70% of the data by choosing longer words only. On the other hand, it is well known that the shorter words are more prone to result in misrecognitions. The question remains, if this choice is advantageous for one of the tasks, or not - corresponding quantitative results should be provided to be able to better evaluate the effect of using this constrained corpus. Without clarification of this I would not readily agree that the error rates are competitive or better than the standard approach, as stated at the end of Sec. 5.\n\nI do see the motivation for introducing open-bigrams in an unordered way due to the corresponding evidence from cognitive research. However, decision theoretically I wonder, why the order should be given up, if the underlying sequential classification problem clearly is of a monotonous nature. It would be interesting to see an experiment, where only the use of the order is varied, to differentiate the effect of the order from the effect of other aspects of the approach.\n\nEnd of page 1: \"whole language method\" - please explain what is meant by this.\n\nPage 6: define your notation for rnn_d(x,t).\n\nThe number of target for the RNNs modeling order 0 (unigrams effectively) and the RNNs modeling order 1 and larger are very much different.  Therefore the precision and recall numbers in Table 2 do not seem to be readily comparable between order 0 and orders >=1. At least, the column for order 0 should be visually separated to highlight this.\n\n\nMinor comments: a spell check is recommended\np. 2: state-of-art -> state-of-the-art\np. 2: predict character sequence -> predict a character sequence\np. 3, top: Their approach include -> Their approach includes\np. 3, top: an handwritten -> a handwritten\np. 3, bottom: consituent -> constituent\np. 4, top: in classical approach -> in the classical approach\np. 4, top: transformed in a vector -> transformed into a vector\np. 5: were build -> were built\nReferences: first authors name written wrongly: Thodore Bluche -> Theodore Bluche\n","model":"human","source":"peerread","label":0,"id":4961}
{"text":"Strengths\n\n- interesting to explore the connection between ReLU DNN and simplified SFNN\n- small task (MNIST)  is used to demonstrate the usefulness of the proposed training methods experimentally\n- the proposed, multi-stage training methods are simple to implement (despite lacking theoretical rigor)\n\n\nWeaknesses\n\n-no results are reported on real tasks with large training set\n\n-not clear exploration on the scalability of the learning methods when training data becomes larger\n\n-when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in \u201cPattern Recognition and Computer Vision\u201d, November 2015, Download PDF). Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as \u201cexplaining away\u201d.\n\n-would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers","model":"human","source":"peerread","label":0,"id":4962}
{"text":"No reviewer was willing to champion the paper and the authors did not adequately address reviewer comments in a revision. Recommend rejection.","model":"human","source":"peerread","label":0,"id":4963}
{"text":"Strengths\n\n- interesting to explore the connection between ReLU DNN and simplified SFNN\n- small task (MNIST)  is used to demonstrate the usefulness of the proposed training methods experimentally\n- the proposed, multi-stage training methods are simple to implement (despite lacking theoretical rigor)\n\n\nWeaknesses\n\n-no results are reported on real tasks with large training set\n\n-not clear exploration on the scalability of the learning methods when training data becomes larger\n\n-when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in \u201cPattern Recognition and Computer Vision\u201d, November 2015, Download PDF). Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as \u201cexplaining away\u201d.\n\n-would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers\n","model":"human","source":"peerread","label":0,"id":4964}
{"text":"This paper builds connections between DNN, simplified stochastic neural network (SFNN) and SFNN and proposes to use DNN as the initialization model for simplified SFNN. The authors evaluated their model on several small tasks with positive results.\n\nThe connection between different models is interesting. I think the connection between sigmoid DNN and Simplified SFNN is the same as mean-field approximation that has been known for decades. However, the connection between ReLU DNN and simplified SFNN is novel.\n\nMy main concern is whether the proposed approach is useful when attacking real tasks with large training set. For tasks with small training set I can see that stochastic units would help generalize well.","model":"human","source":"peerread","label":0,"id":4965}
{"text":"Update: Because no revision of the paper has been provided by the authors, I am reducing my rating to \"marginally below acceptance\".\n\n----------\n\nThis paper addresses the problem of training stochastic feedforward neural networks.  It proposes to transfer weights from a deterministic deep neural network trained using standard procedures (including techniques such as dropout and batch normalization) to a stochastic network having the same topology.  The initial mechanism described for performing the transfer involves a rescaling of unit inputs and layer weights, and appropriate specification of the stochastic latent units if the DNN used for pretraining employs ReLU nonlinearities.  Initial experiments on MNIST classification and a toy generative task with a multimodal target distribution show that the simple transfer process works well if the DNN used for pretraining uses sigmoid nonlinearities, but not if the pretraining DNN uses ReLUs.  To tackle this problem, the paper introduces the \"simplified stochastic feedforward neural network,\" in which every stochastic layer is followed by a layer that takes an expectation over samples from its input, thus limiting the propagation of stochasticity in the network.  A modified process for transferring weights from a pretraining DNN to the simplified SFNN is described and justified.  The training process then occurs in three steps:  (1) pretrain a DNN, (2) transfer weights from the DNN to a simplified SFNN and continue training, and (3) optionally transfer the weights to a full SFNN and continue training or transfer them to a deterministic model (called DNN*) and continue training.  The third step can be skipped and the simplified SFNN may also be used directly as an inference model.  Experimental results on MNIST classification show that the use of simplified SFNN training can improve a deterministic DNN* model over a DNN baseline trained with batch normalization and dropout.  Experiments on two generative tasks (MNIST-half and the Toronto Faces Database) show that the proposed pretraining process improves test set negative log-likelihoods.  Finally, experiments on CIFAR-10, CIFAR-100, and SVHN with the LeNet-5, network-in-network, and wide residual network architectures show that use of a stochastic training step can improve performance of a deterministic (DNN*) model.\n\nIt is a bit confusing to refer to \"multi-modal\" tasks, when what is meant is \"generative tasks with a multimodal target distribution\" because \"multi-modal\" task can also refer to a learning task that crosses sensory modalities such as audio-visual speech recognition, text-based image retrieval, or image captioning.  I recommend that you use the more precise term (\"generative tasks with a multimodal target distribution\") early in the introduction and then say that you will refer to such tasks as \"multi-modal tasks\" in the rest of the paper for the sake of brevity.\n\nThe paper would be easier to read if \"SFNN\" were not used to refer to both the singular (\"stochastic feedforward neural network\") and plural (\"stochastic feedforward neural networks\") cases.  When the plural is meant, write \"SFNNs\".\n\nIn Table 1, why does the 3 hidden layer SFNN initialized from a ReLU DNN have so much worse of a test NLL than the 2 hidden layer SFNN initialized from a ReLU DNN?\n\nThe notation that uses superscripts to indicate layer indexes is confusing.  The reader naturally parses N\u00b2 as \"N squared\" and not as \"the number of units in the second layer.\"\n\nWhen you transfer weights back from the simplified SFNN to the DNN* model, do you need to perform some sort of rescaling that undoes the operations in Equation (8) in the paper?\n\nWhat does NCSFNN stand for in the supplementary material?\n\nPros\n+ The proposed model is easy to implement and apply to other tasks.\n+ The MNIST results showing that the stochastic model training can produce a deterministic model (called DNN* in the paper) that generalizes better than a DNN trained with batch normalization and dropout is quite exciting.\n\nCons\n- For the reasons outlined above, the paper is at times a bit hard to follow.\n- The results CIFAR-10, CIFAR-100, and SVHN would be more convincing if the baselines used dropout and batch normalization.  While this is shown on MINST, demonstration of a similar result on a more challenging task would strengthen the paper.\n\nMinor issues\n\nIt has been believed that stochastic \u2192 It is believed that stochastic\n\nunderlying these successes is on the efficient training methods \u2192 underlying these successes is efficient training methods\n\nnecessary in order to model complex stochastic natures in many real-world tasks \u2192 necessary in to model the complex stochastic nature of many real-world tasks\n\nstructured prediction, image generation and memory networks : memory networks are models, not tasks.\n\nFurthermore, it has been believed that SFNN \u2192 Furthermore, it is believed that SFNN\n\nusing backpropagation under the variational techniques and the reparameterization tricks  \u2192 using backpropagation with variational techniques and reparameterization tricks\n\nThere have been several efforts developing efficient training methods \u2192 There have been several efforts toward developing efficient training methods\n\nHowever, training SFNN is still significantly slower than doing DNN \u2192 However, training a SFNN is still significantly slower than training a DNN\n\ne.g., most prior works on this line have considered a \u2192 consequently most prior works in this area have considered a\n\nInstead of training SFNN directly \u2192 Instead of training a SFNN directly\n\nwhether pre-trained parameters of DNN \u2192 whether pre-trained parameters from a DNN\n\nwith further fine-tuning of light cost \u2192 with further low-cost fine-tuning\n\nrecent advances in DNN on its design and training \u2192 recent advances in DNN design and training\n\nit is rather believed that transferring parameters \u2192  it is believed that transferring parameters\n\nbut the opposite direction is unlikely possible \u2192 but the opposite is unlikely\n\nTo address the issues, we propose \u2192 To address these issues, we propose\n\nwhich intermediates between SFNN and DNN, \u2192 which is intermediate between SFNN and DNN,\n\nin forward pass and computing gradients in backward pass \u2192 in the forward pass and computing gradients in the backward pass\n\nin order to handle the issue in forward pass \u2192  in order to handle the issue in the forward pass\n\nNeal (1990) proposed a Gibbs sampling \u2192 Neal (1990) proposed Gibbs sampling\n\nfor making DNN and SFNN are equivalent \u2192 for making the DNN and SFNN equivalent\n\nin the case when DNN uses the unbounded ReLU \u2192 in the case when the DNN uses the unbounded ReLU\n\nare of ReLU-DNN type due to the gradient vanishing problem \u2192 are of the ReLU-DNN type because they mitigate the gradient vanishing problem\n\nmultiple modes in outupt space y \u2192 multiple modes in output space y\n\nThe only first hidden layer of DNN \u2192 Only the first hidden layer of the DNN\n\nis replaced by stochastic one, \u2192 is replaced by a stochastic layer,\n\nthe former significantly outperforms for the latter for the \u2192 the former significantly outperforms the latter for the\n\nsimple parameter transformations from DNN to SFNN are not clear to work in general, \u2192 simple parameter transformations from DNN to SFNN do not clearly work in general,\n\nis a special form of stochastic neural networks \u2192 is a special form of stochastic neural network\n\nAs like (3), the first layer is \u2192 As in (3), the first layer is\n\nThis connection naturally leads an efficient training procedure \u2192 This connection naturally leads to an efficient training procedure\n","model":"human","source":"peerread","label":0,"id":4966}
{"text":"When you transfer weights back from the simplified SFNN to the DNN* model, do you need to perform some sort of rescaling that undoes the operations in Equation (8) in the paper?\n","model":"human","source":"peerread","label":0,"id":4967}
{"text":"In Table 1, do the 4-layer SFNNs have one or two layers of stochastic units?  What about the 3-layer networks?  I suppose you could take the expectation in the output layer.","model":"human","source":"peerread","label":0,"id":4968}
{"text":"In this paper, citations are appearing with the authors' first initials and last names, e.g. (Hinton, G. et al., 2012a) instead of the authors last names and no initials, e.g. (Hinton et al., 2012a).  I find the first initials to be very distracting.  Please reformat the paper to match the citation style of the ICLR 2017 template.\n","model":"human","source":"peerread","label":0,"id":4969}
{"text":"Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the margins to the correct spacing for your submission to be considered. Thank you!","model":"human","source":"peerread","label":0,"id":4970}
{"text":"The authors propose \"information dropout\", a variation of dropout with an information theoretic interpretation. A dropout layer limits the amount of information that can be passed through it, and the authors quantify this using a variational bound. \n\nIt remains unclear why such an information bottleneck is a good idea from a theoretical standpoint. Bayesian interpretations lend a theoretical basis to parameter noise, but activation noise has no such motivation. The information bottleneck indeed limits the information that can be passed through, but there is no rigorous argument for why this should improve generalization.\n\nThe experiments are not convincing. The CIFAR-10 results are worse than those in the paper that originally proposed the network architecture they use (Springenberg et al). The VAE results on MNIST are also horrible.","model":"human","source":"peerread","label":0,"id":4971}
{"text":"The authors all agree that the theory presented in the paper is of high quality and is promising but the experiments are not compelling. The reviewers are concerned that the presented idea and connections to existing methods, while neat, may not be impactful as the promise of the theory does not bear out in practice. One reviewer is concerned that the presented theory is still not useful, stating that the \"information bottleneck thus only becomes meaningful when the capacity of the encoding network is controlled in some measurable way, which is not discussed in the paper\". In general, they seem to agree that the experimental evaluation is still preliminary and unfinished. As such, it would seem that the authors could make the paper far more compelling by demonstrating more compelling improvements on benchmark experiments and submitting to a future conference.","model":"human","source":"peerread","label":0,"id":4972}
{"text":"Following the suggestions of the reviewers, we updated the paper with new experiments and plots.\n\nFirst, to empirically validate that, by increasing the value of the parameter \\beta, we can obtain representations that are increasingly minimal and invariant while remaining discriminative (sufficient), we created a new dataset, called \u2018Occluded CIFAR\u2019. The experiment in Sec. 6 shows precisely this effect, thus validating the theoretical intuition. Indeed, by increasing \\beta, we also prevent overfitting and the overall quality of the representation actually improves.\n\nIn Figure 5 in Appendix D, we added a comparison between Information Dropout and binary dropout using the same settings as [Springenberg et al., 2014]. For both methods we obtain a slightly better testing error than the original paper and, as also observed in the previous experiments, Information Dropout performs comparably or better than dropout.\n\nIn Figure 6, we plot the amount of information flowing through the dropout layers of a CNN as the number of filters varies. This plots supports some of the theoretical intuitions, and we show empirically that information dropout automatically selects a lower noise level for smaller networks, and that the units in the higher layers contain on average more information relative to the task than units in the bottom layers.","model":"human","source":"peerread","label":0,"id":4973}
{"text":"We thank all the reviewers for their comments. We would like to provide some clarification regarding the experiments in the paper, and address some of the concerns which were raised.\n\n>> The CIFAR-10 results are worse than those in the paper that originally proposed the network architecture they use (Springenberg et al). The VAE results on MNIST are also horrible.\n\nIf we use exactly the same architecture of Springenberg et al., then our results on CIFAR are, as predicted by the theory, comparable asymptotically, and better for smaller nets. We have added experiments that show this in the revised version to be uploaded soon. Also, our results on VAE are comparable to [KW13] for a similar architecture.\n\nNote, however, that the goal of our experiments is not to improve state-of-the-art on CIFAR-10 or MNIST, but to illustrate the effect of Information Dropout when compared to other forms of dropout, and to validate the intuition derived from the theory. For this reason, for the experiments in the paper we chose the simplest empirical settings, and modified the All Convolutional Net to isolate potentially confounding factors: we removed weight decay, increased the batch size to reduce gradient noise, simplified the architecture by removing the initial dropout layer, and used less aggressive learning rates and no fine tuning.  We also replaced ReLU with Softplus to make the results comparable with those of [KSW15]. This also served to validate the theory which applies to both ReLU and Softplus. \n\nMany factors affect empirical performance, only few of which are relevant to validating our theory. To the latter hand, we went to great length to ensure that the experiments are *controlled*. Only under careful control can the experiments be convincing in validating the theory.\n\nNevertheless, as suggested by the reviewers, we are currently exploring other experiments that would further illustrate the tradeoff between invariance to nuisances and sufficiency as mediated by the coefficient \\beta. We will add these along with the further tests using the same architecture of Springerberg, as described above.\n\n>> The results on CIFAR-10 in Figure 3(b) seem to be on a validation set\n\nWe are using the same nomenclature of [KSW15], since we want to make a direct comparison with their experiment. As customary for CIFAR, the data is divided into a disjoint training set (50,000 samples) and validation\/test set (10,000 samples). We feel that \"validation\" here is more appropriate.\n\n[KSW15] Diederik Kingma, Tim Salimans, and Max Welling, \"Variational Dropout and the Local Reparameterization Trick\", 2015\n\n[KW13] Diederik P Kingma, Max Welling, \"Auto-Encoding Variational Bayes\", 2013\n","model":"human","source":"peerread","label":0,"id":4974}
{"text":"Paper summary\nThis paper develops a generalization of dropout using information theoretic\nprinciples. The basic idea is that when learning a representation z of input x\nwith the aim of predicting y, we must choose a z such that it carries the least\namount of information about x, as long as it can predict y. This idea can be\nformalized using the Information Bottleneck Lagrangian. This leads to an\noptimization problem which is similar to the one derived for variational\ndropout, the difference being that Information dropout allows for a scaling\nfactor associated with the KL divergence term that encourages noise. The amount\nof noise being added is made a parameterized function of the data and this\nfunction is optimized along with the rest of the model. Experimental results on\nCIFAR-10 and MNIST show (small) improvements over binary dropout.\n\nStrengths\n- The paper highlights an important conceptual link between probabilistic\n  variational methods and information theoretic methods, showing that dropout\ncan be generalized using both formalisms to arrive at very similar models.\n- The presentation of the model is excellent.\n- The experimental results on cluttered MNIST are impressive.\n\nWeaknesses\n- The results on CIFAR-10 in Figure 3(b) seem to be on a validation set (unless\n  the axis label is a typo). It is not clear why the test set was not used. This\nmakes it hard to compare to results reported in Springenberg et al, as well as\nother results in literature.\n\nQuality\nThe theoretical exposition is high quality. Figure 2 gives a nice qualitative\nassessment of what the model is doing. However, the experimental results\nsection can be made better, for example, by matching the results on CIFAR-10 as\nreported in Springenberg et al. and trying to improve on those using information\ndropout.\n\nClarity\nThe paper is well written and easy to follow.\n\nOriginality\nThe derivation of the information dropout optimization problem using IB\nLagrangian is novel. However, the final model is quite close to variational\ndropout.\n\nSignificance\nThis paper will be of general interest to researchers in representation learning\nbecause it highlights an alternative way to think about latent variables (as\ninformation bottlenecks). However, unless the model can be shown to achieve\nsignificant improvements over simple dropout, its wider impact is likely to be\nlimited.\n\nOverall\nThe paper presents an insightful theoretical derivation and good preliminary\nresults. The experimental section can be improved.\n\nMinor comments and suggestions -\n- expecially -> especially\n- trough -> through\n- There is probably a minus sign missing in the expression for H(y|z) above Eq (2).\n- Figure 3(a) has error bars, but 3(b) doesn't. It might be a good idea to have those\nfor Figure 3(b) as well.\n- Please consider comparing Figure 2 with the activity map of a standard CNN\n  trained with binary dropout, so we can see if similar filtering out is\nhappening there already.","model":"human","source":"peerread","label":0,"id":4975}
{"text":"An interesting connection is made between dropout, Tishby et al's \"information bottleneck\" and VAEs. Specifically, classification of 'y' from 'x' is split in two faces: an inference model z ~ q(z|x), a prior p(z), and a classifier y ~ p(y|z). By optimizing the objective E_{(x,y)~data} [ E_{z~q(z|x)}[log p(x|y)] + lambda * KL(q(z|x)||p(z))], with lambda <= 1, an information bottleneck 'z' is formed, where lambda controls an upper bound on the number of bits traveling through 'z'.\n\nThe objective is equivalent to a VAE objective with downweighted KL(posterior|prior), an encoder that takes as input 'x', and a decoder that only predicts 'x'.\n\n- Related work (section 2) is discussed sufficiently. \n- In section 3, would be better to remind us the definition of mutual information.\n- Connection to VAEs in section 5 is interesting.\n- Unfortunately, the MNIST\/CIFAR-10 results are not great. Since the method is potentially more flexible than other forms of dropout, this is slightly disappointing.\n- It's unclear why the CIFAR-10 results seem to be substantially worse than the results originally reported for that architecture.\n- It's unclear which version of 'beta' was used in figure 3a.\n\nOverall I think the theory presented in the paper is promising. However, the paper lacks sufficiently convincing experimental results, and I encourage the authors to do further experiments that prove significant improvements, at least on CIFAR-10, perhaps on larger problems.","model":"human","source":"peerread","label":0,"id":4976}
{"text":"A personal communication asked whether there are cases in which a stochastic representation of the data can obtain a better value of the IB Lagrangian than any deterministic representation; in response to this, we added a remark in Section 3 saying that this indeed can happen. \n\nIn response to a question by the reviewer, we added to Section 2 a few examples of nuisances that act as a group on the data.\n\nWe updated the MNIST and CIFAR experiments: all the qualitative results are the same as before, but we slightly changed the hyperparameters and the optimization method to provide a more accurate and fairer comparison between the algorithms.\n\nFinally, we added an appendix to fill a gap in the narrative between Equation (2),  where the two distributions in the KL term were the actual prior and posterior of z, and Section 4, where we assume an approximated prior whose parameters are learned independently. Specifically, we show that if the approximated prior of the activations is chosen to be factorized, as we do, then our loss function differs from the actual IB Lagrangian by the total correlation of z. As a consequence, our approximation is correct when the components of z are mutually independent, and the loss function we use actually encourages this independence.\n\nWe would like to thank all the people that gave us early feedback on the paper.","model":"human","source":"peerread","label":0,"id":4977}
{"text":"I agree with the other reviewer that the application areas are limited in the paper. I agree with the overall sentiment of the paper to evaluate effectiveness of some of the more recent techniques in this area, in conjunction with the recurrent networks. \n\nThe paper advertises itself as a method (or a list of methods) of improving the recurrent baselines when performing experiments, however fails (or not shown) to generalize to other tasks. Effectiveness of these methods need to be shown across a wide variety of tasks if we intend to replace traditional baselines in general, rather than a specific subset of applications.\n\nI like the desire to evaluate many of the recent techniques and having many replications of experiments towards this end (which is a strong point of the paper). However, whether there are synergies of some of the enhancements with sentiment analysis or not, we cannot see from these results. It would be interesting to see whether some of these results generalize across a wide variety of tasks.","model":"human","source":"peerread","label":0,"id":4978}
{"text":"The paper attempts to perform an interesting exploration (how to combine different tricks for LSTM training) but does not take it far enough. \n \n Pros:\n - interesting attempt at studying different techniques to improve LSTM training results\n Cons:\n - not very strong baselines\n - limited set of domains were explored\n - low in novelty (which wouldn't be a problem if the comparison was more thorough -- see above 2 points).","model":"human","source":"peerread","label":0,"id":4979}
{"text":"The paper proposes and analyses three methods applied to traditional LSTMs: Monte Carlo test-time model averaging, average pooling, and residual connections. It shows that those methods help to enhance traditional LSTMs on sentiment analysis. \n\nAlthough the paper is well written, the experiment section is definitely its dead point. Firstly, although it shows some improvements over traditional LSTMs, those results are not on par with the state of the art. Secondly, if the purpose is to take those extensions as strong baselines for further research, the experiments are not adequate: the both two datasets which were used are quite similar (though they have different statistics). I thus suggest to carry out more experiments on more diverse tasks, like those in \"LSTM: A Search Space Odyssey\"). \n\nBesides, those extensions are not really novel.","model":"human","source":"peerread","label":0,"id":4980}
{"text":"This paper presents three improvements to the standard LSTM architecture used in many neural NLP models: Monte Carlo averaging, embed average pooling, and residual connections. Each of the modifications is trivial to implement, so the paper is definitely of interest to any NLP researchers experimenting with deep learning. \n\nWith that said, I am concerned about the experiments and their results. The residual connections do not seem to consistently help performance; on SST the vertical residuals help but the lateral residuals hurt, and on IMDB it is the opposite. More fundamentally, there need to be more tasks than just sentiment analysis here. I'm not quite sure why the paper's focus is on text classification, as any NLP task using an LSTM encoder could conceivably benefit from these modifications. It would be great to see a huge variety of tasks like QA, MT, etc., which would really make the paper much stronger. \n\nAt this point, while the experiments that are included in the paper are very thorough and the analysis is interesting, there need to be more tasks to convince me that the modifications generalize, so I don't think the paper is ready for publication.","model":"human","source":"peerread","label":0,"id":4981}
{"text":"I agree with the other reviewer that the application areas are limited in the paper. I agree with the overall sentiment of the paper to evaluate effectiveness of some of the more recent techniques in this area, in conjunction with the recurrent networks. \n\nThe paper advertises itself as a method (or a list of methods) of improving the recurrent baselines when performing experiments, however fails (or not shown) to generalize to other tasks. Effectiveness of these methods need to be shown across a wide variety of tasks if we intend to replace traditional baselines in general, rather than a specific subset of applications.\n\nI like the desire to evaluate many of the recent techniques and having many replications of experiments towards this end (which is a strong point of the paper). However, whether there are synergies of some of the enhancements with sentiment analysis or not, we cannot see from these results. It would be interesting to see whether some of these results generalize across a wide variety of tasks.  ","model":"human","source":"peerread","label":0,"id":4982}
{"text":"I find the experiments not convincing because the two datasets are quite similar (sentiment analysis). I was wondering if you have tried your proposed models\/methods on much more different tasks (e.g. machine translation, question answering, etc.)","model":"human","source":"peerread","label":0,"id":4983}
{"text":"This work proposes a model that can learn short binary codes via paragraph vectors to allow fast retrieval of documents. The experiments show that this is superior to semantic hashing. The approach is simple and not very technically interesting. For a code size of 128, the loss compared to a continuous paragraph vector seems moderate.\n\nThe paper asks the reader to refer to the Salakhutdinov and Hinton paper for the baseline numbers but I think they should be placed in the paper for easy reference. For simplicity, the paper could show the precision at 12.5%, 25% and 50% recall for the proposed model and semantic hashing. It also seems that the semantic hashing paper shows results on RCV2 and not RCV1. RCV1 is twice the size of RCV2 and is English only so it seems that these results are not comparable. It would be interesting to see how many binary bits are required to match the performance of the continuous representation. A comparison to the continuous PV-DBOW trained with bigrams would also make it a more fair comparison.\n\nFigure 7 in the paper shows a loss from using the real-binary PV-DBOW. It seems that if a user needed high quality ranking after the retrieval stage and they could afford the extra space and computation, then it would be better for them to use a standard PV-DBOW to obtain the continuous representation at that stage.\n\nMinor comments:\nFirst line after the introduction: is sheer -> is the sheer\n4th line from the bottom of P1: words embeddings -> word embeddings\nIn table 1: What does code size refer to for PV-DBOW? Is this the number of elements in the continuous vector?\n5th line from the bottom of P5: W -> We\n5th line after section 3.1: covers wide -> covers a wide","model":"human","source":"peerread","label":0,"id":4984}
{"text":"This paper proposes to binarize Paragraph Vector distributed representations in an end-to-end framework. Experiments demonstrate that this beats autoencoder-based binary codes. However, the performance is similar to using paragraph vectors followed by existing binarization techniques, failing to show an advantage of training end-to-end. Therefore, the novel contribution seems too limited for publication.","model":"human","source":"peerread","label":0,"id":4985}
{"text":"We have just uploaded an updated version of our paper. Below we list the most important changes. We will then follow with responses to the reviews.\n\n1) AnonReviewer3 pointed out that, apart from the Krizhevsky's binarization, one can also employ stochastic neurons in the coding layer. When comparing these two approaches we carried out a more thorough investigation of the dropout in the coding layers. In the initial experiments we employed small, 10% dropout. However, larger dropout rates (up to 50%) turned out to improve performance on the validation sets (in both binary and real-valued models). We therefore updated final results to reflect the dropout rates selected in validation experiments.\n\n2) We extended the comparison with baseline methods by adding results for two hashing techniques, namely random hyperplane projection and iterative quantization.\n\n3) Visualization of Binary PV codes (Figure 5 in the previous version of the paper) along with the corresponding text was moved to an appendix.","model":"human","source":"peerread","label":0,"id":4986}
{"text":"The method in this paper introduces a binary encoding level in the PV-DBOW and PV-DM document embedding methods (from Le & Mikolov'14). The binary encoding consists in a sigmoid with trained parameters that is inserted after the standard training stage of the embedding.\n \nFor a document to encode, the binary vector is obtained by forcing the sigmoid to output a binary output for each of the embedding vector components. The binary vector can then be used for compact storage and fast comparison of documents.\n \nPros:\n \n- the binary representation outperforms the Semantic hashing method from Salakhutdinov & Hinton '09\n \n- the experimental approach sound: they compare on the same experimental setup as Salakhutdinov & Hinton '09, but since in the meantime document representations improved (Le & Mikolov'14), they also combine this new representation with an RBM to show the benefit of their binary PV-DBOW\/PV-DM\n \nCons:\n \n- the insertion of the sigmoid to produce binary codes (from Lin & al. '15) in the training process is incremental\n \n- the explanation is too abstract and difficult to follow for a non-expert (see details below)\n \n- a comparison with efficient indexing methods used in image retrieval is missing. For large-scale indexing of embedding vectors, derivations of the Inverted multi-index are probably more interesting than binary codes. See eg. Babenko & Lempitsky, Efficient Indexing of Billion-Scale Datasets of Deep Descriptors, CVPR'16\n \nDetailed comments:\n \nSection 1: the motivation for producing binary codes is not given. Also, the experimental section could give some timings and mem usage numbers to show the benefit of binary embeddings\n \nfigure 1, 2, 3: there is enough space to include more information on the representation of the model: model parameters + training objective + characteristic sizes + dropout. In particular, in fig 2, it is not clear why \"embedding lookup\" and \"linear projection\" cannot be merged in a single smaller lookup table (presumably because there is an intermediate training objective that prevents this).\n \np2: \"This way, the length of binary codes is not tied to the dimensionality of word embeddings.\" -> why not?\n \nsection 3: This is the experimental setup of  Salakhutdinov & Hinton 2009. Specify this and whether there is any difference between the setups.\n \n\"similarity of the inferred codes\": say here that codes are compared using Hamming distances.\n \n\"binary codes perform very well, despite their far lower capacity\" -> do you mean smaller size than real vectors?\n \nfig 5: these plots could be dropped if space is needed.\n \nsection 3.1: one could argue that \"transferring\" from Wikipedia to anything else cannot be called transferring, since Wikipedia's purpose is to include all topics and lexical domains\n \nsection 3.2: specify how the 300D real vectors are compared. L2 distance? inner product?\n \nfig4: specify what the raw performance of the large embedding vectors is (without pre-filtering with binary codes), or equivalently, the perf of (code-size, Hamming dis) = (28, 28), (24, 24), etc.\n","model":"human","source":"peerread","label":0,"id":4987}
{"text":"This paper presents a method to represent text documents and paragraphs as short binary codes to allow fast similarity search and retrieval by using hashing techniques. The real-valued paragraph vectors by Le & Mikolov is extended by adding a stochastic binary layer on top of the neural network architecture. Two methods for binarizing the final activations are compared: (1) simply adding noise to sigmoid activations to encourage discritization. (2) binarizing the activations in the forward pass and keeping them real-valued in the backward pass (straight-through estimation). The paper presents encouraging results by using straight-through estimation on 20 newsgroup and RCV1 text datasets by using 128 and 32 bit binary codes.\n\nOn the plus side, the application presented in the paper is interesting and important. The exposition of the paper is clean and clear. However, the novelty of the approach is limited from a machine learning standpoint. The literature on binary hashing beyond semantic hashing and Krizhevsky's binary autoencoders in 2011 is not explained. An important baseline is missing where real-valued paragraph vectors are learned first, and then converted to binary codes using off-the-shelf hashing methods (e.g. random projection LSH by Charikar, BRE by Kulis & Darrell, ITQ by Gong & Lazebnik, MLH by Norouzi & Fleet, etc.)\n\nGiven the lack of novelty and the missing baseline, I do not recommend this paper in its current for publication in the ICLR conference's proceeding. Moving forward, this paper may be more suitable for NLP conferences as it is more on the applied side.\n\nMore comments:\n- I believe from an practical perspective it may be easier to first learn real-valued paragraph vectors and then quantize them for indexing. That said, an end-to-end approach as proposed in this paper may perform better. I would like to see an empirical comparison between the proposed end-to-end approach and a simpler two stage quantization method suggested here.\n- See \"Estimating or Propagating Gradients Through Stochastic Neurons\" By Bengio et al - discussing straight through estimation and some other alternatives.\n- The paper argues that the length of binary codes cannot be longer than 32 bits because longer codes are not suitable for document hashing. This is not quite right given multi-probe hashing mechanisms, for example see \"Mult-index Hashing\" by Norouzi et al.\n- See \"Hashing for Similarity Search: A Survey\" by Wang et al. for a survey of related work on binary hashing and quantization. You seem to ignore the extensive work done on binary hashing.\n","model":"human","source":"peerread","label":0,"id":4988}
{"text":"This work proposes a model that can learn short binary codes via paragraph vectors to allow fast retrieval of documents. The experiments show that this is superior to semantic hashing. The approach is simple and not very technically interesting. For a code size of 128, the loss compared to a continuous paragraph vector seems moderate.\n\nThe paper asks the reader to refer to the Salakhutdinov and Hinton paper for the baseline numbers but I think they should be placed in the paper for easy reference. For simplicity, the paper could show the precision at 12.5%, 25% and 50% recall for the proposed model and semantic hashing. It also seems that the semantic hashing paper shows results on RCV2 and not RCV1. RCV1 is twice the size of RCV2 and is English only so it seems that these results are not comparable. It would be interesting to see how many binary bits are required to match the performance of the continuous representation. A comparison to the continuous PV-DBOW trained with bigrams would also make it a more fair comparison.\n\nFigure 7 in the paper shows a loss from using the real-binary PV-DBOW. It seems that if a user needed high quality ranking after the retrieval stage and they could afford the extra space and computation, then it would be better for them to use a standard PV-DBOW to obtain the continuous representation at that stage.\n\nMinor comments:\nFirst line after the introduction: is sheer -> is the sheer\n4th line from the bottom of P1: words embeddings -> word embeddings\nIn table 1: What does code size refer to for PV-DBOW? Is this the number of elements in the continuous vector?\n5th line from the bottom of P5: W -> We\n5th line after section 3.1: covers wide -> covers a wide\n","model":"human","source":"peerread","label":0,"id":4989}
{"text":"This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem. \nThe paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. \n\nThe authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches. \nThe experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.\n\nThe authors do not address the out-of-sample problem. This is a problem of kernel-based methods vs LVMs, and thus should be address here.\n\n\nThe paper contains errors:\n\n- The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA. This is simply wrong, as the proposed approach consists of iteratively solving iterativey a set of closed form updates  and Levenberg-Marquard optimizationd. This is not any more closed form!\n\n- In the same paragraph (and later in the text) the authors claim that the proposed approach can be trivially generalized to incorporate other cost functions. This is not true, as in general there will be no more inner loop closed form updates and the authors will need to solve a much more complex optimization problem. \n\n- The third paragraph of section 2 claims that this paper presents a novel energy minimization framework to solve problems of the general form of eq. (2). However, this is not what the authors solve at the end. They solve a different problem that has been subject to at least two relaxations. It is not clear how solving for a local optima of this double relaxed problem is related to the original problem they want to solve. \n\n- The paper says that Geiger et al defined non linearities on a latent space of pre-defined dimensionality. This is wrong. This paper discovers the dimensionality of the latent space by means of a regularizer that encourages the singular values to be sparse. Thus, it does not have a fixed dimensionality, the latent space is just bounded to be smaller or equal than the dimensionality of the original space. \n\n\nIt is not clear to me why the author say for LVMs such as GPLVM that \"the latent space is learned a priority with clean training data\". One can use different noise models within the GP framework. Furthermore, the proposed approach assumes Gaussian noise (see eq. 6), which is also the trivial case for GP-based LVMs.  \n\n\nIt is not clear what the authors mean in the paper by \"pre-training\" or saying that techniques do not have a training phase. KPCA is trained via a closed-form update, but there is still training.","model":"human","source":"peerread","label":0,"id":4990}
{"text":"There is complete consensus among the reviewers that the KPCA formulation in this paper needs better motivation; that the paper has technical errors, and the experimental evaluation is not convincing. As such the paper is not up to ICLR standards. The authors are encouraged to revise the paper based on feedback from the reviews.","model":"human","source":"peerread","label":0,"id":4991}
{"text":"The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. The latent variables (or causal factors) corresponding to the observed data are assumed to lie near a low dimensional subspace in an RKHS induced by a predetermined kernel. The proposed regularizer can be seen as an extension of the linear low-rank assumption on the latent factors. A nuclear norm penalty on the Cholesky factor of the kernel matrix is used as a relaxation for the dimensionality of the subspace. Empirical results are reported on two tasks involving linear inverse problems -- missing feature imputation, and estimating non-rigid 3D structures from a sequence of 2D orthographic projections -- and the proposed method is shown to outperform linear low-rank regularizer. \n\nThe clarity of the paper has scope for improvement (particularly, Introduction) - the back and forth b\/w dimensionality reduction techniques and inverse problems is confusing at times. Clearly defining the ill-posed inverse problem first and then motivating the need for a regularizer (which brings dimensionality reduction techniques into the picture) may be a more clear flow in my opinion. \n\nThe motivation behind relaxation of rank() in Eq 1 to nuclear-norm in Eq 2 is not clear to me in this setting. The relaxation does not yield a convex problem over S,C (Eq 5) and also increases the computations (Algo 2 needs to do full SVD of K(S) every time). The authors should discuss pros\/cons over the alternate approach that fixes the rank of C (which can be selected using cross-validation, in the same way as $\\tau$ is selected), leaving just the first two terms in Eq 5. For this simpler objective, an interesting question to ask would be -- are there kernel functions for which it can solved in a scalable manner? \n\nThe proposed alternating optimization approach in the current form is computationally intensive and seems hard to scale to even moderate sized data -- in every iteration one needs to compute the kernel matrix over S and perform full SVD over the kernel matrix (Algo 2). Empirical evaluations are also not extensive -- (i) the dataset used for feature imputation is old and non-standard, (ii) for structure estimation from motion on CMU dataset, the paper only compares with linear low-rank regularization, (iii) there is no comment\/study on the convergence of the alternating procedure (Algo 1). \n\n\n\n\n\n","model":"human","source":"peerread","label":0,"id":4992}
{"text":"This paper considers an alternate formulation of Kernel PCA with rank constraints incorporated as a regularization term in the objective. The writing is not clear. The focus keeps shifting from estimating \u201ccausal factors\u201d, to nonlinear dimensionality reduction to Kernel PCA to ill-posed inverse problems. The problem reformulation of Kernel PCA uses somewhat standard tricks and it is not clear what are the advantages of the proposed approach over the existing methods as there is no theoretical analysis of the overall approach or empirical comparison with existing state-of-the-art.  \n\n- Not sure what the authors mean by \u201ccausal factors\u201d. There is a reference to it in Abstract and in Problem formulation on page 3 without any definition\/discussion.\n\n- In KPCA, I am not sure why one is interested in step (iii) outlined on page 2 of finding a pre-image for each\n\n- Authors outline two key disadvantages of the existing KPCA approach. The first one, that of low-dimensional manifold assumption not holding exactly, has received lots of attention in the machine learning literature. It is common to assume that the data lies near a low-dimensional manifold rather than on a low-dimensional manifold. Second disadvantage is somewhat unclear as finding \u201ca data point (pre-image) corresponding to each projection in the input space\u201d is not a standard step in KPCA. \n\n- On page 3, you never define $\\mathcal{X} \\times N$, $\\mathcal{Y} \\times N$, $\\mathcal{H} \\times N$. Clearly, they cannot be cartesian products. I have to assume that notation somehow implies N-tuples. \n\n- On page 3, Section 2, $\\mathcal{X}$ and $\\mathcal{Y}$ are sets. What do you mean by $\\mathcal{Y} \\ll \\mathcal{X}$\n\n- On page 5, $\\mathcal{S}^n$ is never defined. \n\n- Experiments: None of the standard algorithms for matrix completion such as OptSpace or SVT were considered \n\n- Experiments: There is no comparison with alternate existing approaches for Non-rigid structure from motion.  \n\n- Proof of the main result Theorem 3.1: To get from (16) to (17) using the Holder inequality (as stated) one would end up with a term that involves sum of fourth powers of weights w_{ij}. Why would they equal to one using the orthonormal constraints? It would be useful to give more details here, as I don\u2019t see how the argument goes through at this point. ","model":"human","source":"peerread","label":0,"id":4993}
{"text":"This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem. \nThe paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. \n\nThe authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches. \nThe experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.\n\nThe authors do not address the out-of-sample problem. This is a problem of kernel-based methods vs LVMs, and thus should be address here.\n\n\nThe paper contains errors:\n\n- The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA. This is simply wrong, as the proposed approach consists of iteratively solving iterativey a set of closed form updates  and Levenberg-Marquard optimizationd. This is not any more closed form!\n\n- In the same paragraph (and later in the text) the authors claim that the proposed approach can be trivially generalized to incorporate other cost functions. This is not true, as in general there will be no more inner loop closed form updates and the authors will need to solve a much more complex optimization problem. \n\n- The third paragraph of section 2 claims that this paper presents a novel energy minimization framework to solve problems of the general form of eq. (2). However, this is not what the authors solve at the end. They solve a different problem that has been subject to at least two relaxations. It is not clear how solving for a local optima of this double relaxed problem is related to the original problem they want to solve. \n\n- The paper says that Geiger et al defined non linearities on a latent space of pre-defined dimensionality. This is wrong. This paper discovers the dimensionality of the latent space by means of a regularizer that encourages the singular values to be sparse. Thus, it does not have a fixed dimensionality, the latent space is just bounded to be smaller or equal than the dimensionality of the original space. \n\n\nIt is not clear to me why the author say for LVMs such as GPLVM that \"the latent space is learned a priority with clean training data\". One can use different noise models within the GP framework. Furthermore, the proposed approach assumes Gaussian noise (see eq. 6), which is also the trivial case for GP-based LVMs.  \n\n\nIt is not clear what the authors mean in the paper by \"pre-training\" or saying that techniques do not have a training phase. KPCA is trained via a closed-form update, but there is still training.  \n","model":"human","source":"peerread","label":0,"id":4994}
{"text":"Overall I think this is an interesting paper which shows empirical performance improvement over baselines. However, my main concern with the paper is regarding its technical depth, as the gist of the paper can be summarized as the following: instead of keeping the batch norm mean and bias estimation over the whole model, estimate them on a per-domain basis. I am not sure if this is novel, as this is a natural extension of the original batch normalization paper. Overall I think this paper is more fit as a short workshop presentation rather than a full conference paper.\n\nDetailed comments:\n\nSection 3.1: I respectfully disagree that the core idea of BN is to align the distribution of training data. It does this as a side effect, but the major purpose of BN is to properly control the scale of the gradient so we can train very deep models without the problem of vanishing gradients. It is plausible that intermediate features from different datasets naturally show as different groups in a t-SNE embedding. This is not the particular feature of batch normalization: visualizing a set of intermediate features with AlexNet and one gets the same results. So the premise in section 3.1 is not accurate.\n\nSection 3.3: I have the same concern as the other reviewer. It seems to be quite detatched from the general idea of AdaBN. Equation 2 presents an obvious argument that the combined BN-fully_connected layer forms a linear transform, which is true in the original BN case and in this case as well. I do not think it adds much theoretical depth to the paper. (In general the novelty of this paper seems low)\n\nExperiments:\n\n- section 4.3.1 is not an accurate measure of the \"effectiveness\" of the proposed method, but a verification of a simple fact: previously, we normalize the source domain features into a Gaussian distribution. the proposed method is explicitly normalizing the target domain features into the same Gaussian distribution as well. So, it is obvious that the KL divergence between these two distributions are closer - in fact, one is *explicitly* making them close. However, this does not directly correlate to the effectiveness of the final classification performance.\n\n- section 4.3.2: the sensitivity analysis is a very interesting read, as it suggests that only a very few number of images are needed to account for the domain shift in the AdaBN parameter estimation. This seems to suggest that a single \"whitening\" operation is already good enough to offset the domain bias (in both cases shown, a single batch is sufficient to recover about 80% of the performance gain, although I cannot get data for even smaller number of examples from the figure). It would thus be useful to have a comparison between these approaches, and also a detailed analysis of the effect from each layer of the model - the current analysis seems a bit thin.","model":"human","source":"peerread","label":0,"id":4995}
{"text":"The paper performs domain adaptation using a very simple trick inspired by BatchNorm. The paper received below margin scores. The reviewers both, liked the simplicity of the approach, and at the same time felt that the contribution was too thin. Given the high bar of ICLR, this paper falls short.","model":"human","source":"peerread","label":0,"id":4996}
{"text":"This work also inspired our another recent work \"Demystifying Neural Style Transfer\"(","model":"human","source":"peerread","label":0,"id":4997}
{"text":"According to the reviewers\u2019 helpful suggestions, we have revised and updated our paper. The main modifications are as follows:\n(1) We have updated the writing of our abstract and revised section 3.3 to make it clearer and merge it to the section 3.2.\n(2) We have removed the original section 4.3.1, and revised section \u201csensitivity to target domain size\u201d by adding the experimental results of using smaller number of images.\n(3) We have added a new analysis section \u201cAdaptation Effect for Different BN Layers\u201d in section 4.3.2.\n","model":"human","source":"peerread","label":0,"id":4998}
{"text":"We do not think simplicity is our drawback of our method. On the contrary, we believe this is a great advantage of our method. Being technically simple does not mean no novelty, and it should never be a reason to reject a paper. In fact, Batch Normalization is a very simple method, while dropout is even simpler. These techniques have had huge impacts to the field despite the simplicity. As the Reviewer2 indicates, there are also prior simple but important methods in domain adaptation. (e.g. \u201cFrustratingly Easy Domain Adaptation\u201d and \u201cReturn of Frustratingly Easy Domain Adaptation\u201d) Further, to our best knowledge, there are no prior works to exploit Batch Normalization for domain adaptation.\n\nThe main contributions in our paper is that we propose a simple yet effective AdaBN method for domain adaptation by modulating the statistics in all BN layers, which outperforms the states-of-the-art methods. Furthermore, we demonstrate that our method is complementary with other existing methods. Thus, we think it is valuable for the researchers in the field.\n","model":"human","source":"peerread","label":0,"id":4999}
{"text":"This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain.\n\n\nPros:\n\nThe method is very simple and easy to understand and apply.\n\nThe experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks.\n\nThe analysis in section 4.3.2 shows that a very small number of target domain samples are needed for adaptation of the network.\n\n\nCons:\n\nThere is little novelty -- the method is arguably too simple to be called a \u201cmethod.\u201d Rather, it\u2019s the most straightforward\/intuitive approach when using a network with batch normalization for domain adaptation.  The alternative -- using the BN statistics from the source domain for target domain examples -- is less natural, to me. (I guess this alternative is what\u2019s done in the Inception BN results in Table 1-2?)\n\nThe analysis in section 4.3.1 is superfluous except as a sanity check -- KL divergence between the distributions should be 0 when each distribution is shifted\/scaled to N(0,1) by BN.\n\nSection 3.3: it\u2019s not clear to me what point is being made here.\n\n\nOverall, there\u2019s not much novelty here, but it\u2019s hard to argue that simplicity is a bad thing when the method is clearly competitive with or outperforming prior work on the standard benchmarks (in a domain adaptation tradition that started with \u201cFrustratingly Easy Domain Adaptation\u201d).  If accepted, Sections 4.3.1 and 3.3 should be removed or rewritten for clarity for a final version.","model":"human","source":"peerread","label":0,"id":5000}
{"text":"Update: I thank the authors for their comments. I still think that the method needs more experimental evaluation: for now, it's restricted to the settings in which one can use pre-trained ImageNet model, but it's also important to show the effectiveness in scenarios where one needs to train everything from scratch. I would love to see a fair comparison of the state-of-the-art methods on toy datasets (e.g. see (Bousmalis et al., 2016), (Ganin & Lempitsky, 2015)). In my opinion, it's a crucial point that doesn't allow me to increase the rating.\n\nThis paper proposes a simple trick turning batch normalization into a domain adaptation technique. The authors show that per-batch means and variances normally computed as a part of the BN procedure are sufficient to discriminate the domain. This observation leads to an idea that adaptation for the target domain can be performed by replacing population statistics computed on the source dataset by the corresponding statistics from the target dataset.\n\nOverall, I think the paper is more suitable for a workshop track rather than for the main conference track. My main concerns are the following:\n\n1. Although the main idea is very simple, it feels like the paper is composed in such a way to make the main contribution less obvious (e.g. the idea could have been described in the abstract but the authors avoided doing so). \n\n2. (This one is from the pre-review questions) The authors are using much stronger base CNN which may account for the bulk of the reported improvement. In order to prove the effectiveness of the trick, the authors would need to conduct a fair comparison against competing methods. It would be highly desirable to conduct this comparison also in the case of a model trained from scratch (as opposed to reusing ImageNet-trained networks).\n","model":"human","source":"peerread","label":0,"id":5001}
{"text":"Strengths\n\uf06e-- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. \n\uf06e-- Balanced exploration of CNN macroarchitecture and microarchitecture with fire modules.\n\uf06e-- x50 less memory usage than AlexNet, keeping similar accuracy \n\uf06e-- strong experimental results\n\nWeaknesses\n\uf06e--Would be nice to test Sqeezenet on multiple tasks\n\n\uf06e--lack of insights and rigorous analysis into what factors are responsible for the success of SqueezeNet. For example, how are ResNet and GoogleNet connected to the current architecture? Another old paper (Analysis of correlation structure for a neural predictive model with application to speech recognition, Neural Networks, 1994) also showed that the \u201cby-pass\u201d architecture by mixing linear and nonlinear prediction terms improves long term dependency in NN based on rigorous perturbation analysis. Can the current work be placed more rigorously on theoretical analysis?","model":"human","source":"peerread","label":0,"id":5002}
{"text":"The paper proposes a ConvNet architecture (\"SqueezeNet\") and a building block (\"Fire module\") aimed at reducing the model size while maintaining the AlexNet level of accuracy. The novelty of the submission is very limited as very similar design choices have already been used for model complexity reduction in Inception and ResNet. Because of this, we recommend rejection and invite the authors to further develop their method.","model":"human","source":"peerread","label":0,"id":5003}
{"text":"Strengths\n\uf06e-- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. \n\uf06e-- Balanced exploration of CNN macroarchitecture and microarchitecture with fire modules.\n\uf06e-- x50 less memory usage than AlexNet, keeping similar accuracy \n\uf06e-- strong experimental results\n\nWeaknesses\n\uf06e--Would be nice to test Sqeezenet on multiple tasks\n\n\uf06e--lack of insights and rigorous analysis into what factors are responsible for the success of SqueezeNet. For example, how are ResNet and GoogleNet connected to the current architecture? Another old paper (Analysis of correlation structure for a neural predictive model with application to speech recognition, Neural Networks, 1994) also showed that the \u201cby-pass\u201d architecture by mixing linear and nonlinear prediction terms improves long term dependency in NN based on rigorous perturbation analysis. Can the current work be placed more rigorously on theoretical analysis?\n","model":"human","source":"peerread","label":0,"id":5004}
{"text":"Summary: The paper presents a smaller CNN architecture called SqueezeNet for embedded deployment. The paper explores CNN macroarchitecture and microarchitecture to develop SqueezeNet, which is composed of fire modules.\n\nPros: \nAchieves x50 less memory usage than AlexNet while keeping similar accuracy.\n\nCons & Questions:\nComplex by-pass has less accuracy than simple by-pass. And simple by-pass is like ResNet bottlenecks and complex by-pass is like inception modules in GoogLeNet. Can we say that these two valiants of SqueezeNet are adaptation of concepts seen in GoogLeNet and ResNet? If so, then shouldn\u2019t be there a SqueezeNet like model that achieves similar accuracy compared with GoogLeNet and ResNet?\n","model":"human","source":"peerread","label":0,"id":5005}
{"text":"The Squeezenet paper came out in Feb 2016, and I read it with interest. It has a series of completely reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition (imagenet). The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet. (Looks like ~500x if combined with Han, 2015). So, very nice results, definitely worth publishing.\n\nSince the arxiv paper came out, people have noticed and worked to extend the paper. This is already evidence that this paper will have impact --- and deserves to have a permanent published home.\n\nOn the negative side, the architecture was only tested on ImageNet -- unclear whether the ideas transfer to other tasks (e.g., audio or text recognition). And, as with many other architecture-tweaking papers, there is no real mathematical or theoretical support for the ideas: they are just sensible and empirically work.\n\nOh the whole, I think the paper deserves to appear at ICLR, being in the mainline of work on deep learning architectures.","model":"human","source":"peerread","label":0,"id":5006}
{"text":"This paper proposes a novel approach ParMAC, a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models which is based on the composition of multiple processing layers (i.e., deep nets). The basic idea of MAC to optimise the nested objective function, which is traditionally learned using methods based on the chain-rule gradients but inconvenient and is hard to parallelise, is to break nested functional relationships judiciously by introducing new variables ( the auxiliary coordinates) as equality constraints, and then to optimise a penalised function using alternating optimisation over the original parameters (W step) and over the coordinates (Z step).  The minimisation (W step) updates the parameters by splitting the nested model into independent submodels and training them using existing algorithms, and the coordination (Z step) ensures that corresponding inputs and outputs of submodels eventually match.  In this paper, the basic assumptions of ParMAC are that with large datasets in distributed systems, it is imperative to minimise data movement over the network because of the communication time generally far exceeds the computation time in modern architectures. Thus, the authors propose the ParMAC to translate the parallelism inherent in MAC into a distributed system by data parallelism and model parallelism. They also analyse its parallel speedup and convergence, and demonstrated it with MPI-based implementation to optimise binary autoencoders. The proposed ParMAC is tested on 3 colour image retrieval datasets. \n\nThe organization of the paper is well written, and the presentation is clear. My questions are included in the following:\n- The MAC framework solves the original problem approximately. If people use the sigmoid function to smooth the stepwise function, the naive optimization methods can be easier applied. What is the difference between these two? Or why do we want to use a new approach to solve it?\n- The authors do not compare their ParMAC model with other distributed approaches for the same nested function optimization problem.","model":"human","source":"peerread","label":0,"id":5007}
{"text":"The work proposes a parallel\/distributed variant of the MAC decomposition method. In presents some theoretical and experimental results supporting the parallelization strategy. The reviews are mixed and indeed a common concern among the reviewers was the choice of test problem. To me it is ok to only concentrate on a single class of problems, but in this case it needs to be a problem that the ICLR community identifies as being of central importance. Otherwise, if a more esoteric problem is chosen then I (and the reviewers) would rather see that the method is useful on multiple problems. Otherwise, it's basically impossible to extrapolate the experiments to new settings and we are forced to re-implement the algorithm. I'm not saying that the authors necessarily need to consider deep networks and there are many alternative possible models (sparse coding, collaborative filtering, etc.). But it should be noted that, without further experimental comparisons, it is impossible to verify the author's claims that the method is effective for deeply-nested models.\n \n Other concerns brought up by the reviewers (beyond the clarity\/presentation issues, which should also be addressed): the experimental comparison would be more convincing with a comparison to an existing approach like a parallel SGD method. I appreciate that the authors have done a lot of work already on this problem, but doing such obvious comparisons should be the job of the author instead of the reader (focusing purely on parallelization would be ok if the MAC model was extremely-widely-used already and parallelizing was an open problem, but my impression is that this is not the case). As a minor aside, the memory issue will be more serious for deeply-nested models, due to the use of the decomposition approach (we don't want to store the activations for all layers for all examples), and this doesn't arise in SGD.","model":"human","source":"peerread","label":0,"id":5008}
{"text":"We thank the reviewers for their reviews. Below we reply individually to each one. Here, we address a comment that several reviewers made, namely that the binary autoencoder model we explore experimentally is not well known by other researchers. It is true that this model is less well known than deep nets, but it was a good choice for this paper for several reasons:\n- This type of binary autoencoders is actually well known in the area of binary hashing, where one wants to learn a fast hash function (e.g. linear) with binary outputs because the goal is to do fast image searches in large image databases or similar retrieval problems. We have worked in this area using the MAC algorithm and it was convenient for us to develop ParMAC for it.\n- The binary autoencoder allows us to highlight the ability of ParMAC to train non-differentiable models, for which the chain rule does not apply.\n- The binary hashing application also provides with large, public training sets (100 million images). This allowed us to test ParMAC in a realistic distributed setting (up to 128 processors over a network). For us it was important to get actual experimental numbers in a distributed cluster (rather than on cores in a machine or simulating network delays).\n\nFinally, perhaps it is not obvious, but implementing and debugging the algorithm in C and MPI costs significant effort, and running the experiments in the UCSD cluster costs real money (around $0.03 per processing core per hour, which quickly becomes hundreds of dollars). This isn't your usual Matlab or GPU experiment... For a team of one student and one faculty member this puts limitations on the size and number of the experiments.\n\nWe provide the full C\/MPI code in our website to recreate the experiments in either a shared- or a distributed-memory system.\n","model":"human","source":"peerread","label":0,"id":5009}
{"text":"The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) (Carreira-Perpinan and Wang, 2012). This method decomposes the optimization into training individual layers and updating the auxiliary coordinates. The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between machines. Relatively good speedup factors are reported especially on larger datasets and a theoretical model of performance is presented that matches with the experiments.\n\nMy main concern is that even though the method is presented as a general framework for nested functions, experiments focus on a restricted family of models (i.e. binary autoencoders with linear or kernel encoders and linear decoders) with only two components. While the speedup factors are encouraging, it is hard to get a sense of their importance as the binary autoencoder model considered is not well studied by other researchers and is not widely used. I encourage the authors to apply this framework to more generic architectures and problems.\n\nQuestions:\n1- Does this framework apply to some form of generic multi-layer neural network? If so, some experimental results are useful.\n2- What is the implication of applying this framework to more than two components (an encoder and a decoder) and non-linear components?\n3- It is desired to see a plot of performance as a function of time for different setups to demonstrate the speedup after convergence. It seems the paper only focuses on the speedup factors per iteration. For example, increasing the mini-batch size may improve the speed per iteration but may hurt the convergence speed.\n4- Did you consider a scenario where the dataset is too big that storing the data and auxiliary variables on multiple machines simultaneously is not possible?\n\nThe paper cites an ArXiv manuscript with the same title by the authors multiple times. Please make the paper self-contained and include any supplementary material in the appendix.\n\nI believe without applying this framework to a more generic architecture beyond binary autoencoders, this paper does not appeal to a wide audience at ICLR, hence weak reject.\n","model":"human","source":"peerread","label":0,"id":5010}
{"text":"UPDATE:\nI looked at the arxiv version of the paper. It is much longer and appears more rigorous. Fig 3 there is indeed more insightful.\nHowever, I am reviewing the submission and my overall assessment does not change. This is not a minor incremental contribution, and if you want to compress it into a conference submission of this type, I would recommend choosing message you want to convey, and focus on that. As you say, \"...ICLR submission focus on the ParMAC algorithm...\", I would focus on this properly - and remove or move to appendix all extensions and theoretical remarks, and have an extra page on explaining the algorithm. Additionally, make sure to clearly explain the relation of the arxiv paper, in particular that the submission was a compressed version.\n\nORIGINAL REVIEW:\nThe submission proposes ParMAC, based on MAC (Method of Auxiliary Coordinates), formulating a distributed variant of the idea.\n\nRelated Work: In the part on convex ERM and methods, I would recommend citing general communication efficient frameworks, COCOA (Ma et al.) and AIDE (Reddi et al.). I believe these works are most related to the practical objectives authors of this paper set, while number of the papers cited are less relevant.\n\nSection 2, explaining MAC, is quite clearly written, but I do not find part on MAC and EM particularly useful.\n\nSection 3 is much less clearly written. I have trouble following notation, particularly in the speedups part, as different symbols were introduced at different places. Perhaps a quick summary or paragraph on notation in the introduction would be helpful. In paragraph 2, you write as if reader knew how data\/anything is distributed, but this was not mentioned yet; it is specified later. It is not clear what is meant by \"submodel\". Perhaps a more precise example pointing back to eqs (1) & (2) would be useful. As far as I understand from what is written, there are P independent sets of submodels, that traverse the machines in circular fashion. I don't understand how are they initialized (identically?), and more importantly I don't understand what would be a single output of the algorithm (averaging? does not seem to make sense). Since this is not addressed, I suppose I get it wrong, leaving me to guess what was actually meant. \nThe fact that I am not able to understand what is actually happening, I see as major issue.\n\nI don't like the later paragraphs on extensions, model for speedup, convergence and topologies. I don't understand whether these are novel contributions or not, as the authors refer to other work for details. If these are novel, the explanation is not sufficient, particularly speedup part, which contains undefined quantities, e.g. T(P) (or I can't find it). If this is not novel, It does not provide enough explanation to understand anything more, compared with a its version compressed to 1\/4 of its size and referring to the other work. The statement that we can recover the original convergence guarantees seems strong and I don't see why it should be trivial to show (but author point to other work which I did not look at). In topologies part, claiming that something does \"true SGD\", without explaining what is \"true SGD\" seems very strange. Other statements in this section seem also very vague and unjustified\/unexplained.\n\nExperimental section seems to suggest that the method is interesting for binary autoencoders, but I don't see how would I conclude anything about any other models. ParMAC is also not compared to alternative methods, only with itself, focusing on scaling properties.\n\nConclusion contains statements that are too strong or misleading based on what I saw. In particular, \"we analysed its parallel speedup and convergence\" seems ungrounded. Further, the claim \"The convergence properties of MAC remain essentially unaltered in ParMAC\" is unsupported, regardless of the meaning of \"essentially unchanged\".\n\nIn summary, the method seems relevant for particular model class, binary autoencoders, but clarity of presentation is insufficient - I wouldn't be able to recreate the algorithm used in experiments - and the paper contains a number of questionable claims.","model":"human","source":"peerread","label":0,"id":5011}
{"text":"This paper proposes an extension of the MAC method in which subproblems are trained on a distributed cluster arranged in a circular configuration. The basic idea of MAC is to decouple the optimization between parameters and the outputs of sub-pieces of the model (auxiliary coordinates); optimization alternates between updating the coordinates given the parameters and optimizing the parameters given the outputs. In the circular configuration. Because each update is independent, they can be massively parallelized.\n\nThis paper would greatly benefit from more concrete examples of the sub-problems and how they decompose. For instance, can this be applied effectively for deep convolutional networks, recurrent models, etc? From a practical perspective, there's not much impact for this paper beyond showing that this particular decoupling scheme works better than others. \n\nThere also seem to be a few ideas worth comparing, at least:\n- Circular vs. parameter server configurations\n- Decoupled sub-problems vs. parallel SGD\n\nParallel SGD also has the benefit that it's extremely easy to implement on top of NN toolboxes, so this has to work a lot better to be practically useful. \n\nAlso, it's a bit hard to understand what exactly is being passed around from round to round, and what the trade-offs would be in a deep feed-forward network. Assuming you have one sub-problem for every hidden unit, then it seems like:\n\n1. In the W step, different bits of the NN walk their way around the cluster, taking SGD steps w.r.t. the coordinates stored on each machine. This means passing around the parameter vector for each hidden unit.\n2. Then there's a synchronization step to gather the parameters from each submodel, requiring a traversal of the circular structure.\n3. Then each machine updates it's coordinates based on the complete model for a slice of the data. This would mean, for a feed-forward network, producing the intermediate activations of each layer for each data point.\n\nSo for something comparable to parallel SGD, you could do the following: put a mini-batch of size B on each machine with ParMAC, compared to running such mini-batches in parallel. Completing steps 1-2-3 above would then be roughly equivalent to one synchronized PS type implementation step (distribute model to workers, get P gradients back, update model.)\n\n It would be really helpful to see how this compares in practice. It's hard for me to understand intuitively why the proposed method is theoretically any better than parallel SGD (except for the issue of non-smooth function optimization); the decoupling also can fundamentally change the problem since you're not doing back-propagation directly anymore, so that seems like it would conflate things as well and it's not necessarily going to just work for other types of architectures.","model":"human","source":"peerread","label":0,"id":5012}
{"text":"Summary\n===\nThis paper presents tic-tac-toe as toy problem for investigating CNNs.\nA dataset is created containing tic-tac-toe boards where one player is one\nmove away from winning and a CNN is trained to label boards according\nto (1) the player who can win (2 choices) and (2) the position they may move\nto win (9 choices), resulting in 18 labels. The CNN evaluated in this paper\nperforms perfectly at the task and the paper's goal is to inspect how the\nCNN works.\n\nThe fundamental mechanism for this inspection is Class Activation\nMapping (CAM) (Zhou et. al. 2016), which identifies regions of implicit attention\nin the CNN. These implicit attention maps (localization heat maps) are used to\nderive actions (which square each player should move). The attention maps  \n\n(1) attend to squares in the tic-tac-toe board rather than arbitrary\nblobs, despite the fact that one square in a board has uniform color, and\n\n(2) they can be used to pick correct (winning) actions.\n\nThis experiment are used to support assertions that the network understands\n(1) chess (tic-tac-toe) boards\n(2) a rule for winning tic-tac-toe\n(3) that there are two players.\n\nSome follow up experiments indicate similar results under various renderings\nof the tic-tac-toe boards and an incomplete training regime.\n\n\nMore Clarifying Questions\n===\n\n* I am not quite sure precisely how CAM is implemented here. In the original CAM\none must identify a class of interest to visualize (e.g., cat or dog). I don't\nthink this paper identifies such a choice. How is one of the 18 possible classes\nchosen for creating the CAM visualization and through that visualization\nchoosing an action?\n\n* How was the test set for this dataset for the table 1 results created?\nHow many of the final 1029 states were used for test and was the\ndistribution of labels the same in train and test?\n\n* How is RCO computed? Is rank correlation or Pearson correlation used?\nIf Pearson correlation is used then it may be good to consider rank correlation,\nas argued in \"Human Attention in Visual Question Answering: Do Humans and\nDeep Networks Look at the Same Regions?\" by Das et. al. in EMNLP 2016.\nIn table 1, what does the 10^3 next to RCO mean?\n\n\nPros\n===\n\n* The proposed method, deriving an action to take from the result of a\nvisualization technique, is very novel.\n\n* This paper provides an experiment that clearly shows a CNN relying on context\nto make accurate predictions.\n\n* The use of a toy tic-tac-toe domain to study attention in CNNs\n(implicit or otherwise) is a potentially fruitful setting that may\nlead to better understanding of implicit and maybe explicit attention mechanisms.\n\n\nCons\n===\n\n* This work distinguishes between predictions about \"what will happen\"\n(will the white player win?) and \"what to do\" (where should the white\nplayer move to win?). The central idea is generalization from \"what will happen\"\nto \"what to do\" indicates concept learning (sec. 2.1). Why should an ability to\nact be any more indicative of a learned concept than an ability to predict\nfuture states. I see a further issue with the presentation of this approach and\na potential correctness problem:\n\n1. (correctness)\nIn the specific setting proposed I see no difference between \"what to do\"\nand \"what will happen.\"\n\nSuppose one created labels dictating \"what to do\" for each example in the\nproposed dataset. How would these differ from the labels of \"what will happen\"\nin the proposed dataset? In this case \"what will happen\" labels include\nboth player identity (who wins) and board position (which position they move\nto win). Wouldn't the \"what to do\" labels need to indicate board position?\nThey could also chosen to indicate player identity, which would make them\nidentical to the \"what will happen\" labels (both 18-way softmaxes).\n\n2. (presentation)\nI think this distinction would usually be handled by the Reinforcement Learning\nframework, but the proposed method is not presented in that framework or\nrelated to an RL based approach. In RL \"what will happen\" is the reward an\nagent will receive for making a particular action and \"what to do\" is the\naction an agent should take. From this point of view, generalization from\n\"what will happen\" to \"what to do\" is not a novel thing to study.\n\nAlternate models include:\n    * A deep Q network (Mnih. et. al. 2015) could predict the value of\n      every possible action where an action is a (player, board position) tuple.\n    * The argmax of the current model's softmax could be used as an action\n      prediction.\nThe deep Q network approach need not be implemented, but differences between\nmethods should be explained because of the uniqueness of the proposed approach.\n\n\n* Comparison to work that uses visualization to investigate deep RL networks\nis missing. In particular, other work in RL has used Simonyan et. al.\n(arXiv 2013) style saliency maps to investigate network behavior. For example, \n\"Dueling Network Architectures for Deep Reinforcement Learning\" by Wang et. al.\nin (ICML 2016) uses saliency maps to identify differences between their\nstate-value and advantage networks. In \"Graying the black box:\nUnderstanding DQNs\" by Zahavy et. al. (ICML 2016) these saliency maps are\nalso used to analyze network behavior.\n\n\n* In section 2.3, saliency maps of Simonyan et. al. are said to not be able to\nactivate on grid squares because they have constant intensity, yet no empirical\nor theoretical evidence is provided for this claim.\n\nOn a related note, what precisely is the notion of information referenced in\nsection 2.3 and why is it relevant? Is it entropy of the distribution of pixel\nintensities in a patch? To me it seems that any measure which depends only\non one patch is irrelevant because the methods discussed (e.g., saliency maps)\ndepend on context as well as the intensities within a patch.\n\n\n* The presentation in the paper would be improved if the results in section 7\nwere presented along with relevant discussion in preceding sections.\n\n\nOverall Evaluation\n===\nThe experiments presented here are novel, but I am not sure they are very\nsignificant or offer clear conclusions. The methods and goals are not presented\nclearly and lack the broader relevant context mentioned above. Furthermore, I\nfind the lines of thought mentioned in the Cons section possibly incorrect\nor incomplete. As detailed with further clarifying questions, upon closer\ninspection I do not see how some aspects of the proposed approach were\nimplemented, so my opinion may change with further details.","model":"human","source":"peerread","label":0,"id":5013}
{"text":"Game of tic-tac-toe is considered. 1029 tic-tac-toe board combinations are chosen so that a single move will result into victory of either the black or the white player. There are 18 possible moves - 2 players x 9 locations. A CNN is trained from a visual rendering of the game board to these 18 possible outputs. CAM technique is used to visualize the salient regions in the inputs responsible for the prediction that CNN makes. Authors find that predictions correspond to the winning board locations. \n\nAuthors claim that this:\n1. is a very interesting finding. \n2. CNN has figured out game rules. \n3. Cross modal supervision is applicable to higher-level semantics. \n\nI don't think (2) be can be claimed because the knowledge of game rules is not tested by any experiment. There is only \"one\" stage of a game - i.e. last move that is considered. Further, the results are on the training set itself - the bare minimum requirement of any implicit or explicit representation of game rules is the ability to act in previously unseen states (i.e. generalization). Even if the CNN did generalize, I would avoid making any claims about knowledge of game rules. \n\nFor (3), author's definition of cross-modal seems to be training from images to games moves. In image-classification we go from images --> labels (i.e. between two different domains). We already know CNNs can perform such mappings. CNNs have been used to map images to actions such as in DQN my Mnih et al., or DDPG by Lillicrap et al. and a lot of other classical work such as ALVIN. It's unclear what points authors are trying to make. \n\nFor (1): how interesting is an implicit attention mechanism is a subjective matter. The authors claim a difference between the concepts of \"what do do\" and \"what will happen\". They claim by supervising for \"what will happen\", the CNN can automatically learn about \"what to do\". This is extensively studied in the model predictive control literature. Where model is \"what will happen next\", and the model is used to infer a control law - \"what to do\". However, in the experimental setup presented in the paper what will happen and what to do seem to be the exact same things. \n\nFor further analysis of what the CNN has learnt I would recommend:\n(a) Visualizing CAM with respect to incorrect classes. For eg, visualize the CAM with respect to player would lose (instead of winning).\n\n(b) Split the data into train\/val and use the predictions on the val-set for visualization. These would be much more informative about what kind of \"generalizable\" features the CNN pays attention to. \n\nIn summary, understanding why CNN's make what decisions they make is a very interesting area of research. While the emergence of an implicit attention mechanism may be considered to be an interesting finding by some, many claims made by the authors are not supported by experiments (see comments above). \n\n\n \n\n\n\n","model":"human","source":"peerread","label":0,"id":5014}
{"text":"1029 tic-tac-toe boards are rendered (in various ways). These 1029 boards are legal boards where the next legal play can end the game. There are 18 categories of such boards -- 9 for the different locations of the next play, and 2 for the color of the next play. The supervision is basically saying \"If you place a black square in the middle right, black will win\" or \"if you place a white square in the upper left, white will win\". A CNN is trained to predict these 18 categories and can do so with 100% accuracy.\n\nThe focus of the paper is using Zhou et al's Class Activation Mapping to show where the CNN focuses when making it's decision. As I understand it, an input to CAM is the class of interest. So let's say it is class 1 (black wins with a play to the bottom right square, if I've deciphered figure 2 correctly. Figure 2 should really be more clear about what each class is). So we ask CAM to determine the area of focus of the CNN for deciding whether class 1 is exhibited. The focus ends up being on the empty bottom right square (because certainly you can't exhibit class 1 if the bottom right square is occupied). The CNN also needs to condition its decision on other parts of the board -- it needs to know whether there will be 3 in a row from some direction. But maybe that conditioning is weaker?\n\nThat's kind of interesting but I'm not sure about the deeper statements about discovering game rules that the paper hints at. I'm also not sure about the connection of this work to weakly supervised learning or multi-modal learning.\n\nThe paper is pretty well written, overall, with some grammatical mistakes, but I simply don't see the surprising discovery of this work. \n\nI also have some concerns about how contrived this scenario is -- using a big, expressive CNN for such a simple game domain and using a particular CNN visualization method.\n\nI am not an expert in reinforcement learning (which isn't happening in this paper, but is in related works on CNN game playing), so maybe I'm not appreciating the paper appropriately.","model":"human","source":"peerread","label":0,"id":5015}
{"text":"SUMMARY \nThis paper studies the expressive power of deep neural networks under various related measures of expressivity. \nIt discusses how these measures relate to the `trajectory length', which is shown to depend exponentially on the depth of the network, in expectation (at least experimentally, at an intuitive level, or theoretically under certain assumptions). \nThe paper also emphasises the importance of the weights in the earlier layers of the network, as these have a larger influence on the represented classes of functions, and demonstrates this in an experimental setting. \n\nPROS \nThe paper further advances on topics related to the expressive power of feedforward neural networks with piecewise linear activation functions, in particular elaborating on the relations between various points of view. \n\nCONS \nThe paper further advances and elaborates on interesting topics, but to my appraisal it does not contribute significantly new aspects to the discussion. \n\nCOMMENTS\n- The paper is a bit long (especially the appendix) and seems to have been written a bit in a rush. \nOverall the main points are presented clearly, but the results and conclusions could be clearer about the assumptions \/ experimental vs theoretical nature. \nThe connection to previous works could also be clearer. \n\n- On page 2 one finds the statement ``Furthermore, architectures are often compared via \u2018hardcoded\u2019 weight values -- a specific function that can be represented efficiently by one architecture is shown to only be inefficiently approximated by another.'' \n\nThis is partially true, but it neglects important parts of the discussion conducted in the cited papers. \nIn particular, the paper [Montufar, Pascanu, Cho, Bengio 2014] discusses not one hard coded function, but classes of functions with a given number of linear regions. \nThat paper shows that deep networks generically* produce functions with at least a given number of linear regions, while shallow networks never do. \n* Generically meaning that, after fixing the number of parameters, any function represented by the network, for parameter values form an open, positive -measure, neighbourhood, belongs to the class of functions which have at least a certain number of linear regions. \nIn particular, such statements can be directly interpreted in terms of networks with random weights. \n\n- One of the measures for expressivity discussed in the present paper is the number of Dichotomies. In statistical learning theory, this notion is used to define the VC-dimension. In that context, a high value is associated with a high statistical complexity, meaning that picking a good hypothesis requires more data. \n\n- On page 2 one finds the statement ``We discover and prove the underlying reason for this \u2013 all three measures are directly proportional to a fourth quantity, trajectory length.'' \nThe expected trajectory length increasing exponentially with depth can be interpreted as the increase (or decrease) in the scale by a composition of the form a*...*a x, which scales the inputs by a^d. Such a scaling by itself certainly is not an underlying cause for an increase in the number of dichotomies or activation patterns or transitions. Here it seems that at least the assumptions on the considered types of trajectories also play an important role. \nThis is probably related to another observation from page 4: ``if the variance of the bias is comparatively too large... then we no longer see exponential growth.''\n\nOTHER SPECIFIC COMMENTS \nIn Theorem 1 \n- Here it would be good to be more specific about ``random neural network'', i.e., fixed connectivity structure with random weights, and also about the kind of one-dimensional trajectory, i.e., finite in length, closed, differentiable almost everywhere, etc. \n\n- The notation ``g \\geq O(f)'' used in the theorem reads literally as |g| \\geq \\leq k |f| for some k>0, for large enough arguments. It could also be read as g being not smaller than some function that is bounded above by f, which holds for instance whenever g\\geq 0. \nFor expressing asymptotic lower bounds one can use the notation \\Omega (see","model":"human","source":"peerread","label":0,"id":5016}
{"text":"While the reviewers saw some value in your contribution, there were also serious issues, so the paper does not reach the acceptance threshold.","model":"human","source":"peerread","label":0,"id":5017}
{"text":"This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks.\n\nRandom networks (deep networks with random Gaussian weights, hard tanh or ReLU activation) are studied according to several criterions: number of neutron transitions, activation patterns, dichotomies and trajectory length.\n\nThere doesn't seem to be a solid justification for why the newly introduced measures of expressivity really measure expressivity.\nFor instance the trajectory length seems a very discutable measure of expressivity. The only justification given for why it should be a good measure of expressivity is proportionality with other measures of expressivity in the specific case of random networks.\n\nThe paper is too obscure and too long. The work may have some interesting ideas but it does not seem to be properly replaced in context.\n\nSome findings seem trivial.\n\ndetailed comments\n\np2 \n\n\"Much of the work examining achievable functions relies on unrealistic architectural assumptions such as layers being exponentially wide\"\n\nI don\u2019t think so. In \"Deep Belief Networks are Compact Universal Approximators\" by Leroux et al., proof is given that deep but narrow feed-forward neural networks with sigmoidal units can represent any Boolean expression i.e. A neural network with 2n\u22121 + 1 layers of n units (with n the number of input neutron).\n\n\u201cComparing architectures in such a fashion limits the generality of the conclusions\u201d\n\nTo my knowledge much of the previous work has focused on mathematical proof, and has led to very general conclusions on the representative power of deep networks (one example being Leroux et al again).\n\nIt is much harder to generalise the approach you propose, based on random networks which are not used in practice.\n\n\u201c[we study] a family of networks arising in practice: the behaviour of networks after random initialisation\u201d\n\nThese networks arise in practice as an intermediate step that is not used to perform computations; this means that the representative power of such intermediate networks is a priori irrelevant. You would need to justify why it is not.\n\n\u201cresults on random networks provide natural baselines to compare trained networks with\u201d\n\nrandom networks are not \u201cnatural\u201d for the study of expressivity of deep networks. It is not clear how the representative power of random networks (what kind of random networks seems an important question here) is linked to the representative power of (i) of the whole class of networks or (ii) the class of networks after training. Those two classes of networks are the ones we would a priori care about and you would need to justify why the study of random networks helps in understanding either (i) or (ii).\n\np5\n\n\u201cAs FW is a random neural network [\u2026] it would suggest that points far enough away from each other would have independent signs, i.e. a direct proportionality between the length of z(n)(t) and the number of times it crosses the decision boundary.\u201d\n\nAs you say, it seems that proportionality of the two measures depends on the network being random. This seems to invalidate generalisation to other networks, i.e. if the networks are not random, one would assume that path lengths are not proportional.\n\np6\n\nthe expressivity w.r.t. remaining depth seems a trivial concerns, completely equivalent to the expressivity w.r.t. depth. This makes the remark in figure 5 that the number of achievable dichotomies only depends *only* on the number of layers above the layer swept seem trivial\n\np7\n\nin figure 6 a network width of 100 for MNIST seems much too small. Accordingly performance is very poor and it is difficult to generalise the results to relevant situations.\n","model":"human","source":"peerread","label":0,"id":5018}
{"text":"SUMMARY \nThis paper studies the expressive power of deep neural networks under various related measures of expressivity. \nIt discusses how these measures relate to the `trajectory length', which is shown to depend exponentially on the depth of the network, in expectation (at least experimentally, at an intuitive level, or theoretically under certain assumptions). \nThe paper also emphasises the importance of the weights in the earlier layers of the network, as these have a larger influence on the represented classes of functions, and demonstrates this in an experimental setting. \n\nPROS \nThe paper further advances on topics related to the expressive power of feedforward neural networks with piecewise linear activation functions, in particular elaborating on the relations between various points of view. \n\nCONS \nThe paper further advances and elaborates on interesting topics, but to my appraisal it does not contribute significantly new aspects to the discussion. \n\nCOMMENTS\n- The paper is a bit long (especially the appendix) and seems to have been written a bit in a rush. \nOverall the main points are presented clearly, but the results and conclusions could be clearer about the assumptions \/ experimental vs theoretical nature. \nThe connection to previous works could also be clearer. \n\n- On page 2 one finds the statement ``Furthermore, architectures are often compared via \u2018hardcoded\u2019 weight values -- a specific function that can be represented efficiently by one architecture is shown to only be inefficiently approximated by another.'' \n\nThis is partially true, but it neglects important parts of the discussion conducted in the cited papers. \nIn particular, the paper [Montufar, Pascanu, Cho, Bengio 2014] discusses not one hard coded function, but classes of functions with a given number of linear regions. \nThat paper shows that deep networks generically* produce functions with at least a given number of linear regions, while shallow networks never do. \n* Generically meaning that, after fixing the number of parameters, any function represented by the network, for parameter values form an open, positive -measure, neighbourhood, belongs to the class of functions which have at least a certain number of linear regions. \nIn particular, such statements can be directly interpreted in terms of networks with random weights. \n\n- One of the measures for expressivity discussed in the present paper is the number of Dichotomies. In statistical learning theory, this notion is used to define the VC-dimension. In that context, a high value is associated with a high statistical complexity, meaning that picking a good hypothesis requires more data. \n\n- On page 2 one finds the statement ``We discover and prove the underlying reason for this \u2013 all three measures are directly proportional to a fourth quantity, trajectory length.'' \nThe expected trajectory length increasing exponentially with depth can be interpreted as the increase (or decrease) in the scale by a composition of the form a*...*a x, which scales the inputs by a^d. Such a scaling by itself certainly is not an underlying cause for an increase in the number of dichotomies or activation patterns or transitions. Here it seems that at least the assumptions on the considered types of trajectories also play an important role. \nThis is probably related to another observation from page 4: ``if the variance of the bias is comparatively too large... then we no longer see exponential growth.''\n\nOTHER SPECIFIC COMMENTS \nIn Theorem 1 \n- Here it would be good to be more specific about ``random neural network'', i.e., fixed connectivity structure with random weights, and also about the kind of one-dimensional trajectory, i.e., finite in length, closed, differentiable almost everywhere, etc. \n\n- The notation ``g \\geq O(f)'' used in the theorem reads literally as |g| \\geq \\leq k |f| for some k>0, for large enough arguments. It could also be read as g being not smaller than some function that is bounded above by f, which holds for instance whenever g\\geq 0. \nFor expressing asymptotic lower bounds one can use the notation \\Omega (see ","model":"human","source":"peerread","label":0,"id":5019}
{"text":"Thank you for the review! We will take the comments into account and endeavour to make the text even clearer.\n\nA quick comment about motivation: our goal in this work improve interpretability in deep neural networks through a better understanding of neural network expressivity. In particular, we look at different \"diagnostics\" (transitions\/activation patterns\/dichotomies) for measuring the expressiveness of different neural network architectures, and their practical consequences. The surprising fact that three natural measures of expressiveness are related by *direct* proportion (see below) to trajectory length suggests consequences on remaining depth (earlier parameters are more important to fit the final function) and a trade off between expressivity and stability during training due to initialization choices.  \n\nResponses inline to other specific comments below:\n\nTrajectory Length: We will add this as a definition before Theorem 1. We take a 1-d trajectory to be a 1-d curve in the high dimensional space, and we measure the length  -- ","model":"human","source":"peerread","label":0,"id":5020}
{"text":"Summary of the paper:\n\nAuthors study in this paper quantities related to the expressivity of neural networks.The analysis is done for a random network. authors define the \u2018trajectory length\u2019 of a one dimensional trajectory as the length of the trajectory as the points (in a m- dimensional space) are embedded by layers of the network. They provide growth factors as function of hidden units k, and number of layers d.  the growth factor is exponential in the number of layers. Authors relates this trajectory length to authors quantities : \u2018transitions\u2019,\u2019activation patterns \u2019 and \u2018Dichotomies\u2019. \nAs a consequence of this study authors suggest that training only  earlier layers in the network  leads higher accuracy then just training later layers. Experiments are presented on MNIST and CIFAR10.\n\nClarity:\n\nThe  paper is a little hard to follow, since  the motivations are not clear in the introduction and the definitions across the paper are not clear. \n\nNovelty:\n\nStudying the trajectory length as function of transforming the data by a multilayer network is   new and interesting idea. The relation to transition numbers is in term of the growth factor, and not as a quantity to quantity relationship. Hence it is hard to understand what are the implications.\n\nSignificance:\n\nThe geometry of the input set (of dimension m)  shows up only weakly in the activation patterns analysis.  The trajectory study should tell us how the network organizes the input set. As observed in the experiments the network becomes contractive\/selective as we train the network. It would be interesting to study those phenomenas using this trajectory length , as a measure for disentangling nuisance factors ( such as invariances etc.). In the supervised setting the network need not to be contractive every where , so it needs to be selective to the class label, a  theoretical study of the selectivity and contraction using the trajectory length would be more appealing.\n\nDetailed comments:\n\nTheorem 1:\n\n- As raised by reviewer one the definition of a one dimensional input trajectory is missing. \n- What does theorem 1 tells us about the design and the architecture to use in neural networks as promised in the introduction is not clear. The connection to transitions in Theorem 2 is rather weak. \n\nTheorem 2:\n\n- in the proof of theorem 2 it not clear what is meant by T and t. Notations are confusing, the expectation is taken with respect to which weight: is it W_{d+1} or (W_{d+1} and W_{d})? I understand you don't want to overload notation but maybe E_{d+1} can help keeping track. I don't see how the recursion is applied if T and t in it, have different definitions. seems T_{d+1} for you is a random variable and t_{d} is fixed. Are you fixing W_d and then looking at W_{d+1} as  random?\n\n- In the same proof:  the recursion  is for d>1  ? your analysis is for W \\in R^{k\\times k}, you don't not study the W \\in \\mathbb{R}^{k\\times m}. In this case you can not assume assume that |z^(0)|=1.\n\n- should d=1, be analyzed alone to know how it scales with m?\n\nTheorem 4 in main text:\n\n- Is the proof missing? or Theorem 4 in the main text is Theorem 6 in the appendix?\n\nFigures 8 and 9:\n\n- the trajectory length reduction in the training isn't that just the network becoming contractive to enable mapping the training points to the labels? See for instance  on contraction in deep networks ","model":"human","source":"peerread","label":0,"id":5021}
{"text":"Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct font type for your submission to be considered. Thank you!","model":"human","source":"peerread","label":0,"id":5022}
{"text":"The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short:\n\n1. Exactly one algorithm is shown for the deep learning example. It would have been more convincing to compare distributions with one or more algorithms. \n\n2. The definition (1),  and much of the work of Section 2.1 seems to have already been covered in Deift (2014), Section 1.3. In that paper, a number of different algorithms for the solution of linear systems are considered, and then the concept of universality becomes more plausible. I do not see enough of such algorithmic comparisons in this paper (same problem setup, different algorithms).\n\n3.  It seems to me that what practitioners might care about in practice are both the mean and variance in running times; these quantities are buried in (1). So I question how useful the distribution itself might be for algorithm tuning. \n\nAt the least, many more empirical comparisons should be provided to convince me that the universality holds across a broad range of algorithms.","model":"human","source":"peerread","label":0,"id":5023}
{"text":"My overall conclusion is that the paper needs more work to be sufficiently convincing. I think the reviews are sufficiently careful. My recommendation is based on the fact that none of the reviewers supports acceptance.","model":"human","source":"peerread","label":0,"id":5024}
{"text":"The authors explore whether the halting time distributions for various algorithms in various settings exhibit \"universality\", i.e. after rescaling to zero mean and unit variance, the distribution does not depend on stopping parameter, dimensionality and ensemble.\n\nThe idea of the described universality is very interesting. However I see several shortcomings in the paper:\n\nIn order to be of practical relevance, the actual stopping time might be more relevant than the scaled one. The discussion of exponential tailed halting time distributions is a good start, but I am not sure how often this might be actually helpful. Still, the findings in the paper might be interesting from a theoretical point of view.\n\nEspecially for ICLR, I think it would have been more interesting to look into comparisons between stochastic gradient descent, momentum, ADAM etc on different deep learning architectures. Over which of those parameters does universality hold?. How can different initializations influence the halting time distribution? I would expect a sensible initialization to cut of part of the right tail of the distribution.\n\nAdditionally, I found the paper quite hard to read. Here are some clarity issues:\n\n- abstract: \"even when the input is changed drastically\": From the abstract I'm not sure what \"input\" refers to, here\n- I. Introduction: \"where the stopping condition is, essentially, the time to find the minimum\": this doesn't seem to make sense, a condition is not a time. I guess the authors wanted to say that the stopping condition is that the minimum has been reached?\n- I.1 the notions of dimension N, epsilon and ensemble E are introduced without any clarification what they are. From the later parts of the paper I got some ideas and examples, but here it is very hard to understand what these parameters should be (just some examples would be already helpful)\n- I.3 \"We use x^\\ell for \\ell \\in Z=\\{1, \\dots, S\\} where Z is a random sample from of training samples\" This formulation doesn't make sense. Either Z is a random sample, or Z={1, ..., S}.\n- II.1 it took me a long time to find the meaning of M. As this parameter seems to be crucial for universality in this case, it would be very helpful to point out more explicitly what it refers to.\n","model":"human","source":"peerread","label":0,"id":5025}
{"text":"Summary\n\n\nFor several algorithms, previous research has shown that the halting time follows a two-parameter distribution (the so-called universal property investigated by the authors). In this work, the authors extend the investigation to new algorithms (spin-glass, gradient descent in deep learning).\n\nAn algorithm is considered to satisfy the universality property when the centered\/scaled halting time fluctuations (empirical distribution of halting times) depend on the algorithm but do not depend on the target accuracy epsilon, an intrinsic measure of dimension N, the probability distribution\/random ensemble. (This is clear from Eq 1 where on the left the empirical halting time distribution depends on epsilon, N, A, E and on the right, the approximation only depends on the algorithm)\n\nThe authors argue that empirically, the universal property is observed when both algorithms (spin glass and deep learning) perform well and that it is not observed when they do not perform well.\n\nA moment-based indicator is introduced to assess whether universality is observed.\n\n\nReview\n\n\nThis paper presents several problems.\n\n\n\npage 2: \u201c[\u2026] for sufficiently large N and eps = eps(N)\u201d\n\nThe dependence of epsilon on N is troubling.\n\n\n\npage 3: \u201cUniversality is a measure of stability in an algorithm [\u2026] For example [\u2026] halting time for the power method [\u2026] has infinite expectation and hence this type of universality is *not* present. One could use this to conclude that the power method is naive. Therefore the presence of universality is a desirable feature of a numerical method\u201d\n\nNo. An algorithm is naive if there are better ways to answer the problem. One could not conclude from a halting time with infinite expectation (e.g. solving a problem extremely quickly 99% of the time, and looping forever in 1% of cases) or infinite variance, that the algorithm is naive.\u2028\nMoreover the universal property is more restrictive than having a finite halting time expectation. Even if in many specific cases, having a finite halting time expectation is a desirable property, showing that the presence of universality is desirable would require a demonstration that the other more restrictive aspects are also desirable.\u2028\nAlso, the paragraph only concerns one algorithm. why would the conclusions generalise to all numerical methods ?\u2028\nEven if the universality property is arguably desirable (i.e. event if the conclusion of this paragraph is assumed correct), the paragraph does not support the given conclusion.\n\n\n\nComparing Eq 1 and figures 2,3,4,5\u2028\nFrom Eq 1, universality means that the centered\/scaled halting time fluctuations (which depend on A, epsilon, N, E) can be approximated by a distribution that only depends on A (not on epsilon, N, E) but in the experiments only E varies (figures 2,3,4,5). The validity of the approximation with varying epsilon or N is never tested\n\n\n\nThe ensembles\/distributions parameter E (on which halting fluctuations should not depend) and the algorithm A (on which halting fluctuations are allowed to depend) are not well defined, especially w.r.t. the common use of the words. In the optimisation setting we are told that the functional form of the landscape function is part of A (in answer to the question of a reviewer) but what is part of the functional form ? what about computations where the landscape has no known functional form (black box) ?\n\n\n\nThe conclusion claims that the paper \u201cattempts to exhibit cases\u201d where one can answer 5 questions in a robust and quantitative way.\n\nQuestion 1: \u201cWhat are the conditions on the ensembles and the model that lead to such universality ?\u201d\u2028The only quantitative way would be to use the moments based indicator however there is only one example of universality not being observed which concerns only one algorithm (conjugate gradient) and one type of failure (when M = N). This does not demonstrate robustness of the method.\n\nQuestion 2: \u201cWhat constitutes a good set of hyper parameters for a given algorithm ?\u201d\nThe proposed way to choose would be to test whether universality is observed. If it is then the hyper parameters are good, if not the hyper parameters are bad. The correspondance between bad hyper-parameters and observing no universality concerns only one algorithm and one type of failure. Other algorithms may fail in the universal regime or perform well in the non universal regime. The paper does not show how to answer this question in a robust way.\n\nQuestion 3: \"How can we go beyond inspection when tuning a system ?\u2028\"\nThe question is too vague and general and there is probably no robust and quantitative way to answer it at all.\n\nQuestion 4: \"How can we infer if an algorithm is a good match to the system at hand ?\u2028\"\nThe paper fails to demonstrate convincingly that universality is either a good or robust way to approach the very few studied algorithms. The suggested generalisation to all systems and algorithms is extremely far fetched.\n\nQuestion 5: \"What is the connection between the universal regime and the structure of the landscape ?\"\n\u2028Same as before, the question is extremely vague and cannot be answered in a robust or quantitative way at all. The fact that what corresponds to A and what corresponds to E is not clear does not help.\n\n\nIn the conclusion it is written that the paper validates the claim that universality is present in all or nearly all sensible computation. It does not. The paper does not properly test whether universality is present (only 1 parameter in 3 that should not vary is tested). The paper does not properly test whether universality is lost when the computation is no longer sensible (only one failure case tested). Finally the experiments do not apply to all or nearly all computations but only to very few  specific algorithms.\n","model":"human","source":"peerread","label":0,"id":5026}
{"text":"It is an interesting paper. However, I would like to describe a hypothesis\/counter-example and I hope you will\neasily reject it. Otherwise, I will fail to see any universality in the discussed \"universality in halting time\".\n\nI would like to present a toy minimization problem where f(x) is defined in [0,1] and has two global\noptima in 0.0 and 1.0 respectively. f(x) has its maximum in 0.5 but f(x) decays somewhat differently to the \"left\"\nand to the \"right\" optima. These somewhat different decays (shapes of f(x)) impact the way some optimizer A minimizes\nf(x). My A can be e.g. a deterministic local search algorithm such that if it is initialized in the basin of attraction [0,0.5], it will stay\/search there. Thus, the starting position will determine in which global optima it will end up. To observe the halting distribution, I run A starting from different starting positions generated uniformly at random in [0,1]. I expect the halting distribution to have two peaks: the first peak corresponds to a typical number of iterations required to reach some epsilon around the \"left\" optima, the second peak would correspond to the \"right\" optima. Question: Why there are two peaks and not one? Answer: Because, as mentioned before, the shapes\/decays of f(x) on the left and right side are different and this affects our algorithm A, i.e., the number of iterations to reach some epsilon precision. Question: Is this a problem to have two peaks in the halting distribution not like the ones shown in figures? Answer: No, it is perfectly fine. What is not fine is that when I run the same algorithm A on another problem which is unimodal, then I will get a completely different halting distribution, e.g., like the \"one peak\" ones shown in the paper. Question: Where is the problem? Answer: The problem is that these observations contradict the discussed universality in halting time distribution which is an intrinsic thing to A and does not depend on the problem. Question: This was stated for large dimension N and not one-dimensional problem. Answer: Increase the dimensionality of the toy problem preserving the idea of the two (multiple) optima with, e.g., basins of attraction of equal size (keep in mind the  course of dimensionality). Question: There is a restriction on the class of ensembles E. Answer: If we restrict our-self to unimodal problems then I barely see \"universality\". Moreover, unimodal non-convex problems may exhibit the same properties as multi-modal ones, e.g., make a horizontal one-dimensional slice of 2-dimensional Rosenbrock function. Question: Why we see these nice-looking distribution for very different problems? Answer: because for some problems the multi-modality is negligeable and the dynamical system behind the optimizer may behave similarly for different values of N, especially for the large ones. Nevertheless, to claim it to be the \"universal law\" is misleading since it depends on the properties of the problem at hand and is not strictly intrinsic to the optimizer. Moreover, for the same (class of) problem the landscape may qualitatively change with increasing N. Rosenbrock function is a good example, it is uni-modal for 2 variables and multi-modal for multiple variables which would affect the halting time distribution. The \"threshold\" dimension value 2 can be turned to 2 billion if needed.","model":"human","source":"peerread","label":0,"id":5027}
{"text":"The paper presents a layer architecture where a single parameter is used to  gate the output response of layer to amplify or suppress it. It is shown that such an architecture can ease optimization of a deep network as it is easy to learn identity mappings in layers helping in better gradient propagation to lower layers (better supervision). \n\nUsing an introduced SDI metric it shown that gated residual networks can most easily learn identity mappings compared to other architectures. \n\nAlthough good theoretical reasoning is presented the observed experimental evidence of learned k values does not seem to strongly support the theory given that learned  k values are mostly very small and not varying much across layers. Also, experimental validation of the approach is not quite strong in terms of reported performances and number of large scale experiments.","model":"human","source":"peerread","label":0,"id":5028}
{"text":"Although this was a borderline paper, the reviewers ultimately concluded that, given how easy it would be for a practitioner to independently devise the methodological trick of the paper, the paper did not demonstrate that the idea was sufficiently useful to merit acceptance.","model":"human","source":"peerread","label":0,"id":5029}
{"text":"We have also added a section (3.3) that connects our technique to the Unrolled Iterative Estimation interpretation of Highway and ResNets. We further elaborate on how the learned k values are indicators of the abstraction jumps in representations defined in Greff et al, and analyze the k values for both experiments (MNIST and CIFAR) under this perspective.","model":"human","source":"peerread","label":0,"id":5030}
{"text":"In the last revision of our work, we have performed the following main changes:\n\n- Added 4 extra experiments comparing Wide ResNet and their augmented counterparts (Table 5) on CIFAR-100. With this we hope to provide stronger indications of the generality and advantages of our technique.\n\nWe are currently running Gated PlainNets on CIFAR-100.\n\nWe thank all the received feedback.","model":"human","source":"peerread","label":0,"id":5031}
{"text":"In the last revision of our work, we have performed the following main changes:\n\n- Added 6 extra experiments comparing Wide ResNet and their augmented counterparts (Table 4). With this we hope to provide stronger indications of the generality and advantages of our technique.\n\n\nWe are currently running the same 6 models on CIFAR-100 to add them to the table. Are have also run Gated PlainNets (u = g(k)f(x) + (1 - g(k))x) on CIFAR-10, as suggested by reviewer 3, and will add results after running on CIFAR-100 as well.\n\n\nWe thank all the received feedback.","model":"human","source":"peerread","label":0,"id":5032}
{"text":"In the last revision of our work, we have performed the following main changes:\n\n- Added results with u = g(k)f(x) + (1 - g(k))x (Gated Plain Networks), along with some intuitions of why it outperformed non-augmented Residual Networks. This result suggests that an extensive study on different gating mechanisms for Highway Neural Networks can be extremely fruitful, once the original design is equivalent to a Highway Net with scalar gates. This also goes against the suggestions in the literature not to add gates to shortcut connections in order to keep an uncorrupted gradient flow through the network.\n\n- Added Gated Plain Networks to the table with mean k values, along with an explanation for the significant difference when compared to mean k's of Gated ResNets.\n\n- Added 'Understanding deep learning requires rethinking generalization' to bibliography.\n\n- Rephrased a few parts when augmented models are compared to Highway Nets, showing more clearly the differences between the two designs. Also added a brief discussion regarding the impact for Highway Nets of the new results (Gated Plain Nets).\n\n\nWe are currently gathering results to introduce the following changes to the next revision:\n\n- Add results for more aggressive depths (200+ layers), in order to better compare different models.\n\n- Add results for Gated Plain Net on CIFAR.\n\nWe thank all the received feedback.","model":"human","source":"peerread","label":0,"id":5033}
{"text":"\nThis paper proposes a network called Gated Residual Networks layer design that adds gating to shortcut connections with a scalar to regulate the gate. The authors claim that this approach will improve the training Residual Networks.\n\nIt seems the authors could get competitive performance on CIFAR-10 to state of art models with only Wide Res Nets. Wide Gated ResNet requires much more parameters than DenseNet (and other Res Net variants) for obtaining a little improvement over Dense Net.  More importantly, the authors state that they obtained the best results on CIFAR-10 and CIFAR-100 but the updated version of DenseNet (Huang et al. (2016b)) has new results for a version called DenseNet-BC which outperforms all of the results that authors reported (3.46 for CIFAR-10 and 17.18 for CIFAR-100 with 25.6M parameters, DenseNet-BC still outperforms with 15.3M parameters which is much less that 36.5M). The Res Net variants papers with state of art results report result for Image Net. Therefore the empirical results need also the Image Net to demonstrate that improvement claimed is achieved.\n\nThe proposed trick adopts Highway Neural Networks and Residual Networks with an intuitive motivation. It is not sufficiently novel and the empirical results do not prove sufficient effectiveness of this incremental approach.\n\n","model":"human","source":"peerread","label":0,"id":5034}
{"text":"This paper proposes to learn a single scalar gating parameter instead of a full gating tensor in highway networks. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation.\n\nThe basic idea of the paper is simple and is clearly presented. It is a natural simplification of highway networks to allow easily \"shutting off\" layers while keeping number of additional parameters low. However, in this regard the paper leaves out a few key points. Firstly, it does not mention that the gates in highway networks are data-dependent which is potentially more powerful than learning a fixed gate for all units and independent of data. Secondly, it does not do a fair comparison with highway networks to show that this simpler formulation is indeed easier to learn.\n\nDid the authors try their original design of u = g(k)f(x) + (1 - g(k))x where f(x) is a plain layer instead of a residual layer? Based on the arguments made in the paper, this should work fine. Why wasn't it tested? If it doesn't work, are the arguments incorrect or incomplete?\n\nFor the MNIST experiments, since the hyperparameters are fixed, the plots are misleading if any dependence on hyperparameters exists for the different models. This experiment appears to be based on Srivastava et al (2015). If it is indeed designed to test optimization at aggressive depths, then apart from doing a hyperparameter search, the authors should not use regularization such as dropout or batch norm, which do not appear in the theoretical arguments for the architecture.\n\nFor CIFAR experiments, the obtained improvements compared to the baseline (wide resnets) are very small and therefore it is important to report the standard deviations (or all results) in both cases. It's not clear that the differences are significant.\n\nSome questions regarding g(): Was g() always ReLU? Doesn't this have potential problems with g(k) becoming 0 and never recovering? Does this also mean that for the wide resnet in Fig 7, most residual blocks are zeroed out since k < 0?","model":"human","source":"peerread","label":0,"id":5035}
{"text":"In the recent revisions of our work, we have performed the following main changes:\n\n- Run complex models on CIFAR-10 and on CIFAR-100, achieving 3.65% and 18.27% test error, respectively. We have then removed our belief that the technique could achieve SOTA results, since now we have empirical indications of it.\n- Added evaluation of the final k parameters for fully-connected and convolutional networks, showing interesting patterns for both architectures.\n- Added layer pruning experiment to Wide GResNet.\n- Made the intuition behind the distance to identity clearer.\n\nWe thank all the received feedback.","model":"human","source":"peerread","label":0,"id":5036}
{"text":"\n- \"assume that the squared parameter-wise distance between .. surrogate to the paths length\". Can you elaborate more on this? Why should this be the case? \n\n- Why do you substitute mean and variance with initialization mean variance in derivation in Eq 15 and 16?\n\n-  Regarding \"A comparison of the Total Squared Distance to Identity\u2026\": where is the comparison?\n\n- Why there is no Highway Neural Networks result in Table 5? \n\n- In Table 5, why does  He et al. (2015b)  give 6.61 which does not match with 6.43 in that paper? \n\n- Regarding \"Our results don\u2019t surpass the state-of-the-art, which was expected considering the hardware limitations. However, taking into account the improvement observed when augmenting a smaller Wide ResNet, we believe that the technique proposed can be used to surpass the state-of-the-art\", you may use that result for sanity check during the empirical work but how can you claim that you will get any improvement over baselines in final comparison? \n\n- Baselines have Imagenet results. In order to obtain satisfactory comparison, you would need that too. Are there any results on Imagenet comparison?\n","model":"human","source":"peerread","label":0,"id":5037}
{"text":"I was holding off on this review hoping to get the missing details from the code at","model":"human","source":"peerread","label":0,"id":5038}
{"text":"The area chair agrees with the reviewers that this paper is not ready for ICLR yet. There are significant issues with the writing, making it difficult to follow the technical details. Writing aside, the technique seems somewhat limited in its applicability. The authors also promised an updated version, but this version was never delivered (latest version is from Nov 13).","model":"human","source":"peerread","label":0,"id":5039}
{"text":"Dear authors,\n\ndo you plan to address the third reviewer's comments? Your responses could help bring some more clarity and improve the confidence for the final decision...\n\nThanks!","model":"human","source":"peerread","label":0,"id":5040}
{"text":"I was holding off on this review hoping to get the missing details from the code at ","model":"human","source":"peerread","label":0,"id":5041}
{"text":"The basic idea of this contribution is very nice and worth pursuing: how to use the powerful \u201cdivide and conquer\u201d algorithm design strategy to learn better programs for tasks such as sorting or planar convex hull. However, the execution of this idea is not convincing and needs polishing before acceptance. As it is right now, the paper has a proof-of-concept feel that makes it great for a workshop contribution.\n\nMy main concern is that the method presented is currently not easily applicable to other tasks. Typically, demonstrations of program induction from input-output examples on well known tasks serves the purpose of proving, that a generic learning machine is able to solve some well known tasks, and will be useful on other tasks due to its generality. This contribution, however, presents a learning machine that is very hand-tailored to the two chosen tasks. The paper essentially demonstrates that with enough engineering (hardcoding the recurrency structure, designing problem-specific rules of supervision at lower recurrency levels) one can get a partially trainable sorter or convex hull solver.\n\nI found the contribution relatively hard to understand. High level ideas are mixed with low-level tricks required to get the model to work and it is not clear either how the models operate, nor how much of them was actually learned, and how much was designed. The answer to the questions did hep, nut didn't make it into the paper. Mixing the descriptions of the tricks required to solve the two tasks makes things even more confusing. I believe that the paper would be much more accessible if instead of promising a general solution it clearly stated the challenges faced by the authors and the possible solutions.\n\nHighlights:\n+ Proof-of-concept of a partially-trainable implementation of the important \u201cdivide and conquer\u201d paradigm\n++ Explicit reasoning about complexity of induced programs\n- The solution isn\u2019t generic enough to be applicable to unknown problems - the networks require tricks specific to each problem\n- The writing style pictures the method as very general, but falls back on very low level details specific to each task\n","model":"human","source":"peerread","label":0,"id":5042}
{"text":"I find this paper extremely hard to read. The main promise of the paper is to train models for combinatorial search procedures, especially for dynamic programming to learn where to split and merge. The present methodology is supposed to make use of some form of scale invariance property which is scarcely motivated for most problems this approach should be relevant for. However, the general research direction is fruitful and important.\n\nThe paper would be much more readable if it would start with a clear, formal problem formulation, followed by some schematic view on the overall flow and description on which parts are supervised, which parts are not. Also a tabular form and sample of the various kinds problems solved by this method could be listed in the beginning as a motivation with some clear description on how they fit the central paradigm and motivate the rest of the paper in a more concrete manner.\n\nInstead, the paper is quite chaotic, switching between low-level and high level details, problem formulations and their solutions in a somewhat random, hard to parse order.\n\nBoth split and merge phases seem to make a lot of discrete choices in a hierarchical manner during training. The paper does not explain how those discrete choices are backpropagated through the network in an unbiased manner, if that is the case at all.\n\nIn general, the direction this paper is exciting, but the paper itself is a frustrating read in its present form. I have spent several hours on it without having to manage to achieve a clear mental image on how all the presented pieces fit together. I would revise my score if the paper would be improved greatly from a readability perspective, but I think it would require a major rewrite.","model":"human","source":"peerread","label":0,"id":5043}
{"text":"This work proposes to use visualization of gradients to further understand the importance of features (i.e. pixels) for visual classification. Overall, this presented visualizations are interesting, however, the approach is very ad hoc. The authors do not explain why visualizing regular gradients isn't correlated with the importance of features relevant to the given visual category and proceed to the interior gradient approach. \n\nOne particular question with regular gradients at features that form the spatial support of the visual class. Is it the case that the gradients of the features that are confident of the prediction remain low, while those with high uncertainty will have strong gradients?\n\nWith regards to the interior gradients, it is unclear how the scaling parameter \\alpha affects the feature importance and how it is related to attention.\n\nFinally, does this model use batch normalization?","model":"human","source":"peerread","label":0,"id":5044}
{"text":"This paper was reviewed by 3 experts. All 3 seem unconvinced of the contributions, point to several shortcomings, and recommend rejection. I see no basis for overturning their recommendation. To be clear, the problem of achieving insight into the inner workings of deep networks is of significant importance and I encourage the authors to use the feedback to improve the manuscript.","model":"human","source":"peerread","label":0,"id":5045}
{"text":"We added two new sections (2.5 and 2.6) to the paper. Section 2.5\nproposes two very desirable axioms for attribution methods,\nand uses them to rule out other attribution methods from the literature.\nSection 2.6 proposes a full axiomatization under which our method is\nunique.\n\nIt is possible that sections 2.3-2.7 may constitute a shorter 6-page\nself-contained paper with the title --- \"attributions using interior\ngradients\".\n","model":"human","source":"peerread","label":0,"id":5046}
{"text":"We thank the reviewers for a detailed review.  The rebuttal below addresses some of the mentioned concerns.\n\nRegarding \u201cfar too long\u201d and \u201cunnecessarily grandiose name for literally, a scaled image\u201d: \n\nWe\u2019d agree that the paper is long for the ideas in it. The length stems from the difficulty of not having a crisp evaluation technique for feature importance. So we try to resort to qualitative discussions together with images. But we  can definitely try to tighten the writing. We are open to changing the title of the paper to \u201cInterior Gradients\u201d or something like it, though it is worth noting that while scaling intensities seems natural for images, analogous scaling for Text or Drug Discovery models results in inputs that are more obviously fake, i.e., counterfactual.\n\nRegarding \u201chow the proposed scheme for feature importance ranking is useful\u201d: \n\nWhile debugging deep networks is hard in general, examining feature importance scores offers a limited but useful insight into the operation of the network on a particular input. For us, the experience with the Drug Discovery network where we found, via our attributions, that the bond features were severely underused (see Section 3.1) was a concrete instance of how feature importance analysis could help debug and improve networks. As we discussed in section 2.7, we do mention the limitations of our technique in understanding what the network does. The same pros and cons would seem to apply to other feature importance techniques (see Section 2.8). The key difference is that ours is much easier to implement--- as simple as computing a gradient.\n\nRegarding \u201cThe quantitative evidence is quite limited and most of the paper is spent on qualitative results\u201d: \n\nWe address with the following multipart response; apologies for the lengthy response.\n\nFirst, we do plan to produce a comparison with side by sides for LRP and our method for the MNIST data set over the next few weeks as a sanity check.\n\nHowever, we don\u2019t think that that there is a strong metric to compare different feature importance techniques. This is acknowledged by Samek et al in their 2015 ICML Visualization Workshop work. We elaborate on this further at the end of this rebuttal.  \n\nMethods like DeepLift and Layer-wise Relevance Propagation (LRP) break a fundamental axiom in our mind: the attributions depend on the implementation, i.e. two networks that implement identical input-output  relationships can have different attributions. This seems odd---see Section 2.4 and Figure 14.  Perhaps, we did not emphasize this enough in the paper. \n\nThe main focus for us in the evaluation conducted so far has been to ensure that our output was sensible. In Section 2.5, we discuss a combination of approaches that we used to assess the attributions, including eyeballing, localization, and ablations. We welcome you to visualize more attributions at: ","model":"human","source":"peerread","label":0,"id":5047}
{"text":"This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks.  The interior gradient is the gradient measured on a scaled version of the input.  The integrated gradient is the integral of interior gradients over all scaling factors.  Visualizations comparing integrated gradients with standard gradients on real images input to the Inception CNN show that integrated gradients correspond to an intuitive notion of feature importance.\n\nWhile motivation and qualitative examples are appealing, the paper lacks both qualitative and quantitative comparison to prior work.  Only the baseline (simply the standard gradient) is presented as reference for qualitative comparison.  Yet, the paper cites numerous other works (DeepLift, layer-wise relevance propagation, guided backpropagation) that all attack the same problem of feature importance.  Lack of comparison to any of these methods is a major weakness of the paper.  I do not believe it is fit for publication without such comparisons.  My pre-review question articulated this same concern and has not been answered.\n","model":"human","source":"peerread","label":0,"id":5048}
{"text":"The authors propose to measure \u201cfeature importance\u201d, or specifically, which pixels contribute most to a network\u2019s classification of an image. A simple (albeit not particularly effective) heuristic for measuring feature importance is to measure the gradients of the predicted class wrt each pixel in an input image I. This assigns a score to each pixel in I (that ranks how much the output prediction would change if a given pixel were to change). In this paper, the authors build on this and propose to measure feature importance by computing gradients of the output wrt scaled version of the input image, alpha*I, where alpha is a scalar between 0 and 1, then summing across all values of alpha to obtain their feature importance score. Here the scaling is simply linear scaling of the pixel values (alpha=0 is all black image, alpha=1 is original image). The authors call these scaled images \u201ccounterfactuals\u201d which seems like quite an unnecessarily grandiose name for literally, a scaled image. \n\nThe authors show a number of visualizations that indicate that the proposed feature importance score is more reasonable than just looking at gradients only with respect to the original image. They also show some quantitative evidence that the pixels highlighted by the proposed measure are more likely to fall on the objects rather than spurious parts of the image (in particular, see figure 5). The method is also applied to other types of networks. The quantitative evidence is quite limited and most of the paper is spent on qualitative results.\n\nWhile the goal of understanding deep networks is of key importance, it is not clear whether this paper really help elucidate much. The main interesting observation in this paper is that scaling an image by a small alpha (i.e. creating a faint image) places more \u201cimportance\u201d on pixels on the object related to the correct class prediction. Beyond that, the paper builds a bit on this, but no deeper insight is gained. The authors propose some hand-wavy explanation of why using small alpha (faint image) may force the network to focus on the object, but the argument is not convincing. It would have been interesting to try to probe a bit deeper here, but that may not be easy.\n\nUltimately, it is not clear how the proposed scheme for feature importance ranking is useful. First, it is still quite noisy and does not truly help understand what a deep net is doing on a particular image. Performing a single gradient descent step on an image (or on the collection of scaled versions of the image) hardly begins to probe the internal workings of a network. Moreover, as the authors admit, the scheme makes the assumption that each pixel is independent, which is clearly false.\n\nConsidering the paper presents a very simple idea, it is far too long. The main paper is 14 pages, up to 19 with references and appendix. In general the writing is long-winded and overly verbose. It detracted substantially from the paper. The authors also define unnecessary terminology. \u201cGradients of Coutnerfactuals\u201d sounds quite fancy, but is not very related to the ideas explored in the writing. I would encourage the authors to tighten up the writing and figures down to a more readable page length, and to more clearly spell out the ideas explored early on.","model":"human","source":"peerread","label":0,"id":5049}
{"text":"This paper is the first (I believe) to establish a simple yet important result that Convnets for NMT encoders can be competitive to RNNs. The authors present a convincing set of results over many translation tasks and compare with very competitive baselines. I also appreciate the detailed report on training and generation speed. I find it's very interesting when position embeddings turn out to be hugely important (beside residual connections); unfortunately, there is little analysis to shed more lights on this aspect and perhaps compare other ways of capturing positions (a wild guess might be to use embeddings that represent some form of relative positions). The only concern I have (similar to the other reviewer) is that this paper perhaps fits better in an NLP conference.\n\nOne minor comment: it's slight strange that this well-executed paper doesn't have a single figure on the proposed architecture :) It will also be even better to draw a figure for the biLSTM architecture as well (it does take some effort to understand the last paragraph in Section 2, especially the part on having a linear layer to compute z).","model":"human","source":"peerread","label":0,"id":5050}
{"text":"This work demonstrates architectural choices to make conv nets work for NMT. In general the reviewers liked the work and were convinced by the results but found the main contributions to be \"incremental\". \n \n Pros:\n - Clarity: The work was clearly presented, and besides for minor comments (diagrams) the reviewers understood the work\n - Quality: The experimental results were thorough, \"very extensive and leaves no doubt that the proposed approach works well\".\n \n Mixed:\n - Novelty: There is appreciation that the work is novel. However as the work is somewhat \"application-specific\" the reviewers felt the technical contribution was not an overwhelming contribution.\n - Impact: While some speed ups were shown, not all reviewers were convinced that the benefit was sufficient, or \"main speed-up factor(s)\" were. \n \n This work is clearly worthwhile, but the reviews place it slightly below the top papers in this area.","model":"human","source":"peerread","label":0,"id":5051}
{"text":"Authors, It would be great to have a rebuttal for this paper as reviewers will be discussing over the next week. Thanks.","model":"human","source":"peerread","label":0,"id":5052}
{"text":"The system described works comparably to bi-directional LSTM baseline for NMT, and CNN's are naturally parallelizable.\n\nKey ideas include the use of two stacked CNN's (one for each of encoding and decoding) for translation, with res connections and position embeddings. The use of CNN's for translation has been attempted previously (as described by the authors), but presumably it is the authors' combination of various architectural choices (attention, position embeddings, etc) that make the present system competitive with RNN's, whereas earlier attempts were not. They describe system's sensitivity to some of these choices (e.g. experiments to choose appropriate number of layers in each of the CNN's).\n\nThe experimental results are well reported in detail.\n\nOne or two figures would definitely be required to help clarify the architecture.\n\nThis paper is less about new ways of learning representations than about the combination of choices made (over the set of existing techniques) in order to get the good results that they do on the reported NMT tasks. In this respect, while I am fairly confident that the paper represents good work in machine learning, I am not quite as confident about its fit for this particular conference.","model":"human","source":"peerread","label":0,"id":5053}
{"text":"The paper reports a very clear and easy to understand result that a convolutional network can be used instead of the recurrent encoder for neural machine translation. \n\nApart from the known architectural elements, such as convolution, pooling, residual connections, position embeddings, the paper features one unexpected architectural twist: two stacks of convolutions, one for computing alignment and another for computing the representations.\nThe empirical evidence that this was necessary is provided, however the question of *why* it is necessary remains open. \n\nThe experimental evaluation is very extensive and leaves no doubt that the proposed approach works well. The convnet-based model was faster at evaluation, but it is not very clear what is the main speed-up factor. It\u2019s however hard to argue against the fact that the speed advantage of convnets is likely to increase if a more parallel implementation is considered. \n\nMy main concern is whether or not the paper is appropriate for ICLR, because the contribution is quite incremental and rather application-specific. ACL, EMNLP and other NLP conferences would be a better venue, I think. ","model":"human","source":"peerread","label":0,"id":5054}
{"text":"Hi,\n\nThe conditional input is denoted by both c_i and c_t in sections 2 and 3.1 respectively. c_i is reused in section 3.2\n","model":"human","source":"peerread","label":0,"id":5055}
{"text":"I want to draw attention to the fact that you compare tokenized BLEU results (with multi-bleu.perl) and detokenized BLEU results (with mteval-v13a.pl). The two should *never* be mixed in the same table, as the tokenization will have a big effect on results. Even when comparing systems that all have tokenized BLEU, and all use the Moses tokenizer, using different parameters (such as the \"-a\" option for aggressive hyphen splitting) will skew the results.\n\nDetokenized BLEU is standard for WMT, and reported by Sennrich et al. (2016a,b).\n\nI re-ran BLEU on our EN-RO system for comparison:\n\ndetokenized BLEU, mteval-v13a.pl: 28.1 BLEU\ntokenized BLEU, multi-bleu.perl: 29.4 BLEU\n\nyour reported result (multi-bleu.perl): 28.5","model":"human","source":"peerread","label":0,"id":5056}
{"text":"The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net.\nAlthough the equations suggest a weighting per example, dropping this weight (alpha_i) works equally well.\nThe proposed approach relates to Batch Norm and weight decay.\nExperiments are given on \"low-shot\" settting.\nThere seem to be two stories in the paper: feature penalty as a soft batch norm version, and low-shot learning; why is feature penalty specifically adapted to low-shot learning and not a more classical supervised task?\nRegarding your result on Omniglot, 91.5, I believe it is still about 2% worse than the Matching Networks, which you refer to but don't put in Table 1. Why?\nOverall, the idea is simple but feels like preliminary: while it is supposed to be a \"soft BN\", BN itself gets better performance than feature penalty, and both together give even better results. Is something still missing in the explanation?\n\n-- edits after revised version:\n\nThank you for adding more information to the paper. I feel it is still too long but hopefully you can reduce it to 9 pages as promised. However, I'm still not convinced the paper is ready to be accepted, mainly for the following reasons:\n- on Omniglot, the paper is still significantly far from the current state of the art.\n- the new experiments do not really confirm\/infirm the relationship with BN.\n- you added an explanation of why FP works for low-shot setting, by showing it controls the VC dimension and hence is good to control overfitting with a small number of training examples, but this discussion is basic and does not really shed more light than the obvious.\nI'm pushing up your score from 4 to 5 for the improved version, but I still think it is below acceptance level.","model":"human","source":"peerread","label":0,"id":5057}
{"text":"The paper extends a regularizer on the gradients recently proposed by Hariharan and Girshick. I agree with the reviewers that while the analysis is interesting, it is unclear why this particular regularizer is especially relevant for low-shot learning. And the experimental validation is not strong enough to warrant acceptance.","model":"human","source":"peerread","label":0,"id":5058}
{"text":"We would like to thank all reviewers for their valuable comments and suggestions. We modify our paper accordingly. \nEspecially, we add classification performance comparison between our methods and batch normalization.\nMoreover, we also add our preliminary analysis why the proposed regularizer could improve generalization performance, which is closely related to the improved \"low-shot\" improvement.\n\nAdmittedly, current version is a little bit beyond page-limit (11 pages now). We managed to include some new experimental results and analysis in our revised paper, and will trim it within 9 pages with some detailed deductions left in supplemental materials.","model":"human","source":"peerread","label":0,"id":5059}
{"text":"Summary\n===\nThis paper extends and analyzes the gradient regularizer of Hariharan and\nGirshick 2016. In that paper a regularizer was proposed which penalizes\ngradient magnitudes and it was shown to aid low-shot learning performance.\nThis work shows that the previous regularizer is equivalent to a direct penalty\non the magnitude of feature values weighted differently per example.\n\nThe analysis goes to to provide two examples where a feature penalty\nfavors a better representation. The first example addresses the XOR\nproblem, constructing a network where a feature penalty encourages\na representation where XOR is linearly separable.\nThe second example analyzes a 2 layer linear network, showing improved stability\nof a 2nd order optimizer when the feature penalty is added.\nOne last bit of analysis shows how this regularizer can be interpreted as\na Gaussian prior on both features and weights. Since the prior can be\ninterpreted as having a soft whitening effect, the feature regularizer\nis like a soft version of Batch Normalization.\n\nExperiments show small improvements on a synthetic XOR test set.\nOn the Omniglot dataset feature regularization is better than most baselines,\nbut is worse than Moment Matching Networks. An experiment on ImageNet similar\nto Hariharan and Girshick 2016 also shows effective low-shot learning.\n\n\nStrengths\n===\n\n* The core proposal is a simple modification of Hariharan and Girshick 2016.\n\n* The idea of feature regularization is analyzed from multiple angles\nboth theoretically and empirically.\n\n* The connection with Batch Normalization could have broader impact.\n\n\nWeaknesses\n===\n\n* In section 2 the gradient regularizer of Hariharan and Girshick is introduced.\nWhile introducing the concept, some concern is expressed about the motivation:\n\"And it is not very clear why small gradients on every sample produces\ngood generalization experimentally.\" This seems to be the central issue to me.\nThe paper details some related analysis, it does not offer a clear answer to\nthis problem.\n\n\n* The purpose and generality of section 2.1 is not clear.\n\nThe analysis provides a specific case (XOR with a non-standard architecture)\nwhere feature regularization intuitively helps learn a better representation.\nHowever, the intended take-away is not clear.\n\nThe take-away may be that since a feature penalty helps in this case it\nshould help in other cases. I am hesitant to buy that argument because of the\nspecific architecture used in this section. The result seems to rely on the\nchoice of an x^2 non-linearity, which is not often encountered in recent neural\nnet literature.\n\nThe point might also be to highlight the difference between a weight\npenalty and a feature penalty because the two seem to encourage\ndifferent values of b in this case. However, there is no comparison to\na weight penalty on b in section 2.1.\n\n\n* As far as I can tell, eq. 3 depends on either assuming an L2 or cross-entropy\nloss. A more general class of losses for which eq. 3 holds is not provided. This\nshould be made clear before eq. 3 is presented.\n\n\n* The Omniglot and ImageNet experiments are performed with Batch Normalization,\nyet the paper points out that feature regularization may be similar in effect\nto Batch Norm. Since the ResNet CNN baseline includes Batch Norm and there are\nclear improvements over that baseline, the proposed regularizer has a clear\nadditional positive effect. However, results should be provided without\nBatch Norm so a 1-1 comparison between the two methods can be performed.\n\n\n* The ImageNet experiment should be more like Hariharan and Girshick.\nIn particular, the same split of classes should be used (provided in\nthe appendix) and performance should be measured using n > 1 novel examples\nper class (using k nearest neighbors).\n\n\nMinor:\n\n* A brief comparison to Matching Networks is provided in section 3.2, but the\nperformance of Matching Networks should also be reported in Table 1.\n\n* From the approach section: \"Intuitively when close to convergence, about half\nof the data-cases recommend to update a parameter to go left, while\nthe other half recommend to go right.\"\n\nCould the intuition be clarified? There are many directions in high\ndimensional space and many ways to divide them into two groups.\n\n* Is the SGM penalty of Hariharan and Girshick implemented for this paper\nor using their code? Either is acceptable, but clarification would be appreciated.\n\n* Should the first equal sign in eq. 13 be proportional to, not equal to?\n\n* The work is dense in nature, but I think the presentation could be improved.\nIn particular, more detailed derivations could be provided in an appendix\nand some details could be removed from the main version in order to increase\nfocus on the results (e.g., the derviation in section 2.2.1).\n\n\nOverall Evaluation\n===\n\nThis paper provides an interesting set of analyses, but their value is not clear.\nThere is no clear reason why a gradient or feature regularizer should improve\nlow-shot learning performance. Despite that, experiments support that conclusion,\nthe analysis is interesting by itself, and the analysis may help lead to a\nclearer explanation.\n\nThe work is a somewhat novel extension and analysis of Hariharan and Girshick 2016.\nSome points are not completely clear, as mentioned above.","model":"human","source":"peerread","label":0,"id":5060}
{"text":"This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm, showing that it is equivalent to another proposed regularization, gradient magnitude loss. They then argue that: 1) it is helpful to low-shot learning, 2) it is numerically stable, 3) it is a soft version of Batch Normalization. Finally, they demonstrate experimentally that such a regularization improves performance on low-shot tasks.\n\nFirst, this is a nice analysis of some simple models, and proposes interesting insights in some optimization issues. Unfortunately, the authors do not demonstrate, nor argue in a convincing manner, that such an analysis extends to deep non-linear computation structures. I feel like the authors could write a full paper about \"results can be derived for \u03c6(x) with convex differentiable non-linear activation functions such as ReLU\", both via analysis and experimentation to measure numerical stability.\n\nSecond, the authors again show an interesting correspondance to batch normalization, but IMO fail to experimentally show its relevance.\n\nFinally, I understand the appeal of the proposed method from a numerical stability point of view, but am not convinced that it has any effect on low-shot learning in the high dimensional spaces that deep networks are used for.\n\nI commend the authors for contributing to the mathematical understanding of our field, but I think they have yet to demonstrate the large scale effectiveness of what they propose. At the same time, I feel like this paper does not have a clear and strong message. It makes various (interesting) claims about a number of things, but they seem more or less disparate, and only loosely related to low-shot learning.\n\nnotes:\n- \"an expectation taken with respect to the empirical distribution generated by the training set\", generally the training set is viewed as a \"montecarlo\" sample of the underlying, unknown data distribution \\mathcal{D}.\n- \"we can see that our model learns meaningful representations\", it gets a 6.5% improvement on the baseline, but there is no analysis of the meaningfulness of the representations.\n- \"Table 13.2\" should be \"Table 2\".\n- please be mindful of formatting, some citations should be parenthesized and there are numerous extraneous and missing spacings between words and sentences.\n","model":"human","source":"peerread","label":0,"id":5061}
{"text":"The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net.\nAlthough the equations suggest a weighting per example, dropping this weight (alpha_i) works equally well.\nThe proposed approach relates to Batch Norm and weight decay.\nExperiments are given on \"low-shot\" settting.\nThere seem to be two stories in the paper: feature penalty as a soft batch norm version, and low-shot learning; why is feature penalty specifically adapted to low-shot learning and not a more classical supervised task?\nRegarding your result on Omniglot, 91.5, I believe it is still about 2% worse than the Matching Networks, which you refer to but don't put in Table 1. Why?\nOverall, the idea is simple but feels like preliminary: while it is supposed to be a \"soft BN\", BN itself gets better performance than feature penalty, and both together give even better results. Is something still missing in the explanation?\n\n-- edits after revised version:\n\nThank you for adding more information to the paper. I feel it is still too long but hopefully you can reduce it to 9 pages as promised. However, I'm still not convinced the paper is ready to be accepted, mainly for the following reasons:\n- on Omniglot, the paper is still significantly far from the current state of the art.\n- the new experiments do not really confirm\/infirm the relationship with BN.\n- you added an explanation of why FP works for low-shot setting, by showing it controls the VC dimension and hence is good to control overfitting with a small number of training examples, but this discussion is basic and does not really shed more light than the obvious.\nI'm pushing up your score from 4 to 5 for the improved version, but I still think it is below acceptance level.\n\n","model":"human","source":"peerread","label":0,"id":5062}
{"text":"This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea).\n\n1) insufficient context of what is known and had been studied before (in shallow RL), for example within the field of \u201crobust RL\u201d. A good place to start might be with the work of Shie Mannor.\n\n2) an ill-defined general problem setup. Does it make sense to do post-hoc labeling of certain actions as \u201ccatastrophic\u201d if the agent is not informed about that metric during learning? Training a system to do one thing (maximize reward), but then evaluating it with a different metric is misleading. On the training metric, it could even be that the baseline outperforms the new algorithm? So I\u2019d want to see plots for \u201caverage reward\u201d in fig 3 as well. Also, what would the baseline learn if it was given large negative rewards for entering these otherwise invisible \u201cdanger states\u201d?\n\n3) a somewhat ad-hoc solution, that introduces new domain-specific hyperparameters (k_r, k_lambda and lambda) a second deep network and and two additional replay memories. In terms of results, I\u2019m also unsure whether I can trust the results, given the long-standing track-record of cart-pole being fully solved by many methods: is DQN an outlier here? Or is the convnet not an appropriate function-approximator? Actually: which exact variant \u201cstate-of-the-art\u201d variant of DQN are you using?\n\nThe good idea that I encourage the authors to pursue further is D_d, this set of rare but dangerous states, that should be kept around in some form. I see it as an ingredient for continual learning that most typical methods lack -- it is also one of the big differences between RL and supervised learning, where such states would generally be discarded as outliers.\n\nGiven my comment a couple of weeks ago, and the prompt response (\u201cwe implemented expected SARSA\u201d), I would have expected that the paper had been revised with the new results by now? In any case, I\u2019m open to discussing all these points and revising my opinion based on an updated version of the paper.\n\nMinor comment: the bibliography is done sloppily, with missing years, conference venues and missing\/misspelled author lists, e.g. \u201cSergey et al. Levine\u201d. I also think it is good form to cite the actual conference publications instead of arXiv where applicable.","model":"human","source":"peerread","label":0,"id":5063}
{"text":"This paper presents a few interesting ideas, namely the idea of keeping around a set of \"danger states\" and treating these states with some special consideration in reply to make sure that their impact is not neglected after collecting a lot of additional data.\n \n However, there are two main problems: 1) the actual implementation here seems fairly ad-hoc, and it's not at all clear to me that this particular algorithm (building a classifier with equal numbers of good and danger states, and then injecting an additional reward into the Q-learning task based upon this classifier), is the right way to go about this. The presentation is also difficult to follow, and the final results imply aren't that compelling (though this is improving after the revisions, but still has a way to go. We therefore encourage the authors to resubmit their work at a future conference venue.\n \n Pros:\n + Interesting idea of keeping around danger states and injecting them into training\n \n Cons:\n - Algorithm doesn't seem that well motivated\n - Presentation is a bit unclear, takes until page 6 to actually present the basic approach.\n - Experiments aren't that convincing (better after revisions, but still need work)","model":"human","source":"peerread","label":0,"id":5064}
{"text":"Thanks for an interesting take on an important question :) \nSome high level comments\/questions about the issue of \"safety\" in RL:\n\n1. What is the justification that there is any hope of behaving safely when a model is unavailable? Some amount of exploration is needed anyway, and without exploration you are doomed to not find the optimal policy. Essentially you either get stuck with a sub-optimal policy or accept some amount of failures. Whether such failures are acceptable or not is highly application specific, and hence I would encourage the authors to consider a concrete application that has some use (as opposed to toy problems) and provide a reasonable solution tailored to the same. A general solution is unlikely to exist, or be useful in practice.\n\n2. If a model is available, then much of the motivations raised become irrelevant. A simulated car can hit simulated pedestrians many times, and learn from the mistake in simulation -- it does not pose any threat. Further, measures like risk sensitive RL or robust RL can be put in place to ensure sim2real transfer.\n\n3. In light of (2), I would actually encourage the authors to think of their danger model as a form of reward shaping. Can we think of fear as a way to guide the exploration space when a model is made available? That way, the question shifts away from \"safety\" to sample efficiency and transfer. I think the danger model could be a good approach for these considerations (e.g. similar to SARSA). Though these have been mentioned in the paper, I look forward to an expanded discussion along these lines.","model":"human","source":"peerread","label":0,"id":5065}
{"text":"This paper presents a heuristic for avoiding large negative rewards which have already been experienced by distilling such events into a \"danger model\". The paper is well written including some rather poetic language [*].\n\nThe heuristic is evaluated in two toy domains. I would think that in order to properly evaluate this one would use a well known benchmark e.g. Atari. Atari seems particularly apt since those games are full of catastrophes (i.e. sudden death).\n\n[*] this reviewer's favourite quotes:\n\"Imagine a self-driving car that had to periodically hit a few pedestrians in order to remember that it\u2019s undesirable.\"\n\"The child can learn to adjust its behaviour without actually having to stab someone.\"\n\"... the catastrophe lurking just past the optimal shave.\"","model":"human","source":"peerread","label":0,"id":5066}
{"text":"- The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. After the pre-review comments, authors do mention that they compared against expected SARSA but I would really like to see these and other extensive baselines before accepting this paper. \n\n- There is also an increasing amount of literature of using reward replay buffers in deep RL agents (c.f. Jaderberg, Max, et al. \"Reinforcement learning with unsupervised auxiliary tasks.\", Blundell, Charles, et al. \"Model-free episodic control.\" , Narasimhan et al. \"Language understanding for text-based games using deep reinforcement learning\"), which could perhaps reinforce the agent to avoid revisiting catastrophic states. \n\n- Overall, the approach presented is not very principled. For instance, why isn't catastrophe directly provided as a signal to the learner instead of a separate model? ","model":"human","source":"peerread","label":0,"id":5067}
{"text":"This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy that tracks the use of certain token types, like identifiers. Additionally they release a Python open source dataset. As expected this augmentation, the fixed attention policy, improves the perplexity of the model. It seems important to dig a bit deeper into these results and show the contribution of different token types to the achieve perplexity. This is alluded to in the text, but a more thorough comparison would be welcome. The idea of an attention policy that takes advantage of expert knowledge is a nice contribution, but perhaps if limited novelty --- for example the Maddison and Tarlow 2014 paper, which the authors cite, has scoping rules that track previously used identifiers in scope.","model":"human","source":"peerread","label":0,"id":5068}
{"text":"This paper augments language models with attention to to capture long range dependencies through a sparse pointer network that is restricted to previously introduced identifiers, and demonstrates the proposed architecture over a new, released large-scale code suggestion corpus of 41M lines of Python code. The addition of long range attention over 20 identifiers improves perplexity compared to an LSTM with an attentional context of 50 words, but degrades accuracy (hit @1), while improving hit@5.\n The experimental validation however requires a more thorough analysis and more detailed ablation experiments and discussions, and more thorough comparison to related work. As is, many choices seem quite arbitrary and make it hard to determine if the model is really performing well (minibatch sizes, size of the memory for the LSTM, choice and number of identifiers for the sparse pointers, etc).","model":"human","source":"peerread","label":0,"id":5069}
{"text":"This paper presents an improved neural language models designed for selected long-term dependency, i.e., to predict more accurately the next identifier for the dynamic programming language such as Python. The improvements are obtained by:\n\n1) replacing the fixed-widow attention with a pointer network, in which the memory only consists of context representation of the previous K identifies introduced for the entire history. \n2) a conventional neural LSTM-based language model is combined with such a sparse pointer network with a controller, which linearly combines the prediction of both components using a dynamic weights, decided by the input, hidden state, and the context representations at the time stamp.\n\nSuch a model avoids the the need of large window size of the attention to predict next identifier, which usually requires a long-term dependency in the programming language. This is partly validated by the python codebase (which is another contribution of this paper) experiments in the paper.\n\nWhile the paper still misses some critical information that I would like to see, including how the sparse pointer network performance chances with different size of K, and how computationally efficient it is for both training and inference time compared to LSTM w\/ attention of various window size, and ablation experiments about how much (1) and (2) contribute respectively, it might be of interest to the ICLR community to see it accepted.\n\n","model":"human","source":"peerread","label":0,"id":5070}
{"text":"This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages. Code suggestion seems an area where attention and\/or pointers truly show an advantage in capturing long term dependencies.\n\nThe sparse pointer method does seem to provide better results than attention for similar window sizes - specifically, comparing a window size of 20 for the attention and sparse pointer method shows the sparse pointer winning fairly definitively across the board. Given a major advantage of the pointer method is being able to use a large window size well thanks to the supervision the pointer provides, it was unfortunate (though understandable due to potential memory issues) not to see larger window sizes. Having a different batch size for the sparse pointer and attention models is unfortunate given it complicates an otherwise straight comparison between the two models.\n\nThe construction and filtering of the Python corpus sounds promising but as of now it is still inaccessible (listed in the paper as TODO). Given that code suggestion seems an interesting area for future long term dependency work, it may be promising as an avenue for future task exploration.\n\nOverall this paper and the dataset are likely an interesting contribution even though there are a few potential issues.","model":"human","source":"peerread","label":0,"id":5071}
{"text":"This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy that tracks the use of certain token types, like identifiers. Additionally they release a Python open source dataset. As expected this augmentation, the fixed attention policy, improves the perplexity of the model. It seems important to dig a bit deeper into these results and show the contribution of different token types to the achieve perplexity. This is alluded to in the text, but a more thorough comparison would be welcome. The idea of an attention policy that takes advantage of expert knowledge is a nice contribution, but perhaps if limited novelty --- for example the Maddison and Tarlow 2014 paper, which the authors cite, has scoping rules that track previously used identifiers in scope. ","model":"human","source":"peerread","label":0,"id":5072}
{"text":"Some of the claimed contributions the paper makes already exist in prior work. For instance:\n\n\u2192 There is already a Python data set available: ","model":"human","source":"peerread","label":0,"id":5073}
{"text":"ResNet and other architectures that use shortcuts have shown empirical success in several domains and therefore, studying the optimization for such architectures is very valuable. This paper is an attempt to address some of the properties of networks that use shortcuts. Some of the experiments in the paper are interesting. However, there are two main issues with the current paper:\n\n1- linear vs non-linear: I think studying linear networks is valuable but we should be careful not to extend the results to networks with non-linear activations without enough evidence. This is especially true for Hessian as the Hessian of non-linear networks have very large condition number (see the ICLR submission \"Singularity of Hessian in Deep Learning\") even in cases where the optimization is not challenging. Therefore, I don't agree with the claims in the paper on non-linear networks. Moreover, one plot on MNIST is not enough to claim that non-linear networks behave similar to linear networks.\n\n2- Hessian at zero initial point: The explanation of why we should be interested in Hessain at zero initial point is not acceptable. The zero initial point is not interesting because it is a very particular point that cannot tell us about the Hessian during optimization.","model":"human","source":"peerread","label":0,"id":5074}
{"text":"This paper endeavors to offer theoretical explanations of the performance of ResNet. Providing better theoretical understanding of existing empirically powerful architectures is very important work and I commend the authors for tackling this. Unfortunately, this paper falls short in its current form: the particular choices and restrictions made (0 weights, linear regime) limit applicability to ResNet, and do not seem to offer insights sufficient to capture the causes of ResNet's performance.","model":"human","source":"peerread","label":0,"id":5075}
{"text":"I think the write-up can be improved. The results of the paper also might be somewhat misleading. The behavior for when weights are 0 is not revealing of how the model works in general. I think the work also underestimates the effect of the nonlinearities on the learning dynamics of the model.","model":"human","source":"peerread","label":0,"id":5076}
{"text":"This paper studies the optimization issue of linear ResNet, and shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth. I skimmed through the proof but have not checked them carefully. \n\nThis result is a nice observation for training deep linear networks.  But I do not think the paper has fully resolved the linear vs nonlinear issue. Some question:\n\n1. Though the revision has added some results using ReLU units, it seems it is only added to the mid positions of the network (sec 5.3), is this how it is typically done in ResNet? Moreover, ReLU is not differentiable at zero point, which does not satisfy the condition you had in Theorem 1. Why not use differentiable activations like sigmoid or tanh?\n\n2. From equation (22) in the appendix, it seems for nonlinear activations, the condition number depends on the derivative \\sigma^\\prime at 0. Therefore, if we use tanh which has derivative 1 at zero, the condition number is the same for linear and tanh activations. But this probably is not enough to explain the bit difference in performance or optimization for linear and nonlinear networks, or how the situations evolve after learning the 0 point.\n\n3. As for the success of ResNet (or convnets in general) in computer vision, I believe there are more types of nonlinearity such as pooling? Can the result here generalizes to pooling as well?\n\nMinor: \n- sec 1 last paragraph, low approximation error typically means more powerful model class and better training error, but not necessarily better test error\n- sec 4.1 what do you mean by \"zero initialization with small random perturbations\"? why not exactly zero initialization, how large is the random perturbation?\n\n","model":"human","source":"peerread","label":0,"id":5077}
{"text":"ResNet and other architectures that use shortcuts have shown empirical success in several domains and therefore, studying the optimization for such architectures is very valuable. This paper is an attempt to address some of the properties of networks that use shortcuts. Some of the experiments in the paper are interesting. However, there are two main issues with the current paper:\n\n1- linear vs non-linear: I think studying linear networks is valuable but we should be careful not to extend the results to networks with non-linear activations without enough evidence. This is especially true for Hessian as the Hessian of non-linear networks have very large condition number (see the ICLR submission \"Singularity of Hessian in Deep Learning\") even in cases where the optimization is not challenging. Therefore, I don't agree with the claims in the paper on non-linear networks. Moreover, one plot on MNIST is not enough to claim that non-linear networks behave similar to linear networks.\n\n2- Hessian at zero initial point: The explanation of why we should be interested in Hessain at zero initial point is not acceptable. The zero initial point is not interesting because it is a very particular point that cannot tell us about the Hessian during optimization. ","model":"human","source":"peerread","label":0,"id":5078}
{"text":"Hi Authors,\n\nI went through your paper in detail and it was very good linking the Hessian at the zero initial point to the success of the ResNet. However, I am a little unsure of the definition of the condition number in the paper. Across several standard refernces, the condition number is defined as the ratio the maximum and minimum singular values where as you defined it as $ |\\lambda|_max \/ |\\lambda|_min  $.  Unless the matrix, which is the Hessian H in your case, is positive semi-definite, these two are not the same and the spectra of the singular values and the eigen values can be quite different in general. Especially, for n=2, the initial point zero is a saddle point as stated in your theorem and as such the Hessian is not positive-definite. Please let me know if I am missing out on something and whether or not this modification affects the analysis in the paper. Thanks!","model":"human","source":"peerread","label":0,"id":5079}
{"text":"This paper presents a generative model for binary images.  Images are composed by placing a set of binary features at locations in the image.  These features are OR'd together to produce an image.  In a hierarchical variant, features\/classes can have a set of possible templates, one of which can be active.  Variables are defined to control which template is present in each layer.  A joint probability distribution over both the feature appearance and instance\/location variables is defined.\n\nOverall, the goal of this work is interesting -- it would be satisfying if semantically meaningful features could be extracted, allowing compositionality in a generative model of images.  However, it isn't clear this would necessarily result from the proposed process.\nWhy would the learned features (building blocks) necessarily semantically meaningful?  In the motivating example of text, rather than discovering letters, features could correspond to many other sub-units (parts of letters), or other features lacking direct semantic meaning.\n\nThe current instantiation of the model is limited.  It models binary image patterns.  The experiments are done on synthetic data and MNIST digits.  The method recovers the structure and is effective at classification on synthetic data that are directly compositional.  On the MNIST data, the test errors are quite large, and worse than a CNN except when synthetic data corruption is added.  Further work to enhance the ability of the method to handle natural images or naturally occuring data variation would enhance the paper.","model":"human","source":"peerread","label":0,"id":5080}
{"text":" The reviewer's opinions were clear for this paper. Mainly it seems that the fact that this work focuses on binary image patterns limited the ability of reviewers to assess the significance of this work based on the instantiation of the model explored in this work. It was also noted that the writing could have been clearer when describing the intuitions for the approach and that the derivations could have been explained in more detail.","model":"human","source":"peerread","label":0,"id":5081}
{"text":"The paper discusses a method to learn interpretable hierarchical template representations from given data. The authors illustrate their approach on binary images.\n\nThe paper presents a novel technique for extracting interpretable hierarchical template representations based on a small set of standard operations. It is then shown how a combination of those standard operations translates into a task equivalent to a boolean matrix factorization. This insight is then used to formulate a message passing technique which was shown to produce accurate results for these types of problems.\n\nSummary:\n\u2014\u2014\u2014\nThe paper presents an novel formulation for extracting hierarchical template representations that has not been discussed in that form. Unfortunately the experimental results are on smaller scale data and extension of the proposed algorithm to more natural images seems non-trivial to me.\n\nQuality: I think some of the techniques could be described more carefully to better convey the intuition.\nClarity: Some of the derivations and intuitions could be explained in more detail.\nOriginality: The suggested idea is reasonable but limited to binary data at this point in time.\nSignificance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge.\n\nDetails:\n\u2014\u2014\u2014\n1. My main concern is related to the experimental evaluation. While the discussed approach is valuable, its application seems limited to binary images at this point in time. Can the authors comment?\n\n2. There are existing techniques to extract representations of images which the authors may want to mention, e.g., work based on grammars.","model":"human","source":"peerread","label":0,"id":5082}
{"text":"This paper presents an approach to learn object representations by composing a set of templates which are leaned from binary images. \nIn particular, a hierarchical model is learned by combining AND, OR and POOL operations. Learning is performed by using approximated inference with MAX-product BP follow by a heuristic to threshold activations to be binary. \n\nLearning hierarchical representations that are interpretable is a very interesting topic, and this paper brings some good intuitions in light of modern convolutional neural nets. \n\nI have however, some concerns about the paper:\n\n1) the paper fails to cite and discuss relevant literature and claims to be the first one that is able to learn interpretable parts. \nI would like to see a discussion of the proposed approach compared to a variety of papers e.g.,:\n\n- Compositional hierarchies of Sanja Fidler\n- AND-OR graphs used by Leo Zhu and Alan Yuille to model objects\n- AND-OR templates of Song-Chun Zhu's group at UCLA \n\nThe claim that this paper is the first to discover such parts should be removed. \n\n2) The experimental evaluation is limited to very toy datasets. The papers I mentioned have been applied to real images (e.g., by using contours to binarize the images). \nI'll also like to see how good\/bad the proposed approach is for classification in more well known benchmarks. \nA comparison to other generative models such as VAE, GANS, etc will also be useful.\n\n3) I'll also like to see a discussion of the relation\/differences\/advantages of the proposed approach wrt to sum product networks and grammars.\n\nOther comments:\n\n- the paper claims that after learning inference is feed-forward, but since message passing is used, it should be a recurrent network. \n\n- the algorithm and tech discussion should be moved from the appendix to the main paper\n\n- the introduction claims that compression is a prove for understanding. I disagree with this statement, and should be removed. \n\n- I'll also like to see a discussion relating the proposed approach to the Deep Rendering model. \n\n- It is not obvious how some of the constraints are satisfied during message passing. Also constraints are well known to be difficult to optimize with max product. How do you handle this?\n\n- The learning and inference algorithms seems to be very heuristic (e.g., clipping to 1, heuristics on which messages are run). Could you analyze the choices you make?\n\n- doing multiple steps of 5) 2) is not a single backward pass \n\nI'll reconsider my score in light of the answers","model":"human","source":"peerread","label":0,"id":5083}
{"text":"This paper presents a generative model for binary images.  Images are composed by placing a set of binary features at locations in the image.  These features are OR'd together to produce an image.  In a hierarchical variant, features\/classes can have a set of possible templates, one of which can be active.  Variables are defined to control which template is present in each layer.  A joint probability distribution over both the feature appearance and instance\/location variables is defined.\n\nOverall, the goal of this work is interesting -- it would be satisfying if semantically meaningful features could be extracted, allowing compositionality in a generative model of images.  However, it isn't clear this would necessarily result from the proposed process.\nWhy would the learned features (building blocks) necessarily semantically meaningful?  In the motivating example of text, rather than discovering letters, features could correspond to many other sub-units (parts of letters), or other features lacking direct semantic meaning.\n\nThe current instantiation of the model is limited.  It models binary image patterns.  The experiments are done on synthetic data and MNIST digits.  The method recovers the structure and is effective at classification on synthetic data that are directly compositional.  On the MNIST data, the test errors are quite large, and worse than a CNN except when synthetic data corruption is added.  Further work to enhance the ability of the method to handle natural images or naturally occuring data variation would enhance the paper.\n","model":"human","source":"peerread","label":0,"id":5084}
{"text":"The authors explore the idea of deep-learning a static analyzer. They do it with a toy programming language and a very simplified analysis problem -- just checking if all variables are initalized.\n\nWhile the idea is interesting and might be developped into a tool in the future, the toy task presented in this paper is too simple to warrant an ICLR submission. Just detecting whether a variable is initialized in a string is a toy algorihtmic task, similar to the ones solved in a number of paper in recent years by models such as the Neural Turing Machine, Stack RNNs, Neural GPU, or Differentiable Neural Computer. All these architectures perform almost perfectly on a number of algorithmic tasks, so it is highly probable that they would also solve this one. Unluckily, the authors only compare to much more basic models, such as HMMs. Since the code for many of the above-mentioned models is available online, a paper without these baselines is not ready for ICLR. Moreover, there is a risk that existing models already solve this problem very well, making the contribution unclear.","model":"human","source":"peerread","label":0,"id":5085}
{"text":"There is a general consensus that, though the idea is interesting, the work is not mature enough for a conference publication (e.g., the problem is too toy, not clear that really solves any, even artificial problem, better than existing techniques).","model":"human","source":"peerread","label":0,"id":5086}
{"text":"This paper takes a first step towards learning to statically analyze source code. It develops a simple toy programming language that includes loops and branching. The aim is to determine whether all variables in the program are defined before they are used. The paper tries a variety of off-the-shelf sequence classification models and develops a new model that makes use of a ``differentiable set'' to keep track of which variables have been defined so far. Result show that an LSTM model can achieve 98% accuracy, and the differentiable set model can achieve 99.3% accuracy with sequence-level supervision and 99.7% accuracy with strong token-level supervision. An additional result is used whereby an LSTM language model is trained over correct code, and then low probability (where a threshold to determine low is tuned by hand) tokens are highlighted as sources of possible error.\n\nOne further question is if the authors could clarify what reasoning patterns are needed to solve these problems. Does the model need to, e.g., statically determine whether an `if` condition can ever evaluate to true in order to solve these tasks? Or is it just as simple as checking whether a variable appears on a LHS before it appears on a RHS later in the textual representation of the program?\n\nStrengths:\n- Learning a static analyzer is an interesting concept, and I think there is good potential for this line of work\n- The ability to determine whether variables are defined before they are used is certainly a prerequisite for more complicated static analysis.\n- The experimental setup seems reasonable\n- The differentiable set seems like a useful (albeit simple) modelling tool\n\nWeaknesses:\n- The setup is very toy, and it's not clear to me that this makes much progress towards the challenges that would arise if one were trying to learn a static analyzer \n- The models are mostly very simple. The one novelty on the modelling front (the differentiable set) provides a small win on this task, but it's not clear if it is a useful general construct or not.\n\nOverall:\nI think it's an interesting start, and I'm eager to see how this line of work progresses. In my opinion, it's a bit too early to accept this work to ICLR, but I'd be excited about seeing what happens as the authors try to push the system to learn to analyze more properties of code, and as they push towards scenarios where the learned static analyzer would be useful, perhaps leveraging strengths of machine learning that are not available to standard programming languages analyses.","model":"human","source":"peerread","label":0,"id":5087}
{"text":"The authors are trying to understand whether static analysis can be learned. As I hinted in my question, I think that all of the interesting complexity of static analysis has been removed in the toy language --- extraordinarily simple logic using a set can solve the problem posed, and an LSTM (unsurprisingly) can learn the extraordinarily simple logic (when given a differentiable set object). This extreme simplicity gives me no confidence that a more realistic static analysis problem can be solved.\n\nLSTMs (and deep learning) have had remarkable successes in solving messy real-world language problems. It's certainly possible that LSTMs could solve static analysis -- but being technically timid is not the right way to go about it.","model":"human","source":"peerread","label":0,"id":5088}
{"text":"-- The motivation behind the work is somewhat unclear: by now, it is very well understood how to design analyzers and what sound\/optimal means. Creating a ``black box'' analyzer that can make basic predictions (that are sometimes incorrect) without being able to modify it would be useful if the predictions were challenging and need not be sound all the time (like in some of the cited papers). Note that when analyzers are not sound there are typically clear  reasons for why this is so, e.g., dealing with native methods, frameworks, dynamic evaluation, etc. They are not unsound for 'random reasons'. \n\n-- There is also related work, already pointed out: the one of Hindle and others which already addresses what is in section 4.\n\n-- Here is also recent related work on learning (the transformers of the) static analyzers from data, one that is more elaborate as it learns the transformers of real analyzers and even finds real-world issues in Facebook's Flow: ","model":"human","source":"peerread","label":0,"id":5089}
{"text":"This paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features, and tried to investigate whether neural networks can also have this kind of ability.  Specifically, the paper proposed to use Auto-Encoder (AE) as the network to reconstruct the low fidelity of visual input. Moreover, similar to Mnih et al. (2014),  the paper also proposed to use a recurrent fashion to mimic the sequential behavior the  human visual system. \n\nI think the paper is well motivated. However, there are several concerns:\n1. The baselines of the paper are too weak. Nearest neighbor, bilinear, bicubic and cubic interpolations without any learning procedure are of course performed worse than AE based models. The author should compare with the STOA methods such as","model":"human","source":"peerread","label":0,"id":5090}
{"text":"The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers find this direction of exploration to be interesting, but a bit preliminary at the moment. Authors are strongly encouraged to incorporate reviewer comments to make future iterations of the work stronger.","model":"human","source":"peerread","label":0,"id":5091}
{"text":"This paper aims to characterize the perceptual ability of a neural network under different input conditions.  This is done by manipulating the input image x in various ways (e.g. downsamplig, foveating), and training an auto-encoder to reconstruct the original full-resolution image.  MSE and qualitative results are shown and compared for the different input conditions.\n\nUnfortunately, this paper seems to lack focus, presenting a set of preliminary inspections with few concrete conclusions.  For example, at the end of sec 4.4, \"This result is not surprising, given that FOV-R contains additional information .... These results suggests that a small number of foveations containing rich details might be all these neural networks need....\".  But this hypothesis is left dangling:  What detailed regions are needed, and from where?  For what sort of tasks?\n\nSecondly, it isn't clear to me what reconstruction behaviors are caused by a fundamental perception of the input, and what are artifacts of the autoencoder and pixelwise l2 loss?  A prime example is texture, which the autoencoder fails to recover.  But with a pixelwise loss, the network must predict high-frequency textures nearly pixel-for-pixel at training time; if this is impossible, then it will generate a pixelwise average of the training samples --- a flat region.  So then the network's inability to reconstruct textures is due to a problem generating them, specifically averaging from the training loss, not necessarily an issue in perceiving textures.  A network trained a different way (perhaps an adversarial network) may infer a texture is there, even if it wouldn't be able to generate it in a pixelwise l2 sense.\n\nSimilarly, the ability to perform color reconstruction given a color glimpse I think has much to do with disambiguating the color of an object\/scene:  If there is an ambiguity, the network won't know which to \"choose\" (white flower or yellow flower?) and output an average, which is why there are so many sepia tones.  However, in its section on this, the paper only measures the reconstruction error for different amounts of color given, and does not drill very far into any hypotheses for why this behavior occurs.\n\nThere are some interesting measurements here, such as the amount of color needed in the foveation to reconstruct a color image, and the discussion on global features, which may start to get at a mechanism by which glimpses may propagate to an entire reconstruction.  But overall it's hard to know what to take away from this paper.  What are larger concrete conclusions that can be garnered from the details, and what mechanisms bring them about?  Can these be more thoroughly explored with more focus?\n","model":"human","source":"peerread","label":0,"id":5092}
{"text":"I like the idea the paper is exploring. Nevertheless I see some issues with the analysis:\n\n- To get a better understanding of the quality of the results, I think at least some state-of-the-art comparisons should be included (e.g. by setting d times d pixel patches too their average and applying a denoising autoencoder). If they perform significantly better, then this indicates that the presented model is not yet taking all the information from the input image that could be used.\n- SCT-R and FOV-R are supposed to test how much information can be restored from the Fovea alone as opposed to the Fovea together with low resolution periphery. However, there is an additional difference between the two conditions: According to the paper, in SCT-R, part of the image was set to zero, while in FOV-R it was removed alltogether. With only one or two hidden layers, I could easily imagine this making a difference.\n- On page 4, you compare the performance of FOV-R (1% error) with that of DS-D (1.5%) and attribute this to information about the periphery that the autoencoder extracts from the fovea. While this might be the case, at least part of the reduced error will be due to the fact that the fovea is (hopefully) perfectly reconstructed. To answer the actual question \"how much additional information about the periphery can be extracted from the fovea\", you should consider calculating the error only in the periphery, i.e. the part of the image where DS-D and FOV-R got exactly the same input for. Then any decreased error is only due to the additional fovea information.\n\nOther issues:\n- The images in Figure 2 (a) and (b) in the rows \"factor 2\", \"factor 4\", \"factor 8\" look very blurry. There seems some interpolation to be going on (although slighly different than the bilinear interpolation). This makes it hard to asses how much information is in these images. I think it would be much more insightfull to print them with \"nearest\" interpolation.\n- Figure 3 caption too vague. Maybe add something like footnote 2?\n- Often figures appear too early in paper which leads to lots of distance between text and figures.\n","model":"human","source":"peerread","label":0,"id":5093}
{"text":"This paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features, and tried to investigate whether neural networks can also have this kind of ability.  Specifically, the paper proposed to use Auto-Encoder (AE) as the network to reconstruct the low fidelity of visual input. Moreover, similar to Mnih et al. (2014),  the paper also proposed to use a recurrent fashion to mimic the sequential behavior the  human visual system. \n\nI think the paper is well motivated. However, there are several concerns:\n1. The baselines of the paper are too weak. Nearest neighbor, bilinear, bicubic and cubic interpolations without any learning procedure are of course performed worse than AE based models. The author should compare with the STOA methods such as ","model":"human","source":"peerread","label":0,"id":5094}
{"text":"Because the authors did not respond to reviewer feedback, I am maintaining my original review score.\n\n-----\n\nThis paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach: they design a flexible parametric (but not generative) model with Gaussian latent factors and fit it using a rich training objective including terms for reconstruction (of observed time series) error, smoothness in the latent state space (via a KL divergence term encouraging neighbor states to be similarly distributed), and a final regularizer that encourages related time series to have similar latent state trajectories. Relations between trajectories are hard coded based on pre-existing knowledge, i.e., latent state trajectories for neighboring (wind speed) base stations should be similar. The model appears to be fit using gradient simple descent. The authors propose several elaborations, including a nonlinear transition function (based on an MLP) and a reconstruction error term that takes variance into account. However, the model is restricted to using a linear decoder. Experimental results are positive but not convincing.\n\nStrengths:\n- The authors target a worthwhile and challenging problem: incorporating the modeling of uncertainty over hidden states with the power of flexible neural net-like models.\n- The idea of representing relationships between hidden states using KL divergence between their (distributions over) corresponding hidden states is clever. Combined with the Gaussian distribution over hidden states, the resulting regularization term is simple and differentiable.\n- This general approach -- focusing on writing down the problem as a neural network-like loss function -- seems robust and flexible and could be combined with other approaches, including variants of variational autoencoders.\n\nWeaknesses:\n- The presentation is a muddled, especially the model definition in Sec. 3.3. The authors introduce four variants of their model with different combinations of decoder (with and without variance term) and linear vs. MLP transition function. It appears that the 2,2 variant is generally better but not on all metrics and often by small margins. This makes drawing a solid conclusions difficult: what each component of the loss contributes, whether and how the nonlinear transition function helps and how much, how in practice the model should be applied, etc. I would suggest two improvements to the manuscript: (1) focus on the main 2,2 variant in Sec. 3.3 (with the hypothesis that it should perform best) and make the simpler variants additional \"baselines\" described in a paragraph in Sec. 4.1; (2) perform more thorough experiments with larger data sets to make a stronger case for the superiority of this approach.\n- The authors only allude to learning (with references to gradient descent and ADAM during model description) in this framework. Inference gets its one subsection but only one sentence that ends in an ellipsis (?).\n- It's unclear what is the purpose of introducing the inequality in Eq. 9.\n- Experimental results are not convincing: given the size of the data, the differences vs. the RNN and KF baselines is probably not significant, and these aren't particularly strong baselines (especially if it is in fact an RNN and not an LSTM or GRU).\n- The position of this paper is unclear with respect to variational autoencoders and related models. Recurrent variants of VAEs (e.g., Krishnan, et al., 2015) seem to achieve most of the same goals as far as uncertainty modeling is concerned. It seems like those could easily be extended to model relationships between time series using the simple regularization strategy used here. Same goes for Johnson, et al., 2016 (mentioned in separate question).\n\nThis is a valuable research direction with some intriguing ideas and interesting preliminary results. I would suggest that the authors restructure this manuscript a bit, striving for clarity of model description similar to the papers cited above and providing greater detail about learning and inference. They also need to perform more thorough experiments and present results that tell a clear story about the strengths and weaknesses of this approach.","model":"human","source":"peerread","label":0,"id":5095}
{"text":"The reviews of this paper seem to be very aligned: many of the ideas presented in the paper are interesting, the problem is important, and the results encouraging but preliminary. R2 thought the paper could be improved in terms of clarity and offered several specific suggestions to this end. R2 and R1 mentioned the limitations of the linear decoder; which is not a critical flaw, in my opinion, but as R1 points out, many recent works have explored nonlinear decoders and these could be at least discussed, if not compared. All of the reviewers have worked in this area and expressed high-confidence reviews.\n \n I was surprised that the authors did not provide feedback or revise the paper at least with reference to the clarity\/presentation suggestions. It seems this may have had an impact on the perception of the reviewers. I encourage the authors to revise the paper in light of the reviews and re-submit to another venue.","model":"human","source":"peerread","label":0,"id":5096}
{"text":"This manuscript proposes an approach for modeling correlated timeseries through a combination of loss functions which depend on neural networks. The loss functions correspond to: data fit term, autoregressive latent state term, and a term which captures relations between pairs of timeseries (relations have to be given as prior information).\n\nModeling relational timeseries is a well-researched problem, however little attention has been given to it in the neural network community. Perhaps the reason for this is the importance of having uncertainty in the representation. The authors correctly identify this need and consider an approach which considers distributions in the state space.\n\nThe formulation is quite straightforward by combining loss functions. The model adds to Ziat et al. 2016 in certain aspects which are well motivated, but unfortunately implemented in an unconvincing way. To start with, uncertainty is not treated in a very principled way, since the inference in the model is rather naive; I'd expect employing a VAE framework [1] for better uncertainty handling. Furthermore, the Gaussian co-variance collapses into a variance, which is the opposite of what one would want for modelling correlated time-series. There are approaches which take these correlations into account in the states, e.g. [2].\n\nMoreover, the treatment of uncertainty only allows for linear decoding function f. This significantly reduces the power of the model. State of the art methods in timeseries modeling have moved beyond this constraint, especially in the Gaussian process community e.g. [2,3,4,5]. Comparing to a few of these methods, or at least discussing them would be useful.\n\n\nReferences:\n[1] Kingma and Welling. Auto-encoding Variational Bayes. arXiv:1312.6114\n[2] Damianou et al. Variational Gaussian process dynamical systems. NIPS 2011.\n[3] Mattos et al. Recurrent Gaussian processes. ICLR 2016.\n[4] Frigola. Bayesian Time Series Learning with Gaussian Processes, University of Cambridge, PhD Thesis, 2015. \n[5] Frigola et al. Variational Gaussian Process State-Space Models. NIPS 2014\n\n\nOne innovation is that the prior structure of the correlation needs to be given. This is a potentially useful and also original structural component. However, it also constitutes a limitation in some sense, since it is unrealistic in many scenarios to have this prior information. Moreover, the particular regularizer that makes \"similar\" timeseries to have closeness in the state space seems problematic. Some timeseries groups might be more \"similar\" than others, and also the similarity might be of different nature across groups. These variations cannot be well captured\/distilled by a simple indicator variable e_ij. Furthermore, these variables are in practice taken to be binary (by looking at the experiments), which would make it even harder to model rich correlations. \n\nThe experiments show that the proposed method works, but they are not entirely convincing. Importantly, they do not shed enough light into the different properties of the model w.r.t its different parts. For example, the effect and sensitivity of the different regularizers. The authors state in a pre-review answer that they amended with some more results, but I can't see a revision in openreview (please let me know if I've missed it). From the performance point of view, the results are not particularly exciting, especially given the fact that it's not clear which loss is better (making it difficult to use the method in practice). \n\nIt would also be very interesting to report the optimized values of the parameters \\lambda, to get an idea of how the different losses behave.\n\nTimeseries analysis is a very well-researched area. Given the above, it's not clear to me why one would prefer to use this model over other approaches. Methodology wise, there are no novel components that offer a proven advantage with respect to past methods. The uncertainty in the states and the correlation of the time-series are the aspects which could add an advantage, but are not adequately researched in this paper.\n","model":"human","source":"peerread","label":0,"id":5097}
{"text":"In absence of authors' response, the rating is maintained.\n\n---\n\nThis paper introduces a nonlinear dynamical model for multiple related multivariate time series. It models a linear observation model conditioned on the latent variables, a linear or nonlinear dynamical model between consecutive latent variables and a similarity constraint between any two time series (provided as prior data and non-learnable). The predictions\/constraints given by the three components of the model are Gaussian, because the model predicts both the mean and the variance or covariance matrix. Inference is forward only.\n\nThe model is evaluated on four datasets, and compared to several baselines: plain auto-regressive models, feed-forward networks, RNN and dynamic factor graphs DFGs, which are RNNs with forward and backward inference of the latent variables.\n\nThe model, which introduces lateral constraints between different time series, and which predicts both the mean and covariance seems interesting, but presents two limitations.\n\nFirst of all, the paper should refer to variational auto-encoders \/ deep gaussian models, which also predict the mean and the variance during inference.\n\nSecondly, the datasets are extremely small. For example, the WHO contains only 91 times series of 52*10 = 520 time points. Although the experiments seem to suggest that the proposed model tends to outperform RNNs, the datasets are very small and the high variance in the results indicates that further experiments, with longer time series, are required. The paper could also easily be extended with more information about the model (what is the architecture of the MLP) as well as time complexity comparison between the models (especially between DFGs and this model).\n\nMinor remark:\nThe footnote 2 on page 5 seems to refer to the structural regularization term, not to the dynamical term.\n\n","model":"human","source":"peerread","label":0,"id":5098}
{"text":"The topic of the paper, model-based RL with a learned model, is important and timely. The paper is well written. I feel that the presented results are too incremental. Augmenting the frame prediction network with another head that predicts the reward is a very sensible thing to do. However neither the methodology not the results are novel \/ surprising, given that the original method of [Oh et al. 2015] already learns to successfully increment score counters in predicted frames in many games.\n\nI\u2019m very much looking forward to seeing the results of applying the learned joint model of frames and rewards to model-based RL as proposed by the authors.","model":"human","source":"peerread","label":0,"id":5099}
{"text":"The authors have combined two known areas of research - frame prediction and reward prediction - and combined them in a feedforward network trained on sequences from Atari games. The fact that this should train well is unsurprising for this domain, and the research yields no other interesting results. Pros - the paper is clearly written and the experiments are sound. Cons - there is very little novelty or contribution.","model":"human","source":"peerread","label":0,"id":5100}
{"text":"This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction. In Atari game playing scenarios, the authors show that this model can successfully predict both reward and next frames.\n\nPros:\n- Paper is well written and easy to follow.\n- Model is clear to understand.\n\nCons:\n- The model is incrementally different than the baseline. The authors state that their purpose is to establish a pre-condition, which they achieve. But this makes the paper quite limited in scope.\n\nThis paper reads like the start of a really good long paper, or a good short paper. Following through on the future work proposed by the authors would make a great paper. As it stands, the paper is a bit thin on new contributions.","model":"human","source":"peerread","label":0,"id":5101}
{"text":"The paper extends a recently proposed video frame prediction method with reward prediction in order to learn the unknown system dynamics and reward structure of an environment. The method is tested on several Atari games and is able to predict the reward quite well within a range of about 50 steps. The paper is very well written, focussed and is quite clear about its contribution to the literature. The experiments and methods are sound. However, the results are not really surprising given that the system state and the reward are linked deterministically in Atari games. In other words, we can always decode the reward from a network that successfully encodes future system states in its latent representation. The contribution of the paper is therefore minor. The paper would be much stronger if the authors could include experiments on the two future work directions they suggest in the conclusions: augmenting training with artificial samples and adding Monte-Carlo tree search. The suggestions might decrease the number of real-world training samples and increase performance, both of which would be very interesting and impactful.","model":"human","source":"peerread","label":0,"id":5102}
{"text":"The topic of the paper, model-based RL with a learned model, is important and timely. The paper is well written. I feel that the presented results are too incremental. Augmenting the frame prediction network with another head that predicts the reward is a very sensible thing to do. However neither the methodology not the results are novel \/ surprising, given that the original method of [Oh et al. 2015] already learns to successfully increment score counters in predicted frames in many games.\n\nI\u2019m very much looking forward to seeing the results of applying the learned joint model of frames and rewards to model-based RL as proposed by the authors. ","model":"human","source":"peerread","label":0,"id":5103}
{"text":"This paper presents experimental results from an EdgeBoxes + Fast R-CNN detector on the task of localizing pedestrians. It uses an AlexNet (CaffeNet) backbone architecture modified to include batch normalization. Experimental results are presented on the INRIA and ETH datasets.\n\nPros\n- The paper is clearly written and easy to follow\n\nCons\n- The paper's two contributions are too minor to merit publication\n- Experimental results should include at least the Caltech pedestrian dataset but likely also the KITTI pedestrian dataset\n- Recent work from ECCV 2016 [a], with superior results and much more experimental evaluation, is not cited or discussed\n\nMy rating is due primarily to the lack luster contributions. The first claimed contribution is the use of EdgeBoxes as proposals for pedestrian detection. Unless the result of this choice produced a truly surprising experimental result, this is simply too minor to be considered a contribution. Moreover, if this choice is important, then the paper should justify it by showing that other proposal methods (of which there are a great many in addition to Selective Search and Edge Boxes) are worse performing in some regard (speed, accuracy, memory, etc.). The second claimed contribution is the use of batch normalization (BN) in their network architecture. There is a case to be made that BN hasn't been explored in Fast R-CNN. However, if the goal of the paper was to thoroughly explore BN + Fast R-CNN, then why focus narrowly on pedestrian detection? Instead, it should focus more broadly on generic object category detection for which there are well established Fast R-CNN baselines on PASCAL VOC and COCO. The use of BN + Fast R-CNN only for pedestrian detection does not provide much signal about this choice. There are also potential technical issues that are not discussed. BN is typically avoided in Fast R-CNN because the batch size seen by most of the network is usually only one or two images. This is likely too few images for the naive application of BN.\n\n\n[a] \"Is Faster R-CNN Doing Well for Pedestrian Detection?\" Zhang et al.","model":"human","source":"peerread","label":0,"id":5104}
{"text":"Four knowledgable reviewers recommend rejection due to too weak of a contribution. The authors did not post a rebuttal. The AC agrees with the reviewers' recommendation.","model":"human","source":"peerread","label":0,"id":5105}
{"text":"Paper summary: the authors proposed to use EdgeBoxes + Fast-RCNN with\nbatch normalization for pedestrian detection\n\nReview summary: results do not cover enough datasets, the reported\nresults do not improve over state of the art, writing is poor, and\noverall the work lacks novelty. This is a clear reject.\n\nPros:\n* Shows that using batch normalization does improve results\n\nCons:\n* Only results on ETH and INRIA. Should include Caltech or KITTI.\n* Reported results are fair, but not improving over state of the art\n* Overall idea of limited interest when considering works like S.\nZhang CVPR 2016 (Fast R-CNN for pedestrian detection) and L. Zhang\nECCV 2017 (Faster R-CNN for pedestrian detection)\n* Issues with the text quality\n* Limited takeaways\n\nQuality: low\nClarity: fair, but poor English\nOriginality: low\nSignificance: low\n\nFor acceptance at future conferences, this work would need more\npolish, improving over best known results on INRA, ETH, and Caltech or\nKITTI. And ideally, present additional new insights.\n\nMinor comments:\n* The text lacks polish. E.g. influent -> influence, has maken ->\nmade, is usually very important -> is important, achieve more\nexcellent results -> achieve better results; etc. Please consider\nasking help from a native speaker for future submissions. There are\nalso non-sense sentences such as \u201cit is computational\u201d.\n* Citations should be in parentheses\n* Some of the citations are incorrect because the family name is in\nthe wrong position, e.g. Joseph Lim, Lawrence Zitnick, and Rodrigo Benenson.","model":"human","source":"peerread","label":0,"id":5106}
{"text":"This paper proposes a pedestrian detection method using Fast RCNN framework with batch normalization, where EdgeBoxes is used to collect pedestrian proposals instead of selective search as used in the original Fast RCNN method. The proposed method is evaluated in INRIA and ETH dataset.\n\nPros:\n- The proposed method shows good performance(but not state-of-the-art).\n\nCons:\n- Lack of novelty. Fast RCNN and its variants (e.g. FasterRCNN, ","model":"human","source":"peerread","label":0,"id":5107}
{"text":"The authors apply the commonly used Fast RCNN detection system to pedestrian detection. They use \u201cEdgeBoxes\u201d object proposals and incorporate batch norm into their network. Results are shown on the INRIA and ETH pedestrian datasets. They are reasonable but not state-of-the-art. Results are not shown on Caltech Pedestrians, the standard modern dataset used to evaluate pedestrian detection. Perhaps more importantly, the paper has no novelty.\n\nThe detection system described in this paper is a standard application of Fast RCNN to pedestrian detection. The implementation is not state-of-the-art, and there is no novelty in this work. EdgeBoxes has been used with Fast RCNN before. The authors don\u2019t seem to be aware of more recent developments in object detection, including Faster RCNN (","model":"human","source":"peerread","label":0,"id":5108}
{"text":"The starting point of this work is the understanding that by having decorrelated neurons (e.g. neurons that only fire on background, or only on foreground regions) one provides independent pieces of information to the subsequent decisions. As such one gives \"complementary viewpoints\" of the input to the subsequent layers, which can be thought of as performing ensembling\/expert combination within the model, rather than using an ensemble of networks. \n\nFor this, the authors propose a sensible method to decorrelate the activations of intermediate neurons, with the aim of delivering complementary inputs to the final classification layers: they split intermediate neurons to a \"foreground\" and a \"background\" subset, and append side-losses that force them to be zero on background and foreground pixels respectively. \n\nThey demonstrate that this can improve classification on a mid-scale classification example (a fraction of imagenet, and a ResNet with 18, rather than 150 layers), when compared to a \"vanilla\" baseline that does not use these losses.\n\nI enjoyed reading the paper because the idea is simple, smart, and seems to be effective. \nBut there are a few concerns;\n-firstly, the way of doing this seems very particular to vision. In vision one knows that masking the features (during both training and testing) helps, e.g.","model":"human","source":"peerread","label":0,"id":5109}
{"text":"This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (lack of clean experiments, clarity in the manuscript, etc) and recommend rejection. I believe there are promising ideas here, and this manuscript will be stronger for a future deadline.","model":"human","source":"peerread","label":0,"id":5110}
{"text":"We've already updated the paper. \n- The abstract and introduction have been rewritten with more explanation (on the motivation) and comparison. \n- The difference from ensemble models was highlighted in the related works.\n- We found that the Fig 1. is a bit confusing and have already updated it in the revised revision.\n- Eqn 3. has been corrected.\n- New results on ImageNet dataset.","model":"human","source":"peerread","label":0,"id":5111}
{"text":"This paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group.  The technique is applied in the setting of image classification with \u201cprivileged information\u201d in the form of foreground segmentation masks, where the model is trained to learn orthogonal groups of foreground and background features using the correlation penalty and an additional \u201cbackground suppression\u201d term.\n\n\nPros:\n\nProposes a \u201cgroup-wise model diversity\u201d loss term which is novel, to my knowledge.\n\nThe use of foreground segmentation masks to improve image classification is also novel.\n\nThe method is evaluated on two standard and relatively large-scale vision datasets: ImageNet and PASCAL VOC 2012.\n\n\nCons:\n\nThe evaluation is lacking.  There should be a baseline that leaves out the background suppression term, so readers know how much that term is contributing to the performance vs. the group orthogonal term.  The use of the background suppression term is also confusing to me -- it seems redundant, as the group orthogonality term should already serve to suppress the use of background features by the foreground feature extractor.\n\nIt would be nice to see the results with \u201cIncomplete Privileged Information\u201d on the full ImageNet dataset (rather than just 10% of it) with the privileged information included for the 10% of images where it\u2019s available.  This would verify that the method and use of segmentation masks remains useful even in the regime of more labeled classification data.\n\nThe presentation overall is a bit confusing and difficult to follow, for me.  For example, Section 4.2 is titled \u201cA Unified Architecture: GoCNN\u201d, yet it is not an overview of the method as a whole, but a list of specific implementation details (even the very first sentence).\n\nMinor: calling eq 3 a \u201cregression loss\u201d and writing \u201c||0 - x||\u201d rather than just \u201c||x||\u201d is not necessary and makes understanding more difficult -- I\u2019ve never seen a norm regularization term written this way or described as a \u201cregression to 0\u201d.\n\nMinor: in fig. 1 I think the FG and BG suppression labels are swapped: e.g., the \u201csuppress foreground\u201d mask has 1s in the FG and 0s in the BG (which would suppress the BG, not the FG).\n\n\nAn additional question: why are the results in Table 4 with 100% privileged information different from those in Table 1-2?  Are these not the same setting?\n\nThe ideas presented in this paper are novel and show some promise, but are currently not sufficiently ablated for readers to understand what aspects of the method are important.  Besides additional experiments, the paper could also use some reorganization and revision for clarity.\n\n===============\n\nEdit (1\/29\/17): after considering the latest revisions -- particularly the full ImageNet evaluation results reported in Table 5 demonstrating that the background segmentation 'privileged information' is beneficial even with the full labeled ImageNet dataset -- I've upgraded my rating from 4 to 6.\n\n(I'll reiterate a very minor point about Figure 1 though: I still think the \"0\" and \"1\" labels in the top part of the figures should be swapped to match the other labels.  e.g., the topmost path in figure 1a, with the text \"suppress foreground\", currently has 0 in the background and 1 in the foreground, when one would want the reverse of this to suppress the foreground.)","model":"human","source":"peerread","label":0,"id":5112}
{"text":"This paper proposes a modification to ConvNet training so that the feature activations before the linear classifier are divided into groups such that all pairs of features across all pairs of groups are encouraged to have low statistical correlation. Instead of discovering the groups automatically, the work proposes to use supervision, which they call privileged information, to assign features to groups in a hand-coded fashion. The developed method is applied to image classification.\n\nPros:\n- The paper is clear and easy to follow\n- The experimental results seem to show some benefit from the proposed approach\n\nCons:\n(1) The paper proposes one core idea (group orthogonality w\/ privileged information), but then introduces background feature suppression without much motivation and without careful experimentation\n(2) No comparison with an ensemble\n(3) Full experiments on ImageNet under the \"partial privileged information\" setting would be more impactful\n\nThis paper is promising and I would be willing to accept an improved version. However, the current version lacks focus and clean experiments.\n\nFirst, the abstract and intro focus on the need to replace ensembles with a single model that has diverse (ensemble like) features. The hope is that such a model will have the same boost in accuracy, while requiring fewer FLOPs and less memory. Based on this introduction, I expect the rest of the paper to focus on this point. But it does not; there are no experimental results on ensembles and no experimental evidence that the proposed approach in able to avoid the speed and memory cost of ensembles while also retaining the accuracy benefit.\n\nSecond, the technical contribution of the paper is presented as group orthogonality (GO). However, in Sec 4.1 the idea of background feature suppression is introduced. While some motivation for it is given, the motivation does not tie into GO. GO does not require bg suppression and the introduction of it seems ad hoc. Moreover, the experiments never decouple GO and bg suppression, so we are unable to understand how GO works on its own. This is a critical experimental flaw in my reading.\n\nMinor suggestions \/ comments:\n- The equation in definition 2 has an incorrect normalizing factor (1\/c^(k)^2)\n- Figure 1 seems to have incorrect mask placements. The top mask is one that will mask out the background and only allow the fg to pass","model":"human","source":"peerread","label":0,"id":5113}
{"text":"The starting point of this work is the understanding that by having decorrelated neurons (e.g. neurons that only fire on background, or only on foreground regions) one provides independent pieces of information to the subsequent decisions. As such one gives \"complementary viewpoints\" of the input to the subsequent layers, which can be thought of as performing ensembling\/expert combination within the model, rather than using an ensemble of networks. \n\nFor this, the authors propose a sensible method to decorrelate the activations of intermediate neurons, with the aim of delivering complementary inputs to the final classification layers: they split intermediate neurons to a \"foreground\" and a \"background\" subset, and append side-losses that force them to be zero on background and foreground pixels respectively. \n\nThey demonstrate that this can improve classification on a mid-scale classification example (a fraction of imagenet, and a ResNet with 18, rather than 150 layers), when compared to a \"vanilla\" baseline that does not use these losses.\n\nI enjoyed reading the paper because the idea is simple, smart, and seems to be effective. \nBut there are a few concerns;\n-firstly, the way of doing this seems very particular to vision. In vision one knows that masking the features (during both training and testing) helps, e.g. ","model":"human","source":"peerread","label":0,"id":5114}
{"text":"This paper addresses the practical problem of generating rare or unseen words in the context of language modeling. Since language follows a Zipf\u2019s law, most approaches limit the vocabulary (because of computation reasons) and hence rare words are often mapped to a UNK token. Rare words are especially important in context of applications such as question answering. MT etc. This paper proposes a language modeling technique which incorporates facts from knowledge bases (KBs) and thus has the ability to generate (potentially unseen) words from KBs. This paper also releases a dataset by aligning words with Freebase facts and corresponding Wikipedia descriptions.\n\nThe model first selects a KB fact based on the previously generated words and facts. Based on the selected fact, it then predicts whether to generate a word based on the vocabulary or to output a symbolic word from the KB. For the latter, the model is trained to predict the position of the word from the fact description.\n\nOverall the paper could use some rewriting especially the notations in section 3. The experiments are well executed and they definitely get good results. The heat maps at the end are very insightful. \n\nComments\n\nThis contributions of this paper would be much stronger if it showed improvements in a practical applications such as Question Answering (although the paper clearly mentions that this technique could be applied to improve QA)\nIn section 3, it is unclear why the authors refer the entity as a \u2018topic'. This makes the text a little confusing since a topic can also be associated with something abstract, but in this case the topic is always a freebase entity. \nIs it really necessary to predict a fact at every step before generating a word. In other words, how many distinct facts on average does the model choose to generate a sentence. Intuitively a natural language sentence would be describe few facts about an entity. If the fact generation step could be avoided (by adding a latent variable which decides if the fact should be generated or not), the model will also be faster.\nIn equation 2, the model has to make a hard decision to choose the fact. For this to be end to end trained, every word needs to be annotated with a corresponding fact which might not be always a realistic scenario. For e.g., in domains such as social media text.\nLearning position embeddings for copying knowledge words seems a little counter-intuitive. Does the sequence of knowledge words follow any particular structure like word O_2 is always the last name (e.g. Obama).\nIt would also be nice to compare to char-level LM's which inherently solves the unknown token problem.","model":"human","source":"peerread","label":0,"id":5115}
{"text":"This work introduces a combination of a LM with knowledge based retrieval system. This builds upon the recent trend of incorporating pointers and external information into generation, but includes some novelty, making the paper \"different and more interesting\". Generally though the reviewers found the clarity of the work to be sufficiently an issue that no one strongly defended its inclusion.\n \n Pros:\n - The reviewers seemed to like the work and particularly the problem space. Issues were mainly on presentation and experiments. \n \n Mixed:\n - Reviewers were divided on experimental quality. The work does introduce a new dataset, but reviewers would also have liked use on some existing tasks. \n \n Cons:\n - Clarity and writing issues primarily. All reviewers found details missing and generally struggled with comprehension.\n - Novelty was a question. Impact of work could also be improved by more clearly defining new contributions","model":"human","source":"peerread","label":0,"id":5116}
{"text":"Dear Reviewers and AreaChair, \n\nApproaching to the deadline, I'd like to remind the reviewers of the fact that the revised version of the paper (that we uploaded in Dec. 27) has significant improvements with respect to most of the comments pointed by the reviewers, which are mainly in terms of the writing clarity in Section 3.","model":"human","source":"peerread","label":0,"id":5117}
{"text":"This paper proposes to incorporate knowledge base facts into language modeling, thus at each time step, a word is either generated from the full vocabulary or relevant KB entities.\n\nThe authors demonstrate the effectiveness on a new generated dataset WikiFacts which aligns Wikipedia articles with Freebase facts.  The authors also suggest a modified perplexity metric which penalizes the likelihood of unknown words.\n\nAt a high level, I do like the motivation of this paper -- named entity words are usually important for downstream tasks, but difficult to learn solely based on statistical co-occurrences. The facts encoded in KB could be a great supply for this.\n\nHowever, I find it difficult to follow the details of the paper (mainly Section 3) and think the paper writing needs to be much improved. \n- I cannot find where  f_{symbkey} \/ f_{voca} \/ f_{copy} are defined\n- w^v, w^s are confusing.\n- e_k seems to be the average of all previous fact embeddings? It is necessary to make it clear enough.\n- (h_t, c_t) = f_LSTM(x_{t\u22121}, h_{t\u22121})  c_t is not used?\n- The notion of \u201cfact embeddings\u201d is also not that clear (I understand that they are taken as the concatenation of relation and entity (object) entities in the end).  For the anchor \/ \u201ctopic-itself\u201d facts, do you learn the embedding for the special relations and use the entity embeddings from TransE?\n\nOn generating words from KB entities (fact description), it sounds a bit strange to me to generate a symbol position first.  Most entities are multiple words, and it is necessary to keep that order. Also it might be helpful to incorporate some prior information, for example, it is common to only mention \u201cObama\u201d for the entity \u201cBarack Obama\u201d?\n","model":"human","source":"peerread","label":0,"id":5118}
{"text":"The paper proposes an evolution upon traditional Recurrent Language Models to give the capability to deal with unknown words. It is done by pairing the traditional RNNLM with a module operating on a KB and able to copy from KB facts to generate unseen words. It is shown to be efficient and much better than plain RNNLM on a new dataset.\n\nThe writing could be improved. The beginning of Section 3 in particular is hard to parse.\n\nThere have been similar efforts recently (like \"Pointer Sentinel Mixture Models\" by Merity et al.) that attempt to overcome limitations of RNNLMs with unknown words; but they usually do it by adding a mechanism to copy from a longer past history. The proposal of the current paper is different and more interesting to me in that it try to bring knowledge from another source (KB) to the language model. This is harder because one needs to leverage the large scale of the KB to do so. Being able to train that conveniently is nice.\n\nThe architecture appears sound, but the writing makes it hard to fully understand completely so I can not give a higher rating. \n\n\nOther comments:\n* How to cope with the dependency on the KB? Freebase is not updated anymore so it is likely that a lot of the new unseen words in the making are not going to be in Freebase.\n* What is the performance on standard benchmarks like Penn Tree Bank?\n* How long is it to train compare to a standard RNNLM?\n* What is the importance of the knowledge context $e$?\n* How is initialized the fact embedding $a_{t-1}$ for the first word?\n* When a word from a fact description has been chosen as prediction (copied), how is it encoded in the generation history for following predictions if it has no embedding (unknown word)? In other words, what happens if \"Michelle\" in the example of Section 3.1 is not in the embedding dictionary, when one wants to predict the next word?\n\n\n\n","model":"human","source":"peerread","label":0,"id":5119}
{"text":"\nThis paper addresses the practical problem of generating rare or unseen words in the context of language modeling. Since language follows a Zipf\u2019s law, most approaches limit the vocabulary (because of computation reasons) and hence rare words are often mapped to a UNK token. Rare words are especially important in context of applications such as question answering. MT etc. This paper proposes a language modeling technique which incorporates facts from knowledge bases (KBs) and thus has the ability to generate (potentially unseen) words from KBs. This paper also releases a dataset by aligning words with Freebase facts and corresponding Wikipedia descriptions.\n\nThe model first selects a KB fact based on the previously generated words and facts. Based on the selected fact, it then predicts whether to generate a word based on the vocabulary or to output a symbolic word from the KB. For the latter, the model is trained to predict the position of the word from the fact description.\n\nOverall the paper could use some rewriting especially the notations in section 3. The experiments are well executed and they definitely get good results. The heat maps at the end are very insightful. \n\nComments\n\nThis contributions of this paper would be much stronger if it showed improvements in a practical applications such as Question Answering (although the paper clearly mentions that this technique could be applied to improve QA)\nIn section 3, it is unclear why the authors refer the entity as a \u2018topic'. This makes the text a little confusing since a topic can also be associated with something abstract, but in this case the topic is always a freebase entity. \nIs it really necessary to predict a fact at every step before generating a word. In other words, how many distinct facts on average does the model choose to generate a sentence. Intuitively a natural language sentence would be describe few facts about an entity. If the fact generation step could be avoided (by adding a latent variable which decides if the fact should be generated or not), the model will also be faster.\nIn equation 2, the model has to make a hard decision to choose the fact. For this to be end to end trained, every word needs to be annotated with a corresponding fact which might not be always a realistic scenario. For e.g., in domains such as social media text.\nLearning position embeddings for copying knowledge words seems a little counter-intuitive. Does the sequence of knowledge words follow any particular structure like word O_2 is always the last name (e.g. Obama).\nIt would also be nice to compare to char-level LM's which inherently solves the unknown token problem. ","model":"human","source":"peerread","label":0,"id":5120}
{"text":"This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context. This function is inferred from bilingual translation agreement.\n\nThe main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality. However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.\n\nRegarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij) is optimal. It would have been nice to see some experiments investigating different choices, in particular some baselines where the effect of f is diminished (so that it reduces to f=1 in the limit) would have been interesting. I also feel like there would be a lot to gain from having f be a function of the nearby word embeddings, though this would obvious incur a significant slowdown. (See for example 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should probably be cited.) As it stands, the experimental results do not clearly distinguish the fuzzy paraphrase approach from prior work, i.e. tables 3 and 4 do not show major trends one way or the other.\n\nRegarding the second question, it is hard to draw many conclusions from analogy tasks alone, especially when effects unrelated to good\/bad paraphrasing such as corpus size\/content, window size, vocabulary size, etc., can have an outsize effect on performance. \n\nOverall, I think this is a good paper presenting a sensible idea, but I am not convinced by the experiments that the specific approach is achieving its goal. With some improved experiments and analysis, I would wholeheartedly recommend this paper for acceptance; as it stands, I am on the fence.","model":"human","source":"peerread","label":0,"id":5121}
{"text":"The reviewers agree that the paper's clarity and experimental evaluation can be improved.","model":"human","source":"peerread","label":0,"id":5122}
{"text":"This paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model. The main idea and model are presented convincingly and seem plausible. The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration. The evaluation does not convincingly determine whether the model is a significant improvement over simpler methods (particularly those that do not require the paraphrase database!). Likewise, the model section did not convince me that this was the most obvious model formulation to try. The paper would be stronger if model choices were explained more convincingly or - better yet - alternatives were explored.\n\nOn balance I lean towards rejecting the paper and encouraging the authors to submit a revised and improved version at a near point in the future.\n\nDetailed\/minor points below:\n\n1) While the paper is grammatically mostly correct, it would benefit from revision with the help of a native English speaker. In its current form long sections are very difficult to understand due to the unconventional sentence structure.\n2) The tables need better and more descriptive labels.\n3) The results are somewhat inconclusive. Particularly in the analogy task in Table 4 it is surprising that CBOW does better on the semantic aspect of the task than your embeddings which are specifically tailored to be good at this?\n4) Why was \"Enriched CBOW\" not included in the analogy task?\n5) In the related work section several papers are mentioned that learn embeddings from a combination of lexica and corpora, yet it is repeatedly said that this was the first work of such a kind \/ that there hasn't been enough work on this. That feels a little misleading.","model":"human","source":"peerread","label":0,"id":5123}
{"text":"This paper tries to leverage an external lexicon \/ knowledge base to improve corpus-based word representations by determining (in a fuzzy way) which potential paraphrase is the most appropriate in a particular context.\n\nI think this paper is a bit lost in translation. The grammatical and storytelling styles made it really difficult for me to concentrate, and even unintelligible at times. One of the most important criteria in a conference paper is to communicate one's ideas clearly; unfortunately, I do not feel that this paper meets that standard.\n\nIn addition, the evaluation is rather lacking. There are many ways to evaluate word representations, and Google's analogy dataset has many issues (see, for example, Linzen's paper from RepEval 2016, as well as Drozd et al., COLING 2016).\n\nFinally, this work does not provide any qualitative result or motivation. Why does this method work better? Where does it fail? What have we learned about word representations \/ lexicons \/ corpus-based methods in general?","model":"human","source":"peerread","label":0,"id":5124}
{"text":"This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification (e.g. VGG), and post-process them for image retrieval. In other words, the network is off-the-shelf and solely acts as a feature extractor. The post-processing strategies are borrowed from traditional retrieval pipelines relying on hand-crafted features (e.g. SIFT + Fisher Vectors), denoted by the authors as \"traditional wisdom\".\n\nSpecifically, the authors examine where to extract features in the network (i.e. features are neurons activations of a convolution layer), which type of feature aggregation and normalization performs best, whether resizing images helps, whether combining multiple scales helps, and so on. \n\nWhile this type of experimental study is reasonable and well motivated, it suffers from a huge problem. Namely it \"ignores\" 2 major recent works that are in direct contradictions with many claims of the paper ([a] \"End-to-end Learning of Deep Visual Representations for Image Retrieval\" by  Gordo et al. and [b] \"CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples\" by Radenovi\u0107 et al., both ECCV'16 papers). These works have shown that training for retrieval can be achieved with a siamese architectures and have demonstrated outstanding performance. As a result, many claims and findings of the paper are either outdated, questionable or just wrong.\n\nHere are some of the misleading claims: \n\n  - \"Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years.\"\n  Until [a] (not cited), the state-of-the-art was still largely dominated by sparse invariant features based methods (see last Table in [a]).\n  \n  - \"the proposed method [...] outperforms the state-of-the-art methods on four typical datasets\"\n  That is not true, for the same reasons than above, and also because the state-of-the-art is now dominated by [a] and [b].\n  \n  - \"Also in situations where a large numbers of training samples are not available, instance retrieval using unsupervised method is still preferable and may be the only option.\".\n  This is a questionable opinion. The method exposed in \"End-to-end Learning of Deep Visual Representations for Image Retrieval\" by Gordo et al. outperforms the state-of-the-art on the UKB dataset (3.84 without QE or DBA) whereas it was trained for landmarks retrieval and not objects, i.e. in a different retrieval context. This demonstrates that in spite of insufficient training data, training is still possible and beneficial.\n\n  - Finally, most findings are not even new or surprising (e.g. aggregate several regions in a multi-scale manner was already achieved by Tolias at al, etc.). So the interest of the paper is limited overall.\n\nIn addition, there are some problems in the experiments. For instance, the tuning experiments are only conducted on the Oxford dataset and using a single network (VGG-19), whereas it is not clear whether these conditions are well representative of all datasets and all networks (it is well known that the Oxford dataset behaves very differently than the Holidays dataset, for instance). In addition, tuning is performed very aggressively, making it look like the authors are tuning on the test set (e.g. see Table 3). \n\nTo conclude, the paper is one year too late with respect to recent developments in the state of the art.","model":"human","source":"peerread","label":0,"id":5125}
{"text":"The paper conducts a detailed evaluation of different CNN architectures applied to visual instance retrieval. The authors consider various deep neural network architectures, with a focus on architectures pre-trained for image classification. \n \n An important concern of the reviewers is the relevance of the evaluation given the recent impressive experimental results of deep neural networks trained end-to-end for visual instance retrieval by Gordo et al. \"End-to-end Learning of Deep Visual Representations for Image Retrieval\". Another concern is the novelty of the proposed evaluation given the evaluation of the performance for visual instance retrieval of deep neural network pre-trained for image classification performed in Paulin et al. \"Convolutional Patch Representations for Image Retrieval: An Unsupervised Approach\". \n \n A revision of the paper, following the reviewers' suggestions, will generate a stronger submission to a future venue.","model":"human","source":"peerread","label":0,"id":5126}
{"text":"The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. The authors focus on testing various architectural choices, but do not propose or compare to end-to-end learning frameworks.\n\nTechnically, the contribution is clear, particularly with the promised clarifications on how multiple scales are handled in the representation. However, I am still not entirely clear whether there would be a difference in the multi-scale settting for full and cropped queries.\n\nWhile the paper focuses on comparing different baseline architectures for CNN-based image retrieval, several recent papers have proposed to learn end-to-end representations specific for this task, with very good result (see for instance the recent work by Gordo et al. \"End-to-end Learning of Deep Visual Representations for Image Retrieval\"). The authors clarify that their work is orthogonal to papers such as Gordo et al. as they assess instead the performance of networks pre-trained from image classification. In fact, they also indicate that image retrieval is more difficult than image classification -- this is because it is performed by using features originally trained for classification. I can partially accept this argument. However, given the results in recent papers, it is clear than end-to-end training is far superior in practice and it is not clear the analysis developed by the authors in this work would transfer or be useful for that case as well.\n","model":"human","source":"peerread","label":0,"id":5127}
{"text":"Authors investigate how to use pretrained CNNs for retrieval and perform an extensive evaluation of the influence of various parameters. For detailed comments on everything see the questions I posted earlier. The summary is here:\n\nI don't think we learn much from this paper: we already knew that we should use the last conv layer, we knew we should use PCA with whitening, we knew we should use original size images (authors say Tolias didn't do this as they resized the images, but they did this exactly for the same reason as authors didn't evaluate on Holidays - the images are too big. So they basically used \"as large as possible\" image sizes, which is what this paper effectively suggests as well), etc. This paper essentially concatenates methods that people have already used, and performs some more parameter tweaking to achieve the state-of-the-art (while the tweaking is actually performed on the test set of some of the tests).\n\nThe setting of the state-of-the-art results is quite misleading as it doesn't really come from the good choice of parameters, but mainly due to the usage of the deeper VGG-19 network. \n\nFurthermore, I don't think it's sufficient to just try one network and claim these are the best practices for using CNNs for instance retrieval - what about ResNet, what about Inception, I don't know how to apply any of these conclusions for those networks, and would these conclusions even hold for them. Furthermore the parameter tweaking was done on Oxford, I really can't tell what conclusions would we get if we tuned on UKB for example. So a more appropriate paper title would be \"What are the best parameter values for VGG-19 on Oxford\/Paris benchmarks?\" - I don't think this is sufficiently novel nor interesting for the community.\n","model":"human","source":"peerread","label":0,"id":5128}
{"text":"\n5)\nI agree with other reviewers on the lack of comparison with Gordo et al and Radenovic et al, though I understand that authors' arguments are that they do not want to train their networks (though then comparing with Arandjelovic et al. also doesn't make sense). It's still worth citing these papers and commenting on them. Also, with such an extensive set of experiments, it's a bit arguable if authors don't really do training - they don't do the canonical SGD, but they essentially perform grid search for parameters on the test (see question 3).\n\n6)\nI'm not sure what did we actually learn from this paper. To use the last conv? We knew that before as all recent papers do this (Arandjelovic et al, Tolias et al, Gordo et al, Radenovic et al, Babenko and Lempitsky, ..). That using original image sizes is important? We knew this as well, early works (Babenko et al 2014, etc) used smaller images while all recent works apply the networks convolutionally over original size images (e.g. Tolias et al have this experiment in table 1). That one should use PCA with whitening (and if possible learn whitening on the test set)? We knew this already as well. So the only two things that haven't been done in exactly the same way as people did it before is the multi-scale pooling (though obviously various other similar versions exist), and the exploration of max\/sum pooling with l1 or l2 normalization (though the experiments in table 1 are basically ignored as sum-l1 works the best there, but authors then say that actually later they notice that for multiscale max-l2 works best). Actually the most interesting part for me, one that I can actually say I didn't know and don't think anyone knew, is figure 3.\n\n7)\nI think it's a bit of an overstatement to call this paper 'best practice for CNNs' when only a single CNN architecture, VGG-19, is considered. What is the best practice for other models, e.g. ResNet, Inception? Presumably the last conv is likely to be best though for ResNet it's not that clear, and I'm not sure if sum vs max pooling would change as those two networks were trained with sum pooling, and I'm not sure if any of the other conclusions hold either. This is more of a surgery of VGG-19 than best practices for CNNs in general.\n\n8)\nOn a more philosophical level, and not only aimed at authors but also at others who are potentially reading this - this conference is about learning representations, while no learning is being performed. Taking CNNs as black boxes and tweaking the inputs and outputs in different ways with different normalizations is much more like using hand-engineered features like SIFT (replace black-box SIFT extractor with black-box CNN) than actually doing Deep Learning. I'm not saying this type of paper shouldn't exist as it's good to know what works best, but my preference in terms of what papers I would like to see in the future is:\na) There have been too many papers for using CNNs as black-boxes, I hoped we are finally over with this\nb) For ICLR I think one should actually do some training, e.g. after we figure out the best image representation, now train the whole system end-to-end and see if you can improve the performance.\nc) Design architectures which are specifically aimed at image retrieval - maybe something different than CNNs for classification pops up?\nd) Figure out ways to train CNNs for retrieval, we know how to do it for classification by paying people to label millions of images, can we do something better for retrieval? (though this is to some extend addressed now by Arandjelovic et al, Gordo et al and Radenovic et al).\n\n\nOther minor comments:\n\n- I was also surprised by the \"harder than category retrieval\" statement, as reviewer 3. I wouldn't go as far as saying that the opposite is true either, the two just cannot be compared so easily.\n- Inconsistencies of references (e.g. \"Y. Lecun\" vs \"Ross Girshick\", \"CVPR\" versus \"Computer\nVision and Pattern Recognition\", ..\n","model":"human","source":"peerread","label":0,"id":5129}
{"text":"In this paper the authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs. For example, the authors highlight the well known but often not sufficiently appreciated fact that the probability mass of high dimensional Gaussian distributions concentrates near a thin hyper-shell with a certain radius. They therefore propose to use spherical interpolations (great arcs) instead of the commonly used linear interpolations. In a similar spirit they propose a visualisation for analogies and techniques to reinforce structure in VAE latent spaces.\n\nI find it hard to give clear recommendation for this paper: On the one hand I enjoyed reading it and I might want use some of the proposals (e.g. spherical interpolations; J-diagrams) in future work of mine. On the other hand, it\u2019s obvious that this paper is not a typical machine learning paper; it does not propose a new model, or training method, or provide (theoretical\/empirical) insight and it does not have the scientific quality and depth I\u2019ve seen in many other ICLR submissions. But it does more than just describing useful \u201ctricks\u201d. And all things considered I think this paper deserves a wider audience (but  I'm not convinced that ICLR is the right venue)","model":"human","source":"peerread","label":0,"id":5130}
{"text":"This paper proposes some interesting ideas about visualizing latent-variable models. The paper is nicely written and presented, but the originality and importance of the work isn't enough. Also, neither the reviewers nor I were convinced that spherical interpolation makes more sense than linear interpolation.","model":"human","source":"peerread","label":0,"id":5131}
{"text":"This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. This paper is somewhat challenging to assess since it doesn't propose a new algorithm, model, application etc. On the one hand these techniques will be highly relevant to the generative modeling community and I think this paper deserves a wide audience. The techniques proposed are simple, well explained, and of immediate use to those working on generative models. However, I'm not sure the paper is appropriate for an ICLR conference track as it doesn't provide any greater theoretical insights into sampling generative models and there are no comparisons \/ quantitative evaluations of the techniques proposed. Overall, I'm very much on the fence since I think the techniques are useful and this paper should be read by those interested in generating modeling. I would be willing to increase my core if the author could present a case for why ICLR is an appropriate venue for this work.","model":"human","source":"peerread","label":0,"id":5132}
{"text":"This paper proposed a set of different things under the name of \"sampling generative models\", focusing on analyzing the learned latent space and synthesizing desirable output images with certain properties for GANs.  This paper does not have one single clear message or idea, but rather proposed a set of techniques that seem to produce visually good looking results.  While this paper has some interesting ideas, it also has a number of problems.\n\nThe spherical interpolation idea is interesting, but after a second thought this does not make much sense.  The proposed slerp interpolation equation (page 2) implicitly assumes that the two points q1 and q2 lie on the same sphere, in which case the parameter theta is the angle corresponding to the great arc connecting the two points on the sphere.  However, the latent space of a GAN, no matter trained with a uniform distribution or a Gaussian distribution, is not a distribution on a sphere, and many points have different distances to the origin.  The author's justification for this comes from the well known fact that in high dimensional space, even with a uniform distribution most points lie on a thin shell in the unit cube.  This is true because in high-dimensional space, the outer shell takes up most of the volume in space, and the inner part takes only a very small fraction of the space, in terms of volume.  This does not mean the density of data in the outer shell is greater than the inner part, though.  In a uniform distribution, the data density should be equal everywhere, a point on the outer shell is not more likely than a point in the inner part.  Under a Gaussian model, the data density is on the other hand higher in the center and much lower on the out side.  If we have a good model of data, then sampling the most likely points from the model should give us plausible looking samples.  In this sense, spherical interpolation should do no better than the normally used linear interpolation.  From the questions and answers it seems that the author does not recognize this distinction.  The results shown in this paper seem to indicate that spherical interpolation is better visually, but it is rather hard to make any concrete conclusions from three pairs of examples.  If this is really the case then there must be something else wrong about our understanding of the learned model.\n\nAside from these, the J-diagram and the nearest neighbor latent space traversal both seems to be good ways to explore the latent space of a learned model.  The attribute vector section on transforming images to new ones with desired attributes is also interesting, and it provides a few new ways to make the GAN latent space more interpretable.\n\nOverall I feel most of the techniques proposed in this paper are nice visualization tools.  The contributions however, are mostly on the design of the visualizations, and not much on the technical and model side.  The spherical interpolation provides the only mathematical equation in the paper, yet the correctness of the technique is arguable.  For the visualization tools, there are also no quantitative evaluation, maybe these results are more art than science.","model":"human","source":"peerread","label":0,"id":5133}
{"text":"In this paper the authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs. For example, the authors highlight the well known but often not sufficiently appreciated fact that the probability mass of high dimensional Gaussian distributions concentrates near a thin hyper-shell with a certain radius. They therefore propose to use spherical interpolations (great arcs) instead of the commonly used linear interpolations. In a similar spirit they propose a visualisation for analogies and techniques to reinforce structure in VAE latent spaces.\n\nI find it hard to give clear recommendation for this paper: On the one hand I enjoyed reading it and I might want use some of the proposals (e.g. spherical interpolations; J-diagrams) in future work of mine. On the other hand, it\u2019s obvious that this paper is not a typical machine learning paper; it does not propose a new model, or training method, or provide (theoretical\/empirical) insight and it does not have the scientific quality and depth I\u2019ve seen in many other ICLR submissions. But it does more than just describing useful \u201ctricks\u201d. And all things considered I think this paper deserves a wider audience (but  I'm not convinced that ICLR is the right venue)\n","model":"human","source":"peerread","label":0,"id":5134}
{"text":"Could you please explain why the spherical interpolation is better when the prior is uniform?","model":"human","source":"peerread","label":0,"id":5135}
{"text":"Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format for your submissions to be considered. Thank you!","model":"human","source":"peerread","label":0,"id":5136}
{"text":"This work proposes to iteratively improve a sentence that has been generated from another MT system (in this case, a phrase-based system). The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word, and predicts the current target word. During testing, the gold words are replaced with the generated words. While this is an interesting area of research, I am not convinced by the proposed approach, and experimental evidence is lacking.\n\nUnder the current framework, it is all but impossible for the model to do anything more than a rudimentary word replacement (e.g. it cannot change \"I went to the fridge even though I was not hungry\" to \"Although I was not hungry, I went to the fridge\"). The fact that only 0.6 words are edited on average supports this. \n\nSpecific comments:\n- It would be interesting to see what the improvements are if the baseline model is a neural system.\n- It seems strange (to me at least) that T^i and L(y^{-i|k}) only look at a window of 2k words. It means that when making the decision to change the i-th word, the model does not know what was generated outside of the window? \n- Relatedly, the idea of changing individual words based on local (i.e. word-level) scores seems counterintuitive. Given that we have the full generated sentence, don't we want a global score? Scoring at the sentence-level could also make room for non-greedy search strategies, which could potentially facilitate richer edits.\n- How does the approach compare to a model that simply re-ranks the k-best output?\n- Instead of editing, did you consider learning an encoder-decoder that takes in x, y_g, and generates y_ref? When decoding you can attend to both x and y_g.\n\nMinor comments:\n- Iteratively improving a generated text was also explored in","model":"human","source":"peerread","label":0,"id":5137}
{"text":"The paper is an interesting first step, but the reviewers found flaws in the overall argument. Further, the method is not contextualized well enough in relation to prior work.","model":"human","source":"peerread","label":0,"id":5138}
{"text":"This paper proposes a method for iteratively improving the output of an existing machine translation by identifying potential mistakes and proposing a substitution, in this case using an attention-based model.  It is motivated by the method in which (it is assumed) human translators operate.\n\nThe paper is interesting and imaginative.  However, in general terms, I am somewhat sceptical of this kind of approach -- whereby a machine learning method is used to identify and correct the predictions of another method, or itself -- because in the first case, if the new method is better, why not use it from the outset in place of the other method?  And in the second case, since the method has no new information compared to previously, why is it more likely to identify more past mistakes and correct them, than identify past correct terms and turn them into new errors?  That is unless there is a specific reason that an iterative approach can be shown to converge to a better solution when run over several epochs.\n\nThis paper does not convince me on these points.  Indeed, unsurprisingly, the authors note that \"the probability of correctly labelling a word as a mistake remains low (62%)\" - this admittedly beats a random-chance baseline, but is not compared to something more meaningful, such as simply contrasting the existing system with a more powerful convolutional model and labelling all discrepancies as mistakes.  The oracle experiments are rather meaningless - they just serve to confirm that improving a translation is very easy when the existing mistakes have been identified, but much harder when they are not. \n\nAlthough I do like the paper on the whole, to really convince me that main objective -- ie. that **iterative** improvement is beneficial -- has been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular, to show that an iterative refinement scheme can really improve over a system closely matched to the attention-based model, both when used in isolation and when used in system combination with a PBMT system, and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention-based model.\n\nMinor comments:\n\nI find the notation excessively fiddly at times - eg F^i = (F^{i,1}, F^{i,|F^i|}) - why use |F^i| here when F is a matrix, so surely the length of the slice is not dependent on i?\n\nIn the discussion in section 4 - it seems that this still creates a mismatch between the training and test conditions - could anything be done about this?\n\n\n\n\n\n\n\n\n ","model":"human","source":"peerread","label":0,"id":5139}
{"text":"This paper proposes a model for iteratively refining translation hypotheses. This has several benefits, including enabling the translation model to condition not only on \u201cleft context\u201d, but also on \u201cright context\u201d, and potentially enabling more rapid and\/or accurate decoding. The motivation given is that often translators (and text generators generally) use a process of refinement in generating outputs.\n\nThis is an important idea that is not currently playing much of a role in neural net models, so this paper is a welcome contribution. However, while I think this is an important first step, I do feel that the lack of in depth analysis suggests this paper is not quite ready for a final publication version. For example, there are many possible connections to prior work in NLP, MT, and other parts of ML that could better contextualize this work (see specifics below). More substantively, the model in Section 3 could be interpreted as a globally normalized, undirected (~CRF) translation model trained using a pseudo-likelihood objective. In this analysis, the model squarely back in the context of traditional discriminative translation models which used \u201cundirected\u201d features, and the decoding algorithm then looks more like a standard greedy hill-climbing algorithm (albeit with an extra heuristic model for selecting which variable to update), which is also nothing unfamiliar.\n\nMy second criticism the limitations of the model are not well discussed. For example, the proposed editing procedure cannot obviously remove or insert a word from a translation. While I think this is a reasonable assumption than can be made for the sake of tractability, it is very unfortunate since missing or extra words (esp. function words) are a common problem in the baseline models that are being used. Second, the standard objections to absolute positional models (vs. relative positional models) seem particularly crucial to bring up in this work, especially since they might make some of the design decisions a bit more justifiable.\n\nOverall, this is an initial step in an interesting direction, but it needs more thorough analysis to demonstrate its value. A more thorough analysis will also likely suggest some important model variants (for example: is a global translation model really the goal? or is a post-editing model that fixes outputs with more complex operations more ideal?)\n\nRelated work:\nI think that more could be done to put this work in the context of what has come before and what is currently going on in other parts of ML. The idea of iterative refinement has been proposed in other problems that have complex output spaces, for example the DRAW model of Gregor et al. and the conditional adversarial network models used to refine images proposed recently by Isola et al. In NLP, there have been several (stochastic) hill climbing approaches that have been proposed, such as the work on parsing by Zhang and Lei et al. (2014) who use random initial guesses and then do greedy hill climbing using a series of local refinements, the structured prediction cascades of Weiss and Taskar (2009) (not to mention general coarse-to-fine modeling strategies). Finally, in MT, Arun et al. (2009) who use a Gibbs sampler to refine an initial guess to do decoding with a more complex model. The use of an explicit error model is rather novel in the context of correction, but I would point out that although the proposed architecture is different, the discriminative word lexicon models of Mauser et al. (2009) and the neural version of the same by Ha et al. (2014) are similar in spirit. There have also been a number of papers on \u201cautomatic post editing\u201d, including the shared task at WMT2016, and there are not only standard test sets and baselines, but also datasets that could actually be used to train a post editing model with human-generated data. Minimally using the techniques they described could be a useful foil for the models presented in this paper.\n\n\u201cthe target sentence is also embedded in distributional space via a lookup table\u201d I think \u201cdistributional space\u201d is a bit unclear. Maybe \u201cthe target sentence is represented in terms of distributed word representations via a lookup table\u201d or something like that. \u201cdistributional\u201d suggests that the representations are derived from how the words are distributed in the corpus, whereas you are learning these representations on this task which isn\u2019t modeling their distribution except only very indirectly.\n\nSection 3 Model: In Section 3, the model computes the distribution over target word types at an absolute position i in the output sentence, given the target language context and the source language context. It is introduced as the model that is used to refine an existing hypothesis, but it is not immediately clear that the training data for this model (at least in this section) are the gold standard translations- \u201ctraining set\u201d could be interpreted in variety of ways. This becomes clearer when reading later in the paper, but it\u2019s a bit less clear when reading from the beginning for the first time.\n\nThe use of a fixed sized window for representing the target word in context also seems to make something like a model 1 assumption since only the lexical features (and not any \u201calignment\u201d or \u201cpositional\u201d features) determine the attention. This should be clarified since it will make the assumptions of the model more transparent (and also suggest possible refinements to the model, e.g., including (representations) of i and j as components of S^j and T^i, which would allow model 2\/3-like responses to be learned- although by leaving them out, the model might behave a bit more like a relative positional model than an absolute positional model, which is probably attractive).\n\nFinally, some discussion for why a fixed window is used to represent the target sentence is worth including (since a global context is apparently used to represent the source sentence).\n\nThe relationship between this training objective and pseudo likelihood (PL; Besag, 1975) might be worth mentioning. Since I believe this is just a PL objective for a certain global model, this suggests alternative decoding algorithms, or certainly a different analysis of the proposed decoding objective.\n\nThe section 4 model conditions on the true context of a position in the true target, the current target guess, and the source. I don\u2019t completely understand the rationale for this model since at test time only two of these variables are available, and the replacement of y_ref with y_g seems hard to justify.","model":"human","source":"peerread","label":0,"id":5140}
{"text":"Disclosure: I am not an expert in machine translation algorithms.\n\nSummary: A human translator does not come up with the final translation right\naway. Instead, (s)he uses an iterative process, starting with a rough draft\nwhich is corrected little by little. The idea behind this paper is to\nimplement a similar framework for an automated system. \n\nThis paper is generally well written. \n\nIt is my opinion however that drawings illustrating the architectures would help\nunderstanding how the different algorithms relate to one another.\n\nI like a lot that you report on a preliminary experiment to give an\nintuition of how difficult the task is. You should highlight the links\nbetween the task of finding the errors in a guess translation and the task\nof iterative refinement. Could you use post-edited text to have a more\nsolid ground-truth?\n\nMy main concern with this paper is that in the experimental section the \niterative approach tries to improve upon only one type of machine translation. \nWhich immediately prompts these questions:\n- why did they choose that approach to improve on?\n- what is the part of the improvement that comes from the choice of the\n  initial draft (maybe it was a very bad draft)? \n\nHere are some minor typos:\n- p.2: ... a lookup table that replace*S* each word... ?\n- p.3: I might be mistanken but it seems to me that j is used for two\n  different things. It is confusing.\n- p.3: ...takes as input these representation*S* and outputs... ?\n","model":"human","source":"peerread","label":0,"id":5141}
{"text":"This work proposes to iteratively improve a sentence that has been generated from another MT system (in this case, a phrase-based system). The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word, and predicts the current target word. During testing, the gold words are replaced with the generated words. While this is an interesting area of research, I am not convinced by the proposed approach, and experimental evidence is lacking.\n\nUnder the current framework, it is all but impossible for the model to do anything more than a rudimentary word replacement (e.g. it cannot change \"I went to the fridge even though I was not hungry\" to \"Although I was not hungry, I went to the fridge\"). The fact that only 0.6 words are edited on average supports this. \n\nSpecific comments:\n- It would be interesting to see what the improvements are if the baseline model is a neural system.\n- It seems strange (to me at least) that T^i and L(y^{-i|k}) only look at a window of 2k words. It means that when making the decision to change the i-th word, the model does not know what was generated outside of the window? \n- Relatedly, the idea of changing individual words based on local (i.e. word-level) scores seems counterintuitive. Given that we have the full generated sentence, don't we want a global score? Scoring at the sentence-level could also make room for non-greedy search strategies, which could potentially facilitate richer edits.\n- How does the approach compare to a model that simply re-ranks the k-best output?\n- Instead of editing, did you consider learning an encoder-decoder that takes in x, y_g, and generates y_ref? When decoding you can attend to both x and y_g.\n\nMinor comments:\n- Iteratively improving a generated text was also explored in ","model":"human","source":"peerread","label":0,"id":5142}
{"text":"This paper introduces a maximum total correlation procedure, adds a target and then adds noise perturbations.\n\nTechnical issues:\n\nThe move from (1) to (2) is problematic. Yes it is a lower bound, but by igoring H(Z), equation (2) ignores the fact that H(Z) will potentially vary more significantly that H(Z|Y). As a result of removing H(Z), the objective (2) encourages Z that are low entropy as the H(Z) term is ignored, doubly so as low entropy Z results in low entropy Z|Y. Yes the -H(X|Z) mitigates against a complete entropy collapse for H(Z), but it still neglects critical terms. In fact one might wonder if this is the reason that semantic noise addition needs to be done anyway, just to push up the entropy of Z to stop it reducing too much.\n\nIn (3) arbitrary balancing paramters lamda_1 and lambda_2 are introduced ex-nihilo - they were not there in (2). This is not ever justified.\n\nThen in (5), a further choice is made by simply adding L_{NLL} to the objective. But in the supervised case, the targets are known and so turn up in H(Z|Y). Hence now H(Z|Y) should be conditioned on the targets. However instead another objective is added again without justification, and the conditional entropy of Z is left disconnected from the data it is to be conditioned on. One might argue the C(X,Y,Z) simply acts as a prior on the networks (and hence implicitly on the weights) that we consider, which is then combined with a likelihood term, but this case is not made. In fact there is no explicit probabilistic or information theoretic motivation for the chosen objective.\n\nGiven these issues, it is then not too surprising that some further things need to be done, such as semantic noise addition to actually get things working properly. It may be the form of noise addition is a good idea, but given the troublesome objective being used in the first place, it is very hard to draw conclusions.\n\nIn summary, substantially better theoretical justification of the chosen model is needed, before any reasonable conclusion on the semantic noise modelling can be made.","model":"human","source":"peerread","label":0,"id":5143}
{"text":"The reviewers all expressed concerns with the technical quality of this work. In particular, the reviewers are concerned that ignoring certain entropy terms in the objective is problematic and would require significantly more justification theoretically and empirically. The reviewers believe that the authors had to resort to unjustified tricks such as adding noise in order to compensate for the missing terms in the objective. Some of the reviewers also had concerns with the choice of experiments, expressing that the authors did not choose the right baseline comparisons to compare to (e.g. convolutional networks vs. fully connected networks on MNIST). Hopefully the thorough feedback and lengthly discussion, along with the authors' responses (both in the text and additions to the paper and appendix), will lead to a stronger submission to a future conference.","model":"human","source":"peerread","label":0,"id":5144}
{"text":"We have made overall revisions to the manuscript in order to clarify mathematical justification, explanations, and several empirical analyses. Among the revised contents, we especially focused on the methodology part (Section 2). Section 2 is divided into two subsections. Section 2.1 explains the base model as an extension of joint learning approaches. This subsection includes details of derivations for mathematical justification of the base model (See Appendix_A1). Section 2.2 shows the proposed latent space modeling method. This subsection focuses on explaining how to model the semantic-preserving perturbation on the latent space. We are sorry for the incompleteness of the early version of manuscript. We ask the reviewers to revisit the updated manuscript once again.\n","model":"human","source":"peerread","label":0,"id":5145}
{"text":"The paper introduces supervised deep learning with layer-wise reconstruction loss (in addition to the supervised loss) and class-conditional semantic additive noise for better representation learning. Total correlation measure and additional insights from auto-encoder are used to derive layer-wise reconstruction loss and is further combined with supervised loss. When combining with supervised loss the class-conditional additive noise model is proposed, which showed consistent improvement over the baseline model. Experiments on MNIST and CIFAR-10 datasets while changing the number of training examples per class are done extensively.\n\nThe derivation of Equation (3) from total correlation is hacky. Moreover, assuming graphical model between X, Y and Z, it should be more carefully derived to estimate H(X|Z) and H(Z|Y). The current proposal, encoding Z and Y from X and decoding from encoded representation is not really well justified.\n\nIs \\sigma in Equation 8 trainable parameter or hyperparameter? If it is trainable how it is trained? If it is not, how are they set? Does j correspond to one of the class? The proposed feature augmentation sounds like simply adding gaussian noise to the pre-softmax neurons. That being said, the proposed method is not different from gaussian dropout (Wang and Manning, ICML 2013) but applied on different layers. In addition, there is a missing reference (DisturbLabel: Regularizing CNN on the Loss Layer, CVPR 2016) that applied synthetic noise process on the loss layer.\n\nExperiments should be done for multiple times with different random subsets and authors should provide mean and standard error. Overall, I believe the proposed method is not very well justified and has limited novelty. ","model":"human","source":"peerread","label":0,"id":5146}
{"text":"The paper presents a new regularization technique for neural networks, which seeks to maximize correlation between input variables, latent variables and outputs. This is achieved by defining a measure of total correlation between these variables and decomposing it in terms of entropies and conditional entropies.\n\nAuthors explain that they do not actually maximize the total correlation, but a lower-bound of it that ignores simple entropy terms, and only considers conditional entropies. It is not clearly explained what is the rationale for discarding these entropy terms.\n\nEntropies measures are applying to probability distributions (i.e. this implies that the variables in the model should be random). The link between the conditional entropy formulation and the reconstruction error is not made explicit. In order to link these two views, I would have expected, for example, a noise model for the units of the network.\n\nLater in the paper, it is claimed that the original ladder network is not suitable for supervised learning with small samples, and some empirical results seek to demonstrate this. But a more theoretical explanation why it is the case would have been welcome.\n\nThe MNIST results are shown for a particular convolutional neural network architecture, however, most ladder network results for this dataset have been produced on standard fully-connected architectures. Results for such neural network architecture would have been desirable for more comparability with original ladder neural network results.","model":"human","source":"peerread","label":0,"id":5147}
{"text":"An updated version of the paper has been uploaded. This version includes more clear explanation of the base model, reflecting the issues raised by AnnoReviewer3.","model":"human","source":"peerread","label":0,"id":5148}
{"text":"This paper evaluates several strategies to reduce output vocabulary size in order to speed up NMT decoding and training. It could be quite useful to practitioners, although the main contributions of the paper seem somewhat orthogonal to representation learning and neural networks, and I am not sure ICLR is the ideal venue for this work.\n\n- Do the reported decoding times take into account the vocabulary reduction step?\n- Aside from machine translation, might there be applications to other settings such as language modeling, where large vocabulary is also a scalability challenge?\n- The proposed methods are helpful because of the difficulties induced by using a word-level model. But (at least in my opinion) starting from a character or even lower-level abstraction seems to be the obvious solution to the huge vocabulary problem.","model":"human","source":"peerread","label":0,"id":5149}
{"text":"The reviewers agree that the method is exciting as practical contributions go, but the case for originality is not strong enough.","model":"human","source":"peerread","label":0,"id":5150}
{"text":"This paper conducts a comprehensive series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation.\n\nA range of techniques are investigated, ranging from very simple methods such as word co-occurences, to the relatively complex use of SVMs.\n\nThe experiments are solid, comprehensive and very useful in practical terms.  It is good to see that the best vocabulary selection method is very effective at achieving a very high proportion of the coverage of the full-vocabulary model (fig 3).  However, I feel that the experiments in section 4.3 (vocabulary selection during training) was rather limited in their scope - I would have liked to see more experiments here.\n\nA major criticism I have with this paper is that there is little novelty here.  The techniques are mostly standard methods and rather simple, and in particular, there it seems that there is not much additional material beyond the work of Mi et al (2016).  So although the work is solid, the lack of originality lets it down.\n\nMinor comments: in 2.1, the word co-occurence measure - was any smoothing used to make this measure more robust to low counts?\n\n\n\n\n\n\n\n\n","model":"human","source":"peerread","label":0,"id":5151}
{"text":"This paper compares several strategies for guessing a short list of vocabulary for the target language in neural machine translation. The primary findings are that word alignment dictionaries work better than a variety of other techniques.\n\nMy take on this paper is that to have a significant impact, it needs to make the case for why one might want vocabulary rather than characters or sub word units like BPE. I think there are likely many very good reasons to do this that could be argued for (synthesize morphology, deal with transliteration, etc), but most of these would suggest some particular models and experiments, which are of course not in this paper. As it is, I think this paper is a useful but minor contribution that shows that word alignment is a good way of getting short lists, but it does not strongly make the case that we should abandon work in other directions.\n\nMinor comments:\nIn addition to the SVM approach for modeling vocabulary, the discriminative word lexicon of Mauser et al. (2009) and the neural version of Ha et al. (2014) are also worth mentioning.\n\nIt would be useful to know what the coverage rate of the actual full vocabulary would be (rather than the 100k \u201cfull vocabulary\u201d). Since presumably this technique could be used to work with much larger vocabularies.\n\nWhen reducing the vocabulary size for training, the Mi et al. (2016) technique of taking the union of all the vocabularies in a mini batch seems like a rather strange objective. If the vocabulary of a single sentence is used, the probabilistic semantics of the translation model can still be preserved since p(e | f, vocab(f)) = p(e | f) if p(vocab(f) | f) = 1, i.e., is deterministic, which it is here. Whereas the objective is no longer a sensible probability model in the mini batch vocabulary case. Thus, while it may be a bit more difficult to implement, it seems like it would at least be a sensible comparison to make.","model":"human","source":"peerread","label":0,"id":5152}
{"text":"In this paper, the authors present several strategies to select a small subset of target vocabulary to work with per source sentence, which results in significant speedup. The results are convincing and I think this paper offers practical values to general seq2seq approaches to language tasks. However, there is little novelty in this work: the authors further mostly extend the work of (Mi et al., 2016) with more vocabulary selection strategies and thorough experiments. This paper will fit better in an NLP venue.","model":"human","source":"peerread","label":0,"id":5153}
{"text":"This paper evaluates several strategies to reduce output vocabulary size in order to speed up NMT decoding and training. It could be quite useful to practitioners, although the main contributions of the paper seem somewhat orthogonal to representation learning and neural networks, and I am not sure ICLR is the ideal venue for this work.\n\n- Do the reported decoding times take into account the vocabulary reduction step?\n- Aside from machine translation, might there be applications to other settings such as language modeling, where large vocabulary is also a scalability challenge?\n- The proposed methods are helpful because of the difficulties induced by using a word-level model. But (at least in my opinion) starting from a character or even lower-level abstraction seems to be the obvious solution to the huge vocabulary problem.\n","model":"human","source":"peerread","label":0,"id":5154}
{"text":"The primary point made by this paper is that given certain architectural characteristics of multi-GPU systems, namely the use of bi-directional PCI-E for communication and the integration of two independent DMA engines on recent GPU devices (providing support for simultaneous independent communications), and given the characteristics of the communications patterns required by synchronous SGD trainers for deep neural networks, namely that the messages are large, dense, and have a fixed length, it makes sense to design communication collectives such as broadcast, reduce, and allreduce specifically for the use case of synchronous SGD training on a multi-GPU system.  The paper describes the implementation of these three collectives (broadcast, reduce, and allreduce) using a linear pipelining (LP) scheme on a (logical) ring topology.  The paper compares the LP collectives to two alternatives:  collectives based on a minimal spanning tree (MST) topology and collectives based on bidirectional exchange (BE).  First, a theoretical comparison is made using a standard cost model used in the high performance computing community.  When assumptions based on multi-GPU system architecture (very low latency for messages) and on the communication characteristics of synchronous SGD training (very large messages) are integrated into the model, the paper finds that the LP collectives should be less costly than BE collectives by a factor of 2 and less costly than MST collectives by a factor of log(p), where p is the number of GPUs being used.  Second, an empirical comparison is performed in which (1) the time required to perform each of the different collectives on a 4-device (k40m) system is measured as a function of message size and (2) the time required to perform each of the different collectives with a 200 MB message length is measured as a function of the number of devices in the system.  These measurements show that the LP-based collectives are consistently the fastest.  Third, DNN training experiments with AlexNet and GoogLeNet are performed on a 4-device system using three different synchronous SGD algorithms with the different implementations of the collectives (a total of 6 different algorithms in all).  Measurements of the communication and computation costs show that the LP collectives reduce communication costs without affecting computation costs (as expected).  Measurements of the convergence of the training loss as a function of time for the two DNN architectures show that use of the LP collectives leads to faster training.\n\nWhile the theory says that the costs of LP collectives should be invariant to the number of devices in a multi-GPU system, the empirical work shows that in practice this does not hold going from 4 to 5 devices (in the tested configuration) because in a 5-device system messages must traverse the QPI.  Are there other practical considerations that the authors are aware of that affect the scaling of the LP collectives?  If so, these should be mentioned in the paper.\n\nIn the sentence \"Worringen (2003) proposed a pipeline collective model in shared memory environment for CPU data, but communications of different MPI processes sharing the same CPU memory bus within the same CPU socket.\" I really can't figure out what the words after \"but communications of different MPI processes\" are trying to convey.  This sentence is not comprehensible.\n\n\"Please note the latency term is log p\u03b1, which is the smallest among algorithms in Table.1. Therefore, MST only suits for high frequent short messages.\"  The claim that MST collectives are only suitable for high-frequency, short messages does not follow from the statement that MST collectives have the smallest latency term.  You also need to consider the way the cost scales with message size (the bandwidth term).  If the MST collectives had a better bandwidth term than the other collectives, then they would also be superior for large messages.\n\n\"Let\u2019s take an appropriate block size b to ensure n\/b \u226a \u03b1.\"  This looks wrong, since n > b.  Should it be b\/n \u226a \u03b1?\n\n\"However, the parameters are not consistent after several iterations due to the precision issues of float multiplications in Gradient Update.\"  Are you sure the inconsistency in weight estimates across devices is due to multiplication?  I would expect that it would be due to gradients being accumulated in different orders; that is, because floating point addition is not commutative.\n\nI recommend replacing the term \"sub-gradients\" in this paper with \"partial gradients.\"  In the optimization literature, the term \"sub-gradient\" has a very specific meaning that differs from this paper's use of the term (see","model":"human","source":"peerread","label":0,"id":5155}
{"text":"The authors propose improvements for the utilization of modern hardware when training using stochastic gradient. However, the reviewers bring up several issues with the paper, including major clarity issues as well as notational issues and some comments about the theory vs. practice.","model":"human","source":"peerread","label":0,"id":5156}
{"text":"To Reviewers,\n\nWe have improved the readability according to the feedback from Reviewer 3. Please check at the revision.","model":"human","source":"peerread","label":0,"id":5157}
{"text":"This paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU. The paper provides both theoretical analysis and experiments. Overall, the results presented in the paper are interesting, but the writing can be improved. \n\nComments:\n\n- The authors compare their proposed approach with several alternative approaches and demonstrate strong performance of the proposed approaches. But it is unclear if the improvement is from the proposed approach or from the implementation.  \n\n- The paper is not easy to follow and the writing can be improved in many place (aside from typos and missing references). Specifically, the authors should provide more intuitions of the proposed approach in the introduction and in Section 3. \n\n- The proposition and the analysis in Section 3.2 do not suggest the communication cost of linear pipeline is approximately 2x and log p faster than BE and MST, respectively, as claimed in many places in the paper. Instead, it suggests LP *cannot* be faster than these methods by 2x and log p  times. More specifically, Eq (2) shows T_broadcase_BE\/ T_broadcase_LP < 2. This does not provide an upper-bound of T_broadcase_LP and it can be arbitrary worse when comparing with T_broadcase_BE from this inequality. Therefore, instead of showing T_broadcase_BE\/ T_broadcase_LP < 2, the authors should state T_broadcase_BE\/ T_broadcase_LP > 1 when n approaches infinity. \n\n- It would be interesting to emphasize more on the differences between designing parallel algorithms on CPU v.s. on GPU to motivate the paper. \n","model":"human","source":"peerread","label":0,"id":5158}
{"text":"This paper analyzes the ring-based AllReduce approach for multi-GPU data parallel training of deep net.\nComments\n1) The name linear pipeline is somewhat confusing to the readers, as the technique is usually referred as ring based approach in Allreduce literature. The author should use the standard name to make the connection easier. \n2) The cost analysis of ring-based Allreduce is already provided in the existing literature. This paper applied the analysis to the case of multi-GPU deep net training, and concluded that the scaling is invariant of number of GPUs.\n3) The ring-based allreduce approach is already supported by NVidia\u2019s NCCL library, although the authors claim that their implementation comes earlier than the NCCL implementation.\n4) The overlap of communication of computation is an already applied technique in systems such as TensorFlow and MXNet. The schedule proposed by the authors exploits the overlap partially, doing backprop of t-1 while doing reduce.  Note that the dependency pattern can be further exploited; with the forward of layer t depend on update of parameter of layer t in last iteration. This can be done by a dependency scheduler.\t\n5) Since this paper is about analysis of Allreduce, it would be nice to include detailed analysis of tree-shape reduction, ring-based approach and all-to-all approach. The discussion of all-to-all approach is missing in the current paper. \nIn summary, this is a paper discussed existing Allreduce techniques for data parallel multi-GPU training of deep net, with cost analysis based on existing results. While I personally find the claimed result not surprising as it follows from existing analysis of Allreduce, the analysis might help some other readers. I view this as a baseline paper. The analysis of Allreduce could also been improved (see comment 5).\n\n\n\n\n\n\n\n\n\n","model":"human","source":"peerread","label":0,"id":5159}
{"text":"The primary point made by this paper is that given certain architectural characteristics of multi-GPU systems, namely the use of bi-directional PCI-E for communication and the integration of two independent DMA engines on recent GPU devices (providing support for simultaneous independent communications), and given the characteristics of the communications patterns required by synchronous SGD trainers for deep neural networks, namely that the messages are large, dense, and have a fixed length, it makes sense to design communication collectives such as broadcast, reduce, and allreduce specifically for the use case of synchronous SGD training on a multi-GPU system.  The paper describes the implementation of these three collectives (broadcast, reduce, and allreduce) using a linear pipelining (LP) scheme on a (logical) ring topology.  The paper compares the LP collectives to two alternatives:  collectives based on a minimal spanning tree (MST) topology and collectives based on bidirectional exchange (BE).  First, a theoretical comparison is made using a standard cost model used in the high performance computing community.  When assumptions based on multi-GPU system architecture (very low latency for messages) and on the communication characteristics of synchronous SGD training (very large messages) are integrated into the model, the paper finds that the LP collectives should be less costly than BE collectives by a factor of 2 and less costly than MST collectives by a factor of log(p), where p is the number of GPUs being used.  Second, an empirical comparison is performed in which (1) the time required to perform each of the different collectives on a 4-device (k40m) system is measured as a function of message size and (2) the time required to perform each of the different collectives with a 200 MB message length is measured as a function of the number of devices in the system.  These measurements show that the LP-based collectives are consistently the fastest.  Third, DNN training experiments with AlexNet and GoogLeNet are performed on a 4-device system using three different synchronous SGD algorithms with the different implementations of the collectives (a total of 6 different algorithms in all).  Measurements of the communication and computation costs show that the LP collectives reduce communication costs without affecting computation costs (as expected).  Measurements of the convergence of the training loss as a function of time for the two DNN architectures show that use of the LP collectives leads to faster training.\n\nWhile the theory says that the costs of LP collectives should be invariant to the number of devices in a multi-GPU system, the empirical work shows that in practice this does not hold going from 4 to 5 devices (in the tested configuration) because in a 5-device system messages must traverse the QPI.  Are there other practical considerations that the authors are aware of that affect the scaling of the LP collectives?  If so, these should be mentioned in the paper.\n\nIn the sentence \"Worringen (2003) proposed a pipeline collective model in shared memory environment for CPU data, but communications of different MPI processes sharing the same CPU memory bus within the same CPU socket.\" I really can't figure out what the words after \"but communications of different MPI processes\" are trying to convey.  This sentence is not comprehensible.\n\n\"Please note the latency term is log p\u03b1, which is the smallest among algorithms in Table.1. Therefore, MST only suits for high frequent short messages.\"  The claim that MST collectives are only suitable for high-frequency, short messages does not follow from the statement that MST collectives have the smallest latency term.  You also need to consider the way the cost scales with message size (the bandwidth term).  If the MST collectives had a better bandwidth term than the other collectives, then they would also be superior for large messages.\n\n\"Let\u2019s take an appropriate block size b to ensure n\/b \u226a \u03b1.\"  This looks wrong, since n > b.  Should it be b\/n \u226a \u03b1?\n\n\"However, the parameters are not consistent after several iterations due to the precision issues of float multiplications in Gradient Update.\"  Are you sure the inconsistency in weight estimates across devices is due to multiplication?  I would expect that it would be due to gradients being accumulated in different orders; that is, because floating point addition is not commutative.\n\nI recommend replacing the term \"sub-gradients\" in this paper with \"partial gradients.\"  In the optimization literature, the term \"sub-gradient\" has a very specific meaning that differs from this paper's use of the term (see ","model":"human","source":"peerread","label":0,"id":5160}
{"text":"This paper proposes a new kind of expert model where a sparse subset of most reliable experts is chosen instead of the usual logarithmic opinion pool of a PoE.\nI find the paper very unclear. I tried to find a proper definition of the joint model p(x,z) but could not extract this from the text. The proposed \u201cEM-like\u201d algorithm should then also follow directly from this definition. At this point I do not see if such as definition even exists. In other words, is there is an objective function on which the iterates of the proposed algorithm are guaranteed to improve on the train data?\nWe also note that the \u201cproduct of unifac models\u201d from Hinton tries to do something very similar where only a subset of the experts will get activated to generate the input:","model":"human","source":"peerread","label":0,"id":5161}
{"text":"This paper is about learning distributed representations. All reviewers agreed that the first draft was not clear enough for acceptance.\n \n Reviewer time is limited and a paper that needed a complete overhaul after the reviews were written is not going to get the same consideration as a paper that was well-drafted from the beginning.\n \n It's still the case that it's unclear from the paper how the learning updates or derived. The results are not visually impressive in themselves. It's also still the case that more is needed to demonstrate that this direction is promising compared to other approaches to representation learning.","model":"human","source":"peerread","label":0,"id":5162}
{"text":"The paper addresses the problem of learning compact binary data representations. I have a hard time understanding the setting and the writing of the paper is not making it any easier. For example I can't find a simple explanation of the problem and I am not familiar with these line of research. I read all the responses provided by authors to reviewer's questions and re-read the paper again and I still do not fully understand the setting and thus can't really evaluate the contributions of these work. The related work section does not exist and instead the analysis of the literature is somehow scattered across the paper. There are no derivations provided. Statements often miss references, e.g. the ones in the fourth paragraph of Section 3. This makes me conclude that the paper still requires significant work before it can be published.","model":"human","source":"peerread","label":0,"id":5163}
{"text":"The goal of this paper is to learn \u201c a collection of experts that are individually\nmeaningful and that have disjoint responsibilities.\u201d Unlike a standard mixture model, they \u201cuse a different mixture for each dimension d.\u201d While the results seem promising, the paper exposition needs significant improvement.\n\nComments:\n\nThe paper jumps in with no motivation at all. What is the application, or even the algorithm, or architecture that this is used for? This should be addressed at the beginning.\n\nThe subsequent exposition is not very clear. There are assertions made with no justification, e.g. \u201cthe experts only have a small variance for some subset of the variables while the variance of the other variables is large.\u201d \n\nSince you\u2019re learning both the experts and the weights, can this be rephrased in terms of dictionary learning? Please discuss the relevant related literature.\n\nThe horse data set is quite small with respect to the feature dimension, and so the conclusions may not necessarily generalize.\n\n","model":"human","source":"peerread","label":0,"id":5164}
{"text":"This paper proposes a new kind of expert model where a sparse subset of most reliable experts is chosen instead of the usual logarithmic opinion pool of a PoE.\nI find the paper very unclear. I tried to find a proper definition of the joint model p(x,z) but could not extract this from the text. The proposed \u201cEM-like\u201d algorithm should then also follow directly from this definition. At this point I do not see if such as definition even exists. In other words, is there is an objective function on which the iterates of the proposed algorithm are guaranteed to improve on the train data?\nWe also note that the \u201cproduct of unifac models\u201d from Hinton tries to do something very similar where only a subset of the experts will get activated to generate the input: ","model":"human","source":"peerread","label":0,"id":5165}
{"text":"The authors introduce a new memory model which allows memory access in O(log n) time.\n\nPros:\n* The paper is well written and everything is clear.\n* It's a new model and I'm not aware of a similar model.\n* It's clear that memory access time is an issue for longer sequences and it is clear how this model solves this problem.\n\nCons:\n* The motivation for O(log n) access time is to be able to use the model on very long sequences. While it is clear from the definition that the computation time is low because of its design, it is not clear that the model will really generalize well to very long sequences.\n* The model was also not tested on any real-world task.\n\nI think such experiments should be added to show whether the model really works on long sequences and real-world tasks, otherwise it is not clear if this is a useful model.","model":"human","source":"peerread","label":0,"id":5166}
{"text":"This paper introduces a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells.  This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks such as sorting from pure input-output examples and dealing with longer sequences.\n\nThe idea of the paper is novel and well-presented, and the memory structure seems reasonable to have advantages in practice. However, the main weakness of the paper is the experiments. There is no experimental comparison with other external memory-based approaches (e.g. those discussed in Related Work), or experimental analysis of computational efficiency given overhead costs (beyond just computational complexity) despite that being one of the main advantages. Furthermore, the experimental setups are relatively weak, all on artificial tasks with moderate increases in sequence length.  Improving on these would greatly strengthen the paper, as the core idea is interesting.","model":"human","source":"peerread","label":0,"id":5167}
{"text":"This paper proposes to use a hierarchical softmax to speed up attention based memory addressing in memory augmented network (e.g. NTM, memNN\u2026).\n\nThe model build a hierarchical softmax on top of the input sequence then at each time step SEARCH for the most relevant input to predict the next output (this search is discrete), and use its corresponding embedding to update the state of an LSTM that will then produce the output. Finally the embedding of the used input is update by a WRITE function (an LSTM working that takes hidden state of the other LSTM as an input). The model has a discrete component (the SEARCH) and is thus trained with REINFORCE. In the experimental section they test their approach on several algorithmic tasks such as search, sort...\n\nThe main advantage of replacing the full softmax by a hierarchical softmax is that during inference, the complexity goes from O(N) to O(log(N)). It would be great to see if the gain in complexity allows to tackle problem which are a few orders of magnitude bigger than the one addressed with full softmax. However the authors only test on toy sequences up to 32 tokens, which is quite small. \n\nThe model requires a relatively complex search mechanism that can only be trained with REINFORCE. While this seems to work on problems with relatively small and simple sequences, it would be great to see how performance changes with the size of the problem. \n\nOverall, while the idea of replacing the softmax in the attention mechanism by a hierachical softmax is appealing, this work is not quite convincing yet. Their approach is not very natural, may be hard to train and may not be that simple to scale. The experiment section is very weak.\n","model":"human","source":"peerread","label":0,"id":5168}
{"text":"The authors introduce a new memory model which allows memory access in O(log n) time.\n\nPros:\n* The paper is well written and everything is clear.\n* It's a new model and I'm not aware of a similar model.\n* It's clear that memory access time is an issue for longer sequences and it is clear how this model solves this problem.\n\nCons:\n* The motivation for O(log n) access time is to be able to use the model on very long sequences. While it is clear from the definition that the computation time is low because of its design, it is not clear that the model will really generalize well to very long sequences.\n* The model was also not tested on any real-world task.\n\nI think such experiments should be added to show whether the model really works on long sequences and real-world tasks, otherwise it is not clear if this is a useful model.\n","model":"human","source":"peerread","label":0,"id":5169}
{"text":"This paper was easy to read, the main idea was presented very clearly.\n\nThe main points of the paper (and my concerns are below) can be summarized as follows:\n1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user's job, it wouldn't be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some node. Also the increasing shape of the curve is somehow implying some weird implementation of communication. Isn't it only because you are somehow serialize the communication? And it would be maybe much faster if a \"MPI_Reduce\" would be used (even if we wait for the slowest guy)?\n2. asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. Moreover, those algorithms can be divergence it special care is not given to stale gradients. Also they have a nice guarantees for convex functions, but the non-convex DNN may cause pain.\n3.they propose to take gradient from the first \"N\" workers out of \"N+b\" \nworkers available. My concern here is that they focused only on the \nworkers, but what if the \"parameter server\" will became to slow? What \nif the parameter server would be the bottleneck? How would you address \nthis situation? But still if the number of nodes (N) is not large, and \nthe deep DNN is used, I can imagine that the communciation will not \ntake more than 30% of the run-time.\n\n\nMy largest concern is with the experiments. Different batch size \nimplies that different learning rate should be chosen, right? How did \nyou tune the learning rates and other parameters for e.g. Figure 5 you \nprovide some formulas in (A2) but clearly this can bias your Figures, \nright? meaning, that if you tune \"\\gamma, \\beta\" for each N, it could \nbe somehow more representative? also it would be nicer if you run the \nexperiment many times and then report average, best and worst case \nbehaviour. because now it can be just coinsidence, right?","model":"human","source":"peerread","label":0,"id":5170}
{"text":"Counter to the current wisdom, this work proposes that synchronous training may be advantageous over asynchronous training (provided that \"backup workers\" are available). This is shown empirical and without theoretical results. The contribution is somewhat straightforward and designed for a specific large-scale hardware scenario, but this is often an important bottleneck in the learning process. However, there is some concern about the long-term impact of this work, due to its dependence on the hardware.","model":"human","source":"peerread","label":0,"id":5171}
{"text":"This paper was easy to read, the main idea was presented very clearly.\n\nThe main points of the paper (and my concerns are below) can be summarized as follows:\n1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user's job, it wouldn't be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some node. Also the increasing shape of the curve is somehow implying some weird implementation of communication. Isn't it only because you are somehow serialize the communication? And it would be maybe much faster if a \"MPI_Reduce\" would be used (even if we wait for the slowest guy)?\n2. asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. Moreover, those algorithms can be divergence it special care is not given to stale gradients. Also they have a nice guarantees for convex functions, but the non-convex DNN may cause pain.\n3.they propose to take gradient from the first \"N\" workers out of \"N+b\" \nworkers available. My concern here is that they focused only on the \nworkers, but what if the \"parameter server\" will became to slow? What \nif the parameter server would be the bottleneck? How would you address \nthis situation? But still if the number of nodes (N) is not large, and \nthe deep DNN is used, I can imagine that the communciation will not \ntake more than 30% of the run-time.\n\n\nMy largest concern is with the experiments. Different batch size \nimplies that different learning rate should be chosen, right? How did \nyou tune the learning rates and other parameters for e.g. Figure 5 you \nprovide some formulas in (A2) but clearly this can bias your Figures, \nright? meaning, that if you tune \"\\gamma, \\beta\" for each N, it could \nbe somehow more representative? also it would be nicer if you run the \nexperiment many times and then report average, best and worst case \nbehaviour. because now it can be just coinsidence, right? \n","model":"human","source":"peerread","label":0,"id":5172}
{"text":"This paper proposed a synchronous parallel SGD by employing several backup machines. The parameter server does not have to wait for the return from all machines to perform the update on the model, which reduce the synchronization overhead. It sounds like a reasonable and straightforward idea. \n\nMy main concern is that this approach is only suitable for some very specific scenario, that is, most learners (except a small number of learners) are at the same pace to return the results. If the efficiency of learners does not follow such distribution, I do not think that the proposed algorithm will work. So I suggest two revisions:\n\n- provide more experiments to show the performance with different efficiency distributions of learners.\n- assuming that all learners follow the same distribution of efficiency and show the expected idle time is minor by using the proposed algorithm.","model":"human","source":"peerread","label":0,"id":5173}
{"text":"The paper claim that, when supported by a number of backup workers, synchronized-SGD \nactually works better than async-SGD. The paper first analyze the problem of staled updates\nin async-SGDs, and proposed the sync-SGD with backup workers. In the experiments, the \nauthors shows the effectiveness of the proposed method in applications to Inception Net\nand PixelCNN.\n\nThe idea is very simple, but in practice it can be quite useful in industry settings where \nadding some backup workders is not a big problem in cost. Nevertheless, I think the \nproposed solution is quite straightforward to come up with when we assume that \neach worker contains the full dataset and we have budge to add more workers. So, \nunder this setting, it seems quite natural to have a better performance with the additional \nbackup workers that avoid the staggering worker problem. And, with this assumtion I'm not \nsure if the proposed solution is solving difficult enough problem with novel enough idea. \n\nIn the experiments, for fair comparison, I think the Async-SGD should also have a mechanism \nto cut off updates of too much staledness just as the proposed method ignores all the remaining \nupdates after having N updates. For example, one can measure the average time spent to \nobtain N updates in sync-SGD setting and use that time as the cut-off threashold in Async-SGD \nso that Async-SGD does not perform so poorly.","model":"human","source":"peerread","label":0,"id":5174}
{"text":"Is this technique efficient? If you are using 5-10% extra resources (backup workers), do you get at least 5-10% speedup over sync SGD? Figure 8(a) does not have any sync results. It is well understood that async may sometimes never converge to the correct results.","model":"human","source":"peerread","label":0,"id":5175}
{"text":"The proposed technique of over-provisioning a few workers to improve the hardware efficiency of synchronous training gives a very welcome boost, as evidenced beautifully by Figure 6.\n\nHowever, as I have discussed in private communication with the authors, the comparison with asynchronous methods leaves a few questions regarding tuning.\u00a0\n\nLearning rate:\u00a0\n\nThe authors carefully describe the process of tuning the synchronous implementation. However no tuning is reported for the asynchronous implementation: the value is set to 0.045 for all configurations. Figure 7(a) shows that tuning the learning rate can cause a difference of almost a whole percentage point in test precision.\n1. What other values of LR have the authors tried for asynchronous training?\n2. Is 0.045 the best one for all configurations they report?\n3. Is there any chance that the 0.5% difference in precision between sync and async is due to insufficient LR tuning (as suggested by Figure 7(a))?\u00a0\n\n\nMomentum:\n\nOur results (","model":"human","source":"peerread","label":0,"id":5176}
{"text":"Authors propose a competitive learning architecture that learn different RNN predictors independently, akin to a committee of experts which are chosen with a hard switch at run-time. This work is applied to the task of predictive different driving behaviors from human drivers, and combines behaviors at test time, often switching behaviors within seconds. Prediction loss is lower than the similar but non-competitive architecture used as a baseline.\nIt is not very clear how to interpret the results, what is the real impact of the model. If behaviors switch very often, can this really be seen as choosing the best driving mode for a given situation? Maybe the motivation needs to be rephrased a little to be more convincing?\nThe competitive approach presented is interesting but not really novel, thus the impact of this paper for a conference such as ICLR may be limited.","model":"human","source":"peerread","label":0,"id":5177}
{"text":"The authors present a prediction framework that involves multiple 'competitive' RNNs, and they claim that they are predicting human intention. It is unclear if this method, which seems quite ad-hoc, is any different from a simple ensemble approach, and it is unclear that the model is predicting human intention. The experiments do not adequately demonstrate either.","model":"human","source":"peerread","label":0,"id":5178}
{"text":"\nThis paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future based on video and vehicle state input. The architecture allows several RNNs to compete to make the best predictions, with only the best prediction receiving back propagation training at each time step. Preliminary experimental results show that this scheme can yield reduced prediction error.\n\n It is not clear how the best-performing RNN is chosen for each time point at test time. That is, how is the \u201cintegrated prediction\u201d obtained in Fig. 7? Is the prediction the one with minimum error over all of the output layers? If so, this means the prediction cannot be made until you already know the value to be predicted.\n\nIt seems possible that a larger generic RNN might be able to generate accurate predictions. If I understand correctly, the competitive architectures have many more parameters than the baseline. Is the improved performance here due to the competitive scheme, or just a larger model? \n\nA large amount of additional work is required to sustain the claim that this scheme is successfully extracting driver \u2018intentions\u2019. It would be interesting to see if the scheme, suitably extended, can automatically infer the intention to stop at a stop sign vs slowing but not stopping due to a car in front, say, or to pass a car vs simply changing lanes. Adding labels to the dataset may enable this comparison more clearly.\n\nMore generally, the intention of the driver seems more related to the goals they are pursuing at the moment; there is a fair amount of work in inverse reinforcement learning that examines this problem (some of it in the context of driving style as well).\n","model":"human","source":"peerread","label":0,"id":5179}
{"text":"This paper proposes a neural network architecture for car state prediction while driving based on competitive learning. Competitive learning creates several duplicates of the baseline neural architecture and during training only updates the architecture with minimum loss. The experiments compare the competitive learning approach to a single baseline architecture on a driving benchmark task. The paper is understandable but could benefit from some copy editing. \n\nThe competitive learning approach seems rather adhoc and this paper feels quite incomplete without significant discussion and comparisons to ensembling. Much recent work has shown that duplicating and ensembling neural architectures can produce gains, and it\u2019s not clear why competitive learning is better than ensembling, it seems less theoretically sound to me.\n\nThere is a huge confound in the experiments due to the competitive learning architecture having many more free parameters than the baseline architecture. Again I think comparing to ensembling with the same number of architectures duplicated and perhaps comparing to a single baseline with larger hidden layers to make the total number of free parameters comparable is critical to validating the proposed approach.\n\nThe graphical model of the driving process depicted in figure 1 seems nonsensical. If e is observed then all variables are known given the dependencies shown. Further, it is at best very poor notation to say that the driving action d decided at time t affects the vehicle state s at that same time. It should be that s_t depends on d_(t-1). Also, according to this figure the driving decision d does not depend on the observed vehicle state x which also seems invalid.\n\nOdd to have a paragraph break in abstract\n\nFigure 1 caption should include a brief explanation of the variables shown\n","model":"human","source":"peerread","label":0,"id":5180}
{"text":"This paper proposes SEM, a simple large-size multilabel learning algorithm which models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network. This in and of itself is not novel, nor is the idea of optimizing this by adagrad. Though it's weird that the paper explicitly derives the gradient and suggests doing alternating adagrad steps instead of the more standard adagrad steps; it's unclear whether this matters at all for performance. The main trick responsible for increasing the efficiency of this model is the candidate label sampling, which is done in a relatively standard way by sampling labels proportionally to their frequency in the dataset.\n\nGiven that neither the model nor the training strategy is novel, it's surprising that the results are better than the state-of-the-art in quality and efficiency (though non-asymptotic efficiency claims are always questionable since implementation effort trades off fairly well against performance). I feel like this paper doesn't quite meet the bar.","model":"human","source":"peerread","label":0,"id":5181}
{"text":"This is largely a well written paper proposing a sensible approach for multilabel learning that is shown to be effective in practice. However, the main technical elements of this work: the model used and its connections to basic MLPs and related methods in the literature, the optimization strategy, and the speedup tricks are all familiar from prior work. Hence the reviewers are somewhat unanimous in their view that the novelty aspect of this paper is its main shortcomings. The authors are encouraged to revise the paper and clarify the precise contributions.","model":"human","source":"peerread","label":0,"id":5182}
{"text":"The paper proposes a semantic embedding based approach to multilabel classification. \nConversely to previous proposals, SEM considers the underlying parameters determining the\nobserved labels are low-rank rather than that the observed label matrix is itself low-rank. \nHowever, It is not clear to what extent the difference between the two assumptions is significant\n\nSEM models the labels for an instance as draws from a multinomial distribution\nparametrized by nonlinear functions of the instance features. As such, it is a neural network.\nThe proposed training algorithm is slightly more complicated than vanilla backprop.  The significance of the results compared to NNML (in particular on large datasets Delicious and EUrlex) is not very clear. \n\nThe paper is well written and the main idea is clearly presented. However, the experimental results are not significant enough to compensate the lack of conceptual novelty. \n\n\n","model":"human","source":"peerread","label":0,"id":5183}
{"text":"The paper presents the semantic embedding model for multi-label prediction.\nIn my questions, I pointed that the proposed approach assumes the number of labels to predict is known, and the authors said this was an orthogonal question, although I don't think it is!\nI was trying to understand how different is SEM from a basic MLP with softmax output which would be trained with a two step approach instead of stochastic gradient descent. It seems reasonable given their similarity to compare to this very basic baseline.\nRegarding the sampling strategy to estimate the posterior distribution, and the difference with Jean et al, I agree it is slightly different but I think you should definitely refer to it and point to the differences.\nOne last question: why is it called \"semantic\" embeddings? usually this term is used to show some semantic meaning between trained embeddings, but this doesn't seem to appear in this paper.\n","model":"human","source":"peerread","label":0,"id":5184}
{"text":"The authors propose to sample from VAEs through a Markov chain [z_t ~ q(z|x=x_{t-1}), x_t ~ p(x|z=z_t)]. The paper uses confusing notation, oversells the novelty, ignoring some relevant previous results. The qualitative difference between regular sampling and this Gibbs chain is not very convincing, judging from the figures. It would be a great workshop paper (perhaps more), if the authors fix the notation, fix the discussion to related work, and produce more convincing (perhaps simply upscaled?) figures.\n\nComments: \n - Rezende et al's (2014) original VAE paper already discusses the Markov chain, which is ignored in this paper\n - Notation is nonstandard \/ confusing. At page 1, it\u2019s unclear what the authors mean with \u201cp(x|z) which is approximated as q(x|z)\u201d.\n- It\u2019s also not clear what\u2019s meant with q(z). At page 2, q(z) is called the learned distribution, while p(z) can in general also be a learned distribution.\n- It\u2019s not true that it\u2019s impossible to draw samples from q(z): one can sample x ~ q(x) from the dataset, then draw z ~ q(z|x).\n- It's not explained whether the analysis only applies to continuous observed spaces, or also discrete observed spaces\n- Figures 3 and 4 are not very convincing.","model":"human","source":"peerread","label":0,"id":5185}
{"text":"This approach taken in this paper is topical, especially since the importance of sampling and generating diverse samples is increasingly discussed in work on generative models. There were several concerns from reviewers, in three areas particularly: connection and comparison to related work; lack of clarity and understanding of the paper; experiments that are not sufficiently convincing. These have been addressed to some extent by the authors, discussing in more detail the related work, especially in connection to Rezende et al., and GSN of Bengio et al., and with improved figures. But these points are still of concern especially in terms of assessing sample diversity in relation to much of the recent work on richer variational posterior methods and other techniques. For these reasons, the paper is not yet ready for acceptance at this years conference.","model":"human","source":"peerread","label":0,"id":5186}
{"text":"We have updated our paper to include changes that reflect comments made by the reviewers. A PDF document detailing our changes  may be found at: ","model":"human","source":"peerread","label":0,"id":5187}
{"text":"This paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z, such that P(X | Z) can be eased to generate samples from a data distribution.\n\nThe paper in its current form is not acceptable due to the following reasons:\n1. No quantitative evaluation. The authors do include samples from the generative model, which however are insufficient to judge performance of the model. See comment 2.\n2. The description of the model is very unclear. I had to indulge in a lot of charity to interpret what the authors \"must be doing\". What does Q(Z) mean? Does it mean the true posterior P(Z | X) ? What is the generative model here? Typically, it's P(Z)P(X|Z). VAEs use a variational approximation Q(Z | X) to the true posterior P(Z | X). Are you trying to say that your model can sample from the true posterior P(Z | X)?\n\nComments:\n1. Using additive noise in the input does not seem like a reasonable idea. Any justification of why this is being done?\n2. Approaches which learn transition operators are usually very amenable to data augmentation-based semi-supervised learning. I encourage the authors to improve their paper by testing their model on semi-supervised learning benchmarks.","model":"human","source":"peerread","label":0,"id":5188}
{"text":"The authors propose to sample from VAEs through a Markov chain [z_t ~ q(z|x=x_{t-1}), x_t ~ p(x|z=z_t)]. The paper uses confusing notation, oversells the novelty, ignoring some relevant previous results. The qualitative difference between regular sampling and this Gibbs chain is not very convincing, judging from the figures. It would be a great workshop paper (perhaps more), if the authors fix the notation, fix the discussion to related work, and produce more convincing (perhaps simply upscaled?) figures.\n\nComments: \n - Rezende et al's (2014) original VAE paper already discusses the Markov chain, which is ignored in this paper\n - Notation is nonstandard \/ confusing. At page 1, it\u2019s unclear what the authors mean with \u201cp(x|z) which is approximated as q(x|z)\u201d.\n- It\u2019s also not clear what\u2019s meant with q(z). At page 2, q(z) is called the learned distribution, while p(z) can in general also be a learned distribution.\n- It\u2019s not true that it\u2019s impossible to draw samples from q(z): one can sample x ~ q(x) from the dataset, then draw z ~ q(z|x).\n- It's not explained whether the analysis only applies to continuous observed spaces, or also discrete observed spaces\n- Figures 3 and 4 are not very convincing.\n","model":"human","source":"peerread","label":0,"id":5189}
{"text":"The authors argues that the standard ancestral sampling from stochastic autoencoders (such as the Variational Autoencoder and the Adversarial\nAutoencoder) imposes the overly-restrictive constraint that the encoder distribution must marginally match the latent variable prior. They propose, as an alternative, a Markov Chain Monte Carlo approach that avoids the need to specify a simple parametric form for the prior.\n\nThe paper is not clearly written. Most critically, the notation the authors use is either deeply flawed, or there are simple misunderstanding with respect to the manipulations of probability distributions. For example, the authors seem to suggest that both distributions Q(Z|X) and Q(X|Z) are parametrized. For this to be true the model must either be trivially simple, or an energy-based model. There is no indication that they are speaking of an energy-based model. Another example of possible confusion is the statement that the ratio of distributions Q(Z|X)\/P(Z) = 1. I believe this is supposed to be a ratio of marginals: Q(Z)\/P(X) = 1. Overall, it seems like there is a confusion of what Q and P represent. The standard notation used in VAEs is to use P to represent the decoder distribution and\nQ to represent the encoder distribution. This seems not to be how the authors are using these terms. Nor does it seem like there is a single consistent interpretation. \n\nThe empirical results consist entirely of qualitative results (samples and reconstructions) from a single dataset (CelebA). The samples are also not at all up to the quality of the SOTA models. The interpolations shown in Figures 1 and 3 both seems to look like interpolation in pixel space for both the VAE model and the proposed DVAE. \n","model":"human","source":"peerread","label":0,"id":5190}
{"text":"While you cite Rezende et al. 2014 when referring to VAEs, you claim the main contribution of your work is the generation of posterior samples from a Markov Chain. However, Rezende et al. 2014 presented a very similar idea. Let me quote them directly:\n\nWe do not integrate over the missing\nvalues, but use a procedure that simulates a Markov\nchain that we show converges to the true marginal distribution\nof missing given observed pixels.\n\n-- Section 5.4\n\nImage completion can be approximatively achieved by\na simple iterative procedure which consists of (i) initializing\nthe non-observed pixels with random values;\n(ii) sampling from the recognition distribution given\nthe resulting image; (iii) reconstruct the image given\nthe sample from the recognition model; (iv) iterate the\nprocedure.\n\n-- Appendix F\n\nHow is this procedure different from your main contribution? I mean, they motivate with missing data imputation and start in a different way, but the main loop seems the same. Even if there is a significant difference between the procedures, I believe this should be addressed in the paper. I also suggest taking a closer look at appendix F, they provide some proofs which seem relevant to your work.\n","model":"human","source":"peerread","label":0,"id":5191}
{"text":"How are the novelty of samples affected as more samples are generated from the Markov Chain? I played around with a similar idea and found that after about 6-7 Monte Carlo samples, the images looked identical to the training data. Do you observe something similar in your experiments?\n\nAlso, the interpolation experiments are interesting. It seems that the proposed method implicitly interpolates along the Fischer metric rather than the Euclidean metric. Some discussion on this might be illuminating. ","model":"human","source":"peerread","label":0,"id":5192}
{"text":"This paper proposed to use GAN for encrypted communications.\n\nIn section 2, the authors proposed a 3 part neural network trained to encode and decode data. This model does not have any practical value except paving the way for describing the next model in section 3: it is strictly worse than any provable cryptography system.\n\nIn section 3, the authors designed a task where they want to hide part of the data, which has correlated fields, while publishing the rest. However, I'm having trouble thinking of an application where this system is better than simply decorrelating the data and encrypting the fields one wants to hide with a provable cryptography system while publishing the rest in plain text.","model":"human","source":"peerread","label":0,"id":5193}
{"text":"Interesting paper but not over the accept bar.","model":"human","source":"peerread","label":0,"id":5194}
{"text":"The submission proposes to modify the typical GAN architecture slightly to include \"encrypt\" (Alice) and \"decrypt\" (Bob) modules as well as a module trying to decrypt the signal without a key (Eve).  Through repeated transmission of signals, the adversarial game is intended to converge to a system in which Alice and Bob can communicate securely (or at least a designated part of the signal should be secure), while a sophisticated Eve cannot break their code.  Examples are given on toy data:\n\"As a proof-of-concept, we implemented Alice, Bob, and Eve networks that take N-bit random plain-text and key values, and produce N-entry floating-point ciphertexts, for N = 16, 32, and 64.  Both plaintext and key values are uniformly distributed.\"\n\nThe idea considered here is cute.  If some, but not necessarily all of the signal is meant to be secure, the modules can learn to encrypt and decrypt a signal, while an adversary is simultaneously learned that tries to break the encryption.  In this way, some of the data can remain unencrypted, while the portion that is e.g. correlated with the encrypted signal will have to be encrypted in order for Eve to not be able to predict the encrypted part.\n\nWhile this is a nice thought experiment, there are significant barriers to this submission having a practical impact:\n1) GANs, and from the convergence figures also the objective considered here, are quite unstable to optimize.  The only guarantees of privacy are for an Eve that is converged to a very strong adversary (stronger than a dedicated attack over time).  I do not see how one can have any sort of reliable guarantee of the safety of the data transmission from the proposed approach, at least the paper does not outline such a guarantee.\n2) Public key encryption systems are readily available, computationally feasible, and successfully applied almost anywhere.  The toy examples given in the paper do not at all convince me that this is solving a real-world problem at this point.  Perhaps a good example will come up in the near future, and this work will be shown to be justified, but until such an example is shown, the approach is more of an interesting thought experiment.","model":"human","source":"peerread","label":0,"id":5195}
{"text":"The paper deals with an interesting application of adversarial training to encryption. It considers the standard scenario of Alice, Eve and Bob, where A and B aim to exchange messages conditioned on a shared key, while Eve should be unable to encrypt the message. Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy. The concepts, ideas and previous literature are quite nicely and carefully presented.\n\nThe only major concern I have - and I apologize to the authors for not raising this earlier - are the experiments in section 3. In particular, I don't quite get the scenario. The reasoning here seems to be as follows: given information < A, B, C, D >, I want to give the public the value of D (e.g. movies watched) without releasing information about C (e.g. gender). In this scenario, Eve would need to be able to reconstruct D as good as possible without gaining information about C. What is described in section 3, however, is that D and D-public are both reconstructed by Bob, but why would Bob reconstruct the latter (he is not public, in particular because he is allowed to reconstruct C, which is not tested here)? Also, Eve only tries to estimate C, thus rendering the scenario not different in any way to the scenario considered in section 2.\n\nI have two more minor concerns:\n\n1) As raised in the pre-review, Eve should actually be stronger then Alice and Bob in order to be able to compensate for the missing key. The authors noted they have been doing these experiments and are going to add the results.\n\n2) In any natural encryption case I would expect the length of the key to be much shorter then the length of the message. This, however, could potentially make the scenario much easier for Eve (although I doubt any of the results will change if the key is long enough).\n\nI like the creative application of adversarial training to a completely different domain, and I believe it could be the starting point of a very interesting direction in cryptographic systems or in privacy applications (although it is unclear whether the weak guarantees of neural network based approaches can ever be overcome). At the same time the application in the privacy setting leaves me quite confused, and the symmetric encryption example is not particularly strong either. I'd appreciate if the authors could address the major concern I raised above, and I will be quite happy to raise the score in case this confusion can be resolved.","model":"human","source":"peerread","label":0,"id":5196}
{"text":"This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself. In this sense it is similar in spirit to scheduled sampling (Bengio et al) and SEARN (Daume et al) DAgger (Ross et al) which consider a \"roll-in\" mixture of the target and model distributions during training. It was clarified in the pre-review questions that these targets are generated on-line rather than from a lagged distribution, which I think makes the algorithm pseudocode somewhat misleading if I understand it correctly.\n\nThis is an incremental improvement on the idea of label softening\/smoothing that has recently been revived, and so the novelty is not that high. The author points out that co-label similarity is better preserved by this method but it doesn't follow that this is causal re: regularization; a natural baseline would be a fixed, soft label distribution, as well as one where the softening\/temperature of the label distribution is gradually reduced (as one would expect for this method to do as the model gets closer and closer to reproducing the target distribution).\n\nIt's an interesting and somewhat appealing idea but the case is not clearly made that this is all that useful. The dropout baselines for MNIST seem quite far from results already in the literature (Srivastava et al 2014 achieves 1.06% with a 3x1024 MLP with dropout and a simple max norm constraint; the dropout baselines here fail to break 1.3% which is rather high by contemporary standards on the permutation-invariant task), and results for CIFAR10 are quite far from the current state of the art, making it difficult to judge the contribution in light of other innovations. The largest benchmark considered is SVHN where the reported accuracies are quite bad indeed; SOTA for single net performance has been less than half the reported error rates for 3-4 years now. It's unclear what conclusions can be drawn about how this would help (or even hurt) in a better-tuned setting.\n\nI have remaining reservations about data hygiene, namely reporting minimum test loss\/maximum test accuracy rather than an unbiased method for model selection (minimum validation set error, for example). Relatedly, the regularization potential of early stopping on a validation set is not considered. See, e.g. the protocol in Goodfellow et al (2013).","model":"human","source":"peerread","label":0,"id":5197}
{"text":"The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction. Very similar method was proposed in Section 6 from (Hinton et al. 2016, Distilling the Knowledge in a Neural Network). \n\nPros: \n+ Comprehensive analysis on the co-label similarity.\n\nCons:\n- Weak baselines. I am not sure the authors have found the best hyper-parameters in their experiments. I just trained a 5 layer fully connected MNIST model with 512 hidden units without any regularizer and achieved 0.986 acc. using Adam and He initialization, where the paper reported 0.981 for such architecture. \n- The authors failed to bring the novel idea. It is very similar to (Hinton et al. 2016). This is probably not enough for ICLR.","model":"human","source":"peerread","label":0,"id":5198}
{"text":"Inspired by the analysis on the effect of the co-label similarity (Hinton et al., 2015), this paper proposes a soft-target regularization that iteratively trains the network using weighted average of the exponential moving average of past labels and hard labels as target argument of loss. They claim that this prevents the disappearing of co-label similarity after early training and  yields a competitive regularization to dropout without sacrificing network capacity.\n\nIn order to make a fair comparison to dropout,  the dropout should be tuned carefully. Showing that it performs better than dropout regularization for some particular values of dropout (Table 2) does not demonstrate a convincing advantage. It is possible that dropout performs better after a reasonable tuning with cross-validation.\n\nThe baseline architectures used in the experiments do not belong the recent state of art methods thus yielding significantly lower accuracy. It seems also that experiment setup does not involve any data augmentation, the results can also change with augmentation. It is not clear why number of epochs are set to a small number like 100 without putting some convergence tests.. Therefore the significance of the method is not convincingly demonstrated in empirical study.\n\nCo-label similarities could be calculated using softmax results at final layer rather than using predicted labels.  The advantage over dropout is not clear in Figure 4, the dropout is set to 0.2 without any cross-validation.  \n\n\nRegularizing by enforcing the training steps to keep co-label similarities is interesting idea but not very novel and the results are not significant.\n\nPros : \n- provides an investigation of regularization on co-label similarity during training\n\nCons:\n-The empirical results do not support the intuitive claims regarding proposed procedure\nIterative version can be unstable in practice\n\n\n","model":"human","source":"peerread","label":0,"id":5199}
{"text":"\n- You definitely need to report misclassification error results on test data for obvious reasons related to losses and final test misclassification error. Currently comparisons are not conclusive.\n\n-  Can you explain better the reason for using the particular updates in (3) and (4) better? Why don't you do for example totally corrective update, e.g. take convex combination of all \\cal{F}'s (or some portion) up to current iteration in (3)? Therefore \\beta and \\gamma should be tuned reasonably well to see whether (3) and (4) is really helping or not and the range for cross validation should be reported.\n\n- The reason to set n_t n_b is not satisfactory.  It is crucial to cross-validate such parameters. Isn't  n_t = {1,2} unreasonably small number that can cause unstable results? why all n_b and n_t are equal?Are there results on other n_b and n_t's that were tried?\n\n- It is stated that colabel similarities disappear when network starts to overfit. However distillation ( Hinton et.al. ,2015 ) captures colabel similarities after training a model and using distillation. This method seems an iterative extension of distillation without using a bigger teacher model. Does proposed method gives better results then a two step version of distillation ?  \n\n- How do you tune \\lambda for weight decay? \n\n- From paper: \"We considered a frozen set of hyper-parameters for the SoftTarget regularization to show that SoftTarget regularization can still work without a having to conduct a large grid search\". This argument is not valid in ML, maybe if you did a reasonable search, you would get worse results (since you should not look test error until you finish the cross-validation).   Why a common hyper parameter tuning procedure is not used e.g. random search (Bergstra and Bengio, JMLR 2012) or Bayesian optimization (Snoek et al ,NIPS 2012) ?  Setting the hyper parameters to some numbers without searching a range or set can dramatically ruin fair comparison. ","model":"human","source":"peerread","label":0,"id":5200}